<doc id="23643" url="http://en.wikipedia.org/wiki?curid=23643" title="Propane">
Propane

Propane () is a three-carbon alkane with the molecular formula C3H8, normally a gas, but compressible to a transportable liquid. A by-product of natural gas processing and petroleum refining, it is commonly used as a fuel for engines, oxy-gas torches, portable stoves, and residential central heating. Propane is one of a group of liquefied petroleum gases (LP gases). The others include butane, propylene, butadiene, butylene, isobutylene and mixtures thereof.
Propane containing too much propene (also called propylene) is not suited for most vehicle fuels. HD-5 is a specification that establishes a maximum concentration of 5% propene in propane. Propane and other LP gas specifications are established in ASTM D-1835. All propane fuels include an odorant, almost always ethanethiol, so that people can easily smell the gas in case of a leak. Propane as HD-5 was originally intended for use as vehicle fuel. HD-5 is currently being used in all propane applications.
History.
Propane was first identified as a volatile component in gasoline by Walter O. Snelling of the U.S. Bureau of Mines in 1910. The volatility of these lighter hydrocarbons caused them to be known as "wild" because of the high vapor pressures of unrefined gasoline. On March 31, the "New York Times" reported on Snelling's work with liquefied gas and that "a steel bottle will carry enough gas to light an ordinary home for three weeks."
It was during this time that Snelling, in cooperation with Frank P. Peterson, Chester Kerr, and Arthur Kerr, created ways to liquefy the LP gases during the refining of natural gasoline. Together they established American Gasol Co., the first commercial marketer of propane. Snelling had produced relatively pure propane by 1911, and on March 25, 1913, his method of processing and producing LP gases was issued patent #1,056,845. A separate method of producing LP gas through compression was created by Frank Peterson and patented in 1912.
The 1920s saw increased production of LP gas, with the first year of recorded production totaling 223,000 USgal in 1922. In 1927, annual marketed LP gas production reached 1 e6USgal, and by 1935, the annual sales of LP gas had reached 56 e6USgal. Major industry developments in the 1930s included the introduction of railroad tank car transport, gas odorization, and the construction of local bottle-filling plants. The year 1945 marked the first year that annual LP gas sales reached a billion gallons. By 1947, 62% of all U.S. homes had been equipped with either natural gas or propane for cooking.
In 1950, 1,000 propane-fueled buses were ordered by the Chicago Transit Authority, and by 1958, sales in the U.S. had reached 7 e9USgal annually. In 2004 it was reported to be a growing $8-billion to $10-billion industry with over 15 e9USgal of propane being used annually in the U.S.
The "prop-" root found in "propane" and names of other compounds with three-carbon chains was derived from "propionic acid".
Sources.
Propane is produced as a by-product of two other processes, natural gas processing and petroleum refining. The processing of natural gas involves removal of butane, propane, and large amounts of ethane from the raw gas, in order to prevent condensation of these volatiles in natural gas pipelines. Additionally, oil refineries produce some propane as a by-product of cracking petroleum into gasoline or heating oil. The supply of propane cannot easily be adjusted to meet increased demand, because of the by-product nature of propane production. About 90% of U.S. propane is domestically produced. The United States imports about 10% of the propane consumed each year, with about 70% of that coming from Canada via pipeline and rail. The remaining 30% of imported propane comes to the United States from other sources via ocean transport.
After it is produced, North American propane is stored in huge salt caverns. Examples of these are Fort Saskatchewan, Alberta; Mont Belvieu, Texas and Conway, Kansas. These salt caverns were hollowed out in the 1940s, and they can store 80000000 oilbbl or more of propane. When the propane is needed, much of it is shipped by pipelines to other areas of the United States. Propane is also shipped by truck, ship, barge, and railway to many U.S. areas.
Properties and reactions.
Propane undergoes combustion reactions in a similar fashion to other alkanes. In the presence of excess oxygen, propane burns to form water and carbon dioxide.
When not enough oxygen is present for complete combustion, incomplete combustion occurs, allowing carbon monoxide and/or soot (carbon) to be formed as well:
Unlike natural gas, propane is heavier than air (1.5 times as dense). In its raw state, propane sinks and pools at the floor. Liquid propane will flash to a vapor at atmospheric pressure and appears white due to moisture condensing from the air.
When properly combusted, propane produces about 50 MJ/kg of heat. The gross heat of combustion of one normal cubic meter of propane is around 91 megajoules.
Propane is nontoxic; however, when abused as an inhalant, it poses a mild asphyxiation risk through oxygen deprivation. Commercial products contain hydrocarbons beyond propane, which may increase risk. Commonly stored under pressure at room temperature, propane and its mixtures expand and cool when released and may cause mild frostbite.
Propane combustion is much cleaner than gasoline combustion, though not as clean as natural gas combustion. The presence of C–C bonds, plus the multiple bonds of propylene and butylene, create organic exhausts besides carbon dioxide and water vapor during typical combustion. These bonds also cause propane to burn with a visible flame.
Energy content.
The enthalpy of combustion of propane gas where all products return to standard state, for example where water returns to its liquid state at standard temperature, (known as higher heating value) is (−2219.2 ± 0.5) kJ/mol, or (50.33 ± 0.01) MJ/kg.
The enthalpy of combustion of propane gas where products do not return to standard state, for example where the hot gases including water vapor exit a chimney, (known as lower heating value) is −2043.455 kJ/mol. The lower heat value is the amount of heat available from burning the substance where the combustion products are vented to the atmosphere. For example, the heat from a fireplace when the flue is open.
Density.
The density of liquid propane at 25 °C (77 °F) is 0.493 g/cm3, which is equivalent to 4.11 pounds per U.S. liquid gallon or 493 kg/m3. Propane expands at 1.5% per 10 °F. Thus, liquid propane has a density of approximately 4.2 pounds per gallon (504 kg/m3) at 60 °F (15.6 °C).
Uses.
Propane is a popular choice for barbecues and portable stoves because the low boiling point of -42 C makes it vaporize as soon as it is released from its pressurized container. Therefore, no carburetor or other vaporizing device is required; a simple metering nozzle suffices. Propane powers some locomotives, buses, forklifts, taxis and ice resurfacing machines and is used for heat and cooking in recreational vehicles and campers. Since it can be transported easily, it is a popular fuel for home heat and backup electrical generation in sparsely populated areas that do not have natural gas pipelines.
Propane is generally stored and transported in steel cylinders as a liquid with a vapor space above the liquid. The vapor pressure in the cylinder is a function of temperature. When gaseous propane is drawn at a high rate, the latent heat of vaporisation required to create the gas will cause the bottle to cool. (This is why water often condenses on the sides of the bottle and then freezes). In addition, the lightweight, high-octane compounds vaporize before the heavier, low-octane ones. Thus, the ignition properties change as the cylinder empties. For these reasons, the liquid is often withdrawn using a dip tube. Propane is used as fuel in furnaces for heat, in cooking, as an energy source for water heaters, laundry dryers, barbecues, portable stoves, and motor vehicles.
Commercially available "propane" fuel, or LPG, is not pure. Typically in the United States and Canada, it is primarily propane (at least 90%), with the rest mostly ethane, propylene, butane, and odorants including ethyl mercaptan. This is the HD-5 standard, (Heavy Duty-5% maximum allowable propylene content, and no more than 5% butanes and ethane) defined by the American Society for Testing and Materials by its for internal combustion engines. Not all products labeled "LPG" conform to this standard however. In Mexico, for example, gas labeled "LPG" may consist of 60% propane and 40% butane. "The exact proportion of this combination varies by country, depending on international prices, on the availability of components and, especially, on the climatic conditions that favor LPG with higher butane content in warmer regions and propane in cold areas".
Domestic and industrial fuel.
Propane use is growing rapidly in non-industrialized areas of the world. Propane has completely replaced wood and other traditional fuel sources, now it's commonly known as 'cooking gas'. The "propane" sold outside North America is actually a mixture of propane and butane. The warmer the country, the higher the butane content, commonly 50/50 and sometimes reaching 75% butane. Usage is calibrated to the different-sized nozzles found in non-U.S. grills. Americans who take their grills overseas — such as military personnel — can find U.S.-specification propane at AAFES military post exchanges.
North American industries using propane include glass makers, brick kilns, poultry farms and other industries that need portable heat.
In rural areas of North America, as well as northern Australia and some parts of southern India propane is used to heat livestock facilities, in grain dryers, and other heat-producing appliances. When used for heating or grain drying it is usually stored in a large, permanently placed cylinder which is recharged by a propane-delivery truck. s of 2007[ [update]], 9.7 million American households use propane as their primary heating fuel.
In North America, local delivery trucks with an average cylinder size of 3,000 USgal, fill up large cylinders that are permanently installed on the property, or other service trucks exchange empty cylinders of propane with filled cylinders. Large tractor-trailer trucks, with an average cylinder size of 10,000 USgal, transport the propane from the pipeline or refinery to the local bulk plant. The bobtail and transport are not unique to the North American market, though the practice is not as common elsewhere, and the vehicles are generally called "tankers". In many countries, propane is delivered to consumers via small or medium-sized individual cylinders, while empty cylinders are removed for refilling at a central location.
Refrigeration.
Propane is also instrumental in providing off-the-grid refrigeration, usually by means of a gas absorption refrigerator.
Blends of pure, dry "isopropane" (R-290a) (isobutane/propane mixtures) and isobutane (R-600a) have negligible ozone depletion potential and very low Global Warming Potential (having a value of 3.3 times the GWP of carbon dioxide) and can serve as a functional replacement for R-12, R-22, R-134a, and other chlorofluorocarbon or hydrofluorocarbon refrigerants in conventional stationary refrigeration and air conditioning systems.
In motor vehicles.
Such substitution is widely prohibited or discouraged in motor vehicle air conditioning systems, on the grounds that using flammable hydrocarbons in systems originally designed to carry non-flammable refrigerant presents a significant risk of fire or explosion.
Vendors and advocates of hydrocarbon refrigerants argue against such bans on the grounds that there have been very few such incidents relative to the number of vehicle air conditioning systems filled with hydrocarbons.
Motor fuel.
Propane is also being used increasingly for vehicle fuels. In the U.S., over 190,000 on-road vehicles use propane, and over 450,000 forklifts use it for power. It is the third most popular vehicle fuel in the world, behind gasoline and Diesel fuel. In other parts of the world, propane used in vehicles is known as autogas. In 2007, approximately 13 million vehicles worldwide use autogas.
The advantage of propane in cars is its liquid state at a moderate pressure. This allows fast refill times, affordable fuel cylinder construction, and price ranges typically just over half that of gasoline. Meanwhile it is noticeably cleaner (both in handling, and in combustion), results in less engine wear (due to carbon deposits) without diluting engine oil (often extending oil-change intervals), and until recently was a relative bargain in North America. Octane rating of propane is relatively high at 110. In the United States the propane fueling infrastructure is the most developed of all alternative vehicle fuels. Many converted vehicles have provisions for topping off from "barbecue bottles". Purpose-built vehicles are often in commercially owned fleets, and have private fueling facilities. A further saving for propane fuel vehicle operators, especially in fleets, is that pilferage is much more difficult than with gasoline or Diesel fuels.
Propane is also used as fuel for small engines, especially those used indoors or in areas with insufficient fresh air and ventilation to carry away the more toxic exhaust of an engine running on gasoline or Diesel fuel. More recently, there have been lawn care products like string trimmers, lawn mowers and leaf blowers intended for outdoor use, but fueled by propane to reduce air pollution.
Propane risks and alternate gas fuels.
Propane is denser than air. If a leak in a propane fuel system occurs, the gas will have a tendency to sink into any enclosed area and thus poses a risk of explosion and fire. The typical scenario is a leaking cylinder stored in a basement; the propane leak drifts across the floor to the pilot light on the furnace or water heater, and results in an explosion or fire. This property makes propane generally unsuitable as a fuel for boats.
Propane is bought and stored in a liquid form (LPG), and thus fuel energy can be stored in a relatively small space. Compressed Natural Gas (CNG), largely methane, is another gas used as fuel, but it cannot be liquefied by compression at normal temperatures, as these are well above its critical temperature. As a gas, very high pressure is required to store useful quantities. This poses the hazard that, in an accident, just as with any compressed gas cylinder (such as a CO2 cylinder used for a soda concession) a CNG cylinder may burst with great force, or leak rapidly enough to become a self-propelled missile. Therefore, CNG is much less efficient to store, due to the large cylinder volume required. An alternative means of storing natural gas is as a cryogenic liquid in an insulated container as Liquefied Natural Gas (LNG). This form of storage is at low pressure and is around 3.5 times as efficient as storing it as CNG. Unlike propane, if a spill occurs, CNG will evaporate and dissipate harmlessly because it is lighter than air. Propane is much more commonly used to fuel vehicles than is natural gas because the equipment required costs less. Propane requires just 1220 kPa of pressure to keep it liquid at 37.8 C.
Retail cost.
United States.
s of 2013[ [update]], the retail cost of propane was approximately $2.37 per gallon, or roughly $25.95 per 1 million BTUs. This means that filling a 500-gallon propane tank, which is what households that use propane as their main source of energy usually require, costs $948 (80% of 500 gallons or 400 gallons), a 7.5% increase on the 2012–2013 winter season average US price. However, propane costs per gallon change significantly from one state to another: the Energy Information Administration quotes a $2.995 per gallon average on the East Coast for October 2013, while the figure for the Midwest was $1.860 for the same period.

</doc>
<doc id="23645" url="http://en.wikipedia.org/wiki?curid=23645" title="Precambrian">
Precambrian

The Precambrian (or Pre-Cambrian; sometimes abbreviated pЄ) is the largest span of time in Earth's history before the current Phanerozoic Eon, and is a Supereon divided into several eons of the geologic time scale. It spans from the formation of Earth about 4.6 billion years ago (Ga) to the beginning of the Cambrian Period, about 541.0 ± 1.0 million years ago (Ma), when hard-shelled creatures first appeared in abundance. The Precambrian is so named because it precedes the Cambrian, the first period of the Phanerozoic Eon, which is named after Cambria, the classical name for Wales, where rocks from this age were first studied. The Precambrian accounts for 88% of geologic time.
Overview.
Relatively little is known about the Precambrian, despite it making up roughly seven-eighths of the Earth's history, and what is known has largely been discovered in the past 50 years. The Precambrian fossil record is poorer than that of the succeeding Phanerozoic, and those fossils present (e.g. stromatolites) are of limited biostratigraphic use. This is because many Precambrian rocks have been heavily metamorphosed, obscuring their origins, while others have been destroyed by erosion, or remain deeply buried beneath Phanerozoic strata.
It is thought that the Earth itself coalesced from material in orbit around the Sun roughly 4500 Ma, or 4.5 billion years ago (Ga), and may have been struck by a very large (Mars-sized) planetesimal shortly after it formed, splitting off material that formed the Moon (see Giant impact hypothesis). A stable crust was apparently in place by 4400 Ma, since zircon crystals from Western Australia have been dated at 4404 Ma.
The term "Precambrian" is recognized by the International Commission on Stratigraphy as a general term including the Archean and Proterozoic eons. It is still used by geologists and paleontologists for general discussions not requiring the more specific eon names. It was briefly also called the "Cryptozoic" eon.
Life before the Cambrian.
It is not known when life originated, but carbon in 3.8 billion year old rocks from islands off western Greenland may be of organic origin. Well-preserved bacteria older than 3.46 billion years have been found in Western Australia. Probable fossils 100 million years older have been found in the same area. There is a fairly solid record of bacterial life throughout the remainder of the Precambrian.
Excluding a few contested reports of much older forms from USA and India, the first complex multicellular life forms seem to have appeared roughly 600 Ma. The oldest fossil evidence of complex life comes from the Lantian formation, at least 580 million years ago. A quite diverse collection of soft-bodied forms is known from a variety of locations worldwide between 542 and 600 Ma. These are referred to as Ediacaran or Vendian biota. Hard-shelled creatures appeared toward the end of that time span. By the middle of the later Cambrian period a very diverse fauna is recorded in the Burgess Shale, including some which may represent stem groups of modern taxa. The rapid radiation of lifeforms during the early Cambrian is called the Cambrian explosion of life.
While land seems to have been devoid of plants and animals, cyanobacteria and other microbes formed prokaryotic mats that covered terrestrial areas.
Planetary environment and the oxygen catastrophe.
Evidence illuminating the details of plate motions and other tectonic functions in the Precambrian has been poorly preserved. It is generally believed that small proto-continents existed prior to 3000 Ma, and that most of the Earth's landmasses collected into a single supercontinent around 1000 Ma. The supercontinent, known as Rodinia, broke up around 600 Ma. A number of glacial periods have been identified going as far back as the Huronian epoch, roughly 2200 Ma. One of the most delved into is the Sturtian-Varangian glaciation, around 600 Ma, which may have brought glacial conditions all the way to the equator, resulting in a "Snowball Earth".
The atmosphere of the early Earth is not well understood. Most geologists believe it was composed primarily of nitrogen, carbon dioxide, and other relatively inert gases, lacking in free oxygen. This has been disputed with evidence in support of an oxygen-rich atmosphere since the early Archean.
Molecular oxygen was not present as a significant fraction of Earth's atmosphere until after photosynthetic life forms evolved and began to produce it in large quantities as a byproduct of their metabolism. This radical shift from an inert to an oxidizing atmosphere caused an ecological crisis sometimes called the oxygen catastrophe. At first, oxygen would quickly combine with other elements in Earth's crust, primarily iron, as it was produced. After the supply of oxidizable surfaces ran out, oxygen began to accumulate in the atmosphere, and the modern high-oxygen atmosphere developed. Evidence for this lies in older rocks that contain massive banded iron formations, laid down as iron and oxygen first combined.
Subdivisions.
An established terminology has evolved covering the early years of the Earth's existence, as radiometric dating allows plausible real dates to be assigned to specific formations and features. The Precambrian Supereon is divided into three Precambrian eons: the Hadean (4500-3950 Ma), Archean (4000-2500 Ma) and Proterozoic (2500-541.0 ± 1.0 Ma). See Timetable of the Precambrian.
It has been proposed that the Precambrian should be divided into eons and eras that reflect stages of planetary evolution, rather than the current scheme based upon numerical ages. Such a system could rely on events in the stratigraphic record and be demarcated by GSSPs. The Precambrian could be divided into five "natural" eons, characterized as follows.
Precambrian super-continents.
The movement of plates has caused the formation and break-up of continents over time, including occasional formation of a super-continent containing most or all of the continents. The earliest known super-continent was Vaalbara. It formed from proto-continents and was a super-continent by 3.1 billion years ago (3.1 Ga). Vaalbara broke up ~2.8 Ga ago. The super-continent Kenorland was formed ~2.7 Ga ago and then broke sometime after 2.5 Ga into the proto-continent cratons called Laurentia, Baltica, Australia, and Kalahari. The super-continent Columbia or Nuna formed during a period of 2.0–1.8 billion years and broke up about 1.5–1.3 billion years ago. The super-continent Rodinia is thought to have formed about 1 billion years ago, to have embodied most or all of Earth's continents and to have broken up into eight continents around 600 million years ago.

</doc>
<doc id="23647" url="http://en.wikipedia.org/wiki?curid=23647" title="Polymerase chain reaction">
Polymerase chain reaction

The polymerase chain reaction (PCR) is a technology in molecular biology used to amplify a single copy or a few copies of a piece of DNA across several orders of magnitude, generating thousands to millions of copies of a particular DNA sequence.
Developed in 1983 by Kary Mullis, PCR is now a common and often indispensable technique used in medical and biological research labs for a variety of applications. These include DNA cloning for sequencing, DNA-based phylogeny, or functional analysis of genes; the diagnosis of hereditary diseases; the identification of genetic fingerprints (used in forensic sciences and paternity testing); and the detection and diagnosis of infectious diseases. In 1993, Mullis was awarded the Nobel Prize in Chemistry along with Michael Smith for his work on PCR.
The method relies on thermal cycling, consisting of cycles of repeated heating and cooling of the reaction for DNA melting and enzymatic replication of the DNA. Primers (short DNA fragments) containing sequences complementary to the target region along with a DNA polymerase, which the method is named after, are key components to enable selective and repeated amplification. As PCR progresses, the DNA generated is itself used as a template for replication, setting in motion a chain reaction in which the DNA template is exponentially amplified. PCR can be extensively modified to perform a wide array of genetic manipulations.
Almost all PCR applications employ a heat-stable DNA polymerase, such as Taq polymerase (an enzyme originally isolated from the bacterium "Thermus aquaticus"). This DNA polymerase enzymatically assembles a new DNA strand from DNA building-blocks, the nucleotides, by using single-stranded DNA as a template and DNA oligonucleotides (also called DNA primers), which are required for initiation of DNA synthesis. The vast majority of PCR methods use thermal cycling, i.e., alternately heating and cooling the PCR sample through a defined series of temperature steps.
In the first step, the two strands of the DNA double helix are physically separated at a high temperature in a process called DNA melting. In the second step, the temperature is lowered and the two DNA strands become templates for DNA polymerase to selectively amplify the target DNA. The selectivity of PCR results from the use of primers that are complementary to the DNA region targeted for amplification under specific thermal cycling conditions.
PCR principles and procedure.
PCR amplifies a specific region of a DNA strand (the DNA target). Most PCR methods typically amplify DNA fragments of between 0.1 and 10 kilo base pairs (kbp), although some techniques allow for amplification of fragments up to 40 kbp in size. The amount of amplified product is determined by the available substrates in the reaction, which become limiting as the reaction progresses.
A basic PCR set up requires several components and reagents. These components include:
The PCR is commonly carried out in a reaction volume of 10–200 μl in small reaction tubes (0.2–0.5 ml volumes) in a thermal cycler. The thermal cycler heats and cools the reaction tubes to achieve the temperatures required at each step of the reaction (see below). Many modern thermal cyclers make use of the Peltier effect, which permits both heating and cooling of the block holding the PCR tubes simply by reversing the electric current. Thin-walled reaction tubes permit favorable thermal conductivity to allow for rapid thermal equilibration. Most thermal cyclers have heated lids to prevent condensation at the top of the reaction tube. Older thermocyclers lacking a heated lid require a layer of oil on top of the reaction mixture or a ball of wax inside the tube.
Procedure.
Typically, PCR consists of a series of 20-40 repeated temperature changes, called cycles, with each cycle commonly consisting of 2-3 discrete temperature steps, usually three (Figure below). The cycling is often preceded by a single temperature step at a high temperature (>90 °C), and followed by one hold at the end for final product extension or brief storage. The temperatures used and the length of time they are applied in each cycle depend on a variety of parameters. These include the enzyme used for DNA synthesis, the concentration of divalent ions and dNTPs in the reaction, and the melting temperature (Tm) of the primers.
To check whether the PCR generated the anticipated DNA fragment (also sometimes referred to as the amplimer or amplicon), agarose gel electrophoresis is employed for size separation of the PCR products. The size(s) of PCR products is determined by comparison with a DNA ladder (a molecular weight marker), which contains DNA fragments of known size, run on the gel alongside the PCR products (see Fig. 3).
PCR stages.
The PCR process can be divided into three stages:
"Exponential amplification": At every cycle, the amount of product is doubled (assuming 100% reaction efficiency). The reaction is very sensitive: only minute quantities of DNA must be present.
"Leveling off stage": The reaction slows as the DNA polymerase loses activity and as consumption of reagents such as dNTPs and primers causes them to become limiting.
"Plateau": No more product accumulates due to exhaustion of reagents and enzyme.
PCR optimization.
In practice, PCR can fail for various reasons, in part due to its sensitivity to contamination causing amplification of spurious DNA products. Because of this, a number of techniques and procedures have been developed for optimizing PCR conditions. Contamination with extraneous DNA is addressed with lab protocols and procedures that separate pre-PCR mixtures from potential DNA contaminants. This usually involves spatial separation of PCR-setup areas from areas for analysis or purification of PCR products, use of disposable plasticware, and thoroughly cleaning the work surface between reaction setups. Primer-design techniques are important in improving PCR product yield and in avoiding the formation of spurious products, and the usage of alternate buffer components or polymerase enzymes can help with amplification of long or otherwise problematic regions of DNA. Addition of reagents, such as formamide, in buffer systems may increase the specificity and yield of PCR. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.
Application of PCR.
Selective DNA isolation.
PCR allows isolation of DNA fragments from genomic DNA by selective amplification of a specific region of DNA. This use of PCR augments many methods, such as generating hybridization probes for Southern or northern hybridization and DNA cloning, which require larger amounts of DNA, representing a specific DNA region. PCR supplies these techniques with high amounts of pure DNA, enabling analysis of DNA samples even from very small amounts of starting material.
Other applications of PCR include DNA sequencing to determine unknown PCR-amplified sequences in which one of the amplification primers may be used in Sanger sequencing, isolation of a DNA sequence to expedite recombinant DNA technologies involving the insertion of a DNA sequence into a plasmid, phage, or cosmid (depending on size) or the genetic material of another organism. Bacterial colonies (E. coli) can be rapidly screened by PCR for correct DNA vector constructs. PCR may also be used for genetic fingerprinting; a forensic technique used to identify a person or organism by comparing experimental DNAs through different PCR-based methods.
Some PCR 'fingerprints' methods have high discriminative power and can be used to identify genetic relationships between individuals, such as parent-child or between siblings, and are used in paternity testing (Fig. 4). This technique may also be used to determine evolutionary relationships among organisms when certain molecular clocks are used (i.e., the 16S rRNA and recA genes of microorganisms). 
Amplification and quantification of DNA.
Because PCR amplifies the regions of DNA that it targets, PCR can be used to analyze extremely small amounts of sample. This is often critical for forensic analysis, when only a trace amount of DNA is available as evidence. PCR may also be used in the analysis of ADNA that is tens of thousands of years old. These PCR-based techniques have been successfully used on animals, such as a forty-thousand-year-old mammoth, and also on human DNA, in applications ranging from the analysis of Egyptian mummies to the identification of a Russian tsar and the body of English king Richard III.
Quantitative PCR methods allow the estimation of the amount of a given sequence present in a sample—a technique often applied to quantitatively determine levels of gene expression. Quantitative PCR is an established tool for DNA quantification that measures the accumulation of DNA product after each round of PCR amplification.
PCR in diagnosis of diseases.
PCR permits early diagnosis of malignant diseases such as leukemia and lymphomas, which is currently the highest-developed in cancer research and is already being used routinely. PCR assays can be performed directly on genomic DNA samples to detect translocation-specific malignant cells at a sensitivity that is at least 10,000 fold higher than that of other methods.
PCR allows for rapid and highly specific diagnosis of infectious diseases, including those caused by bacteria or viruses. PCR also permits identification of non-cultivatable or slow-growing microorganisms such as mycobacteria, anaerobic bacteria, or viruses from tissue culture assays and animal models. The basis for PCR diagnostic applications in microbiology is the detection of infectious agents and the discrimination of non-pathogenic from pathogenic strains by virtue of specific genes.
Viral DNA can likewise be detected by PCR. The primers used must be specific to the targeted sequences in the DNA of a virus, and the PCR can be used for diagnostic analyses or DNA sequencing of the viral genome. The high sensitivity of PCR permits virus detection soon after infection and even before the onset of disease. Such early detection may give physicians a significant lead time in treatment. The amount of virus ("viral load") in a patient can also be quantified by PCR-based DNA quantitation techniques (see below).
Limitations.
DNA polymerase is prone to error, which in turn causes mutations in the PCR fragments that are made. Additionally, the specificity of the PCR fragments can mutate to the template DNA, due to nonspecific binding of primers. Furthermore prior information on the sequence is necessary in order to generate the primers. <ref name="10.1038/jid.2013.1"></ref>
History.
A 1971 paper in the Journal of Molecular Biology by Kleppe and co-workers first described a method using an enzymatic assay to replicate a short DNA template with primers "in vitro". However, this early manifestation of the basic PCR principle did not receive much attention, and the invention of the polymerase chain reaction in 1983 is generally credited to Kary Mullis.
When Mullis developed the PCR in 1983, he was working in Emeryville, California for Cetus Corporation, one of the first biotechnology companies. There, he was responsible for synthesizing short chains of DNA. Mullis has written that he conceived of PCR while cruising along the Pacific Coast Highway one night in his car. He was playing in his mind with a new way of analyzing changes (mutations) in DNA when he realized that he had instead invented a method of amplifying any DNA region through repeated cycles of duplication driven by DNA polymerase. In "Scientific American", Mullis summarized the procedure: "Beginning with a single molecule of the genetic material DNA, the PCR can generate 100 billion similar molecules in an afternoon. The reaction is easy to execute. It requires no more than a test tube, a few simple reagents, and a source of heat." He was awarded the Nobel Prize in Chemistry in 1993 for his invention, seven years after he and his colleagues at Cetus first put his proposal to practice. However, some controversies have remained about the intellectual and practical contributions of other scientists to Mullis' work, and whether he had been the sole inventor of the PCR principle (see below).
At the core of the PCR method is the use of a suitable DNA polymerase able to withstand the high temperatures of >90 °C required for separation of the two DNA strands in the DNA double helix after each replication cycle. The DNA polymerases initially employed for in vitro experiments presaging PCR were unable to withstand these high temperatures. So the early procedures for DNA replication were very inefficient and time consuming, and required large amounts of DNA polymerase and continuous handling throughout the process.
The discovery in 1976 of Taq polymerase — a DNA polymerase purified from the thermophilic bacterium, "Thermus aquaticus", which naturally lives in hot (50 to) environments such as hot springs — paved the way for dramatic improvements of the PCR method. The DNA polymerase isolated from "T. aquaticus" is stable at high temperatures remaining active even after DNA denaturation, thus obviating the need to add new DNA polymerase after each cycle. This allowed an automated thermocycler-based process for DNA amplification.
Patent disputes.
The PCR technique was patented by Kary Mullis and assigned to Cetus Corporation, where Mullis worked when he invented the technique in 1983. The "Taq" polymerase enzyme was also covered by patents. There have been several high-profile lawsuits related to the technique, including an unsuccessful lawsuit brought by DuPont. The pharmaceutical company Hoffmann-La Roche purchased the rights to the patents in 1992 and currently holds those that are still protected.
A related patent battle over the Taq polymerase enzyme is still ongoing in several jurisdictions around the world between Roche and Promega. The legal arguments have extended beyond the lives of the original PCR and Taq polymerase patents, which expired on March 28, 2005.

</doc>
<doc id="23648" url="http://en.wikipedia.org/wiki?curid=23648" title="Polymerase">
Polymerase

A polymerase is an enzyme (EC 2.7.7.6/7/19/48/49) that synthesizes long chains or polymers of nucleic acids. DNA polymerase and RNA polymerase are used to assemble DNA and RNA molecules, respectively, by copying a DNA or RNA template strand using base-pairing interactions.
It is an accident of history that the enzymes responsible for the generation of other biopolymers are not also referred to as polymerases. For example, the enzymatic complex that assembles amino acids into proteins is termed the ribosome, rather than "protein polymerase".
A polymerase from the thermophilic bacterium, "Thermus aquaticus" ("Taq") (PDB , EC 2.7.7.7) is used in the polymerase chain reaction, an important technique of molecular biology.
Other well-known polymerases include:

</doc>
<doc id="23649" url="http://en.wikipedia.org/wiki?curid=23649" title="Pacific Scandal">
Pacific Scandal

The Pacific Scandal was a political scandal in Canada involving allegations of bribes being accepted by members of the Conservative government in the attempts of private interests to influence the bidding for a national rail contract. As part of British Columbia's 1871 agreement to join Canadian Confederation, the government had agreed to build a transcontinental railway linking the Pacific Province to the eastern provinces. The proposed rail project, when completed, was the most intensive and ambitious of its kind ever undertaken to date. However as a new nation with limited capital resources, financing for the project was sought after both at home and abroad, naturally attracting interest from Great Britain and the United States.
The scandal ultimately led to the resignation of Canada's first Prime Minister, Sir John A. Macdonald, and a transfer of power from his Conservative government to a Liberal government led by Alexander Mackenzie. One of the new government's first measures was to introduce secret ballots in an effort to improve the integrity of future elections.
Background.
For a young and loosely defined nation, the building of a national railway was an active attempt at state-making, as well as an aggressive capitalist venture. Canada, a nascent country with a population of 3.5 million in 1871, lacked the means to exercise meaningful "de facto" control within the "de jure" political boundaries of the recently acquired Rupert's Land; building a transcontinental railway was national policy of high order to change this situation. Moreover, after the American Civil War the American frontier rapidly expanded west with land-hungry settlers, exacerbating talk of annexation. Indeed, sentiments of Manifest Destiny were abuzz in this time: in 1867, year of Confederation, US Secretary of State W.H. Seward surmised that the whole North American continent "shall be, sooner or later, within the magic circle of the American Union." Therefore, preventing American investment into the project was considered as being in Canada's national interest. Thus the federal government favoured an "all Canadian route" through the rugged Canadian Shield of northern Ontario, refusing to consider a less costly route passing south through Wisconsin and Minnesota.
However, a route across the Canadian Shield was highly unpopular with potential investors, not only in the United States but also in Canada and especially Great Britain, the only other viable source of financing. For would-be investors, the objections were not primarily based on politics or nationalism but economics. At the time, national governments lacked the finances needed to undertake such large projects. For the First Transcontinental Railroad, the United States government had made extensive grants of public land to the railway's builders, inducing private financiers to fund the railway on the understanding that they would acquire rich farmland along the route, which could then be sold for a large profit. However, the eastern terminus of the proposed Canadian Pacific route, unlike that of the First Transcontinental, was not in rich Nebraskan farmland, but deep within the Canadian Shield. Copying the American financing model whilst insisting on an all-Canadian route would require the railway's backers to build hundreds of miles of track across rugged shield terrain (with little economic value) at considerable expense before they could expect to access lucrative farmland in Manitoba, which then was part of the newly created Northwest Territories. Many financiers, who had expected to make a relatively quick profit, were not willing to make this sort of long-term commitment.
Nevertheless, the Montreal capitalist Sir Hugh Allan, with his syndicate Canada Pacific Railway Company, sought the potentially lucrative charter for the project. The problem lay in that Allan and Sir John A. Macdonald highly, and secretly, were in cahoots with American financiers such as George W. McMullen and Jay Cooke, men who were deeply interested in the rival American undertaking, the Northern Pacific Railroad.
Scandal.
Two groups competed for the contract to build the railway, Sir Hugh Allan's Canada Pacific Railway Company and David Lewis Macpherson's Inter-Oceanic Railway Company. On April 2, 1873, Lucius Seth Huntington, a Liberal Member of Parliament, created an uproar in the House of Commons. He announced he had uncovered evidence that Sir Hugh Allan and his associates had been granted the Canadian Pacific Railway contract in return for political donations of $360,000.
In 1873, it became known that Allan had contributed a large sum of money to the Conservative government's re-election campaign of 1872; some sources quote a sum over $360,000. Allan had promised to keep American capital out of the railway deal, but had lied to Macdonald over this vital point, and Macdonald later discovered the lie. The Liberal party, at this time the opposition party in Parliament, accused the Conservatives of having made a tacit agreement to give the contract to Hugh Allan in exchange for money.
In making such allegations, the Liberals and their allies in the press (in particular, George Brown's newspaper the Globe) presumed that most of the money had been used to bribe voters in the 1872 election. The secret ballot, then considered a novelty, had not yet been introduced in Canada. Although it was illegal to offer, solicit or accept bribes exchange for votes, effective enforcement of this prohibition proved impossible.
Despite Macdonald's claims that he was innocent, evidence came to light showing receipts of money from Allan to Macdonald and some of his political colleagues. Perhaps even more damaging to Macdonald was when the Liberals discovered a telegram, through a former employee of Sir Hugh Allan, which was thought to have had been stolen from the safe of Allan's lawyer, Sir John Abbott.
The scandal proved fatal to Macdonald's government. Macdonald's control of Parliament was already tenuous following the 1872 election. In a time when party discipline was not as strong as it is today, once Macdonald's culpability in the scandal became known he could no longer expect to retain the confidence of the House of Commons.
Macdonald resigned as prime minister on 5 November 1873. He also offered his resignation as the head of the Conservative party, but it was not accepted and he was convinced to stay. Perhaps as a direct result of this scandal, the Conservative party fell in the eyes of the public and was relegated to being the Official Opposition in the federal election of 1874. This election, in which secret ballots were used for the first time, gave Alexander Mackenzie a firm mandate to succeed Macdonald as the new prime minister of Canada.
Despite the short-term defeat, the scandal was not a mortal wound to Macdonald, the Conservative Party, or the Canadian Pacific Railway. An economic depression gripped Canada shortly after Macdonald left office, and although the causes of the depression were largely external to Canada many Canadians nevertheless blamed Mackenzie for the ensuing hard times. Macdonald would return as prime minister in the 1878 election thanks to his National Policy. He would hold the office of prime minister to his death in 1891, and the Canadian Pacific would be completed by 1885 with Macdonald still in office.

</doc>
<doc id="23650" url="http://en.wikipedia.org/wiki?curid=23650" title="Primer (molecular biology)">
Primer (molecular biology)

A primer is a strand of short nucleic acid sequences (generally about 10 base pairs) that serves as a starting point for DNA synthesis. It is required for DNA replication because the enzymes that catalyze this process, DNA polymerases, can only add new nucleotides to an existing strand of DNA. The polymerase starts replication at the 3'-end of the primer, and copies the opposite strand.
In most cases of natural DNA replication, the primer for DNA synthesis and replication is a short strand of RNA (which can be made "de novo").
Many of the laboratory techniques of biochemistry and molecular biology that involve DNA polymerase, such as DNA sequencing and the polymerase chain reaction (PCR), require DNA primers. These primers are usually short, chemically synthesized oligonucleotides, with a length of about twenty bases. They are hybridized to a target DNA, which is then copied by the polymerase.
Mechanism "in vivo".
The lagging strand of DNA is that strand of the DNA double helix that is orientated in a 5' to 3' manner. Therefore, its complement must be synthesized in a 3'→5' manner. Because DNA polymerase III cannot synthesize in the 3'→5' direction, the lagging strand is synthesized in short segments known as Okazaki fragments. Along the lagging strand's template, primase builds RNA primers in short bursts. DNA polymerases are then able to use the free 3'-OH groups on the RNA primers to synthesize DNA in the 5'→3' direction. 
The RNA fragments are then removed by DNA polymerase I for prokaryotes or DNA polymerase δ for eukaryotes (different mechanisms are used in eukaryotes and prokaryotes) and new deoxyribonucleotides are added to fill the gaps where the RNA was present. DNA ligase then joins the deoxyribonucleotides together, completing the synthesis of the lagging strand.
Primer removal.
In eukaryotic primer removal, DNA polymerase δ extends the Okazaki fragment in 5' to 3' direction, and when it encounters the RNA primer from the previous Okazaki fragment, it displaces the 5′ end of the primer into a single-stranded RNA flap, which is removed by nuclease cleavage. Cleavage of the RNA flaps involves either endonuclease 1 (FEN1) cleavage of short flaps, or coating of long flaps by the single-stranded DNA binding protein replication protein A (RPA) and sequential cleavage by Dna2 nuclease and FEN1.
This mechanism is a potential explanation of how the HIV virus can transform its genome into double-stranded DNA from the RNA-DNA formed after reverse transcription of its RNA. However, the HIV-encoded reverse transcriptase has its own ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that copies the sense cDNA strand into "antisense" DNA to form a double-stranded DNA intermediate.
Uses of synthetic primers.
DNA sequencing is used to determine the nucleotides in a DNA strand. The Sanger chain termination method of sequencing uses a primer to start the chain reaction.
In PCR, primers are used to determine the DNA fragment to be amplified by the PCR process. The length of primers is usually not more than 30 (usually 18–24) nucleotides, and they need to match the beginning and the end of the DNA fragment to be amplified. They direct replication towards each other – the extension of one primer by polymerase then becomes the template for the other, leading to an exponential increase in the target segment.
It is worth noting that primers are not always for DNA synthesis, but can in fact be used by viral polymerases, e.g. influenza, for RNA synthesis.
PCR primer design.
Pairs of primers should have similar melting temperatures since annealing in a PCR occurs for both simultaneously. A primer with a "T"m (melting temperature) significantly higher than the reaction's annealing temperature may mishybridize and extend at an incorrect location along the DNA sequence, while "T"m significantly lower than the annealing temperature may fail to anneal and extend at all. 
Primer sequences need to be chosen to uniquely select for a region of DNA, avoiding the possibility of mishybridization to a similar sequence nearby. A commonly used method is BLAST search whereby all the possible regions to which a primer may bind can be seen. Both the nucleotide sequence as well as the primer itself can be BLAST searched. The free NCBI tool Primer-BLAST integrates primer design tool and BLAST search into one application, so does commercial software product such as ePrime, Beacon Designer. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.
Many online tools are freely available for primer design, some of which focus on specific applications of PCR. The popular tools and can be used to find primers matching a wide variety of specifications. Highly degenerate primers for targeting a wide variety of DNA templates can be interactively designed using . Primers with high specificity for a subset of DNA templates in the presence of many similar variants can be designed using .
Mononucleotide and dinucleotide repeats should be avoided, as loop formation can occur and contribute to mishybridization. Primers should not easily anneal with other primers in the mixture (either other copies of same or the reverse direction primer); this phenomenon can lead to the production of 'primer dimer' products contaminating the mixture. Primers should also not anneal strongly to themselves, as internal hairpins and loops could hinder the annealing with the template DNA.
When designing a primer for use in TA cloning, efficiency can be increased by adding AG tails to the 5' and the 3' end.
The reverse primer has to be the reverse complement of the given cDNA sequence. The reverse complement can be easily determined, e.g. with online calculators.
Degenerate primers.
Sometimes "degenerate primers" are used. These are actually mixtures of similar, but not identical primers. They may be convenient if the same gene is to be amplified from different organisms, as the genes themselves are probably similar but not identical. The other use for degenerate primers is when primer design is based on protein sequence. As several different codons can code for one amino acid, it is often difficult to deduce which codon is used in a particular case. Therefore primer sequence corresponding to the amino acid isoleucine might be "ATH", where A stands for adenine, T for thymine, and H for adenine, thymine, or cytosine, according to the genetic code for each codon, using the IUPAC symbols for degenerate bases. Use of degenerate primers can greatly reduce the specificity of the PCR amplification. The problem can be partly solved by using touchdown PCR.
"Degenerate primers" are widely used and extremely useful in the field of microbial ecology. They allow for the amplification of genes from thus far uncultivated microorganisms or allow the recovery of genes from organisms where genomic information is not available. Usually, degenerate primers are designed by aligning gene sequencing found in GenBank. Differences among sequences are accounted for by using IUPAC degeneracies for individual bases. PCR primers are then synthesized as a mixture of primers corresponding to all permutations.
There are a number of programs available to perform these primer predictions; 

</doc>
<doc id="23652" url="http://en.wikipedia.org/wiki?curid=23652" title="Purine">
Purine

A purine is a heterocyclic aromatic organic compound. It consists of a pyrimidine ring fused to an imidazole ring. Purines, which include substituted purines and their tautomers, are the most widely occurring nitrogen-containing heterocycle in nature.
Purines and pyrimidines make up the two groups of nitrogenous bases, including the two groups of nucleotide bases. Two of the four deoxyribonucleotides and two of the four ribonucleotides, the respective building-blocks of deoxyribonucleic acid - DNA, and ribonucleic acid - RNA, are purines.
Notable purines.
There are many naturally occurring purines. Two of the five bases in nucleic acids, adenine (2) and guanine (3), are purines. In DNA, these bases form hydrogen bonds with their complementary pyrimidines thymine and cytosine, respectively. This is called complementary base pairing. In RNA, the complement of adenine is uracil instead of thymine.
Other notable purines are hypoxanthine (4), xanthine (5), theobromine (6), caffeine (7), uric acid (8) and isoguanine (9).
Functions.
Aside from the crucial roles of purines (adenine and guanine) in DNA and RNA, purines are also significant components in a number of other important biomolecules, such as ATP, GTP, cyclic AMP, NADH, and coenzyme A. Purine (1) itself, has not been found in nature, but it can be produced by organic synthesis.
They may also function directly as neurotransmitters, acting upon purinergic receptors. Adenosine activates adenosine receptors.
History.
The word "purine" ("pure urine") was coined by the German chemist Emil Fischer in 1884. He synthesized it for the first time in 1899. The starting material for the reaction sequence was uric acid (8), which had been isolated from kidney stones by Scheele in 1776. Uric acid (8) was reacted with PCl5 to give 2,6,8-trichloropurine (10), which was converted with HI and PH4I to give 2,6-diiodopurine (11). The product was reduced to purine (1) using zinc-dust.
Metabolism.
Many organisms have metabolic pathways to synthesize and break down purines.
Purines are biologically synthesized as nucleosides (bases attached to ribose).
Accumulation of modified purine nucleotides is defective to various cellular processes, especially those involving DNA and RNA. To be viable, organisms possess a number of (deoxy)purine phosphohydrolases, which hydrolyze these purine derivatives removing them from the active NTP and dNTP pools. Deamination of purine bases can result in accumulation of such nucleotides as ITP, dITP, XTP and dXTP.
Defects in enzymes that control purine production and breakdown can severely alter a cell’s DNA sequences, which may explain why people who carry certain genetic variants of purine metabolic enzymes have a higher risk for some types of cancer.
Purine Sources.
Purines are found in high concentration in meat and meat products, especially internal organs such as liver and kidney. In general, plant-based diets are low in purines.
Examples of high-purine sources include: sweetbreads, anchovies, sardines, liver, beef kidneys, brains, meat extracts (e.g., Oxo, Bovril), herring, mackerel, scallops, game meats, beer (from the yeast) and gravy.
A moderate amount of purine is also contained in beef, pork, poultry, other fish and seafood, asparagus, cauliflower, spinach, mushrooms, green peas, lentils, dried peas, beans, oatmeal, wheat bran, wheat germ, and hawthorn.
Higher levels of meat and seafood consumption are associated with an increased risk of gout, whereas a higher level of consumption of dairy products is associated with a decreased risk. Moderate intake of purine-rich vegetables or protein is not associated with an increased risk of gout.
Laboratory synthesis.
In addition to in vivo synthesis of purines in purine metabolism, purine can also be created artificially.
Purine (1) is obtained in good yield when formamide is heated in an open vessel at 170 °C for 28 hours.
This remarkable reaction and others like it have been discussed in the context of the origin of life.
Oro, Orgel and co-workers have shown that four molecules of HCN tetramerize to form diaminomaleodinitrile (12), which can be converted into almost all natural-occurring purines. For example, five molecules of HCN condense in an exothermic reaction to make Adenine, especially in the presence of ammonia.
The Traube purine synthesis (1900) is a classic reaction (named after Wilhelm Traube) between an amine-substituted pyrimidine and formic acid.

</doc>
<doc id="23653" url="http://en.wikipedia.org/wiki?curid=23653" title="Pyrimidine">
Pyrimidine

Pyrimidine is an aromatic heterocyclic organic compound similar to pyridine. One of the three diazines (six-membered heterocyclics with two nitrogen atoms in the ring), it has the nitrogens at positions 1 and 3 in the ring. The other diazines are pyrazine (nitrogens 1 and 4) and pyridazine (nitrogens 1 and 2). In nucleic acids, three types of nucleobases are pyrimidine derivatives: cytosine (C), thymine (T), and uracil (U).
Occurrence and history.
The pyrimidine ring system has wide occurrence in nature
as substituted and ring fused compounds and derivatives, including the nucleotides, thiamine (vitaminB1) and alloxan. It is also found in many synthetic compounds such as barbiturates and the HIV drug, zidovudine. Although pyrimidine derivatives such as uric acid and alloxan were known in the early 19th century, a laboratory synthesis of a pyrimidine was not carried out until 1879, when Grimaux reported the preparation of barbituric acid from Ivy urea and malonic acid in the presence of phosphorus oxychloride.
The systematic study of pyrimidines began in 1884 with Pinner,
who synthesized derivatives by condensing ethyl acetoacetate with amidines. Pinner first proposed the name “pyrimidin” in 1885. The parent compound was first prepared by Gabriel & Colman in 1900,
by conversion of barbituric acid to 2,4,6-trichloropyrimidine followed by reduction using zinc dust in hot water.
Nomenclature.
The nomenclature of pyrimidines is straightforward. However, like other heterocyclics, tautomeric hydroxyl groups yield complications since they exist primarily in the cyclic amide form. For example, 2-hydroxypyrimidine is more properly named 2-pyrimidone [structures]. A partial list of trivial names of various pyrimidines exists.
Physical properties.
Physical properties are shown in the data box. A more extensive discussion, including spectra, can be found in Brown "et al."
Chemical properties.
Per the classification by Albert six-membered heterocyclics can be described as π-deficient. Substitution by electronegative groups or additional nitrogen atoms in the ring significantly increase the π-deficiency. These effects also decrease the basicity.
Like pyridines, in pyrimidines the π-electron density is decreased to an even greater extent. Therefore electrophilic aromatic substitution is more difficult while nucleophilic aromatic substitution is facilitated. An example of the last reaction type is the displacement of the amino group in 2-aminopyrimidine by chlorine and its reverse.
Electron lone pair availability (basicity) is decreased compared to pyridine. Compared to pyridine, N-alkylation and N-oxidation are more difficult. The pKa value for protonated pyrimidine is 1.23 compared to 5.30 for pyridine. Protonation and other electrophilic additions will occur at only one nitrogen due to further deactivation by the second nitrogen. The 2-, 4-, and 6- positions on the pyrimidine ring are electron deficient analogous to those in pyridine and nitro- and dinitrobenzene. The 5-position is less electron deficient and substitutents there are quite stable. However, electrophilic substitution is relatively facile at the 5-position, including nitration and halogenation.
Reduction in resonance stabilization of pyrimidines may lead to addition and ring cleavage reactions rather than substitutions. One such manifestation is observed in the Dimroth rearrangement.
Pyrimidine is also found in meteorites, but scientists still do not know its origin. Pyrimidine also photolytically decomposes into uracil under UV light.
Synthesis.
As is often the case with parent heterocyclic ring systems, the synthesis of pyrimidine is not that common and is usually performed by removing functional groups from derivatives. Primary syntheses in quantity involving formamide have been reported.
As a class, pyrimidines are typically synthesized by the “Principal Synthesis” involving cyclization of beta-dicarbonyl compounds with N-C-N compounds. Reaction of the former with amidines to give 2-substituted pyrimidines, with urea to give 2-pyrimidiones, and guanidines to give 2-aminopyrimidines are typical.
Pyrimidines can be prepared via the Biginelli reaction. Many other methods rely on condensation of carbonyls with diamines for instance the synthesis of 2-Thio-6-methyluracil from thiourea and ethyl acetoacetate or the synthesis of 4-methylpyrimidine with 4,4-dimethoxy-2-butanone and formamide.
A novel method is by reaction of certain amides with carbonitriles under electrophilic activation of the amide with 2-chloro-pyridine and trifluoromethanesulfonic anhydride:
Reactions.
Because of the decreased basicity compared to pyridine, electrophilic substitution of pyrimidine is less facile. Protonation or alkylation typically takes place at only one of the ring nitrogen atoms. Mono N-oxidation occurs by reaction with peracids.
Electrophilic C-substitution of pyrimidine occurs at the 5-position, the least electron deficient. Nitration, nitrosation, azo coupling, halogenation, sulfonation, formylation, hydroxymethylation, and aminomethylation have been observed with substituted pyrimidines.
Nucleophilic C-substitution should be facilitated at the 2-, 4-, and 6-positions but there are only a few examples. Amination and hydroxylation has been observed for substituted pyrimidines. Reactions with Grignard or alkyllithium reagents yield 4-alkyl- or 4-aryl pyrimidine after aromatization.
Free radical attack has been observed for pyrimidine and photochemical reactions have been observed for substituted pyrimidines. Pyrimidine can be hydrogenated to give tetrahydropyrimidine.
Nucleotides.
Three nucleobases found in nucleic acids, cytosine (C), thymine (T), and
uracil (U), are pyrimidine derivatives:
In DNA and RNA, these bases form hydrogen bonds with their complementary purines. Thus, in DNA, the purines adenine (A) and guanine (G) pair up with the pyrimidines thymine (T) and cytosine (C), respectively.
In RNA, the complement of adenine (A) is uracil (U) instead of thymine (T), so the pairs that form are adenine:uracil and guanine:cytosine.
Very rarely, thymine can appear in RNA, or uracil in DNA,but when other three major pyrimidine bases presented, some minor pyrimidine bases can also occur in nucleic acids. These minor pyrimidines are usually methylated versions of major ones and are postulated to have regulatory functions.
These hydrogen bonding modes are for classical Watson-Crick base pairing. Other hydrogen bonding modes ("wobble pairings") are available in both DNA and RNA, although the additional 2'-hydroxyl group of RNA expands the configurations, through which RNA can form hydrogen bonds.
Theoretical aspects.
In March 2015, NASA Ames scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.

</doc>
<doc id="23654" url="http://en.wikipedia.org/wiki?curid=23654" title="Play-by-mail game">
Play-by-mail game

Play-by-mail games, sometimes known as "Play-by-post", are games, of any type, played through postal mail or email. One example, chess, has been played by mail for centuries (when played in this way, it is known as correspondence chess). Another example, Diplomacy, has been played by mail since the 1960s, starting with a printed newsletter (a fanzine) written by John Boardman. More complex games, moderated entirely or partially by computer programs, were pioneered by Rick Loomis of Flying Buffalo in 1970. The first such game offered via major e-mail services was WebWar II (based on Starweb and licensed from Flying Buffalo) from Neolithic Enterprises who accepted e-mail turns from all of the major E-Mail services including CompuServe in 1983.
Play by mail games are often referred to as PBM games, and play by email is sometimes abbreviated PBeM—as opposed to face to face (FTF) or over the board (OTB) games which are played in person. Another variation on the name is Play-by-Internet (PBI) or Play-by-Web (PBW). In all of these examples, player instructions can be either executed by a human moderator, a computer program, or a combination of the two.
In the 1980s, play-by-mail games reached their peak of popularity with the advent of Gaming Universal, Paper Mayhem and Flagship magazine, the first professional magazines devoted to play-by-mail games. (An earlier fanzine, Nuts & Bolts of PBM, was the first publication to exclusively cover the hobby.) Bob McLain, the publisher and editor of Gaming Universal, further popularized the hobby by writing articles that appeared in many of the leading mainstream gaming magazines of the time. Flagship later bought overseas right to Gaming Universal, making it the leading magazine in the field. Flagship magazine was founded by Chris Harvey and Nick Palmer (now an MP) of the UK. The magazine still exists, under a new editor, but health concerns have led to worries over the publication's long term viability.
In the late 1990s, computer and Internet games marginalized play-by-mail conducted by actual postal mail, but the postal hobby still exists with an estimated 2000–3000 adherents worldwide.
Postal gaming.
Postal gaming developed as a way for geographically separated gamers to compete with each other. It was especially useful for those living in isolated areas and those whose tastes in games were uncommon.
In the case of a two player game such as chess, players would simply send their moves to each other alternately. In the case of a multi-player game such as Diplomacy, a central game master would run the game, receiving the moves and publishing adjudications. Such adjudications were often published in postal game zines, some of which contained far more than just games.
The commercial market for play-by-mail games grew to involve computer servers set up to host potentially thousands of players at once. Players would typically be split up into parallel games in order to keep the number of players per game at a reasonable level, with new games starting as old games ended. A typical closed game session might involve one to two dozen players, although some games claimed to have as many as five hundred people simultaneously competing in the same game world. While the central company was responsible for feeding in moves and mailing the processed output back to players, players were also provided with the mailing addresses of others so that direct contact could be made and negotiations performed. With turns being processed every few weeks (a two week turnaround being standard), more advanced games could last over a year.
Game themes are heavily varied, and may range from those based on historical or real events to those taking place in alternate or fictional worlds.
Inevitably, the onset of the computer-moderated PBM game (primarily the Legends game system) meant that the human moderated games became "boutique" games with little chance of matching the gross revenues that larger, automated games could produce.
Mechanics.
The mechanics of play-by-mail games require that players think and plan carefully before making moves. Because planned actions can typically only be submitted at a fixed maximum frequency (e.g., once every few days or every few weeks), the number of discrete actions is limited compared to real-time games. As a result, players are provided with a variety of resources to assist in turn planning, including game aids, maps, and results from previous turns. Using this material, planning a single turn may take a number of hours.
Actual move/turn submission is traditionally carried out by filling in a "turn card". This card has formatted entry areas where players enter their planned actions (using some form of encoding) for the upcoming turn. Players are limited to some finite number of actions, and in some cases must split their resources between these actions (so that additional actions make each less effective). The way the card is filled in often implies an ordering between each command, so that they are processed in-order, one after another. Once completed, the card is then mailed (or, in more modern times, emailed) to the game master, where it is either processed, or held until the next turn processing window begins.
By gathering turn cards from a number of players and processing them all at the same time, games can provide simultaneous actions for all players. However, for this same reason, co-ordination between players can be difficult to achieve. For example, player A might attempt to move to player B's current location to do something with (or to) player B, while player B might simultaneously attempt to move to player A's current location. As such, the output/results of the turn can differ significantly from the submitted plan. Whatever the results, they are mailed back to the player to be studied and used as the basis for the next turn (often along with a new blank turn card).
While billing is sometimes done using a flat per-game rate (when the length of the game is known and finite), games more typically use a per-turn cost schedule. In such cases, each turn submitted depletes a pool of credit which must periodically be replenished in order to keep playing. Some games have multiple fee schedules, where players can pay more to perform advanced actions, or to take a greater number of actions in a turn.
Some role-playing PBM games also include an element whereby the player may describe actions of their characters in a free text form. The effect and effectiveness of the action is then based on the judgement of the GM who may allow or partially allow the action. This gives the player more flexibility beyond the normal fixed actions at the cost of more complexity and, usually, expense.
Play-by-Email.
With the rise of the Internet, email and websites have largely replaced postal gaming and postal games zines. Play by mail games differ from popular online multiplayer games in that, for most computerized multiplayer games, the players have to be online at the same time - also known as synchronous play. With a play by mail game, the players can play whenever they choose, since responses need not be immediate; this is sometimes referred to as turn-based gaming and is common among browser-based games. Some video games can be played in turn-based mode: one makes one's "move", then play for that player stops, and the turn passes to another player who to makes his or her move in response.
Several non-commercial email games played on the Internet and BITNET predate these.
Some cards games like poker can also be played by Email using cryptography (see for example a patent application from FXTOP, another algorithm exists in Applied Cryptography from Bruce Schneier in case of 2 players°
Play-by-web.
An increasingly popular format for play-by-email games is play-by-web. As with play-by-email games the players are notified by email when it becomes their turn, but they must then return to the game's website to continue playing what is essentially a browser-based game. The main advantage of this is that the players can be presented with a graphical representation of the game and an interactive interface to guide them through their turn. Since the notifications only have to remind the players that it is their turn they can just as easily be sent via instant messaging.
Some sites have extended this gaming style by allowing the players to see each other's actions as they are made. This allows for real time playing while everyone is online and active, or slower progress if not.
Increasingly, this format is being adopted by social and mobile games, often described using the term "asynchronous multiplayer."

</doc>
<doc id="23658" url="http://en.wikipedia.org/wiki?curid=23658" title="Philip K. Dick Award">
Philip K. Dick Award

The Philip K. Dick Award is a science fiction award given annually at Norwescon sponsored by the Philadelphia Science Fiction Society and (since 2005) supported by the Philip K. Dick Trust, and named after science fiction and fantasy writer Philip K. Dick. It has been awarded since 1983, the year after Dick's death. Works that have received the award are identified on their covers as "Best Original SF Paperback". They are awarded to the best original paperback published each year in the US.
The award was founded by Thomas Disch with assistance from David G. Hartwell, Paul S. Williams, and Charles N. Brown. It is currently administered by David G. Hartwell & Gordon Van Gelder. Past administrators include Algis J. Budrys and David Alexander Smith.
Winners and nominees.
Winning authors are listed in bold.<br>
Authors of special citation entries are listed in "italics". The year in the table below indicates the year the book was published. The award was presented the following year.

</doc>
<doc id="23659" url="http://en.wikipedia.org/wiki?curid=23659" title="Plug-in (computing)">
Plug-in (computing)

In computing, a plug-in (or add-in / addin, plugin, extension or add-on / addon) is a software component that adds a specific feature to an existing software application. When an application supports plug-ins, it enables customization. The common examples are the plug-ins used in web browsers to add new features such as search-engines, virus scanners, or the ability to utilize a new file type such as a new video format. Well-known browser plug-ins include the Adobe Flash Player, the QuickTime Player, and the Java plug-in, which can launch a user-activated Java applet on a web page to its execution on a local Java virtual machine.
A theme or skin is a preset package containing additional or changed graphical appearance details, achieved by the use of a graphical user interface (GUI) that can be applied to specific software and websites to suit the purpose, topic, or tastes of different users to customize the look and feel of a piece of computer software or an operating system front-end GUI (and window managers).
Purpose and examples.
Applications support plug-ins for many reasons. Some of the main reasons include:
Types of applications and why they use plug-ins:
Mechanism.
As shown in the figure, the host application provides services which the plug-in can use, including a way for plug-ins to register themselves with the host application and a protocol for the exchange of data with plug-ins. Plug-ins depend on the services provided by the host application and do not usually work by themselves. Conversely, the host application operates independently of the plug-ins, making it possible for end-users to add and update plug-ins dynamically without needing to make changes to the host application.
Programmers typically implement plug-in functionality using shared libraries installed in a place prescribed by the host application. HyperCard supported a similar facility, but more commonly included the plug-in code in the HyperCard documents (called "stacks") themselves. Thus the HyperCard stack became a self-contained application in its own right, distributable as a single entity that end-users could run without the need for additional installation-steps. Programs may also implement plugins by loading a directory of simple script files written in a scripting language like Python or Lua.
Compared to extensions.
In the Mozilla Firefox web browser, extensions differ slightly from plug-ins. Firefox supports plug-ins using NPAPI. When the browser encounters references to content a plug-in specializes in, the data is handed off to be processed by that plug-in. An extension, or "addon" is a user interface enhancement that may add abilities such as a download manager, toolbar buttons, or enhancements specific to a particular web site.
The original impetus behind the development of Mozilla Firefox was the pursuit of a small baseline application, leaving exotic or personalized functionality to be implemented by extensions to avoid feature creep. This is in contrast to the "kitchen sink" approach in its predecessors, the Mozilla Application Suite and Netscape 6 and 7. 
The same distinction between plug-ins and extensions is in use by other web browsers, such as Microsoft Internet Explorer, where a typical extension might be a new toolbar, and a plug-in might embed a video player on the page. Since plug-ins and extensions both increase the utility of the original application, Mozilla uses the term "add-on" as an inclusive category of augmentation modules that consists of plug-ins, extensions and themes.
History.
Plug-ins appeared as early as the mid 1970s, when the EDT text editor running on the Unisys VS/9 operating system using the UNIVAC Series 90 mainframe computers provided the ability to run a program from the editor and to allow such a program to access the editor buffer, thus allowing an external program to access an edit session in memory. The plug-in program could make calls to the editor to have it perform text-editing services upon the buffer that the editor shared with the plug-in. The Waterloo Fortran compiler used this feature to allow interactive compilation of Fortran programs edited by EDT.
Very early PC software applications to incorporate plug-in functionality included HyperCard and QuarkXPress on the Macintosh, both released in 1987. In 1988, Silicon Beach Software included plug-in functionality in Digital Darkroom and SuperPaint, and Ed Bomke coined the term "plug-in".

</doc>
<doc id="23660" url="http://en.wikipedia.org/wiki?curid=23660" title="Pierre Teilhard de Chardin">
Pierre Teilhard de Chardin

Pierre Teilhard de Chardin SJ (]; May 1, 1881 – April 10, 1955) was a French philosopher and Jesuit priest who trained as a paleontologist and geologist and took part in the discovery of Peking Man. He conceived the idea of the Omega Point (a maximum level of complexity and consciousness towards which he believed the universe was evolving) and developed Vladimir Vernadsky's concept of noosphere.
Many of Teilhard's writings were censored by the Catholic Church during his lifetime because of his views on original sin. However, in July 2009, Vatican spokesman Fr. Federico Lombardi said: "By now, no one would dream of saying that [Teilhard] is a heterodox author who shouldn’t be studied." and he has been praised by Pope Benedict XVI.
Life.
Early years.
Pierre Teilhard de Chardin was born in the Château of Sarcenat at Orcines, close to Clermont-Ferrand, France, on May 1, 1881. On the Teilhard side he is descended from an ancient family of magistrates from Auvergne originating in Murat, Cantal, and on the de Chardin side he is descended from a family that was ennobled under Louis XVIII. He was the fourth of eleven children. His father, Emmanuel Teilhard (1844–1932), an amateur naturalist, collected stones, insects and plants and promoted the observation of nature in the household. Pierre Teilhard's spirituality was awakened by his mother, Berthe de Dompiere. When he was 12, he went to the Jesuit college of Mongré, in Villefranche-sur-Saône, where he completed baccalaureates of philosophy and mathematics. Then, in 1899, he entered the Jesuit novitiate at Aix-en-Provence, where he began a philosophical, theological and spiritual career.
As of the summer 1901, the Waldeck-Rousseau laws, which submitted congregational associations' properties to state control, prompted some of the Jesuits to exile themselves in the United Kingdom. Young Jesuit students continued their studies in Jersey. In the meantime, Teilhard earned a licentiate in literature in Caen in 1902.
Academic career.
From 1905 to 1908, he taught physics and chemistry in Cairo, Egypt, at the Jesuit College of the Holy Family. He wrote "... it is the dazzling of the East foreseen and drunk greedily ... in its lights, its vegetation, its fauna and its deserts." ("Letters from Egypt" (1905–1908) — "Éditions Aubier")
Teilhard studied theology in Hastings, in Sussex (United Kingdom), from 1908 to 1912. There he synthesized his scientific, philosophical and theological knowledge in the light of evolution. His reading of "L'Évolution Créatrice" (The Creative Evolution) by Henri Bergson was, he said, the "catalyst of a fire which devoured already its heart and its spirit." His views on evolution and religion particularly inspired the evolutionary biologist Theodosius Dobzhansky. Teilhard was ordained a priest on August 24, 1911, at age 30.
Paleontology.
From 1912 to 1914, Teilhard worked in the paleontology laboratory of the "Museum National d'Histoire Naturelle", in Paris, studying the mammals of the middle Tertiary period. Later he studied elsewhere in Europe. In June 1912 he formed part of the original digging team, with Arthur Smith Woodward and Charles Dawson, to perform follow-up investigations at the Piltdown site, after the discovery of the first fragments of the (fraudulent) "Piltdown Man", with some even suggesting he participated in the hoax. Professor Marcellin Boule (specialist in Neanderthal studies), who so early as 1915 astutely recognised the non-hominid origins of the Piltdown finds, gradually guided Teilhard towards human paleontology. At the museum's Institute of Human Paleontology, he became a friend of Henri Breuil and took part with him, in 1913, in excavations in the prehistoric painted caves in the northwest of Spain, at the Cave of Castillo.
Service in World War I.
Mobilised in December 1914, Teilhard served in World War I as a stretcher-bearer in the 8th Moroccan Rifles. For his valour, he received several citations including the Médaille militaire and the Legion of Honour.
Throughout these years of war he developed his reflections in his diaries and in letters to his cousin, Marguerite Teillard-Chambon, who later edited them into a book: "Genèse d'une pensée" ("Genesis of a thought"). He confessed later: "...the war was a meeting ... with the Absolute." In 1916, he wrote his first essay: "La Vie Cosmique" ("Cosmic life"), where his scientific and philosophical thought was revealed just as his mystical life. He pronounced his solemn vows as a Jesuit in Sainte-Foy-lès-Lyon, on May 26, 1918, during a leave. In August 1919, in Jersey, he would write "Puissance spirituelle de la Matière" ("the spiritual Power of Matter"). The complete essays written between 1916 and 1919 are published under the following titles:
Teilhard followed at the Sorbonne three unit degrees of natural science: geology, botany and zoology. His thesis treated of the mammals of the French lower Eocene and their stratigraphy. After 1920, he lectured in geology at the Catholic Institute of Paris, then became an assistant professor after being granted a science doctorate in 1922.
Research in China.
In 1923 he traveled to China with Father Emile Licent, who was in charge in Tianjin of a significant laboratory collaboration between the Natural History Museum in Paris and Marcellin Boule's laboratory. Licent carried out considerable basic work in connection with missionaries who accumulated observations of a scientific nature in their spare time. He was known as 德日進 (pinyin: Dérìjìn) in China.
Teilhard wrote several essays, including "La Messe sur le Monde" (the "Mass on the World"), in the Ordos Desert. In the following year he continued lecturing at the Catholic Institute and participated in a cycle of conferences for the students of the Engineers' Schools. Two theological essays on Original Sin sent to a theologian at his request on a purely personal basis were wrongly understood.
The Church required him to give up his lecturing at the Catholic Institute and to continue his geological research in China.
Teilhard traveled again to China in April 1926. He would remain there more or less twenty years, with many voyages throughout the world. He settled until 1932 in Tientsin with Emile Licent then in Beijing. From 1926 to 1935, Teilhard made five geological research expeditions in China. They enabled him to establish a general geological map of China.
In 1926 Teilhard’s superiors in the Jesuit Order forbade him to teach any longer. In 1926–1927 after a missed campaign in Gansu, he traveled in the Sang-Kan-Ho valley near Kalgan (Zhangjiakou) and made a tour in Eastern Mongolia. He wrote "Le Milieu Divin" ("the divine Medium"). Teilhard prepared the first pages of his main work "Le Phénomène Humain" ("The Human Phenomenon"). The Holy See refused the Imprimatur for "Le Milieu Divin" in 1927.
He joined the ongoing excavations of the Peking Man Site at Zhoukoudian as an advisor in 1926 and continued in the role for the Cenozoic Research Laboratory of the Geological Survey of China following its founding in 1928.
He resided in Manchuria with Emile Licent, then stayed in Western Shansi (Shanxi) and northern Shensi (Shaanxi) with the Chinese paleontologist C. C. Young and with Davidson Black, Chairman of the Geological Survey of China.
After a tour in Manchuria in the area of Great Khingan with Chinese geologists, Teilhard joined the team of American Expedition Center-Asia in the Gobi Desert organised in June and July, by the American Museum of Natural History with Roy Chapman Andrews.
Henri Breuil and Teilhard discovered that the Peking Man, the nearest relative of "Pithecanthropus" from Java, was a "faber" (worker of stones and controller of fire). Teilhard wrote "L'Esprit de la Terre" ("the Spirit of the Earth").
Teilhard took part as a scientist in the Croisiere Jaune (Yellow Cruise) financed by André Citroën in Central Asia. Northwest of Beijing in Kalgan, he joined the Chinese group who joined the second part of the team, the Pamir group, in Aksu. He remained with his colleagues for several months in Urumqi, capital of Sinkiang. The following year the Sino-Japanese War (1937–1945) began.
In 1933, Rome ordered him to give up his post in Paris.
Teilhard undertook several explorations in the south of China. He traveled in the valleys of Yangtze River and Sichuan in 1934, then, the following year, in Kwang-If and Guangdong. The relationship with Marcellin Boule was disrupted; the museum cut its financing on the grounds that Teilhard worked more for the Chinese Geological Service than for the museum.
During all these years, Teilhard strongly contributed to the constitution of an international network of research in human paleontology related to the whole Eastern and south Eastern zone of the Asian continent. He would be particularly associated in this task with two friends, the English/Canadian Davidson Black and the Scot George B. Barbour. Many times he would visit France or the United States only to leave these countries to go on further expeditions.
World travels.
From 1927 to 1928 Teilhard stayed in France, based in Paris. He journeyed to Leuven, Belgium, to Cantal, and to Ariège, France. Between several articles in reviews, he met new people such as Paul Valéry and Bruno de Solages, who were to help him in issues with the Catholic Church.
Answering an invitation from Henry de Monfreid, Teilhard undertook a journey of two months in Obock, in Harrar and in Somalia with his colleague Pierre Lamarre, a geologist, before embarking in Djibouti to return to Tianjin. While in China, Teilhard developed a deep and personal friendship with Lucile Swan.
From 1930–1931 Teilhard stayed in France and in the United States. During a conference in Paris, Teilhard stated: "For the observers of the Future, the greatest event will be the sudden appearance of a collective humane conscience and a human work to make."
From 1932–1933 he began to meet people to clarify issues with the Congregation for the Doctrine of the Faith, regarding "Le Milieu divin" and "L'Esprit de la Terre". He met Helmut de Terra, a German geologist in the International Geology Congress in Washington, DC.
Teilhard participated in the 1935 Yale–Cambridge expedition in northern and central India with the geologist Helmut de Terra and Patterson, who verified their assumptions on Indian Paleolithic civilisations in Kashmir and the Salt Range Valley.
He then made a short stay in Java, on the invitation of Professor Ralph van Koenigswald to the site of Java man. A second cranium, more complete, was discovered. This Dutch paleontologist had found (in 1933) a tooth in a Chinese apothecary shop in 1934 that he believed belonged to a giant tall ape that lived around half a million years ago.
In 1937 Teilhard wrote "Le Phénomène spirituel" ("The Phenomenon of the Spirit") on board the boat "the Empress of Japan", where he met the Raja of Sarawak. The ship conveyed him to the United States. He received the Mendel Medal granted by Villanova University during the Congress of Philadelphia in recognition of his works on human paleontology. He made a speech about evolution, origins and the destiny of Man. The "New York Times" dated March 19, 1937 presented Teilhard as the Jesuit who held that man descended from monkeys. Some days later, he was to be granted the "Doctor Honoris Causa" distinction from Boston College. Upon arrival in that city, he was told that the award had been cancelled.
1939: Rome banned his work "L’Énergie Humaine".
He then stayed in France, where he was immobilized by malaria. During his return voyage to Beijing he wrote "L'Energie spirituelle de la Souffrance" ("Spiritual Energy of Suffering") (Complete Works, tome VII).
1941: Teilhard submitted to Rome his most important work, "Le Phénomène Humain".
1947: Rome forbade him to write or teach on philosophical subjects.
1948: Teilhard was called to Rome by the Superior General of the Jesuits who hoped to acquire permission from the Holy See for the publication of his most important work "Le Phénomène Humain". But the prohibition to publish it issued in 1944, was again renewed. Teilhard was also forbidden to take a teaching post in the College de France.
1949: Permission to publish "Le Groupe Zoologique" was refused.
1950: Teilhard was named to the French Academy of Sciences.
1955: Teilhard was forbidden by his Superiors to attend the International Congress of
Paleontology.
1957: The Supreme Authority of the Holy Office, in a decree dated 15 November 1957, forbade the works of de Chardin to be retained in libraries, including those of religious institutes. His books were not to be sold in Catholic bookshops and were not to be translated into other languages.
1958: In April of this year, all Jesuit publications in Spain (“Razón y Fe”, “Sal Terrae”,“Estudios de Deusto”) etc., carried a notice from the Spanish Provincial of the Jesuits, that de Chardin’s works had been published in Spanish without previous ecclesiastical examination and in defiance of the decrees of the Holy See.
1962: A decree of the Holy Office dated 30 June, under the authority of Pope John XXIII warned that “... it is obvious that in philosophical and theological matters, the said works [Teilhard’s] are replete with ambiguities or rather with serious errors which offend Catholic doctrine. That is why ... the Rev. Fathers of the Holy Office urge all Ordinaries, Superiors, and Rectors ... to effectively protect, especially the minds of the young, against the dangers of the works of Fr. Teilhard de Chardin and his followers”. (AAS, 6 August 1962).
1963: The Vicariate of Rome (a diocese ruled in the name of Pope Paul VI (who had just become Pope in 1963) by his Cardinal Vicar) in a decree dated 30 September, required that Catholic booksellers in Rome should withdraw from circulation the works of Teilhard, together with those books which favour his erroneous doctrines. The text of this document was published in daily "L’Aurore" of Paris, dated 2 October 1963, and was reproduced in "Nouvelles de Chretiente", 10 October 1963, p. 35.
Death.
Pierre Teilhard de Chardin died in New York City, where he was in residence at the Jesuit Church of St. Ignatius Loyola, Park Avenue. On 15 March 1955, at the house of his diplomat cousin Jean de Lagarde, Teilhard told friends he hoped he would die on Easter Sunday. In the Easter Sunday evening of 10 April 1955, during an animated discussion at the apartment of Rhoda de Terra, his personal assistant since 1949, the 73-year-old priest suffered a heart attack; regaining consciousness for a moment, he died a few minutes later. He was buried in the cemetery for the New York Province of the Jesuits at the Jesuit novitiate, St. Andrew's-on-the-Hudson in Poughkeepsie, upstate New York.
Teachings.
Teilhard de Chardin has two comprehensive works. The first, "The Phenomenon of Man", sets forth a sweeping account of the unfolding of the cosmos and the evolution of matter to humanity to ultimately a reunion with Christ. Chardin abandoned literal interpretations of creation in the Book of Genesis in favor of allegorical and theological interpretations.
In his posthumously published book, "The Phenomenon of Man", Teilhard writes of the unfolding of the material cosmos, from primordial particles to the development of life, human beings and the noosphere, and finally to his vision of the Omega Point in the future, which is "pulling" all creation towards it. He was a leading proponent of orthogenesis, the idea that evolution occurs in a directional, goal-driven way, argued in terms that today go under the banner of convergent evolution. Teilhard argued in Darwinian terms with respect to biology, and supported the synthetic model of evolution, but argued in Lamarckian terms for the development of culture, primarily through the vehicle of education.
Teilhard makes sense of the universe by its evolutionary process. He interprets complexity as the axis of evolution of matter into a geosphere, a biosphere, into consciousness (in man), and then to supreme consciousness (the Omega Point.)
Teilhard's life work was predicated on the conviction that human spiritual development is moved by the same universal laws as material development. He wrote, "...everything is the sum of the past" and "...nothing is comprehensible except through its history. 'Nature' is the equivalent of 'becoming', self-creation: this is the view to which experience irresistibly leads us. ... There is nothing, not even the human soul, the highest spiritual manifestation we know of, that does not come within this universal law." There is no doubt that "The Phenomenon of Man" represents Teilhard's attempt at reconciling his religious faith with his academic interests as a paleontologist. One particularly poignant observation in Teilhard's book entails the notion that evolution is becoming an increasingly optional process. Teilhard points to the societal problems of isolation and marginalization as huge inhibitors of evolution, especially since evolution requires a unification of consciousness. He states that "no evolutionary future awaits anyone except in association with everyone else." Teilhard argued that the human condition necessarily leads to the psychic unity of humankind, though he stressed that this unity can only be voluntary; this voluntary psychic unity he termed "unanimization." Teilhard also states that "evolution is an ascent toward consciousness", giving encephalization as an example of early stages, and therefore, signifies a continuous upsurge toward the Omega Point, which for all intents and purposes, is God. Our century is probably more religious than any other. How could it fail to be, with such problems to be solved? The only trouble is that it has not yet found a God it can adore.
Relationship with the Catholic Church.
In 1925, Teilhard was ordered by the Jesuit Superior General Wlodimir Ledóchowski to leave his teaching position in France and to sign a statement withdrawing his controversial statements regarding the doctrine of original sin. Rather than leave the Jesuit order, Teilhard signed the statement and left for China.
This was the first of a series of condemnations by certain ecclesiastical officials that would continue until after Teilhard's death. The climax of these condemnations was a 1962 monitum (reprimand) of the Holy Office cautioning on Teilhard's works. From the monitum:
 "The above-mentioned works abound in such ambiguities and indeed even serious errors, as to offend Catholic doctrine... For this reason, the most eminent and most revered Fathers of the Holy Office exhort all Ordinaries as well as the superiors of Religious institutes, rectors of seminaries and presidents of universities, effectively to protect the minds, particularly of the youth, against the dangers presented by the works of Fr. Teilhard de Chardin and of his followers".
However, it is noteworthy that the Holy Office did not place any of Teilhard's writings on the Index Librorum Prohibitorum (Index of Forbidden Books), which existed during Teilhard's lifetime and at the time of the 1962 decree.
Shortly thereafter, prominent clerics began a strong theological defense of Teilhard's works. Henri de Lubac (later a Cardinal) wrote three comprehensive books on the theology of Teilhard de Chardin in the 1960s. While de Lubac mentioned that Teilhard was less than precise in some of his concepts he nevertheless affirmed the orthodoxy of Teilhard de Chardin with a stinging rebuke to Teilhard's critics "We need not concern ourselves with a number of detractors of Teilhard, in whom emotion has blunted intelligence". Later that decade a German theologian, Joseph Ratzinger (later Pope Benedict XVI) spoke glowingly of Teilhard's Christology in Ratzinger's famous "Introduction to Christianity":
“It must be regarded as an important service of Teilhard de Chardin’s that he rethought these ideas from the angle of the modern view of the world and, in spite of a not entirely unobjectionable tendency toward the biological approach, nevertheless on the whole grasped them correctly and in any case made them accessible once again. Let us listen to his own words: The human monad “can only be absolutely itself by ceasing to be alone”. In the background is the idea that in the cosmos, alongside the two orders or classes of the infinitely small and the infinitely big, there is a third order, which determines the real drift of evolution, namely, the order of the infinitely complex. It is the real goal of the ascending process of growth or becoming; it reaches a first peak in the genesis of living things and then continues to advance to those highly complex creations that give the cosmos a new center: “Imperceptible and accidental as the position they hold may be in the history of the heavenly bodies, in the last analysis the planets are nothing less than the vital points of the universe. It is through them that the axis now runs, on them is henceforth concentrated the main effort of an evolution aiming principally at the production of large molecules.” The examination of the world by the dynamic criterion of complexity thus signifies “a complete inversion of values. A reversal of the perspective...
This leads to a further passage in Teilhard de Chardin that is worth quoting in order to give at least some indication here, by means of a few fragmentary excerpts, of his general outlook. “The Universal Energy must be a Thinking Energy if it is not to be less highly evolved than the ends animated by its action. And consequently ... the attributes of cosmic value with which it is surrounded in our modern eyes do not affect in the slightest the necessity obliging us to recognize in it a transcendent form of Personality.”
Over the next several decades prominent theologians and Church leaders, including leading Cardinals, Pope John Paul II and Pope Benedict XVI all wrote approvingly of Teilhard's ideas. In 1981, Cardinal Agostino Casaroli, on behalf of Pope John Paul II, wrote on the front page of the Vatican newspaper, "l'Osservatore Romano":
"What our contemporaries will undoubtedly remember, beyond the difficulties of conception and deficiencies of expression in this audacious attempt to reach a synthesis, is the testimomy of the coherent life of a man possessed by Christ in the depths of his soul. He was concerned with honoring both faith and reason, and anticipated the response to John Paul II's appeal: 'Be not afraid, open, open wide to Christ the doors of the immense domains of culture, civilization, and progress.
Cardinal Avery Dulles, S.J. said in 2004:
"In his own poetic style, the French Jesuit Teilhard de Chardin liked to meditate on the Eucharist as the firstfruits of the new creation. In an essay called The Monstrance he describes how, kneeling in prayer, he had a sensation that the Host was beginning to grow until at last, through its mysterious expansion, 'the whole world had become incandescent, had itself become like a single giant Host.' Although it would probably be incorrect to imagine that the universe will eventually be transubstantiated, Teilhard correctly identified the connection between the Eucharist and the final glorification of the cosmos."""
Cardinal Christoph Schönborn wrote in 2007:
"Hardly anyone else has tried to bring together the knowledge of Christ and the idea of evolution as the scientist (paleontologist) and theologian Fr. Pierre Teilhard de Chardin, S.J., has done. ... His fascinating vision ... has represented a great hope, the hope that faith in Christ and a scientific approach to the world can be brought together. ... These brief references to Teilhard cannot do justice to his efforts. The fascination which Teilhard de Chardin exercised for an entire generation stemmed from his radical manner of looking at science and Christian faith together."""
Pope Benedict XVI, in his book "Spirit of the Liturgy" incorporates Teilhard's vision as a touchstone of the Catholic Mass:
“And so we can now say that the goal of worship and the goal of creation as a whole are one and the same—divinization, a world of freedom and love. But this means that the historical makes its appearance in the cosmic. The cosmos is not a kind of closed building, a stationary container in which history may by chance take place. It is itself movement, from its one beginning to its one end. In a sense, creation is history. Against the background of the modern evolutionary world view, Teilhard de Chardin depicted the cosmos as a process of ascent, a series of unions. From very simple beginnings the path leads to ever greater and more complex unities, in which multiplicity is not abolished but merged into a growing synthesis, leading to the “Noosphere”, in which spirit and its understanding embrace the whole and are blended into a kind of living organism. Invoking the epistles to the Ephesians and Colossians, Teilhard looks on Christ as the energy that strives toward the Noosphere and finally incorporates everything in its “fullness’. From here Teilhard went on to give a new meaning to Christian worship: the transubstantiated Host is the anticipation of the transformation and divinization of matter in the christological “fullness”. In his view, the Eucharist provides the movement of the cosmos with its direction; it anticipates its goal and at the same time urges it on.”""
Evaluations by scientists.
Teilhard was a profound paleontological and geological scientist. He made a total commitment to the evolutionary process in the 1920s as the core of his spirituality, at a time when other religious thinkers felt evolutionary thinking challenged the structure of conventional Christian faith. He committed himself to what the evidence showed. Sir Julian Huxley, evolutionary biologist and contributor to the modern synthesis, praised the thought of Teilhard de Chardin for looking at the way in which human development needs to be examined within a larger integrated universal sense of evolution.
In 1961, the Nobel Prize-winner Peter Medawar, a British immunologist, wrote a scornful review of "The Phenomenon Of Man" for the journal "Mind", calling it "a bag of tricks" and saying that the author had shown "an active willingness to be deceived": "the greater part of it, I shall show, is nonsense, tricked out with a variety of metaphysical conceits, and its author can be excused of dishonesty only on the grounds that before deceiving others he has taken great pains to deceive himself".
The evolutionary biologist Richard Dawkins called Medawar's review "devastating" and "The Phenomenon of Man" "the quintessence of bad poetic science". Similarly, Steven Rose wrote that "Teilhard is revered as a mystic of genius by some, but amongst most biologists is seen as little more than a charlatan."
"Nothing in Biology Makes Sense Except in the Light of Evolution" by Theodosius Dobzhansky draws upon Teilhard's insistence that evolutionary theory provides the core of how man understands his relationship to nature, calling him "one of the great thinkers of our age". Key researchers credit Teilhard with the development of the modern evolutionary synthesis that accounts for natural selection in the light of Mendelian genetics.
Evolutionary biologist Jeremy Griffith described Teilhard as a "visionary" philosopher and a contemporary "truth-sayer" or "prophet".
Legacy.
Brian Swimme wrote "Teilhard was one of the first scientists to realize that the human and the universe are inseparable. The only universe we know about is a universe that brought forth the human." 
Pierre Teilhard de Chardin is honored with a feast day on the liturgical calendar of the Episcopal Church (USA) on April 10. George Gaylord Simpson named the most primitive and ancient genus of true primate, the Eocene genus "Teilhardina".
Teilhard and his work continue to influence the arts and culture. Characters based on Teilhard appear in several novels, including Jean Telemond in Morris West's "The Shoes of the Fisherman" (mentioned by name and quoted by Oskar Werner playing Fr. Telemond in the movie version of the novel) and Father Lankester Merrin in William Peter Blatty's "The Exorcist". In Dan Simmons' 1989–97 "Hyperion Cantos", Teilhard de Chardin has been canonized a saint in the far future. His work inspires the anthropologist priest character, Paul Duré. When Duré becomes Pope, he takes "Teilhard I" as his regnal name. Teilhard appears as a minor character in the play "Fake" by Eric Simonson, staged by Chicago's Steppenwolf Theatre Company in 2009, involving a fictional solution to the infamous Piltdown Man hoax.
References range from occasional quotations—an auto mechanic quotes Teilhard in Philip K. Dick's "A Scanner Darkly" – to serving as the philosophical underpinning of the plot, as Teilhard's work does in Julian May's 1987–94 Galactic Milieu Series. Teilhard also plays a major role in Annie Dillard's 1999 "For the Time Being". Teilhard is mentioned by name and the Omega Point briefly explained in Arthur C. Clarke's and Stephen Baxter's "The Light of Other Days".
The title of the short-story collection "Everything That Rises Must Converge" by Flannery O'Connor is a reference to Teilhard's work. The American novelist Don DeLillo's 2010 novel "Point Omega" borrows its title and some of its ideas from Teilhard de Chardin. Robert Wright, in his book "", compares his own naturalistic thesis that biological and cultural evolution are directional and, possibly, purposeful, with Teilhard's ideas.
Teilhard's work also inspired philosophical ruminations by Italian laureate architect Paolo Soleri, artworks such as French painter Alfred Manessier's "L'Offrande de la terre ou Hommage à Teilhard de Chardin" and American sculptor Frederick Hart's acrylic sculpture "The Divine Milieu: Homage to Teilhard de Chardin". A sculpture of the Omega Point by Henry Setter, with a quote from Teilhard de Chardin, can be found at the entrance to the Roesch Library at the University of Dayton. Edmund Rubbra's 1968 Symphony No. 8 is titled "Hommage à Teilhard de Chardin".
Several college campuses honor Teilhard. A building at the University of Manchester is named after him, as are residence dormitories at Gonzaga University and Seattle University.
Bibliography.
The dates in parentheses are the dates of first publication in French and English. Most of these works were written years earlier, but Teilhard's ecclesiastical order forbade him to publish them because of their controversial nature. The essay collections are organized by subject rather than date, thus each one typically spans many years.
See also.
</dl>
Further reading.
</dl>

</doc>
<doc id="23661" url="http://en.wikipedia.org/wiki?curid=23661" title="Phutball">
Phutball

Phutball (short for Philosopher's Football) is a two-player strategy board game described in Elwyn Berlekamp, John Horton Conway, and Richard K. Guy's "Winning Ways for your Mathematical Plays".
Rules.
Phutball is played on the intersections of a 19×15 grid using one white stone and as many black stones as needed. 
In this article the two players are named Ohs (O) and Eks (X).
The board is labeled A through P (omitting I) from left to right and 1 to 19 from bottom to top from Ohs' perspective. 
Rows 0 and 20 represent "off the board" beyond rows 1 and 19 respectively.
As specialized phutball boards are hard to come by, the game is usually played on a 19×19 Go board, with a white stone representing the football and black stones representing the men.
The objective is to score goals by using the men (the black stones) to move the football (the white stone) onto or over the opponent's goal line. Ohs tries to move the football to rows 19 or 20 and Eks to rows 1 or 0. 
At the start of the game the football is placed on the central point, unless one player gives the other a handicap, in which case the ball starts nearer one player's goal.
Players alternate making moves. 
A move is either to add a man to any vacant point on the board or to move the ball. 
There is no difference between men played by Ohs and those played by Eks.
The football is moved by a series of jumps over adjacent men. 
Each jump is to the first vacant point in a straight line horizontally, vertically, or diagonally over one or more men. 
The jumped men are then removed from the board (before any subsequent jump occurs). 
This process repeats for as long as there remain men available to be jumped and the player desires. Jumping is optional: there is no requirement to jump. 
In contrast to checkers, multiple men in a row are jumped and removed as a group.
The diagram on the right illustrates a jump. 
If the football ends the move on or over the opponent's goal line then a goal has been scored. 
If the football passes through a goal line, but ends up elsewhere due to further jumps, the game continues.
Strategy.
The game is sufficiently complex that checking whether there is a win in one (on an m×n board) is NP-complete. It is not known whether any player has a winning strategy or both players have a drawing strategy.

</doc>
<doc id="23664" url="http://en.wikipedia.org/wiki?curid=23664" title="Papyrus">
Papyrus

The word papyrus refers to a thin paper-like material made from the pith of the papyrus plant, "Cyperus papyrus". "Papyrus" can also refer to a document written on sheets of papyrus joined together side by side and rolled up into a scroll, an early form of a book. The plural for such documents is papyri. 
Papyrus is first known to have been used in ancient Egypt (at least as far back as the First Dynasty), as the "Cyperus papyrus" plant was a wetland sedge that was once abundant in the Sudd of Southern Sudan along with the Nile Delta of Egypt. 
Papyrus was also used throughout the Mediterranean region and in Kingdom of Kush. Ancient Egyptians are thought to have used papyrus as a writing material, as well as employing it commonly in the construction of other artifacts such as reed boats, mats, rope, sandals, and baskets.
History.
Papyrus was first manufactured in Egypt and Southern Sudan as far back as the fourth millennium BCE. The earliest archaeological evidence of papyrus was excavated in 2012 and 2013 at Wadi al-Jarf, an ancient Egyptian harbor located on the Red Sea coast. These documents date from c. 2560–2550 BCE (end of the reign of Khufu). In the first centuries BCE and CE, papyrus scrolls gained a rival as a writing surface in the form of parchment, which was prepared from animal skins. Sheets of parchment were folded to form quires from which book-form codices were fashioned. Early Christian writers soon adopted the codex form, and in the Græco-Roman world, it became common to cut sheets from papyrus rolls to form codices.
Codices were an improvement on the papyrus scroll, as the papyrus was not pliable enough to fold without cracking and a long roll, or scroll, was required to create large-volume texts. Papyrus had the advantage of being relatively cheap and easy to produce, but it was fragile and susceptible to both moisture and excessive dryness. Unless the papyrus was of perfect quality, the writing surface was irregular, and the range of media that could be used was also limited.
Papyrus was replaced in Europe by the cheaper, locally produced products parchment and vellum, of significantly higher durability in moist climates, though Henri Pirenne's connection of its disappearance with the Muslim overrunning of Egypt is contended. Its last appearance in the Merovingian chancery is with a document of 692, though it was known in Gaul until the middle of the following century. The latest certain dates for the use of papyrus are 1057 for a papal decree (typically conservative, all papal bulls were on papyrus until 1022), under Pope Victor II, and 1087 for an Arabic document. Its use in Egypt continued until it was replaced by more inexpensive paper introduced by Arabs. By the 12th century, parchment and paper were in use in the Byzantine Empire, but papyrus was still an option.
Papyrus was made in several qualities and prices; these are listed, with minor differences, both by Pliny and Isidore of Seville.
Until the middle of the 19th century, only some isolated documents written on papyrus were known. They did not contain literary works. The first discovery of papyri rolls in modern days was made at Herculaneum in 1752. Before that date, the only papyri known were a few surviving from medieval times.
Etymology.
The English word "papyrus" derives, via Latin, from Greek πάπυρος ("papuros"), a loanword of unknown (perhaps Pre-Greek) origin. Greek has a second word for it, βύβλος ("bublos", said to derive from the name of the Phoenician city of Byblos). The Greek writer Theophrastus, who flourished during the 4th century BC, uses "papuros" when referring to the plant used as a foodstuff and "bublos" for the same plant when used for nonfood products, such as cordage, basketry, or writing surfaces. The more specific term βίβλος "biblos", which finds its way into English in such words as 'bibliography', 'bibliophile', and 'bible', refers to the inner bark of the papyrus plant. "Papyrus" is also the etymon of 'paper', a similar substance.
In the Egyptian language, papyrus was known by the definition "wadj" ("w3ḏ"), "tjufy" ("ṯwfy"), and "djet" ("ḏt").
Documents written on papyrus.
The word for the material papyrus is also used to designate documents written on sheets of it, often rolled up into scrolls. The plural for such documents is papyri. Historical papyri are given identifying names—generally the name of the discoverer, first owner or institution where they are kept—and numbered, such as "Papyrus Harris I". Often an abbreviated form is used, such as "pHarris I". These documents provide important information on ancient writings; they give us the only extant copy of Menander, the Egyptian Book of the Dead, Egyptian treatises on medicine (the Ebers Papyrus) and on surgery (the Edwin Smith papyrus), Egyptian mathematical treatises (the Rhind papyrus), and Egyptian folk tales (the Westcar papyrus). When, in the 18th century, a library of ancient papyri was found in Herculaneum, ripples of expectation spread among the learned men of the time. However, since these papyri were badly charred, their unscrolling and deciphering is still going on today.
Manufacture and use.
Papyrus is made from the stem of the papyrus plant, "Cyperus papyrus". The outer rind is first removed, and the sticky fibrous inner pith is cut lengthwise into thin strips of about 40 cm long. The strips are then placed side by side on a hard surface with their edges slightly overlapping, and then another layer of strips is laid on top at a right angle. The strips may have been soaked in water long enough for decomposition to begin, perhaps increasing adhesion, but this is not certain. The two layers possibly were glued together. While still moist, the two layers are hammered together, mashing the layers into a single sheet. The sheet is then dried under pressure. After drying, the sheet is polished with some rounded object, possibly a stone or seashell or round hardwood.
Sheets could be cut to fit the obligatory size or glued together to create a longer roll. A wooden stick would be attached to the last sheet in a roll, making it easier to handle. To form the long strip scrolls required, a number of such sheets were united, placed so all the horizontal fibres parallel with the roll's length were on one side and all the vertical fibres on the other. Normally, texts were first written on the "recto", the lines following the fibres, parallel to the long edges of the scroll. Secondarily, papyrus was often reused, writing across the fibres on the "verso". Pliny the Elder describes the methods of preparing papyrus in his "Naturalis Historia".
In a dry climate, like that of Egypt, papyrus is stable, formed as it is of highly rot-resistant cellulose; but storage in humid conditions can result in molds attacking and destroying the material. Library papyrus rolls were stored in wooden boxes and chests made in the form of statues. Papyrus scrolls were organized according to subject or author, and identified with clay labels that specified their contents without having to unroll the scroll. In European conditions, papyrus seems to have lasted only a matter of decades; a 200-year-old papyrus was considered extraordinary. Imported papyrus once commonplace in Greece and Italy has since deteriorated beyond repair, but papyrus is still being found in Egypt; extraordinary examples include the Elephantine papyri and the famous finds at Oxyrhynchus and Nag Hammadi. The Villa of the Papyri at Herculaneum, containing the library of Lucius Calpurnius Piso Caesoninus, Julius Caesar's father-in-law, was preserved by the eruption of Mount Vesuvius, but has only been partially excavated.
Sporadic attempts to revive the manufacture of papyrus have been made since the mid-18th century. Scottish explorer James Bruce experimented in the late 18th century with papyrus plants from the Sudan, for papyrus had become extinct in Egypt. Also in the 18th century, Sicilian Saverio Landolina manufactured papyrus at Syracuse, where papyrus plants had continued to grow in the wild. During the 1920s, when Egyptologist Battiscombe Gunn lived in Maadi, outside Cairo, he experimented with the manufacture of papyrus, growing the plant in his garden. He beat the sliced papyrus stalks between two layers of linen, and produced successful examples of papyrus, one of which was exhibited in the Egyptian Museum in Cairo. The modern technique of papyrus production used in Egypt for the tourist trade was developed in 1962 by the Egyptian engineer Hassan Ragab using plants that had been reintroduced into Egypt in 1872 from France. Both Sicily and Egypt have centres of limited papyrus production.
Papyrus is still used by communities living in the vicinity of swamps, to the extent that rural householders derive up to 75% of their income from swamp goods. Particularly in East and Central Africa, people harvest papyrus, which is used to manufacture items that are sold or used locally. Examples include baskets, hats, fish traps, trays or winnowing mats and floor mats. Papyrus is also used to make roofs, ceilings, rope and fences. Although alternatives, such as eucalyptus, are increasingly available, papyrus is still used as fuel.
See also.
Other ancient writing materials:
References.
</dl>

</doc>
<doc id="23665" url="http://en.wikipedia.org/wiki?curid=23665" title="Pixel">
Pixel

In digital imaging, a pixel, pel, or picture element is a physical point in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen. The address of a pixel corresponds to its physical coordinates. LCD pixels are manufactured in a two-dimensional grid, and are often represented using dots or squares, but CRT pixels correspond to their timing mechanisms and sweep rates.
Each pixel is a sample of an original image; more samples typically provide more accurate representations of the original. The intensity of each pixel is variable. In color image systems, a color is typically represented by three or four component intensities such as red, green, and blue, or cyan, magenta, yellow, and black.
In some contexts (such as descriptions of camera sensors), the term "pixel" is used to refer to a single scalar element of a multi-component representation (more precisely called a "photosite" in the camera sensor context, although the neologism "sensel" is sometimes used to describe the elements of a digital camera's sensor), while in others the term may refer to the entire set of such component intensities for a spatial position. In color systems that use chroma subsampling, the multi-component concept of a pixel can become difficult to apply, since the intensity measures for the different color components correspond to different spatial areas in such a representation.
The word "pixel" is based on a contraction of "pix" ("pictures") and "el" (for "element"); similar formations with "el"  for "element" include the words voxel
and texel.
Etymology.
The word "pixel" was first published in 1965 by Frederic C. Billingsley of JPL, to describe the picture elements of video images from space probes to the Moon and Mars. However, Billingsley did not coin the term himself. Instead, he got the word "pixel" from Keith E. McFarland, at the Link Division of General Precision in Palo Alto, who did not know where the word originated. McFarland said simply it was "in use at the time" (circa 1963).
The word is a combination of "pix", for picture, and "element". The word "pix" appeared in "Variety" magazine headlines in 1932, as an abbreviation for the word "pictures", in reference to movies. By 1938, "pix" was being used in reference to still pictures by photojournalists.
The concept of a "picture element" dates to the earliest days of television, for example as "Bildpunkt" (the German word for "pixel", literally 'picture point') in the 1888 German patent of Paul Nipkow. According to various etymologies, the earliest publication of the term "picture element" itself was in "Wireless World" magazine in 1927, though it had been used earlier in various U.S. patents filed as early as 1911.
Some authors explain "pixel" as "picture cell," as early as 1972. In graphics and in image and video processing, "pel" is often used instead of "pixel". For example, IBM used it in their Technical Reference for the original PC.
Pixilation, spelled with a second "i", is an unrelated filmmaking technique that dates to the beginnings of cinema, in which live actors are posed frame by frame and photographed to create stop-motion animation. An archaic British word meaning "possession by spirits (pixies)," the term has been used to describe the animation process since the early 1950s; various animators, including Norman McLaren and Grant Munro, are credited with popularizing it.
Technical.
A pixel is generally thought of as the smallest single component of a digital image. However, the definition is highly context-sensitive. For example, there can be "printed pixels" in a page, or pixels carried by electronic signals, or represented by digital values, or pixels on a display device, or pixels in a digital camera (photosensor elements). This list is not exhaustive and, depending on context, synonyms include pel, sample, byte, bit, dot, and spot. "Pixels" can be used as a unit of measure such as: 2400 pixels per inch, 640 pixels per line, or spaced 10 pixels apart.
The measures dots per inch (dpi) and pixels per inch (ppi) are sometimes used interchangeably, but have distinct meanings, especially for printer devices, where dpi is a measure of the printer's density of dot (e.g. ink droplet) placement. For example, a high-quality photographic image may be printed with 600 ppi on a 1200 dpi inkjet printer. Even higher dpi numbers, such as the 4800 dpi quoted by printer manufacturers since 2002, do not mean much in terms of achievable resolution.
The more pixels used to represent an image, the closer the result can resemble the original. The number of pixels in an image is sometimes called the resolution, though resolution has a more specific definition. Pixel counts can be expressed as a single number, as in a "three-megapixel" digital camera, which has a nominal three million pixels, or as a pair of numbers, as in a "640 by 480 display", which has 640 pixels from side to side and 480 from top to bottom (as in a VGA display), and therefore has a total number of 640×480 = 307,200 pixels or 0.3 megapixels.
The pixels, or color samples, that form a digitized image (such as a JPEG file used on a web page) may or may not be in one-to-one correspondence with screen pixels, depending on how a computer displays an image. In computing, an image composed of pixels is known as a "bitmapped image" or a "raster image". The word "raster" originates from television scanning patterns, and has been widely used to describe similar halftone printing and storage techniques.
Sampling patterns.
For convenience, pixels are normally arranged in a regular two-dimensional grid. By using this arrangement, many common operations can be implemented by uniformly applying the same operation to each pixel independently. Other arrangements of pixels are possible, with some sampling patterns even changing the shape (or kernel) of each pixel across the image. For this reason, care must be taken when acquiring an image on one device and displaying it on another, or when converting image data from one pixel format to another.
For example:
Resolution of computer monitors.
Computers can use pixels to display an image, often an abstract image that represents a GUI. The resolution of this image is called the display resolution and is determined by the video card of the computer. LCD monitors also use pixels to display an image, and have a native resolution. Each pixel is made up of triads, with the number of these triads determining the native resolution. On some CRT monitors, the beam sweep rate may be fixed, resulting in a fixed native resolution. Most CRT monitors do not have a fixed beam sweep rate, meaning they do not have a native resolution at all - instead they have a set of resolutions that are equally well supported.
To produce the sharpest images possible on an LCD, the user must ensure the display resolution of the computer matches the native resolution of the monitor.
Resolution of telescopes.
The pixel scale used in astronomy is the angular distance between two objects on the sky that fall one pixel apart on the detector (CCD or infrared chip). The scale "s" measured in radians is the ratio of the pixel spacing "p" and focal length "f" of the preceding optics, "s"="p/f". (The focal length is the product of the focal ratio by the diameter of the associated lens or mirror.)
Because "p" is usually expressed in units of arcseconds per pixel, because 1 radian equals "180/π*3600≈206,265" arcseconds, and because diameters are often given in millimeters and pixel sizes in micrometers which yields another factor of 1,000, the formula is often quoted as "s=206p/f".
Bits per pixel.
The number of distinct colors that can be represented by a pixel depends on the number of bits per pixel (bpp). A 1 bpp image uses 1-bit for each pixel, so each pixel can be either on or off. Each additional bit doubles the number of colors available, so a 2 bpp image can have 4 colors, and a 3 bpp image can have 8 colors:
For color depths of 15 or more bits per pixel, the depth is normally the sum of the bits allocated to each of the red, green, and blue components. Highcolor, usually meaning 16 bpp, normally has five bits for red and blue, and six bits for green, as the human eye is more sensitive to errors in green than in the other two primary colors. For applications involving transparency, the 16 bits may be divided into five bits each of red, green, and blue, with one bit left for transparency. A 24-bit depth allows 8 bits per component. On some systems, 32-bit depth is available: this means that each 24-bit pixel has an extra 8 bits to describe its opacity (for purposes of combining with another image).
Subpixels.
Many display and image-acquisition systems are, for various reasons, not capable of displaying or sensing the different color channels at the same site. Therefore, the pixel grid is divided into single-color regions that contribute to the displayed or sensed color when viewed at a distance. In some displays, such as LCD, LED, and plasma displays, these single-color regions are separately addressable elements, which have come to be known as "subpixels". For example, LCDs typically divide each pixel horizontally into three subpixels. When the square pixel is divided into three subpixels, each subpixel is necessarily rectangular. In the display industry terminology, subpixels are often referred to as "pixels", as they are the basic addressable elements in a viewpoint of hardware, and they call "pixel circuits" rather than "subpixel circuits".
Most digital camera image sensors use single-color sensor regions, for example using the Bayer filter pattern, and in the camera industry these are known as "pixels" just like in the display industry, not "subpixels".
For systems with subpixels, two different approaches can be taken:
This latter approach, referred to as subpixel rendering, uses knowledge of pixel geometry to manipulate the three colored subpixels separately, producing an increase in the apparent resolution of color displays. While CRT displays use red-green-blue-masked phosphor areas, dictated by a mesh grid called the shadow mask, it would require a difficult calibration step to be aligned with the displayed pixel raster, and so CRTs do not currently use subpixel rendering.
The concept of subpixels is related to samples.
Megapixel.
A megapixel (MP) is a million pixels; the term is used not only for the number of pixels in an image, but also to express the number of image sensor elements of digital cameras or the number of display elements of digital displays. For example, a camera that makes a 2048×1536 pixel image (3,145,728 finished image pixels) typically uses a few extra rows and columns of sensor elements and is commonly said to have "3.2 megapixels" or "3.4 megapixels", depending on whether the number reported is the "effective" or the "total" pixel count.
Digital cameras use photosensitive electronics, either charge-coupled device (CCD) or complementary metal–oxide–semiconductor (CMOS) image sensors, consisting of a large number of single sensor elements, each of which records a measured intensity level. In most digital cameras, the sensor array is covered with a patterned color filter mosaic having red, green, and blue regions in the Bayer filter arrangement, so that each sensor element can record the intensity of a single primary color of light. The camera interpolates the color information of neighboring sensor elements, through a process called demosaicing, to create the final image. These sensor elements are often called "pixels", even though they only record 1 channel (only red, or green, or blue) of the final color image. Thus, two of the three color channels for each sensor must be interpolated and a so-called "N-megapixel" camera that produces an N-megapixel image provides only one-third of the information that an image of the same size could get from a scanner. Thus, certain color contrasts may look fuzzier than others, depending on the allocation of the primary colors (green has twice as many elements as red or blue in the Bayer arrangement).
DxO Labs invented the Perceptual MegaPixel (P-MPix) to measure the sharpness that a camera produces when paired to a particular lens – as opposed to the MP a manufacturer states for a camera product which is based only on the camera's sensor. The new P-MPix claims to be a more accurate and relevant value for photographers to consider when weighing-up camera sharpness. As of mid-2013, the Sigma 35mm F1.4 DG HSM mounted on a Nikon D800 has the highest measured P-MPix. However, with a value of 23 MP, it still wipes-off more than one-third of the D800's 36.3 MP sensor.
A camera with a full-frame image sensor, and a camera with an APS-C image sensor, may have the same pixel count (for example, 16 MP), but the full-frame camera may allow better dynamic range, less noise, and improved low-light shooting performance than an APS-C camera. This is because the full-frame camera has a larger image sensor than the APS-C camera, therefore more information can be captured per pixel. A full-frame camera that shoots photographs at 36 megapixels has roughly the same pixel size as an APS-C camera that shoots at 16 megapixels.
One new method to add Megapixels have been introduced in a Micro Four Thirds System camera which only use 16MP sensor, but can produce 64MP RAW (40MP JPEG) by expose-shift-expose-shift the sensor a half pixel each times to both directions. It should still use tripod and take one second for multiple shots and then combine it into one bigger MP image, no interpolation.

</doc>
<doc id="23666" url="http://en.wikipedia.org/wiki?curid=23666" title="Prime number">
Prime number

A prime number (or a prime) is a natural number greater than 1 that has no positive divisors other than 1 and itself. A natural number greater than 1 that is not a prime number is called a composite number. For example, 5 is prime because 1 and 5 are its only positive integer factors, whereas 6 is composite because it has the divisors 2 and 3 in addition to 1 and 6. The fundamental theorem of arithmetic establishes the central role of primes in number theory: any integer greater than 1 can be expressed as a product of primes that is unique up to ordering. The uniqueness in this theorem requires excluding 1 as a prime because one can include arbitrarily many instances of 1 in any factorization, e.g., 3, 1 × 3, 1 × 1 × 3, etc. are all valid factorizations of 3.
The property of being prime (or not) is called primality. A simple but slow method of verifying the primality of a given number "n" is known as trial division. It consists of testing whether "n" is a multiple of any integer between 2 and formula_1. Algorithms much more efficient than trial division have been devised to test the primality of large numbers. Particularly fast methods are available for numbers of special forms, such as Mersenne numbers. s of 2014[ [update]], the largest known prime number has 17,425,170 decimal digits.
There are infinitely many primes, as demonstrated by Euclid around 300 BC. There is no known useful formula that sets apart all of the prime numbers from composites. However, the distribution of primes, that is to say, the statistical behaviour of primes in the large, can be modelled. The first result in that direction is the prime number theorem, proven at the end of the 19th century, which says that the probability that a given, randomly chosen number "n" is prime is inversely proportional to its number of digits, or to the logarithm of "n".
Many questions regarding prime numbers remain open, such as Goldbach's conjecture (that every even integer greater than 2 can be expressed as the sum of two primes), and the twin prime conjecture (that there are infinitely many pairs of primes whose difference is 2). Such questions spurred the development of various branches of number theory, focusing on analytic or algebraic aspects of numbers. Primes are used in several routines in information technology, such as public-key cryptography, which makes use of properties such as the difficulty of factoring large numbers into their prime factors. Prime numbers give rise to various generalizations in other mathematical domains, mainly algebra, such as prime elements and prime ideals.
Definition and examples.
A natural number (i.e. 1, 2, 3, 4, 5, 6, etc.) is called a prime number (or a prime) if it has exactly two positive divisors, 1 and the number itself. Natural numbers greater than 1 that are not prime are called "composite".
Among the numbers 1 to 6, the numbers 2, 3, and 5 are the prime numbers, while 1, 4, and 6 are not prime. 1 is excluded as a prime number, for reasons explained below. 2 is a prime number, since the only natural numbers dividing it are 1 and 2. Next, 3 is prime, too: 1 and 3 do divide 3 without remainder, but 3 divided by 2 gives remainder 1. Thus, 3 is prime. However, 4 is composite, since 2 is another number (in addition to 1 and 4) dividing 4 without remainder:
5 is again prime: none of the numbers 2, 3, or 4 divide 5. Next, 6 is divisible by 2 or 3, since
Hence, 6 is not prime. The image at the right illustrates that 12 is not prime: . No even number greater than 2 is prime because by definition, any such number "n" has at least three distinct divisors, namely 1, 2, and "n". This implies that "n" is not prime. Accordingly, the term "odd prime" refers to any prime number greater than 2. Similarly, when written in the usual decimal system, all prime numbers larger than 5 end in 1, 3, 7, or 9, since even numbers are multiples of 2 and numbers ending in 0 or 5 are multiples of 5.
If "n" is a natural number, then 1 and "n" divide "n" without remainder. Therefore, the condition of being a prime can also be restated as: a number is prime if it is greater than one and if none of
divides "n" (without remainder). Yet another way to say the same is: a number "n" > 1 is prime if it cannot be written as a product of two integers "a" and "b", both of which are larger than 1:
In other words, "n" is prime if "n" items cannot be divided up into smaller equal-size groups of more than one item.
The set of all primes is often denoted by P.
The first 168 prime numbers (all the prime numbers less than 1000) are:
Fundamental theorem of arithmetic.
The crucial importance of prime numbers to number theory and mathematics in general stems from the "fundamental theorem of arithmetic", which states that every integer larger than 1 can be written as a product of one or more primes in a way that is unique except for the order of the prime factors. Primes can thus be considered the “basic building blocks” of the natural numbers. For example:
As in this example, the same prime factor may occur multiple times. A decomposition:
of a number "n" into (finitely many) prime factors "p"1, "p"2, ... to "p""t" is called "prime factorization" of "n". The fundamental theorem of arithmetic can be rephrased so as to say that any factorization into primes will be identical except for the order of the factors. So, albeit there are many prime factorization algorithms to do this in practice for larger numbers, they all have to yield the same result.
If "p" is a prime number and "p" divides a product "ab" of integers, then "p" divides "a" or "p" divides "b". This proposition is known as Euclid's lemma. It is used in some proofs of the uniqueness of prime factorizations.
Primality of one.
Most early Greeks did not even consider 1 to be a number, and so they did not consider it a prime. In the 19th century, however, many mathematicians did consider the number 1 a prime. For example, Derrick Norman Lehmer's list of primes up to 10,006,721, reprinted as late as 1956, started with 1 as its first prime. Henri Lebesgue is said to be the last professional mathematician to call 1 prime.
Although a large body of mathematical work would still be valid when calling 1 a prime, the fundamental theorem of arithmetic (mentioned above) would not hold as stated. For example, the number 15 can be factored as 3 · 5 and 1 · 3 · 5; if 1 were admitted as a prime, these two presentations would be considered different factorizations of 15 into prime numbers, so the statement of that theorem would have to be modified. Similarly, the sieve of Eratosthenes would not work correctly if 1 were considered a prime: a modified version of the sieve that considers 1 as prime would eliminate all multiples of 1 (that is, all numbers) and produce as output only the single number 1. Furthermore, the prime numbers have several properties that the number 1 lacks, such as the relationship of the number to its corresponding value of Euler's totient function or the sum of divisors function.
History.
There are hints in the surviving records of the ancient Egyptians that they had some knowledge of prime numbers: the Egyptian fraction expansions in the Rhind papyrus, for instance, have quite different forms for primes and for composites. However, the earliest surviving records of the explicit study of prime numbers come from the Ancient Greeks. Euclid's Elements (circa 300 BC) contain important theorems about primes, including the infinitude of primes and the fundamental theorem of arithmetic. Euclid also showed how to construct a perfect number from a Mersenne prime. The Sieve of Eratosthenes, attributed to Eratosthenes, is a simple method to compute primes, although the large primes found today with computers are not generated this way.
After the Greeks, little happened with the study of prime numbers until the 17th century. In 1640 Pierre de Fermat stated (without proof) Fermat's little theorem (later proved by Leibniz and Euler). Fermat also conjectured that all numbers of the form 22"n" + 1 are prime (they are called Fermat numbers) and he verified this up to "n" = 4 (or 216 + 1). However, the very next Fermat number 232 + 1 is composite (one of its prime factors is 641), as Euler discovered later, and in fact no further Fermat numbers are known to be prime. The French monk Marin Mersenne looked at primes of the form 2"p" − 1, with "p" a prime. They are called Mersenne primes in his honor.
Euler's work in number theory included many results about primes. He showed the infinite series 1/2 + 1/3 + 1/5 + 1/7 + 1/11 + … is divergent.
In 1747 he showed that the even perfect numbers are precisely the integers of the form 2"p"−1(2"p" − 1), where the second factor is a Mersenne prime.
At the start of the 19th century, Legendre and Gauss independently conjectured that as "x" tends to infinity, the number of primes up to "x" is asymptotic to "x"/ln("x"), where ln("x") is the natural logarithm of "x". Ideas of Riemann in his 1859 paper on the zeta-function sketched a program that would lead to a proof of the prime number theorem. This outline was completed by Hadamard and de la Vallée Poussin, who independently proved the prime number theorem in 1896.
Proving a number is prime is not done (for large numbers) by trial division. Many mathematicians have worked on primality tests for large numbers, often restricted to specific number forms. This includes Pépin's test for Fermat numbers (1877), Proth's theorem (around 1878), the Lucas–Lehmer primality test (originated 1856), and the generalized Lucas primality test. More recent algorithms like APRT-CL, ECPP, and AKS work on arbitrary numbers but remain much slower.
For a long time, prime numbers were thought to have extremely limited application outside of pure mathematics. This changed in the 1970s when the concepts of public-key cryptography were invented, in which prime numbers formed the basis of the first algorithms such as the RSA cryptosystem algorithm.
Since 1951 all the largest known primes have been found by computers. The search for ever larger primes has generated interest outside mathematical circles. The Great Internet Mersenne Prime Search and other distributed computing projects to find large primes have become popular, while mathematicians continue to struggle with the theory of primes.
Number of prime numbers.
There are infinitely many prime numbers. Another way of saying this is that the sequence
of prime numbers never ends. This statement is referred to as "Euclid's theorem" in honor of the ancient Greek mathematician Euclid, since the first known proof for this statement is attributed to him. Many more proofs of the infinitude of primes are known, including an analytical proof by Euler, Goldbach's proof based on Fermat numbers, Furstenberg's proof using general topology, and Kummer's elegant proof.
Euclid's proof.
Euclid's proof (Book IX, Proposition 20) considers any finite set "S" of primes. The key idea is to consider the product of all these numbers plus one:
Like any other natural number, "N" is divisible by at least one prime number (it is possible that "N" itself is prime).
None of the primes by which "N" is divisible can be members of the finite set "S" of primes with which we started, because dividing "N" by any one of these leaves a remainder of 1. Therefore the primes by which "N" is divisible are additional primes beyond the ones we started with. Thus any finite set of primes can be extended to a larger finite set of primes.
It is often erroneously reported that Euclid begins with the assumption that the set initially considered contains all prime numbers, leading to a contradiction, or that it contains precisely the "n" smallest primes rather than any arbitrary finite set of primes. Today, the product of the smallest "n" primes plus 1 is conventionally called the "n"th Euclid number.
Euler's analytical proof.
Euler's proof uses the sum of the reciprocals of primes,
This sum becomes larger than any arbitrary real number provided that "p" is big enough. This shows that there are infinitely many primes, since otherwise this sum would grow only until the biggest prime "p" is reached. The growth of "S"("p") is quantified by Mertens' second theorem. For comparison, the sum
does not grow to infinity as "n" goes to infinity (see Basel problem). In this sense, prime numbers occur more often than squares of natural numbers. Brun's theorem states that the sum of the reciprocals of twin primes,
is finite.
Testing primality and integer factorization.
There are various methods to determine whether a given number "n" is prime. The most basic routine, trial division, is of little practical use because of its slowness. One group of modern primality tests is applicable to arbitrary numbers, while more efficient tests are available for particular numbers. Most such methods only tell whether "n" is prime or not. Routines also yielding one (or all) prime factors of "n" are called factorization algorithms.
Trial division.
The most basic method of checking the primality of a given integer "n" is called "trial division". This routine consists of dividing "n" by each integer "m" that is greater than 1 and less than or equal to the square root of "n". If the result of any of these divisions is an integer, then "n" is not a prime, otherwise it is a prime. Indeed, if formula_6 is composite (with "a" and "b" ≠ 1) then one of the factors "a" or "b" is necessarily at most formula_1. For example, for formula_8, the trial divisions are by None of these numbers divides 37, so 37 is prime. This routine can be implemented more efficiently if a complete list of primes up to formula_1 is known—then trial divisions need to be checked only for those "m" that are prime. For example, to check the primality of 37, only three divisions are necessary ("m" = 2, 3, and 5), given that 4 and 6 are composite.
While a simple method, trial division quickly becomes impractical for testing large integers because the number of possible factors grows too rapidly as "n" increases. According to the prime number theorem explained below, the number of prime numbers less than formula_1 is approximately given by formula_11, so the algorithm may need up to this number of trial divisions to check the primality of "n". For , this number is 450 million—too large for many practical applications.
Sieves.
An algorithm yielding all primes up to a given limit, such as required in the trial division method, is called a prime number sieve. The oldest example, the sieve of Eratosthenes (see above) is useful for relatively small primes. The modern sieve of Atkin is more complicated, but faster when properly optimized. Before the advent of computers, lists of primes up to bounds like 107 were also used.
Primality testing versus primality proving.
Modern primality tests for general numbers "n" can be divided into two main classes, probabilistic (or "Monte Carlo") and deterministic algorithms. Deterministic algorithms provide a way to tell for sure whether a given number is prime or not. For example, trial division is a deterministic algorithm because, if it performed correctly, it will always identify a prime number as prime and a composite number as composite. Probabilistic algorithms are normally faster, but do not completely prove that a number is prime. These tests rely on testing a given number in a partly random way. For example, a given test might pass all the time if applied to a prime number, but pass only with probability "p" if applied to a composite number. If we repeat the test "n" times and pass every time, then the probability that our number is composite is "1/(1-p)n", which decreases exponentially with the number of tests, so we can be as sure as we like (though never perfectly sure) that the number is prime. On the other hand, if the test ever fails, then we know that the number is composite.
A particularly simple example of a probabilistic test is the Fermat primality test, which relies on the fact (Fermat's little theorem) that "np≡n (mod p)" for any "n" if "p" is a prime number. If we have a number "b" that we want to test for primality, then we work out "nb (mod b)" for a random value of "n" as our test. A flaw with this test is that there are some composite numbers (the Carmichael numbers) that satisfy the Fermat identity even though they are not prime, so the test has no way of distinguishing between prime numbers and Carmichael numbers. Carmichael numbers are substantially rarer than prime numbers, though, so this test can be useful for practical purposes. More powerful extensions of the Fermat primality test, such as the Baillie-PSW, Miller-Rabin, and Solovay-Strassen tests, are guaranteed to fail at least some of the time when applied to a composite number.
Deterministic algorithms do not erroneously report composite numbers as prime. In practice, the fastest such method is known as elliptic curve primality proving. Analyzing its run time is based on heuristic arguments, as opposed to the rigorously proven complexity of the more recent AKS primality test. Deterministic methods are typically slower than probabilistic ones, so the latter ones are typically applied first before a more time-consuming deterministic routine is employed.
The following table lists a number of prime tests. The running time is given in terms of "n", the number to be tested and, for probabilistic algorithms, the number "k" of tests performed. Moreover, ε is an arbitrarily small positive number, and log is the logarithm to an unspecified base. The big O notation means that, for example, elliptic curve primality proving requires a time that is bounded by a factor (not depending on "n", but on ε) times log5+ε("n").
Special-purpose algorithms and the largest known prime.
In addition to the aforementioned tests applying to any natural number "n", a number of much more efficient primality tests is available for special numbers. For example, to run Lucas' primality test requires the knowledge of the prime factors of "n" − 1, while the Lucas–Lehmer primality test needs the prime factors of "n" + 1 as input. For example, these tests can be applied to check whether
are prime. Prime numbers of this form are known as factorial primes. Other primes where either "p" + 1 or "p" − 1 is of a particular shape include the Sophie Germain primes (primes of the form 2"p" + 1 with "p" prime), primorial primes, Fermat primes and Mersenne primes, that is, prime numbers that are of the form 2"p" − 1, where "p" is an arbitrary prime. The Lucas–Lehmer test is particularly fast for numbers of this form. This is why the largest "known" prime has almost always been a Mersenne prime since the dawn of electronic computers.
Fermat primes are of the form
with "k" an arbitrary natural number. They are named after Pierre de Fermat who conjectured that all such numbers "Fk" are prime. This was based on the evidence of the first five numbers in this series—3, 5, 17, 257, and 65,537—being prime. However, "F"5 is composite and so are all other Fermat numbers that have been verified as of 2015. A regular "n"-gon is constructible using straightedge and compass if and only if
where "m" is a product of any number of distinct Fermat primes and "i" is any natural number, including zero.
The following table gives the largest known primes of the mentioned types. Some of these primes have been found using distributed computing. In 2009, the Great Internet Mersenne Prime Search project was awarded a US$100,000 prize for first discovering a prime with at least 10 million digits. The Electronic Frontier Foundation also offers $150,000 and $250,000 for primes with at least 100 million digits and 1 billion digits, respectively. Some of the largest primes not known to have any particular form (that is, no simple formula such as that of Mersenne primes) have been found by taking a piece of semi-random binary data, converting it to a number n, multiplying it by 256k for some positive integer k, and searching for possible primes within the interval [256"k""n" + 1, 256"k"("n" + 1) − 1].
Integer factorization.
Given a composite integer "n", the task of providing one (or all) prime factors is referred to as "factorization" of "n". Elliptic curve factorization is an algorithm relying on arithmetic on an elliptic curve.
Distribution.
In 1975, number theorist Don Zagier commented that primes both
The distribution of primes in the large, such as the question how many primes are smaller than a given, large threshold, is described by the prime number theorem, but no efficient formula for the "n"-th prime is known.
There are arbitrarily long sequences of consecutive non-primes, as for every positive integer formula_12 the formula_12 consecutive integers from formula_14 to formula_15 (inclusive) are all composite (as formula_16 is divisible by formula_17 for formula_17 between formula_19 and formula_20).
Dirichlet's theorem on arithmetic progressions, in its basic form, asserts that linear polynomials
with coprime integers "a" and "b" take infinitely many prime values. Stronger forms of the theorem state that the sum of the reciprocals of these prime values diverges, and that different such polynomials with the same "b" have approximately the same proportions of primes.
The corresponding question for quadratic polynomials is less well-understood.
Formulas for primes.
There is no known efficient formula for primes. For example, Mills' theorem and a theorem of Wright assert that there are real constants "A>1" and μ such that
are prime for any natural number "n". Here formula_23 represents the floor function, i.e., largest integer not greater than the number in question. The latter formula can be shown using Bertrand's postulate (proven first by Chebyshev), which states that there always exists at least one prime number "p" with "n" < "p" < 2"n" − 2, for any natural number "n" > 3. However, computing "A" or μ requires the knowledge of infinitely many primes to begin with. Another formula is based on Wilson's theorem and generates the number 2 many times and all other primes exactly once.
There is no non-constant polynomial, even in several variables, that takes "only" prime values. However, there is a set of Diophantine equations in 9 variables and one parameter with the following property: the parameter is prime if and only if the resulting system of equations has a solution over the natural numbers. This can be used to obtain a single formula with the property that all its "positive" values are prime.
Number of prime numbers below a given number.
The prime counting function π("n") is defined as the number of primes not greater than "n". For example π(11) = 5, since there are five primes less than or equal to 11. There are known algorithms to compute exact values of π("n") faster than it would be possible to compute each prime up to "n". The "prime number theorem" states that π("n") is approximately given by
in the sense that the ratio of π("n") and the right hand fraction approaches 1 when "n" grows to infinity. This implies that the likelihood that a number less than "n" is prime is (approximately) inversely proportional to the number of digits in "n". A more accurate estimate for π("n") is given by the offset logarithmic integral
The prime number theorem also implies estimates for the size of the "n"-th prime number "pn" (i.e., "p"1 = 2, "p"2 = 3, etc.): up to a bounded factor, "pn" grows like "n" log("n"). In particular, the prime gaps, i.e. the differences "p""n" − "p""n"−1 of two consecutive primes, become arbitrarily large. This latter statement can also be seen in a more elementary way by noting that the sequence "n"! + 2, "n"! + 3, …, "n"! + "n" (for the notation "n"! read factorial) consists of "n" − 1 composite numbers, for any natural number "n".
Arithmetic progressions.
An arithmetic progression is the set of natural numbers that give the same remainder when divided by some fixed number "q" called modulus. For example,
is an arithmetic progression modulo . Except for 3, none of these numbers is prime, since 3 + 9"n" = 3(1 + 3"n") so that the remaining numbers in this progression are all composite. (In general terms, all prime numbers above "q" are of the form "q"#·"n" + "m", where 0 < "m" < "q"#, and "m" has no prime factor ≤ "q".) Thus, the progression
can have infinitely many primes only when "a" and "q" are coprime, i.e., their greatest common divisor is one. If this necessary condition is satisfied, "Dirichlet's theorem on arithmetic progressions" asserts that the progression contains infinitely many primes. The picture below illustrates this with : the numbers are "wrapped around" as soon as a multiple of 9 is passed. Primes are highlighted in red. The rows (=progressions) starting with , 6, or 9 contain at most one prime number. In all other rows (, 2, 4, 5, 7, and 8) there are infinitely many prime numbers. What is more, the primes are distributed equally among those rows in the long run—the density of all primes congruent "a" modulo 9 is 1/6.
The Green–Tao theorem shows that there are arbitrarily long arithmetic progressions consisting of primes.
An odd prime "p" is expressible as the sum of two squares, , exactly if "p" is congruent 1 modulo 4 (Fermat's theorem on sums of two squares).
Prime values of quadratic polynomials.
Euler noted that the function
gives prime numbers for 0 ≤ "n" < 40, a fact leading into deep algebraic number theory, more specifically Heegner numbers. For bigger "n", it does take composite values. The Hardy-Littlewood conjecture F makes an asymptotic prediction about the density of primes among the values of quadratic polynomials (with integer coefficients "a", "b", and "c")
in terms of Li("n") and the coefficients "a", "b", and "c". However, progress has proved hard to come by: no quadratic polynomial (with "a" ≠ 0) is known to take infinitely many prime values. The Ulam spiral depicts all natural numbers in a spiral-like way. Surprisingly, prime numbers cluster on certain diagonals and not others, suggesting that some quadratic polynomials take prime values more often than other ones.
Open questions.
Zeta function and the Riemann hypothesis.
The Riemann zeta function ζ("s") is defined as an infinite sum
where "s" is a complex number with real part bigger than 1. It is a consequence of the fundamental theorem of arithmetic that this sum agrees with the infinite product
The zeta function is closely related to prime numbers. For example, the aforementioned fact that there are infinitely many primes can also be seen using the zeta function: if there were only finitely many primes then ζ(1) would have a finite value. However, the harmonic series 1 + 1/2 + 1/3 + 1/4 + ... diverges (i.e., exceeds any given number), so there must be infinitely many primes. Another example of the richness of the zeta function and a glimpse of modern algebraic number theory is the following identity (Basel problem), due to Euler,
The reciprocal of ζ(2), 6/π2, is the probability that two numbers selected at random are relatively prime.
The unproven "Riemann hypothesis", dating from 1859, states that except for all zeroes of the ζ-function have real part equal to 1/2. The connection to prime numbers is that it essentially says that the primes are as regularly distributed as possible. From a physical viewpoint, it roughly states that the irregularity in the distribution of primes only comes from random noise. From a mathematical viewpoint, it roughly states that the asymptotic distribution of primes (about x/log "x" of numbers less than "x" are primes, the prime number theorem) also holds for much shorter intervals of length about the square root of "x" (for intervals near "x"). This hypothesis is generally believed to be correct. In particular, the simplest assumption is that primes should have no significant irregularities without good reason.
Other conjectures.
In addition to the Riemann hypothesis, many more conjectures revolving about primes have been posed. Often having an elementary formulation, many of these conjectures have withstood a proof for decades: all four of Landau's problems from 1912 are still unsolved. One of them is Goldbach's conjecture, which asserts that every even integer "n" greater than 2 can be written as a sum of two primes. s of February 2011[ [update]], this conjecture has been verified for all numbers up to . Weaker statements than this have been proven, for example Vinogradov's theorem says that every sufficiently large odd integer can be written as a sum of three primes. Chen's theorem says that every sufficiently large even number can be expressed as the sum of a prime and a semiprime, the product of two primes. Also, any even integer can be written as the sum of six primes. The branch of number theory studying such questions is called additive number theory.
Other conjectures deal with the question whether an infinity of prime numbers subject to certain constraints exists. It is conjectured that there are infinitely many Fibonacci primes and infinitely many Mersenne primes, but not Fermat primes. It is not known whether or not there are an infinite number of Wieferich primes and of prime Euclid numbers.
A third type of conjectures concerns aspects of the distribution of primes. It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2 (twin prime conjecture). Polignac's conjecture is a strengthening of that conjecture, it states that for every positive integer "n", there are infinitely many pairs of consecutive primes that differ by 2"n". It is conjectured there are infinitely many primes of the form "n"2 + 1. These conjectures are special cases of the broad Schinzel's hypothesis H. Brocard's conjecture says that there are always at least four primes between the squares of consecutive primes greater than 2. Legendre's conjecture states that there is a prime number between "n"2 and ("n" + 1)2 for every positive integer "n". It is implied by the stronger Cramér's conjecture.
Applications.
For a long time, number theory in general, and the study of prime numbers in particular, was seen as the canonical example of pure mathematics, with no applications outside of the self-interest of studying the topic with the exception of use of prime numbered gear teeth to distribute wear evenly. In particular, number theorists such as British mathematician G. H. Hardy prided themselves on doing work that had absolutely no military significance. However, this vision was shattered in the 1970s, when it was publicly announced that prime numbers could be used as the basis for the creation of public key cryptography algorithms. Prime numbers are also used for hash tables and pseudorandom number generators.
Some rotor machines were designed with a different number of pins on each rotor, with the number of pins on any one rotor either prime, or coprime to the number of pins on any other rotor. This helped generate the full cycle of possible rotor positions before repeating any position.
The International Standard Book Numbers work with a check digit, which exploits the fact that 11 is a prime.
Arithmetic modulo a prime and finite fields.
"Modular arithmetic" modifies usual arithmetic by only using the numbers
where "n" is a fixed natural number called modulus.
Calculating sums, differences and products is done as usual, but whenever a negative number or a number greater than "n" − 1 occurs, it gets replaced by the remainder after division by "n". For instance, for "n" = 7, the sum 3 + 5 is 1 instead of 8, since 8 divided by 7 has remainder 1. This is referred to by saying "3 + 5 is congruent to 1 modulo 7" and is denoted
Similarly, 6 + 1 ≡ 0 (mod 7), 2 − 5 ≡ 4 (mod 7), since −3 + 7 = 4, and 3 · 4 ≡ 5 (mod 7) as 12 has remainder 5. Standard properties of addition and multiplication familiar from the integers remain valid in modular arithmetic. In the parlance of abstract algebra, the above set of integers, which is also denoted Z/"n"Z, is therefore a commutative ring for any "n".
Division, however, is not in general possible in this setting. For example, for "n" = 6, the equation
a solution "x" of which would be an analogue of 2/3, cannot be solved, as one can see by calculating 3 · 0, ..., 3 · 5 modulo 6. The distinctive feature of prime numbers is the following: division "is" possible in modular arithmetic if and only if "n" is a prime. Equivalently, "n" is prime if and only if all integers "m" satisfying 2 ≤ "m" ≤ "n" − 1 are "coprime" to "n", i.e. their only common divisor is one. Indeed, for "n" = 7, the equation
has a unique solution, . Because of this, for any prime "p", Z/"p"Z (also denoted F"p") is called a field or, more specifically, a finite field since it contains finitely many, namely "p", elements.
A number of theorems can be derived from inspecting F"p" in this abstract way. For example, Fermat's little theorem, stating
for any integer "a" not divisible by "p", may be proved using these notions. This implies
Giuga's conjecture says that this equation is also a sufficient condition for "p" to be prime. Another consequence of Fermat's little theorem is the following: if "p" is a prime number other than 2 and 5, 1/"p" is always a recurring decimal, whose period is "p" − 1 or a divisor of "p" − 1. The fraction 1/"p" expressed likewise in base "q" (rather than base 10) has similar effect, provided that "p" is not a prime factor of "q". Wilson's theorem says that an integer "p" > 1 is prime if and only if the factorial ("p" − 1)! + 1 is divisible by "p". Moreover, an integer "n" > 4 is composite if and only if ("n" − 1)! is divisible by "n".
Other mathematical occurrences of primes.
Many mathematical domains make great use of prime numbers. An example from the theory of finite groups are the Sylow theorems: if "G" is a finite group and "pn" is the highest power of the prime "p" that divides the order of "G", then "G" has a subgroup of order "pn". Also, any group of prime order is cyclic (Lagrange's theorem).
Public-key cryptography.
Several public-key cryptography algorithms, such as RSA and the Diffie–Hellman key exchange, are based on large prime numbers (for example 512 bit primes are frequently used for RSA and 1024 bit primes are typical for Diffie–Hellman.). RSA relies on the assumption that it is much easier (i.e., more efficient) to perform the multiplication of two (large) numbers "x" and "y" than to calculate "x" and "y" (assumed coprime) if only the product "xy" is known. The Diffie–Hellman key exchange relies on the fact that there are efficient algorithms for modular exponentiation, while the reverse operation the discrete logarithm is thought to be a hard problem.
Prime numbers in nature.
The evolutionary strategy used by cicadas of the genus "Magicicada" make use of prime numbers. These insects spend most of their lives as grubs underground. They only pupate and then emerge from their burrows after 7, 13 or 17 years, at which point they fly about, breed, and then die after a few weeks at most. The logic for this is believed to be that the prime number intervals between emergences make it very difficult for predators to evolve that could specialize as predators on "Magicicadas". If "Magicicadas" appeared at a non-prime number intervals, say every 12 years, then predators appearing every 2, 3, 4, 6, or 12 years would be sure to meet them. Over a 200-year period, average predator populations during hypothetical outbreaks of 14- and 15-year cicadas would be up to 2% higher than during outbreaks of 13- and 17-year cicadas. Though small, this advantage appears to have been enough to drive natural selection in favour of a prime-numbered life-cycle for these insects.
There is speculation that the zeros of the zeta function are connected to the energy levels of complex quantum systems.
Generalizations.
The concept of prime number is so important that it has been generalized in different ways in various branches of mathematics. Generally, "prime" indicates minimality or indecomposability, in an appropriate sense. For example, the prime field is the smallest subfield of a field "F" containing both 0 and 1. It is either Q or the finite field with "p" elements, whence the name. Often a second, additional meaning is intended by using the word prime, namely that any object can be, essentially uniquely, decomposed into its prime components. For example, in knot theory, a prime knot is a knot that is indecomposable in the sense that it cannot be written as the knot sum of two nontrivial knots. Any knot can be uniquely expressed as a connected sum of prime knots. Prime models and prime 3-manifolds are other examples of this type.
Prime elements in rings.
Prime numbers give rise to two more general concepts that apply to elements of any commutative ring "R", an algebraic structure where addition, subtraction and multiplication are defined: "prime elements" and "irreducible elements". An element "p" of "R" is called prime element if it is neither zero nor a unit (i.e., does not have a multiplicative inverse) and satisfies the following requirement: given "x" and "y" in "R" such that "p" divides the product "xy", then "p" divides "x" or "y". An element is irreducible if it is not a unit and cannot be written as a product of two ring elements that are not units. In the ring Z of integers, the set of prime elements equals the set of irreducible elements, which is
In any ring "R", any prime element is irreducible. The converse does not hold in general, but does hold for unique factorization domains.
The fundamental theorem of arithmetic continues to hold in unique factorization domains. An example of such a domain is the Gaussian integers Z["i"], that is, the set of complex numbers of the form "a" + "bi" where "i" denotes the imaginary unit and "a" and "b" are arbitrary integers. Its prime elements are known as Gaussian primes. Not every prime (in Z) is a Gaussian prime: in the bigger ring Z["i"], 2 factors into the product of the two Gaussian primes (1 + "i") and (1 − "i"). Rational primes (i.e. prime elements in Z) of the form 4"k" + 3 are Gaussian primes, whereas rational primes of the form 4"k" + 1 are not.
Prime ideals.
In ring theory, the notion of number is generally replaced with that of ideal. "Prime ideals", which generalize prime elements in the sense that the principal ideal generated by a prime element is a prime ideal, are an important tool and object of study in commutative algebra, algebraic number theory and algebraic geometry. The prime ideals of the ring of integers are the ideals (0), (2), (3), (5), (7), (11), … The fundamental theorem of arithmetic generalizes to the Lasker–Noether theorem, which expresses every ideal in a Noetherian commutative ring as an intersection of primary ideals, which are the appropriate generalizations of prime powers.
Prime ideals are the points of algebro-geometric objects, via the notion of the spectrum of a ring. Arithmetic geometry also benefits from this notion, and many concepts exist in both geometry and number theory. For example, factorization or ramification of prime ideals when lifted to an extension field, a basic problem of algebraic number theory, bears some resemblance with ramification in geometry. Such ramification questions occur even in number-theoretic questions solely concerned with integers. For example, prime ideals in the ring of integers of quadratic number fields can be used in proving quadratic reciprocity, a statement that concerns the solvability of quadratic equations
where "x" is an integer and "p" and "q" are (usual) prime numbers. Early attempts to prove Fermat's Last Theorem climaxed when Kummer introduced regular primes, primes satisfying a certain requirement concerning the failure of unique factorization in the ring consisting of expressions
where "a"0, ..., "ap"−1 are integers and ζ is a complex number such that .
Valuations.
Valuation theory studies certain functions from a field "K" to the real numbers R called valuations. Every such valuation yields a topology on "K", and two valuations are called equivalent if they yield the same topology. A "prime of K" (sometimes called a "place of K") is an equivalence class of valuations. For example, the "p"-adic valuation of a rational number "q" is defined to be the integer "vp"("q"), such that
where both "r" and "s" are not divisible by "p". For example, The "p"-adic norm is defined as
In particular, this norm gets smaller when a number is multiplied by "p", in sharp contrast to the usual absolute value (also referred to as the infinite prime). While completing Q (roughly, filling the gaps) with respect to the absolute value yields the field of real numbers, completing with respect to the "p"-adic norm |−|"p" yields the field of "p"-adic numbers. These are essentially all possible ways to complete Q, by Ostrowski's theorem. Certain arithmetic questions related to Q or more general global fields may be transferred back and forth to the completed (or local) fields. This local-global principle again underlines the importance of primes to number theory.
In the arts and literature.
Prime numbers have influenced many artists and writers. The French composer Olivier Messiaen used prime numbers to create ametrical music through "natural phenomena". In works such as "La Nativité du Seigneur" (1935) and "Quatre études de rythme" (1949–50), he simultaneously employs motifs with lengths given by different prime numbers to create unpredictable rhythms: the primes 41, 43, 47 and 53 appear in the third étude, "Neumes rythmiques". According to Messiaen this way of composing was "inspired by the movements of nature, movements of free and unequal durations".
In his science fiction novel "Contact", NASA scientist Carl Sagan suggested that prime numbers could be used as a means of communicating with aliens, an idea that he had first developed informally with American astronomer Frank Drake in 1975. In the novel "The Curious Incident of the Dog in the Night-Time" by Mark Haddon, the narrator arranges the sections of the story by consecutive prime numbers.
Many films, such as "Cube", "Sneakers", "The Mirror Has Two Faces" and "A Beautiful Mind" reflect a popular fascination with the mysteries of prime numbers and cryptography. Prime numbers are used as a metaphor for loneliness and isolation in the Paolo Giordano novel "The Solitude of Prime Numbers", in which they are portrayed as "outsiders" among integers.

</doc>
<doc id="23669" url="http://en.wikipedia.org/wiki?curid=23669" title="Piers Anthony">
Piers Anthony

Piers Anthony Dillingham Jacob (born 6 August 1934 in Oxford, England) is an English American author in the science fiction and fantasy genres, publishing under the name Piers Anthony. He is most famous for his set in the fictional realm of Xanth.
Many of his books have appeared on the New York Times Best Seller list. He has claimed that one of his greatest achievements has been to publish a book for every letter of the alphabet, from "Anthonology" to "Zombie Lover".
Early life.
Anthony's family emigrated to the United States from Britain when he was six. He graduated from Goddard College in Vermont in 1956. Anthony had not had an easy childhood. On "This American Life" on July 27, 2012, Anthony revealed that his parents had divorced, he was bullied, and had poor grades in school. Anthony referred to his high school as "a very fancy private school", and refuses to donate money to the school, because as a student, he recalls being part of "the lower crust", and that no one paid attention to or cared about him. He said, "I didn't like being a member of the under class, of the peons like that". He became a naturalized U.S. citizen while serving in the United States Army in 1958. After completing a two-year stint in military service, he briefly taught school at Admiral Farragut Academy in St. Petersburg, Florida before becoming a full-time writer.
Marriage and early career.
Anthony met his future wife, Carol Marble, while both were attending college. They were married in 1956, the same year he graduated from Goddard College, Plainfield, Vermont. After a series of odd jobs, Anthony decided to join the U.S. Army in 1957 for a steady source of income and medical coverage for his pregnant wife. He would stay in the Army until 1959; he became a U.S. citizen during this time. While in the army, he became an editor and cartoonist for the battalion newspaper. After leaving the army, he spent a brief stint as a public school teacher before trying his hand at becoming a full-time writer.
Anthony and his wife made a deal: if he could sell a piece of writing within one year, she would continue to work to support him. But if he could not sell anything in that year, then he would forever give up his dream of being a writer. At the end of the year, he managed to get a short story published. He credits his wife as the person who made his writing career possible, and he advises aspiring writers that they need to have a source of income other than their writing in order to get through the early years of a writing career.
Writing.
On multiple occasions Anthony has moved from one publisher to another (taking a profitable hit series with him), when he says he felt the editors were unduly tampering with his work. He has sued publishers for accounting malfeasance and won judgments in his favor. Anthony maintains an Internet Publishers Survey in the interest of helping aspiring writers. For this service, he won the 2003 "Friend of EPIC" award for service to the electronic publishing community. His website won the "Special Recognition for Service to Writers" award from Preditors and Editors, an author's guide to publishers and writing services.
Anthony was at one time an angel investor in Xlibris.
Many of his popular novel series have been optioned for movies. His popular series Xanth inspired a video game, "Companions of Xanth", by Legend Entertainment for DOS. The series also spawned a board game called "Xanth" by Mayfair Games.
Anthony's novels usually end with a chapter-long Author's Note, in which he talks about himself, his life, and his experiences as they related to the process of writing the novel. He often discusses correspondence with readers and any real-world issues that influenced the novel.
Since about 2000, Anthony has written his novels in a Linux environment.
Anthony's "Xanth" series was ranked No. 99 in a 2011 NPR readers' poll of best science fiction and fantasy books.
In other media.
Act One of episode 470 of the radio program "This American Life" is an account of boyhood obsessions with Piers Anthony. The act is written and narrated by writer Logan Hill who, as a 12-year old, was consumed with reading Anthony’s novels. For a decade he felt he must have been Anthony's number one fan, until, when he was 22, he met "Andy" at a wedding and discovered their mutual interest in the writer. Andy is interviewed for the story and explains that, as a teenager, he had used escapist novels in order to cope with his alienating school and home life in Buffalo, New York. In 1987, at age 15, he decided to run away to Florida in order to try to live with Piers Anthony. The story includes Piers Anthony’s reflections on these events.
Personal life.
Anthony currently lives with his wife on a tree farm which he owns in Florida. He and his wife had two daughters, Penny and Cheryl, and one grandchild, Logan. Regarding his religious beliefs, Anthony wrote in the October 2004 entry of his personal website, "I'm agnostic, which means I regard the case as unproven, but I'm much closer to the atheist position than to the theist one."
On 3 September 2009, their daughter Penelope "Penny" Carolyn Jacob died from apparent respiratory paralysis following surgery for melanoma which had metastasized to her brain. She is survived by her husband and her daughter, Logan.
Bibliography.
For autobiography refer to autobiographical subsection.

</doc>
<doc id="23670" url="http://en.wikipedia.org/wiki?curid=23670" title="Perfect number">
Perfect number

In number theory, a perfect number is a positive integer that is equal to the sum of its proper positive divisors, that is, the sum of its positive divisors excluding the number itself (also known as its aliquot sum). Equivalently, a perfect number is a number that is half the sum of all of its positive divisors (including itself) i.e. "σ"1("n") = 2"n".
This definition is ancient, appearing as early as Euclid's Elements (VII.22) where it is called "τέλειος ἀριθμός" ("perfect", "ideal", or "complete number"). Euclid also proved a formation rule (IX.36) whereby formula_1 is an even perfect number whenever formula_2 is what is now called a Mersenne prime. Much later, Euler proved that all even perfect numbers are of this form. This is known as the Euclid–Euler theorem.
It is not known whether there are any odd perfect numbers, nor whether infinitely many perfect numbers exist.
Examples.
The first perfect number is 6, because 1, 2, and 3 are its proper positive divisors, and 1 + 2 + 3 = 6. Equivalently, the number 6 is equal to half the sum of all its positive divisors: ( 1 + 2 + 3 + 6 ) / 2 = 6. The next perfect number is 28 = 1 + 2 + 4 + 7 + 14. This is followed by the perfect numbers 496 and 8128 (sequence in OEIS).
Discovery.
These first four perfect numbers were the only ones known to early Greek mathematics, and the mathematician Nicomachus had noted 8128 as early as 100 AD. In a manuscript written between 1456 and 1461, an unknown mathematician recorded the earliest reference to a fifth perfect number, with 33,550,336 being correctly identified for the first time. In 1588, the Italian mathematician Pietro Cataldi identified the sixth (8,589,869,056) and the seventh (137,438,691,328) perfect numbers.
Even perfect numbers.
Euclid proved that 2"p"−1(2"p" − 1) is an even perfect number whenever 2"p" − 1 is prime (Euclid, Prop. IX.36).
For example, the first four perfect numbers are generated by the formula 2"p"−1(2"p" − 1), with "p" a prime number, as follows:
Prime numbers of the form 2"p" − 1 are known as Mersenne primes, after the seventeenth-century monk Marin Mersenne, who studied number theory and perfect numbers. For 2"p" − 1 to be prime, it is necessary that "p" itself be prime. However, not all numbers of the form 2"p" − 1 with a prime "p" are prime; for example, 211 − 1 = 2047 = 23 × 89 is not a prime number. In fact, Mersenne primes are very rare—of the 9,592 prime numbers "p" less than 100,000,
2"p" − 1 is prime for only 28 of them.
Over a millennium after Euclid, Ibn al-Haytham (Alhazen) "circa" 1000 AD conjectured that "every" even perfect number is of the form 2"p"−1(2"p" − 1) where 2"p" − 1 is prime, but he was not able to prove this result. It was not until the 18th century that Leonhard Euler proved that the formula 2"p"−1(2"p" − 1) will yield all the even perfect numbers. Thus, there is a one-to-one relationship between even perfect numbers and Mersenne primes; each Mersenne prime generates one even perfect number, and vice versa. This result is often referred to as the Euclid–Euler theorem. s of 2015[ [update]], 48 Mersenne primes are known, and therefore 48 even perfect numbers (the largest of which is 257885160 × (257885161 − 1) with 34,850,340 digits).
An exhaustive search by the GIMPS distributed computing project has shown that the first 44 even perfect numbers are 2"p"−1(2"p" − 1) for
Four higher perfect numbers have also been discovered, namely "p" = 37156667, 42643801, 43112609, and 57885161, though there may be others within this range. It is not known whether there are infinitely many perfect numbers, nor whether there are infinitely many Mersenne primes.
As well as having the form 2"p"−1(2"p" − 1), each even perfect number is the (2"p" − 1)th triangular number (and hence equal to the sum of the integers from 1 to 2"p" − 1) and the 2"p"−1th hexagonal number. Furthermore, each even perfect number except for 6 is the ((2"p" + 1)/3)th centered nonagonal number and is equal to the sum of the first 2("p"−1)/2 odd cubes:
Even perfect numbers (except 6) are of the form
with each resulting triangular number (after subtracting 1 from the perfect number and dividing the result by 9) ending in 3 or 5, the sequence starting with 3, 55, 903, 3727815, ... This can be reformulated as follows: adding the digits of any even perfect number (except 6), then adding the digits of the resulting number, and repeating this process until a single digit (called the digital root) is obtained, always produces the number 1. For example, the digital root of 8128 is 1, because 8 + 1 + 2 + 8 = 19, 1 + 9 = 10, and 1 + 0 = 1. This works with all perfect numbers 2"p"−1(2"p" − 1) with odd prime "p" and, in fact, with all numbers of the form 2"m"−1(2"m" − 1) for odd integer (not necessarily prime) "m".
Owing to their form, 2"p"−1(2"p" − 1), every even perfect number is represented in binary as "p" ones followed by "p" − 1  zeros:
Thus every even perfect number is a pernicious number.
Note that every even perfect number is also a practical number (c.f. Related concepts).
Odd perfect numbers.
It is unknown whether there are any odd perfect numbers, though various results have been obtained. Carl Pomerance has presented a heuristic argument that suggests that no odd perfect numbers exist. All perfect numbers are also Ore's harmonic numbers, and it has been conjectured as well that there are no odd Ore's harmonic numbers other than 1.
Any odd perfect number "N" must satisfy the following conditions:
In 1888, Sylvester stated:
...a prolonged meditation on the subject has satisfied me that the existence of any one such [odd perfect number] — its escape, so to say, from the complex web of conditions which hem it in on all sides — would be little short of a miracle.
Minor results.
All even perfect numbers have a very precise form; odd perfect numbers either do not exist or are rare. There are a number of results on perfect numbers that are actually quite easy to prove but nevertheless superficially impressive; some of them also come under Richard Guy's strong law of small numbers:
Related concepts.
The sum of proper divisors gives various other kinds of numbers. Numbers where the sum is less than the number itself are called deficient, and where it is greater than the number, abundant. These terms, together with "perfect" itself, come from Greek numerology. A pair of numbers which are the sum of each other's proper divisors are called amicable, and larger cycles of numbers are called sociable. A positive integer such that every smaller positive integer is a sum of distinct divisors of it is a practical number.
By definition, a perfect number is a fixed point of the restricted divisor function "s"("n") = "σ"("n") − "n", and the aliquot sequence associated with a perfect number is a constant sequence. All perfect numbers are also formula_13-perfect numbers, or Granville numbers.
A semiperfect number is a natural number that is equal to the sum of all or some of its proper divisors. A semiperfect number that is equal to the sum of all its proper divisors is a perfect number. Most abundant numbers are also semiperfect; abundant numbers which are not semiperfect are called weird numbers.
References.
</dl>

</doc>
<doc id="23672" url="http://en.wikipedia.org/wiki?curid=23672" title="Parthenon">
Parthenon

The Parthenon (; Ancient Greek: Παρθενών; Modern Greek: Παρθενώνας) is a former temple on the Athenian Acropolis, Greece, dedicated to the goddess Athena, whom the people of Athens considered their patron. Construction began in 447 BC when the Athenian Empire was at the height of its power. It was completed in 438 BC although decoration of the building continued until 432 BC. It is the most important surviving building of Classical Greece, generally considered the zenith of the Doric order. Its decorative sculptures are considered some of the high points of Greek art. The Parthenon is regarded as an enduring symbol of Ancient Greece, Athenian democracy and western civilization, and one of the world's greatest cultural monuments. The Greek Ministry of Culture is currently carrying out a program of selective restoration and reconstruction to ensure the stability of the partially ruined structure.
The Parthenon itself replaced an older temple of Athena, which historians call the Pre-Parthenon or Older Parthenon, that was destroyed in the Persian invasion of 480 BC. The temple is archaeoastronomically aligned to the Hyades. While a sacred building dedicated to the city's patron goddess, the Parthenon was actually used primarily as a treasury. For a time, it served as the treasury of the Delian League, which later became the Athenian Empire. In the final decade of the sixth century AD, the Parthenon was converted into a Christian church dedicated to the Virgin Mary.
After the Ottoman conquest, it was turned into a mosque in the early 1460s. On 26 September 1687, an Ottoman ammunition dump inside the building was ignited by Venetian bombardment. The resulting explosion severely damaged the Parthenon and its sculptures. In 1806, Thomas Bruce, 7th Earl of Elgin removed some of the surviving sculptures with the alleged permission of the Ottoman Empire. These sculptures, now known as the Elgin Marbles or the Parthenon Marbles, were sold in 1816 to the British Museum in London, where they are now displayed. Since 1983 (on the initiative of Culture Minister Melina Mercouri), the Greek government has been committed to the return of the sculptures to Greece.
Etymology.
The origin of the Parthenon's name is from the Greek word παρθενών ("parthenon"), which referred to the "unmarried women's apartments" in a house and in the Parthenon's case seems to have been used at first only for a particular room of the temple; it is debated which room this is and how the room acquired its name. The Liddell–Scott–Jones "Greek–English Lexicon" states that this room was the western cella of the Parthenon. Jamauri D. Green holds that the parthenon was the room in which the peplos presented to Athena at the Panathenaic Festival was woven by the arrephoroi, a group of four young girls chosen to serve Athena each year. Christopher Pelling asserts that Athena Parthenos may have constituted a discrete cult of Athena, intimately connected with, but not identical to, that of Athena Polias. According to this theory, the name of the Parthenon means the "temple of the virgin goddess" and refers to the cult of Athena Parthenos that was associated with the temple. The epithet "parthénos" (παρθένος), whose origin is also unclear, meant "maiden, girl", but also "virgin, unmarried woman" and was especially used for Artemis, the goddess of wild animals, the hunt, and vegetation, and for Athena, the goddess of strategy and tactics, handicraft, and practical reason. It has also been suggested that the name of the temple alludes to the maidens ("parthenoi"), whose supreme sacrifice guaranteed the safety of the city.
The first instance in which "Parthenon" definitely refers to the entire building is found in the writings of the 4th century BC orator Demosthenes. In 5th century building accounts, the structure is simply called "ho naos" ("the temple"). The architects Mnesikles and Callicrates are said to have called the building "Hekatompodos" ("the hundred footer") in their lost treatise on Athenian architecture, and, in the 4th century and later, the building was referred to as the "Hekatompedos" or the "Hekatompedon" as well as the Parthenon; the 1st century AD writer Plutarch referred to the building as the "Hekatompedon Parthenon".
Because the Parthenon was dedicated to the Greek goddess Athena, it has sometimes been referred to as the Temple of Minerva, the Roman name for Athena, particularly during the 19th century.
Function.
Although the Parthenon is architecturally a temple and is usually called so, it is not really one in the conventional sense of the word. A small shrine has been excavated within the building, on the site of an older sanctuary probably dedicated to Athena as a way to get closer to the goddess, but the Parthenon never hosted the cult of Athena Polias, patron of Athens: the cult image, which was bathed in the sea and to which was presented the "peplos", was an olivewood "xoanon", located at an older altar on the northern side of the Acropolis.
The colossal statue of Athena by Phidias was not related to any cult and is not known to have inspired any religious fervour. It did not seem to have any priestess, altar or cult name. According to Thucydides, Pericles once referred to the statue as a gold reserve, stressing that it "contained forty talents of pure gold and it was all removable". The Athenian statesman thus implies that the metal, obtained from contemporary coinage, could be used again without any impiety. The Parthenon should then be viewed as a grand setting for the votive statue of Phidias rather than a cult site. It is said in many writings of the Greeks that there were many treasures stored inside the temple, such as Persian swords and small statue figures made of precious metals.
Archaeologist Joan Breton Connelly has recently argued for the coherency of the Parthenon’s sculptural program in presenting a succession of genealogical narratives that track Athenian identity back through the ages: from the birth of Athena, through cosmic and epic battles, to the final great event of the Athenian Bronze Age, the war of Erechtheus and Eumolpos. She argues a pedagogical function for the Parthenon’s sculptured decoration, one that establishes and perpetuates Athenian foundation myth, memory, values and identity. Connelly's thesis is controversial and has been doubted or rejected by some prominent classicists, including Mary Beard, Peter Green, and Garry Wills.
Early history.
Older Parthenon.
The first endeavor to build a sanctuary for on the site of the present Parthenon was begun shortly after the Battle of Marathon (c. 490–488 BC) upon a solid limestone foundation that extended and leveled the southern part of the Acropolis summit. This building replaced a "hekatompedon" (meaning "hundred-footer") and would have stood beside the archaic temple dedicated to "Athena Polias" ("of the city"). The Older or Pre-Parthenon, as it is frequently referred to, was still under construction when the Persians sacked the city in 480 BC and razed the Acropolis.
The existence of both the proto-Parthenon and its destruction were known from Herodotus, and the drums of its columns were plainly visible built into the curtain wall north of the Erechtheum. Further material evidence of this structure was revealed with the excavations of Panagiotis Kavvadias of 1885–90. The findings of this dig allowed Wilhelm Dörpfeld, then director of the German Archaeological Institute, to assert that there existed a distinct substructure to the original Parthenon, called Parthenon I by Dörpfeld, not immediately below the present edifice as had been previously assumed. Dörpfeld's observation was that the three steps of the first Parthenon consisted of two steps of Poros limestone, the same as the foundations, and a top step of Karrha limestone that was covered by the lowest step of the Periclean Parthenon. This platform was smaller and slightly to the north of the final Parthenon, indicating that it was built for a wholly different building, now completely covered over. This picture was somewhat complicated by the publication of the final report on the 1885–90 excavations, indicating that the substructure was contemporary with the Kimonian walls, and implying a later date for the first temple.
If the original Parthenon was indeed destroyed in 480, it invites the question of why the site was left a ruin for thirty-three years. One argument involves the oath sworn by the Greek allies before the Battle of Plataea in 479 BC declaring that the sanctuaries destroyed by the Persians would not be rebuilt, an oath from which the Athenians were only absolved with the Peace of Callias in 450. The mundane fact of the cost of reconstructing Athens after the Persian sack is at least as likely a cause. However, the excavations of Bert Hodge Hill led him to propose the existence of a second Parthenon, begun in the period of Kimon after 468 BC. Hill claimed that the Karrha limestone step Dörpfeld thought was the highest of Parthenon I was in fact the lowest of the three steps of Parthenon II, whose stylobate dimensions Hill calculated at 23.51 x.
One difficulty in dating the proto-Parthenon is that at the time of the 1885 excavation the archaeological method of seriation was not fully developed; the careless digging and refilling of the site led to a loss of much valuable information. An attempt to discuss and make sense of the potsherds found on the acropolis came with the two-volume study by Graef and Langlotz published in 1925–33. This inspired American archaeologist William Bell Dinsmoor to attempt to supply limiting dates for the temple platform and the five walls hidden under the re-terracing of the Acropolis. Dinsmoor concluded that the latest possible date for Parthenon I was no earlier than 495 BC, contradicting the early date given by Dörpfeld. Further, Dinsmoor denied that there were two proto-Parthenons, and held that the only pre-Periclean temple was what Dörpfeld referred to as Parthenon II. Dinsmoor and Dörpfeld exchanged views in the "American Journal of Archaeology" in 1935.
Present building.
In the mid-5th century BC, when the Athenian Acropolis became the seat of the Delian League and Athens was the greatest cultural centre of its time, Pericles initiated an ambitious building project that lasted the entire second half of the century. The most important buildings visible on the Acropolis today—the Parthenon, the Propylaia, the Erechtheion and the temple of Athena Nike—were erected during this period. The Parthenon was built under the general supervision of the artist Phidias, who also had charge of the sculptural decoration. The architects Ictinos and Callicrates began their work in 447 BC, and the building was substantially completed by 432, but work on the decorations continued until at least 431. Some of the financial accounts for the Parthenon survive and show that the largest single expense was transporting the stone from Mount Pentelicus, about 16 km from Athens, to the Acropolis. The funds were partly drawn from the treasury of the Delian League, which was moved from the Panhellenic sanctuary at Delos to the Acropolis in 454 BC.
Architecture.
The Parthenon is a peripteral octastyle Doric temple with Ionic architectural features. It stands on a platform or stylobate of three steps. In common with other Greek temples, it is of post and lintel construction and is surrounded by columns ("peripteral") carrying an entablature. There are eight columns at either end ("octastyle") and seventeen on the sides. There is a double row of columns at either end. The colonnade surrounds an inner masonry structure, the "cella", which is divided into two compartments. At either end of the building the gable is finished with a triangular pediment originally filled with sculpture. The columns are of the Doric order, with simple capitals, fluted shafts and no bases. Above the architrave of the entablature is a frieze of carved pictorial panels (metopes), separated by formal architectural triglyphs, typical of the Doric order. Around the cella and across the lintels of the inner columns runs a continuous sculptured frieze in low relief. This element of the architecture is Ionic in style rather than Doric.
Measured at the stylobate, the dimensions of the base of the Parthenon are 69.5 by. The cella was 29.8 metres long by 19.2 metres wide (97.8 × 63.0 ft), with internal colonnades in two tiers, structurally necessary to support the roof. On the exterior, the Doric columns measure 1.9 m in diameter and are 10.4 m high. The corner columns are slightly larger in diameter. The Parthenon had 46 outer columns and 23 inner columns in total, each column containing 20 flutes. (A flute is the concave shaft carved into the column form.) The stylobate has an upward curvature towards its centre of 60 mm on the east and west ends, and of 110 mm on the sides. The roof was covered with large overlapping marble tiles known as imbrices and tegulae.
The Parthenon is regarded as the finest example of Greek architecture. The temple, wrote John Julius Cooper, "enjoys the reputation of being the most perfect Doric temple ever built. Even in antiquity, its architectural refinements were legendary, especially the subtle correspondence between the curvature of the stylobate, the taper of the naos walls and the "entasis" of the columns." "Entasis" refers to the slight diminution in diameter of the columns as they rise, though the observable effect on the Parthenon is considerably more subtle than on earlier temples. The stylobate is the platform on which the columns stand. As in many other classical Greek temples, it has a slight parabolic upward curvature intended to shed rainwater and reinforce the building against earthquakes. The columns might therefore be supposed to lean outwards, but they actually lean slightly inwards so that if they carried on, they would meet almost exactly a mile above the centre of the Parthenon; since they are all the same height, the curvature of the outer stylobate edge is transmitted to the architrave and roof above: "All follow the rule of being built to delicate curves", Gorham Stevens observed when pointing out that, in addition, the west front was built at a slightly higher level than that of the east front. It is not universally agreed what the intended effect of these "optical refinements" was; they may serve as a sort of "reverse optical illusion". As the Greeks may have been aware, two parallel lines appear to bow, or curve outward, when intersected by converging lines. In this case, the ceiling and floor of the temple may seem to bow in the presence of the surrounding angles of the building. Striving for perfection, the designers may have added these curves, compensating for the illusion by creating their own curves, thus negating this effect and allowing the temple to be seen as they intended. It is also suggested that it was to enliven what might have appeared an inert mass in the case of a building without curves, but the comparison ought to be with the Parthenon's more obviously curved predecessors than with a notional rectilinear temple.
Some studies of the Acropolis, including the Parthenon, conclude that many of its proportions approximate the golden ratio. The Parthenon's façade as well as elements of its façade and elsewhere can be circumscribed by golden rectangles. This view that the golden ratio was employed in the design has been disputed in more recent studies.
Sculpture.
The cella of the Parthenon housed the chryselephantine statue of Athena Parthenos sculpted by Phidias and dedicated in 439 or 438 BC.
The decorative stonework was originally highly coloured. The temple was dedicated to Athena at that time, though construction continued until almost the beginning of the Peloponnesian War in 432. By the year 438, the sculptural decoration of the Doric metopes on the frieze above the exterior colonnade, and of the Ionic frieze around the upper portion of the walls of the cella, had been completed. The richness of the Parthenon's frieze and metope decoration is in agreement with the function of the temple as a treasury. In the "opisthodomus" (the back room of the cella) were stored the monetary contributions of the Delian League, of which Athens was the leading member.
The surviving sculptures today are kept in Athens Acropolis Museum and London British Museum, but a few pieces are also kept in Paris Louvre, Rome, Vienna and Palermo.
Metopes.
The frieze of the Parthenon's entablature contained ninety-two metopes, fourteen each on the east and west sides, thirty-two each on the north and south sides. They were carved in high relief, a practice employed until then only in treasuries (buildings used to keep votive gifts to the gods). According to the building records, the metope sculptures date to the years 446–440 BC. The metopes of the east side of the Parthenon, above the main entrance, depict the Gigantomachy (mythical battles between the Olympian gods and the Giants). The metopes of the west end show Amazonomachy (mythical battle of the Athenians against the Amazons). The metopes of the south side show the Thessalian Centauromachy (battle of the Lapiths aided by Theseus against the half-man, half-horse Centaurs). Metopes 13–21 are missing, but drawings from 1674 attributed to Jaques Carrey indicate a series of humans; these have been variously interpreted as scenes from the Lapith wedding, scenes from the early history of Athens and various myths. On the north side of the Parthenon, the metopes are poorly preserved, but the subject seems to be the sack of Troy.
The metopes present examples of the Severe Style in the anatomy of the figures' heads, in the limitation of the corporal movements to the contours and not to the muscles, and in the presence of pronounced veins in the figures of the Centauromachy. Several of the metopes still remain on the building, but, with the exception of those on the northern side, they are severely damaged. Some of them are located at the Acropolis Museum, others are in the British Museum, and one is at the Louvre museum.
In March 2011, archaeologists announced that they had discovered five metopes of the Parthenon in the south wall of the Acropolis, which had been extended when the Acropolis was used as a fortress. According to "Eleftherotypia" daily, the archaeologists claimed the metopes had been placed there in the 18th century when the Acropolis wall was being repaired. The experts discovered the metopes while processing 2250 photos with modern photographic methods, as the white Pentelic marble they are made of differed from the other stone of the wall. It was previously presumed that the missing metopes were destroyed during the Morosini explosion of the Parthenon in 1687.
Frieze.
The most characteristic feature in the architecture and decoration of the temple is the Ionic frieze running around the exterior walls of the cella, which is the inside structure of the Parthenon. The bas-relief frieze was carved in situ; it is dated to 442 BC-438 BC.
One interpretation is that it depicts an idealized version of the Panathenaic procession from the Dipylon Gate in the Kerameikos to the Acropolis. In this procession held every year, with a special procession taking place every four years, Athenians and foreigners were participating to honour the goddess Athena, offering sacrifices and a new peplos (dress woven by selected noble Athenian girls called "ergastines").
Joan Breton Connelly offers a mythological interpretation for the frieze, one that is in harmony with the rest of the temple’s sculptural program which shows Athenian genealogy through a series of succession myths set in the remote past. She identifies the central panel above the door of the Parthenon as the pre-battle sacrifice of the daughter of King Erechtheus, a sacrifice that ensured Athenian victory over Eumolpos and his Thracian army. The great procession marching toward the east end of the Parthenon shows the post-battle thanksgiving sacrifice of cattle and sheep, honey and water, followed by the triumphant army of Erechtheus returning from their victory. This represents the very first Panathenaia set in mythical times, the model on which historic Panathenaic processions was based.[47]
Pediments.
The traveller Pausanias, when he visited the Acropolis at the end of the 2nd century AD, only mentioned briefly the sculptures of the pediments (gable ends) of the temple, reserving the majority of his description for the gold and ivory statue of the goddess inside.
East pediment.
The east pediment narrates the birth of Athena from the head of her father, Zeus. According to Greek mythology, Zeus gave birth to Athena after a terrible headache prompted him to summon Hephaestus (the god of fire and the forge) for assistance. To alleviate the pain, he ordered Hephaestus to strike him with his forging hammer, and when he did, Zeus's head split open and out popped the goddess Athena in full armour. The sculptural arrangement depicts the moment of Athena's birth.
Unfortunately, the centrepieces of the pediment were destroyed even before Jacques Carrey created otherwise useful documentary drawings in 1674, so all reconstructions are subject to conjecture and speculation. The main Olympian gods must have stood around Zeus and Athena watching the wondrous event, with Hephaestus and Hera probably near them. The Carrey drawings are instrumental in reconstructing the sculptural arrangement beyond the center figures to the north and south.
West pediment.
The west pediment faced the Propylaia and depicted the contest between Athena and Poseidon during their competition for the honor of becoming the city's patron. Athena and Poseidon appear at the center of the composition, diverging from one another in strong diagonal forms, with the goddess holding the olive tree and the god of the sea raising his trident to strike the earth. At their flanks, they are framed by two active groups of horses pulling chariots, while a crowd of legendary personalities from Athenian mythology fills the space out to the acute corners of the pediment.
The work on the pediments lasted from 438 to 432 BC, and the sculptures of the Parthenon pediments are some of the finest examples of classical Greek art. The figures are sculpted in natural movement with bodies full of vital energy that bursts through their flesh, as the flesh in turn bursts through their thin clothing. The thin chitons reveal the body underneath as the focus of the composition. The distinction between gods and humans is blurred in the conceptual interplay between the idealism and naturalism bestowed on the stone by the sculptors. The pediments no longer exist.
Athena Parthenos.
The only piece of sculpture from the Parthenon known to be from the hand of Phidias was the statue of Athena housed in the "naos". This massive chryselephantine sculpture is now lost and known only from copies, vase painting, gems, literary descriptions and coins.
Later history.
Late antiquity.
A major fire broke out in the Parthenon shortly after the middle of the third century AD which destroyed the Parthenon's roof and much of the sanctuary's interior. Repairs were made in the fourth century AD, possibly during the reign of Julian the Apostate. A new wooden roof overlaid with clay tiles was installed to cover the sanctuary. It sloped at a greater incline than the original roof and left the building's wings exposed.
The Parthenon survived as a temple dedicated to Athena for close to a thousand years until Theodosius II decreed in 435 AD that all pagan temples in the Byzantine Empire be closed. At some point in the fifth century, Athena's great cult image was looted by one of the emperors and taken to Constantinople, where it was later destroyed, possibly during the siege of Constantinople during the Fourth Crusade in 1204 AD.
Christian church.
The Parthenon was converted into a Christian church in the final decade of the sixth century AD to become the Church of the Parthenos Maria (Virgin Mary), or the Church of the Theotokos (Mother of God). The orientation of the building was changed to face towards the east; the main entrance was placed at the building's western end and the Christian altar and iconostasis were situated towards the building's eastern side adjacent to an apse built where the temple's pronaos was formerly located. A large central portal with surrounding side-doors was made in the wall dividing the cella, which became the church's nave, from the rear chamber, the church's narthex. The spaces between the columns of the "opisthodomus" and the peristyle were walled up though a number of doorways still permitted access. Icons were painted on the walls and many Christian inscriptions were carved into the Parthenon's columns. These renovations inevitably led to the removal and dispersal of some of the sculptures. Those depicting gods were either possibly re-interpreted according to a Christian theme, or removed and destroyed.
The Parthenon became the fourth most important Christian pilgrimage destination in the Eastern Roman Empire after Constantinople, Ephesos and Thessalonica. In 1018, the emperor Basil II went on a pilgrimage to Athens directly after his final victory over the Bulgarians for the sole purpose of worshipping at the Parthenon. In medieval Greek accounts it is called the Temple of Theotokos Atheniotissa and often indirectly referred to as famous without explaining exactly which temple they were referring to, thus establishing that it was indeed well-known.
At the time of the Latin occupation, it became for about 250 years a Roman Catholic church of Our Lady. During this period a tower, used either as a watchtower or bell tower and containing a spiral staircase, was constructed at the southwest corner of the cella, and vaulted tombs were built beneath the Parthenon's floor.
Islamic mosque.
In 1456, Ottoman Turkish forces invaded Athens and laid siege to a Florentine army defending the Acropolis until June 1458, when it surrendered to the Turks. The Turks may have briefly restored the Parthenon to the Greek Orthodox Christians for continued use as a church. Some time before the close of the fifteenth century the Parthenon became a mosque.
The precise circumstances under which the Turks appropriated it for use as a mosque are unclear; one account states that Mehmed II ordered its conversion as punishment for an Athenian plot against Ottoman rule. The apse became a mihrab, the tower previously constructed during the Roman Catholic occupation of the Parthenon was extended upwards to become a minaret, a minbar was installed, the Christian altar and iconostasis were removed, and the walls were whitewashed to cover icons of Christian saints and other Christian imagery.
Despite the alterations accompanying the Parthenon's conversion into a church and subsequently a mosque, its structure had remained basically intact. In 1667 the Turkish traveler Evliya Çelebi expressed marvel at the Parthenon's sculptures and figuratively described the building as "like some impregnable fortress not made by human agency". He composed a poetic supplication that it, as "a work less of human hands than of Heaven itself, should remain standing for all time". The French artist Jacques Carrey in 1674 visited the Acropolis and sketched the Parthenon's sculptural decorations. Early in 1687, an engineer named Plantier sketched the Parthenon for the Frenchman Graviers d’Ortières. These depictions, particularly those made by Carrey, provide important, and sometimes the only, evidence of the condition of the Parthenon and its various sculptures prior to the devastation it suffered in late 1687 and the subsequent looting of its art objects.
Destruction.
In 1687, the Parthenon was extensively damaged in the greatest catastrophe to befall it in its long history. The Venetians sent an expedition led by Francesco Morosini to attack Athens and capture the Acropolis. The Ottoman Turks fortified the Acropolis and used the Parthenon as a gunpowder magazine – despite having been forewarned of the dangers of this use by the 1656 explosion that severely damaged the Propylaea – and as a shelter for members of the local Turkish community. On 26 September a Venetian mortar round, fired from the Hill of Philopappus, blew up the magazine, and the building was partly destroyed. The explosion blew out the building's central portion and caused the cella's walls to crumble into rubble. Greek architect and archaeologist Kornilia Chatziaslani writes that "...three of the sanctuary’s four walls nearly collapsed and three-fifths of the sculptures from the frieze fell. Nothing of the roof apparently remained in place. Six columns from the south side fell, eight from the north, as well as whatever remained from eastern porch, except for one column. The columns brought down with them the enormous marble architraves, triglyphs and metopes." About three hundred people were killed in the explosion, which showered marble fragments over nearby Turkish defenders and caused large fires that burned until the following day and consumed many homes.
Accounts written at the time conflict over whether this destruction was deliberate or accidental; one such account, written by the German officer Sobievolski, states that a Turkish deserter revealed to Morosini the use to which the Turks had put the Parthenon expecting that the Venetians would not target a building of such historic importance. Morosini was said to have responded by directing his artillery to aim at the Parthenon. Subsequently, Morosini sought to loot sculptures from the ruin and caused further damage in the process. Sculptures of Poseidon and Athena's horses fell to the ground and smashed as his soldiers tried to detach them from the building's west pediment.
The following year, the Venetians abandoned Athens to avoid a confrontation with a large force the Turks had assembled at Chalcis; at that time, the Venetians had considered blowing up what remained of the Parthenon along with the rest of the Acropolis to deny its further use as a fortification to the Turks, but that idea was not pursued.
After the Turks had recaptured the Acropolis they used some of the rubble produced by this explosion to erect a smaller mosque within the shell of the ruined Parthenon. For the next century and a half, portions of the remaining structure were looted for building material and any remaining objects of value.
The 18th century was a period of Ottoman stagnation; as a result, many more Europeans found access to Athens, and the picturesque ruins of the Parthenon were much drawn and painted, spurring a rise in philhellenism and helping to arouse sympathy in Britain and France for Greek independence. Amongst those early travellers and archaeologists were James Stuart and Nicholas Revett, who were commissioned by the Society of Dilettanti to survey the ruins of classical Athens. What they produced was the first measured drawings of the Parthenon published in 1787 in the second volume of "Antiquities of Athens Measured and Delineated". In 1801, the British Ambassador at Constantinople, the Earl of Elgin, obtained a questionable firman "firman" (edict) from the Sultan, whose existence or legitimacy has not been proved until today, to make casts and drawings of the antiquities on the Acropolis, to demolish recent buildings if this was necessary to view the antiquities, and to remove sculptures from them.
Independent Greece.
When independent Greece gained control of Athens in 1832, the visible section of the minaret was demolished; only its base and spiral staircase up to the level of the architrave remain intact. Soon all the medieval and Ottoman buildings on the Acropolis were destroyed. However, the image of the small mosque within the Parthenon's cella has been preserved in Joly de Lotbinière's photograph, published in Lerebours's "Excursions Daguerriennes" in 1842: the first photograph of the Acropolis. The area became a historical precinct controlled by the Greek government. Today it attracts millions of tourists every year, who travel up the path at the western end of the Acropolis, through the restored Propylaea, and up the Panathenaic Way to the Parthenon, which is surrounded by a low fence to prevent damage.
Dispute over the marbles.
The dispute centres around the Parthenon Marbles removed by the Earl of Elgin, which are in the British Museum. A few sculptures from the Parthenon are also in the Louvre in Paris, in Copenhagen, and elsewhere, but over fifty percent are in the Acropolis Museum in Athens. A few can still be seen on the building itself. The Greek government has campaigned since 1983 for the British Museum to return the sculptures to Greece. The British Museum has steadfastly refused to return the sculptures, and successive British governments have been unwilling to force the Museum to do so (which would require legislation). Nevertheless, talks between senior representatives from Greek and British cultural ministries and their legal advisors took place in London on 4 May 2007. These were the first serious negotiations for several years, and there were hopes that the two sides may move a step closer to a resolution.
Reconstruction.
In 1975, the Greek government began a concerted effort to restore the Parthenon and other Acropolis structures. After some delay, a Committee for the Conservation of the Acropolis Monuments was established in 1983. The project later attracted funding and technical assistance from the European Union. An archaeological committee thoroughly documented every artifact remaining on the site, and architects assisted with computer models to determine their original locations. Particularly important and fragile sculptures were transferred to the Acropolis Museum. A crane was installed for moving marble blocks; the crane was designed to fold away beneath the roofline when not in use. In some cases, prior re-construction was found to be incorrect. These were dismantled, and a careful process of restoration began. Originally, various blocks were held together by elongated iron H pins that were completely coated in lead, which protected the iron from corrosion. Stabilizing pins added in the 19th century were not so coated, and corroded. Since the corrosion product (rust) is expansive, the expansion caused further damage by cracking the marble. All new metalwork uses titanium, a strong, lightweight, and corrosion resistant material.
The Parthenon will not be restored to a pre-1687 state, but the explosion damage will be mitigated as much as possible, both in the interest of restoring the structural integrity of the edifice (important in this earthquake-prone region) and to restore the aesthetic integrity by filling in chipped sections of column drums and lintels, using precisely sculpted marble cemented in place. New Pentelic marble from the original quarry is being used. Ultimately, almost all major pieces of marble will be placed in the structure where they originally would have been, supported as needed by modern materials. While the repairs initially show as white against the weathered tan of original surfaces, they will become less prominent as they age.
References.
Printed sources.
</dl>
Online sources.
</dl>
Further reading.
</dl>

</doc>
<doc id="23673" url="http://en.wikipedia.org/wiki?curid=23673" title="Pachomius the Great">
Pachomius the Great

Saint Pachomius (Greek: Παχώμιος, ca. 292–348), also known as Pachome and Pakhomius (/pəˈkoʊmiəs/), is generally recognized as the founder of Christian cenobitic monasticism. Coptic churches celebrate his feast day on 9 May, and Eastern Orthodox and Roman Catholic churches mark his feast on 15 May. In the Lutheran Church, the saint is remembered as a renewer of the church, along with his contemporary (and fellow desert saint), Anthony of Egypt on January 17.
Life.
Saint Pachomius was born in 292 in Thebes (Luxor, Egypt) to pagan parents. According to his hagiography, at age 21, Pachomius was swept up against his will in a Roman army recruitment drive, a common occurrence during this period of turmoil and civil war. With several other youths, he was put onto a ship that floated down the Nile river and arrived at Thebes in the evening. Here he first encountered local Christians, who customarily brought food and comfort daily to the impressed troops. This made a lasting impression, and Pachomius vowed to investigate Christianity further when he got out. He was able to leave the army without ever having to fight, was converted and baptized (314). 
Pachomius then came into contact with several well known ascetics and decided to pursue that path under the guidance of the hermit named Palaemon (317). One of his devotions, popular at the time, was praying with his arms stretched out in the form of a cross. After studying seven years with Palaemon, Pachomius set out to lead the life of a hermit near St. Anthony of Egypt, whose practices he imitated until Pachomius heard a voice in Tabennisi that told him to build a dwelling for the hermits to come to. An earlier ascetic named Macarius had created a number of proto-monasteries called lavra, or cells where holy men would live in a community setting who were physically or mentally unable to achieve the rigors of Anthony's solitary life. 
Pachomius established his first monastery between 318 and 323 at Tabennisi, Egypt. His elder brother John joined him, and soon more than 100 monks lived nearby. Pachomius set about organizing these cells into a formal organization. Until then, Christian asceticism had been solitary or "eremitic"-- male or female monastics lived in individual huts or caves and met only for occasional worship services. Pachomius created the community or "cenobitic" organization, in which male or female monastics lived together and held their property in common under the leadership of an abbot or abbess. Pachomius realized that some men, acquainted only with the eremitical life, might speedily become disgusted, if the distracting cares of the cenobitical life were thrust too abruptly upon them. He therefore allowed them to devote their whole time to spiritual exercises, undertaking all the community's administrative tasks himself. The community hailed Pachomius as "Abba" (father), from which "Abbot" derives. 
The monastery at Tabennisi, though enlarged several times, soon became too small and a second was founded at Pabau (Faou). After 336, Pachomius spent most of his time at Pabau. Though Pachomius sometimes acted as lector for nearby shepherds, neither he nor any of his monks became priests. St Athanasius visited and wished to ordain him in 333, but Pachomius fled from him. Athanasius' visit was probably a result of Pachomius' zealous defence of orthodoxy against Arianism. Basil of Caesarea visited, then took many of Pachomius' ideas, which he adapted and implemented in Caesarea. This ascetic rule, or Ascetica, is still used today by the Eastern Orthodox Church, comparable to that of the Rule of St. Benedict in the West.
Death and legacy.
Pachomius continued as abbot to the cenobites for some forty years. During an epidemic (probably plague), Pachomius called the monks, strengthened their faith, and appointed his successor. Pachomius then died on 14 Pashons, 64 A.M. (9 May 348 A.D.)
By the time Pachomius died (c. 345) eight monasteries and several hundred monks followed his guidance. Within a generation, cenobic practices spread from Egypt to Palestine and the Judean Desert, Syria, North Africa and eventually Western Europe. The number of monks, rather than the number of monasteries, may have reached 7000. 
His reputation as a holy man has endured. As mentioned above, several liturgical calendars commemorate Pachomius. Among many miracles attributed to Pachomius, that though he had never learned the Greek or Latin tongues, he sometimes miraculously spoke them. Pachomius is also credited with being the first Christian to use and recommend use of a prayer rope.
Coptic literature.
Examples of purely Coptic literature are the works of Abba Antonius and Abba Pachomius, who spoke only Coptic, and the sermons and preachings of Abba Shenouda, who chose to write only in Coptic.
The Pachomian system tended to treat religious literature as mere written instructions.
See also.
Further reading
References.
Notes

</doc>
<doc id="23674" url="http://en.wikipedia.org/wiki?curid=23674" title="Philosophical Investigations">
Philosophical Investigations

Philosophical Investigations ("Philosophische Untersuchungen") is a highly influential work by the 20th-century philosopher Ludwig Wittgenstein. In it, Wittgenstein discusses numerous problems and puzzles in the fields of semantics, logic, philosophy of mathematics, philosophy of psychology, philosophy of action, and the philosophy of mind. He puts forth the view that conceptual confusions surrounding language use are at the root of most philosophical problems, contradicting or discarding much of what he argued in his earlier work, the "Tractatus Logico-Philosophicus".
He alleges that the problems are traceable to a set of related assumptions about the nature of language, which themselves presuppose a particular conception of the essence of language. This conception is considered and ultimately rejected for being too general; that is, as an essentialist account of the nature of language it is simply too narrow to be able to account for the variety of things we do with language. Wittgenstein begins the book with a quotation from St. Augustine, whom he cites as a proponent of the generalized and limited conception that he then summarizes:
The individual words in language name objects—sentences are combinations of such names. In this picture of language we find the roots of the following idea: Every word has a meaning. This meaning is correlated with the word. It is the object for which the word stands.
He then sets out throughout the rest of the book to demonstrate the limitations of this conception, including, he argues, with many traditional philosophical puzzles and confusions that arise as a result of this limited picture. Within the Anglo-American tradition, the book is considered by many as being one of the most important philosophical works of the 20th century, and it continues to influence contemporary philosophers, especially those studying mind and language.
The text.
Editions.
The book was not ready for publication when Wittgenstein died in 1951. G. E. M. Anscombe translated Wittgenstein's manuscript, and it was first published in 1953. There are two popular editions of "Philosophical Investigations", both translated by Anscombe:
The text is divided into two parts, consisting of what Wittgenstein calls, in the preface, "Bemerkungen", translated by Anscombe as "remarks". In the first part, these remarks are rarely more than a paragraph long and are numbered sequentially. In the second part, the remarks are longer and numbered using Roman numerals. In the index, remarks from the first part are referenced by their number rather than page; however, references from the second part are cited by page number. The comparatively unusual nature of the second part is due to the fact that it comprises notes that Wittgenstein may have intended to re-incorporate into the first part. Subsequent to his death it was published as a "Part II" in the first, second and third editions. However, in light of continuing uncertainty about Wittgenstein's intentions regarding this material, the fourth edition (2009) re-titles "Part I" as "Philosophical Investigations" proper, and "Part II" as "Philosophy of Psychology – A Fragment."
Method and presentation.
"Philosophical Investigations" is unique in its approach to philosophy. A typical philosophical text presents a philosophical problem, summarizes and critiques various alternative approaches to solving it, presents its own approach, and then argues in favour of that approach. In contrast, Wittgenstein's book treats philosophy as an activity, rather along the lines of Socrates's famous method of maieutics; he has the reader work through various problems, participating actively in the investigation. Rather than presenting a philosophical problem and its solution, Wittgenstein engages in a dialogue, where he provides a thought experiment (a hypothetical example or situation), describes how one might be inclined to think about it, and then shows why that inclination suffers from conceptual confusion. The following is an excerpt from the first entry in the book that exemplifies this method:
...think of the following use of language: I send someone shopping. I give him a slip marked 'five red apples'. He takes the slip to the shopkeeper, who opens the drawer marked 'apples', then he looks up the word 'red' in a table and finds a colour sample opposite it; then he says the series of cardinal numbers—I assume that he knows them by heart—up to the word 'five' and for each number he takes an apple of the same colour as the sample out of the drawer.—It is in this and similar ways that one operates with words—"But how does he know where and how he is to look up the word 'red' and what he is to do with the word 'five'?" Well, I assume that he "acts" as I have described. Explanations come to an end somewhere.—But what is the meaning of the word 'five'? No such thing was in question here, only how the word 'five' is used.
This example is typical of the book's style. We can see each of the steps in Wittgenstein's method:
Similarly, Wittgenstein often uses the device of framing many of the remarks as a dialogue between himself and a disputant. For example, Remark 258 proposes a thought experiment in which a certain sensation is associated with the sign "S" written in a calendar. He then sets up a dialogue in which the disputant offers a series of ways of defining "S", and he meets each with a suitable objection, so drawing the conclusion that in such a case there is no "right" definition of "S".
Through such thought experiments, Wittgenstein attempts to get the reader to come to certain difficult philosophical conclusions independently; he does not simply argue in favor of his own theories.
Language, meaning, and use.
The "Investigations" deals largely with the difficulties of language and meaning. Wittgenstein viewed the tools of language as being fundamentally simple, and he believed that philosophers had obscured this simplicity by misusing language and by asking meaningless questions. He attempted in the "Investigations" to make things clear: "Der Fliege den Ausweg aus dem Fliegenglas zeigen"—to show the fly the way out of the fly bottle.
Meaning is use.
A common summary of his argument is that meaning is use—words are not defined by reference to the objects they designate, nor by the mental representations one might associate with them, but by how they are used. For example, this means there is no need to postulate that there is something called "good" that exists independently of any good deed. This anthropological perspective contrasts with Platonic realism and with Gottlob Frege's notions of sense and reference. This argument has been labeled by some authors as "anthropological holism."
Meaning and definition.
Wittgenstein rejects a variety of ways of thinking about what the meaning of a word is, or how meanings can be identified. He shows how, in each case, the "meaning" of the word presupposes our ability to use it. He first asks the reader to perform a thought experiment: to come up with a definition of the word "game". While this may at first seem a simple task, he then goes on to lead us through the problems with each of the possible definitions of the word "game". Any definition which focuses on amusement leaves us unsatisfied since the feelings experienced by a world class chess player are very different from those of a circle of children playing Duck Duck Goose. Any definition which focuses on competition will fail to explain the game of catch, or the game of solitaire. And a definition of the word "game" which focuses on rules will fall on similar difficulties.
The essential point of this exercise is often missed. Wittgenstein's point is not that it is impossible to define "game", but that "we don't have a definition, and we don't need one", because even without the definition, we "use" the word successfully. Everybody understands what we mean when we talk about playing a game, and we can even clearly identify and correct inaccurate uses of the word, all without reference to any definition that consists of necessary and sufficient conditions for the application of the concept of a game. The German word for "game", "Spiele/Spiel", has a different sense than in English; the meaning of "Spiele" also extends to the concept of "play" and "playing." This German sense of the word may help readers better understand Wittgenstein's context in the remarks regarding games.
Wittgenstein argues that definitions emerge from what he termed "forms of life", roughly the culture and society in which they are used. Wittgenstein stresses the social aspects of cognition; to see how language works for most cases, we have to see how it functions in a specific social situation. It is this emphasis on becoming attentive to the social backdrop against which language is rendered intelligible that explains Wittgenstein's elliptical comment that "If a lion could talk, we could not understand him." However, in proposing the thought experiment involving the fictional character, Robinson Crusoe, a captain shipwrecked on a desolate island with no other inhabitant, Wittgenstein shows that language is not in all cases a social phenomenon (although, they are for most case); instead the criterion for a language is grounded in a set of interrelated normative activities: teaching, explanations, techniques and criteria of correctness. In short, it is essential that a language is shareable, but this does not imply that for a language to function that it is in fact already shared.
Wittgenstein rejects the idea that ostensive definitions can provide us with the meaning of a word. For Wittgenstein, the thing that the word stands for does "not" give the meaning of the word. Wittgenstein argues for this making a series of moves to show that to understand an ostensive definition presupposes an understanding of the way the word being defined is used. So, for instance, there is no difference between pointing to a piece of paper, to its colour, or to its shape; but understanding the difference is crucial to using the paper in an ostensive definition of a shape or of a colour.
Family resemblances.
Why is it that we are sure a particular activity — e.g. Olympic target shooting — is a game while a similar activity — e.g. military sharp shooting — is not? Wittgenstein's explanation is tied up with an important analogy. How do we recognize that two people we know are related to one another? We may see similar height, weight, eye color, hair, nose, mouth, patterns of speech, social or political views, mannerisms, body structure, last names, etc. If we see enough matches we say we've noticed a family resemblance. It is perhaps important to note that this is not always a conscious process — generally we don't catalog various similarities until we reach a certain threshold, we just intuitively "see" the resemblances. Wittgenstein suggests that the same is true of language. We are all familiar (i.e. socially) with enough things which "are games" and enough things which "are not games" that we can categorize new activities as either games or not.
This brings us back to Wittgenstein's reliance on indirect communication, and his reliance on thought-experiments. Some philosophical confusions come about because we aren't able to "see" family resemblances. We've made a mistake in understanding the vague and intuitive rules that language uses, and have thereby tied ourselves up in philosophical knots. He suggests that an attempt to untangle these knots requires more than simple deductive arguments pointing out the problems with some particular position. Instead, Wittgenstein's larger goal is to try to divert us from our philosophical problems long enough to become aware of our intuitive ability to "see" the family resemblances.
Language-games.
Wittgenstein develops this discussion of games into the key notion of a "language-game". Wittgenstein introduces the term using simple examples, but intends it to be used for the many ways in which we use language. The central component of language games is that they are uses of language, and language is used in multifarious ways. For example, in one language-game, a word might be used to stand for (or refer to) an object, but in another the same word might be used for giving orders, or for asking questions, and so on. The famous example is the meaning of the word "game". We speak of various kinds of games: board games, betting games, sports, "war games". These are all different uses of the word "games". Wittgenstein also gives the example of "Water!", which can be used as an exclamation, an order, a request, or as an answer to a question. The meaning the word has depends on the language-game in which it is used. Another way Wittgenstein puts the point is that the word "water" has no meaning apart from its use within a language-game. One might use the word as an order to have someone else bring you a glass of water. But it can also be used to warn someone that the water has been poisoned. One might even use the word as code by members of a secret society.
Wittgenstein does not limit the application of his concept of language games to word-meaning. He also applies it to sentence-meaning. For example, the sentence "Moses did not exist" (§79) can mean various things. Wittgenstein argues that independently of use the sentence does not yet 'say' anything. It is 'meaningless' in the sense of being insignificant for a particular purpose. It only acquires significance if we fix it within some context of use. Thus, it fails to say anything because the sentence as such does not yet determine some particular use. The sentence is only meaningful when it is used to say something. For instance, it can be used so as to say that no person or historical figure fits the set of descriptions attributed to the person that goes by the name of "Moses". But it can also mean that the leader of the Israelites was not called Moses. Or that there cannot have been anyone who accomplished all that the Bible relates of Moses. Etc. What the sentence means thus depends on its context of use.
Rules.
One general characteristic of games that Wittgenstein considers in detail is the way in which they consist in following rules. Rules constitute a family, rather than a class that can be explicitly defined. As a consequence, it is not possible to provide a definitive account of what it is to follow a rule. Indeed, he argues that "any" course of action can be made out to accord with some particular rule, and that therefore a rule cannot be used to explain an action. Rather, that one is following a rule or not is to be decided by looking to see if the actions conform to the expectations in the particular "form of life" in which one is involved. Following a rule is a social activity.
Private language.
Wittgenstein also ponders the possibility of a language that talks about those things that are known only to the user, whose content is inherently private. The usual example is that of a language in which one names one's sensations and other subjective experiences, such that the meaning of the term is decided by the individual alone. For example, the individual names a particular sensation, on some occasion, 'S', and intends to use that word to refer to that sensation. Such a language Wittgenstein calls a "private language".
Wittgenstein presents several perspectives on the topic. One point he makes is that it is incoherent to talk of "knowing" that one is in some particular mental state. Whereas others can learn of my pain, for example, I simply "have" my own pain; it follows that one does not "know" of one's own pain, one simply "has" a pain. For Wittgenstein, this is a grammatical point, part of the way in which the language-game involving the word "pain" is played.
Although Wittgenstein certainly argues that the notion of private language is incoherent, because of the way in which the text is presented the exact nature of the argument is disputed. First, he argues that a private language is not really a language at all. This point is intimately connected with a variety of other themes in his later works, especially his investigations of "meaning". For Wittgenstein, there is no single, coherent "sample" or "object" that we can call "meaning". Rather, the supposition that there are such things is the source of many philosophical confusions. Meaning is a complicated phenomenon that is woven into the fabric of our lives. A good first approximation of Wittgenstein's point is that meaning is a "social" event; meaning happens "between" language users. As a consequence, it makes no sense to talk about a private language, with words that "mean" something in the absence of other users of the language.
Wittgenstein also argues that one couldn't possibly "use" the words of a private language. He invites the reader to consider a case in which someone decides that each time she has a particular sensation she will place a sign S in a diary. Wittgenstein points out that in such a case one could have no criteria for the correctness of one's use of S. Again, several examples are considered. One is that perhaps using S involves mentally consulting a table of sensations, to check that one has associated S correctly; but in this case, how could the mental table be checked for its correctness? It is "[a]s if someone were to buy several copies of the morning paper to assure himself that what it said was true", as Wittgenstein puts it. One common interpretation of the argument is that while one may have direct or privileged access to one's "current" mental states, there is no such infallible access to identifying previous mental states that one had in the past. That is, the only way to check to see if one has applied the symbol S correctly to a certain mental state is to introspect and determine whether the current sensation is identical to the sensation previously associated with S. And while identifying one's current mental state of remembering may be infallible, whether one remembered correctly is not infallible. Thus, for a language to be used at all it must have some public criterion of identity.
Often, what is widely regarded as a deep philosophical problem will vanish, argues Wittgenstein, and eventually be seen as a confusion about the significance of the words that philosophers use to frame such problems and questions. It is only in this way that it is interesting to talk about something like a "private language" — i.e., it is helpful to see how the "problem" results from a misunderstanding.
To sum up: Wittgenstein asserts that, if something is a language, it "cannot" be (logically) private; and if something "is" private, it is not (and cannot be) a language.
Wittgenstein's beetle.
Another point that Wittgenstein makes against the possibility of a private language involves the beetle-in-a-box thought experiment. He asks the reader to imagine that each person has a box, inside of which is something that everyone intends to refer to with the word "beetle". Further, suppose that no one can look inside another's box, and each claims to know what a "beetle" is only by examining their own box. Wittgenstein suggests that, in such a situation, the word "beetle" could not be the name of a thing, because supposing that each person has something completely different in their boxes (or nothing at all) does not change the meaning of the word; the beetle as a private object "drops out of consideration as irrelevant". Thus, Wittgenstein argues, if we can talk about something, then it is not "private", in the sense considered. And, contrapositively, if we consider something to be indeed private, it follows that we "cannot talk about it".
Kripke's account.
The discussion of private languages was revitalized in 1982 with the publication of Saul Kripke's book "Wittgenstein on Rules and Private Language". In this work, Kripke uses Wittgenstein's text to develop a particular type of skepticism about rules that stresses the "communal" nature of language-use as grounding meaning. Kripke's version of Wittgenstein, although philosophically interesting, has been facetiously called Kripkenstein, with some scholars such as Gordon Baker, Peter Hacker, Colin McGinn, and John McDowell seeing it as a radical misinterpretation of Wittgenstein's text.
Mind.
Wittgenstein's investigations of language lead to several issues concerning the mind. His key target of criticism is any form of extreme mentalism which posits mental states that are entirely unconnected to the subject's environment. For Wittgenstein, thought is inevitably tied to language, which is inherently social; therefore, there is no 'inner' space in which thoughts can occur. Part of Wittgenstein's credo is captured in the following proclamation: "An 'inner process' stands in need of outward criteria." This follows primarily from his conclusions about private languages: similarly, a private mental state (a sensation of pain, for example) cannot be adequately discussed without public criteria for identifying it.
According to Wittgenstein, those who insist that consciousness (or any other apparently subjective mental state) is conceptually unconnected to the external world are mistaken. Wittgenstein explicitly criticizes so-called conceivability arguments: "Could one imagine a stone's having consciousness? And if anyone can do so—why should that not merely prove that such image-mongery is of no interest to us?" He considers and rejects the following reply as well:
"But if I suppose that someone is in pain, then I am simply supposing that he has just the same as I have so often had." — That gets us no further. It is as if I were to say: "You surely know what 'It is 5 o'clock here' means; so you also know what 'It's 5 o'clock on the sun' means. It means simply that it is just the same there as it is here when it is 5 o'clock." — The explanation by means of "identity" does not work here.
Thus, according to Wittgenstein, mental states are intimately connected to a subject's environment, especially their linguistic environment, and conceivability or imaginability arguments that claim otherwise are misguided. Wittgenstein has also said that "language is inherent and transcendental", which is also not difficult to understand, since we can only comprehend and explain transcendental affairs through language.
Wittgenstein and behaviorism.
From his remarks on the importance of public, observable behavior (as opposed to private experiences), it may seem that Wittgenstein is simply a behaviorist—one who thinks that mental states are nothing over and above certain behavior. However, Wittgenstein resists such a characterization; he writes (considering what an objector might say):
"Are you not really a behaviourist in disguise? Aren't you at bottom really saying that everything except human behaviour is a fiction?" — If I do speak of a fiction, then it is of a "grammatical" fiction.
Clearly, Wittgenstein did not want to be a behaviorist, nor did he want to be a cognitivist or a phenomenologist. He is, of course, primarily concerned with facts of linguistic usage. However, some argue that Wittgenstein is basically a behaviorist because he considers facts about language use as all there is. Such a claim is controversial, since it is not explicitly endorsed in the "Investigations".
"Seeing that" vs. "seeing as".
In addition to ambiguous sentences, Wittgenstein discussed figures which can be seen and understood in two different ways. Often one can see something in a straightforward way — seeing "that" it is a rabbit, perhaps. But, at other times, one notices a particular aspect — seeing it "as" something.
An example Wittgenstein uses is the "duckrabbit", an ambiguous image that can be "seen as" either a duck or a rabbit. When one looks at the duck-rabbit and sees a rabbit, one is not "interpreting" the picture as a rabbit, but rather "reporting" what one sees. One just sees the picture as a rabbit. But what occurs when one sees it first as a duck, then as a rabbit? As the gnomic remarks in the "Investigations" indicate, Wittgenstein isn't sure. However, he is sure that it could not be the case that the external world stays the same while an 'internal' cognitive change takes place.
Relation to the "Tractatus".
According to the standard reading, in the "Philosophical Investigations" Wittgenstein repudiates many of his own earlier views, expressed in the "Tractatus Logico-Philosophicus". The "Tractatus", as Bertrand Russell saw it (though it should be noted that Wittgenstein took strong exception to Russell's reading), had been an attempt to set out a logically perfect language, building on Russell's own work. In the years between the two works Wittgenstein came to reject the idea that underpinned logical atomism, that there were ultimate "simples" from which a language should, or even could, be constructed.
In remark #23 of "Philosophical Investigations" he points out that the practice of human language is more complex than the simplified views of language that have been held by those who seek to explain or simulate human language by means of a formal system. It would be a disastrous mistake, according to Wittgenstein, to see language as being in any way analogous to formal logic.
Besides stressing the Investigation's opposition to the "Tractatus", there are critical approaches which have argued that there is much more continuity and similarity between the two works than supposed. One of these is the New Wittgenstein approach.
Norman Malcolm credits Piero Sraffa with providing Wittgenstein with the conceptual break that founded the "Philosophical Investigations", by means of a rude gesture on Sraffa's part:
"Wittgenstein was insisting that a proposition and that which it describes must have the same 'logical form', the same 'logical multiplicity', Sraffa made a gesture, familiar to Neapolitans as meaning something like disgust or contempt, of brushing the underneath of his chin with an outward sweep of the finger-tips of one hand. And he asked: 'What is the logical form of that?'"
Notes.
"Remarks in Part I of "Investigations" are preceded by the symbol "§". Remarks in Part II are referenced by their Roman numeral or their page number in the third edition.

</doc>
<doc id="23677" url="http://en.wikipedia.org/wiki?curid=23677" title="Poul Anderson">
Poul Anderson

Poul William Anderson (November 25, 1926 – July 31, 2001) was an American science fiction author who began his career during the Golden Age of the genre and continued to write and remain popular into the 21st century. Anderson also authored several works of fantasy, historical novels, and a prodigious number of short stories. He received numerous awards for his writing, including seven Hugo Awards and three Nebula Awards.
Biography.
Poul Anderson was born on November 25, 1926, in Bristol, Pennsylvania, of Scandinavian parents.
Shortly after his birth, his father, Anton Anderson, an engineer, moved the family to Texas, where they lived for over ten years. Following Anton Anderson's death, his widow took her children to Denmark. The family returned to the United States after the outbreak of World War II, settling eventually on a Minnesota farm. (The frame story of Three Hearts and Three Lions, before the fantasy part begins, is partly set in the pre-WWII Denmark which the young Anderson personally experienced.) 
While he was an undergraduate student at the University of Minnesota, Anderson's first stories were published by John W. Campbell in "Astounding Science Fiction": "Tomorrow's Children" by Anderson and F. N. Waldrop in March 1947 and a sequel, "Chain of Logic" by Anderson alone, in July. He earned his B.A. in physics with honors but made no serious attempt to work as a physicist; instead he became a free-lance writer after his graduation in 1948—and placed his third story in the December "Astounding".
Anderson married Karen Kruse in 1953 and moved with her to the San Francisco Bay area. Their daughter Astrid (now married to science fiction author Greg Bear) was born in 1954. They made their home in Orinda, California. Over the years Poul gave many readings at The Other Change of Hobbit bookstore in Berkeley, and his wife later donated his typewriter and desk to the store. He died of cancer on July 31, 2001, after a month in the hospital. A few of his novels were first published posthumously.
Anderson was a founding member of the Society for Creative Anachronism in 1966 and of the Swordsmen and Sorcerers' Guild of America, also in the mid-1960s. The latter was a loose-knit group of Heroic Fantasy authors led by Lin Carter, originally eight in number, with entry by credentials as a fantasy writer alone. He was the sixth President of Science Fiction and Fantasy Writers of America, taking office in 1972. 
Robert A. Heinlein dedicated his 1985 novel "The Cat Who Walks Through Walls" to Anderson and eight of the other members of the Citizens' Advisory Council on National Space Policy.
The Science Fiction Writers of America made him its 16th SFWA Grand Master in 1998 and the Science Fiction and Fantasy Hall of Fame inducted him in 2000, its fifth class of two deceased and two living writers.
Political, moral and literary themes.
Anderson is probably best known for adventure stories in which larger-than-life characters succeed gleefully or fail heroically. His characters were nonetheless thoughtful, often introspective, and well developed. His plot lines frequently involved the application of social and political issues in a speculative manner appropriate to the science fiction genre. He also wrote some quieter works, generally of shorter length, which appeared more often during the latter part of his career.
Much of his science fiction is thoroughly grounded in science (with the addition of unscientific but standard speculations such as faster-than-light travel). A specialty was imagining scientifically plausible non-Earthlike planets. Perhaps the best known was the planet of "The Man Who Counts"; Anderson adjusted its size and composition so that humans could live in the open air but flying intelligent aliens could evolve, and he explored consequences of these adjustments.
Space and liberty.
In many stories, Anderson commented on society and politics. Whatever other vicissitudes his views went through, he firmly retained his belief in the direct and inextricable connection between human liberty and expansion into space, for which reason he strongly cried out against any idea of space exploration being "a waste of money" or "unnecessary luxury".
The connection between space flight and freedom is clearly (as is stated explicitly in some of the stories) an extension of the nineteenth-century American concept of the Frontier, where malcontents can advance further and claim some new land, and pioneers either bring life to barren asteroids (as in "Tales of the Flying Mountains") or settle on Earth-like planets teeming with life, but not intelligent forms (such as New Europe in "Star Fox").
As he repeatedly expressed in his nonfiction essays, Anderson firmly held that going into space was not an unnecessary luxury but an existential need, and that abandoning space would doom humanity to "a society of brigands ruling over peasants".
This is graphically expressed in the chilling short story "Welcome". In it, humanity has abandoned space and is left with an overcrowded Earth where a small elite not only treats all the rest as chattel slaves, but also regularly practices cannibalism, its members getting their chefs to prepare "roast suckling coolie" for their banquets.
Conversely, in the bleak Orwellian world of "The High Ones" where the Soviets have won the Third World War and gained control of the whole world, the dissidents still have some hope, precisely because space flight has not been abandoned. By the end of the story, rebels have established themselves at another stellar system—where their descendants, the reader is told, would eventually build a liberating fleet and set out back to Earth.
World government.
While horrified by the prospect of the Soviets winning complete rule over the Earth, Anderson was not enthusiastic about having Americans in that role either. Several stories and books describing the aftermath of a total American victory in another world war, such as "Sam Hall" and its loose sequel "Three Worlds to Conquer" as well as "Shield", are scarcely less bleak than the above-mentioned depictions of a Soviet victory. Like Heinlein in "Solution Unsatisfactory", Anderson assumed that the imposition of an American military rule over the rest of the world would necessarily entail the destruction of American democracy and the imposition of a harsh tyrannical rule over the United States' own citizens.
Both Anderson's depiction of a Soviet-dominated world and that of an American-dominated one mention a rebellion breaking out in Brazil in the early 21st century, which is in both cases brutally put down by the dominant world power—the Brazilian rebels being characterized as "counter-revolutionaries" in the one case and as "communists" in the other.
In the early years of the Cold War—when he had been, as described by his later, more conservative self, a "flaming liberal"—Anderson pinned his hopes on the United Nations developing into a true world government. This is especially manifest in "Un-man", a future thriller where the Good Guys are agents of the UN Secretary General working to establish a world government while the Bad Guys are nationalists (especially American nationalists) who seek to preserve their respective nations' sovereignty at all costs. (The title has a double meaning: the hero is literally a UN man and has superhuman abilities which make his enemies fear him as an "un-man").
In later years Anderson completely repudiated this idea (a half-humorous remnant is the beginning of "Tau Zero": a future where the nations of the world entrusted Sweden with overseeing disarmament and found themselves living under the rule of the Swedish Empire). In "The Star Fox", his unfavorable depiction of a future peace group called "World Militants for Peace" indicates clearly where he stood with regard to the Vietnam War, raging when the book was published. A more explicit expression of the same appears in the later "The Shield of Time" where a time-traveling young American woman from the 1990s pays a brief visit to a university campus of the 1960s and is not enthusiastic about what she sees there.
Libertarianism.
Instead of a world government, the above-mentioned "Shield" resolves the problem of an American-dominated world dictatorship in a truly libertarian manner: The protagonist, who is hunted by various power groups for the secret of a personal impregnable force field which he brought from Mars, finally decides to simply reveal it to the entire world, so that every individual could thumb his or her nose at each and every Authority.
Anderson often returned to libertarianism (which accounts for his Prometheus Awards) and to the business leader as hero, most notably his character Nicholas van Rijn. Van Rijn is, however, far from the modern type of business executive, being a kind of throwback to the merchant venturer of the Dutch Golden Age of the 17th century. If he spends any time in boardrooms or plotting corporate takeovers, the reader remains ignorant of it, since virtually all his appearances are in the wilds of a space frontier.
Beginning in the 1970s, Anderson's historically grounded works were influenced by the theories of the historian John K. Hord, who argued that all empires follow the same broad cyclical pattern, in which the Terran Empire of the Dominic Flandry spy stories fit neatly.
The writer Sandra Miesel (1978) has argued that Anderson's overarching theme is the struggle against entropy and the heat death of the universe, a condition of perfect uniformity where nothing can happen.
The Israeli–Palestinian conflict.
In 'Withit Collegiate Dictionary', a nonfiction essay that is embedded in "There Will Be Time" and attributed to the book's fictional protagonist, but seems to reflect Anderson's own views, sharply criticizes the American Left of 1972 (when it was written), also defines Napalm as 'Napalm: Jellied gasoline, ignited and propelled against enemy personnel, condemned by all true liberals except when used by Israelis upon Arabs.'
References to the Israeli–Palestinian conflict crop up quite frequently in Anderson's fiction, through various analogues and the conflict's past, future, and alternate permutations. Significantly, Anderson's position on the Middle East conflict was considerably more dovish than his stance towards the United States' own wars, such as his aforementioned support for the military involvement in Vietnam. Consistently, he regarded the conflict as one in which both Israelis and Palestinians have some measure of justice on their side, and Israeli characters often express criticism of their country's policies.
Thus, in the story "Ivory, and Apes, and Peacocks", the Time Patrol's resident agents in the Tyre of King Hiram are a twentieth-century Israeli couple, who express their wish to help the ancient Tyrians "in order to compensate a bit for what our country is going to do here." (The story was written during the Lebanon War of 1982, when Israeli planes bombed the modern Tyre and caused heavy civilian casualties).
The aggressive mutants of Dromm in "Inside Straight", who totally subdued their own planet and embarked on interstellar conquest, had started as a persecuted minority. The Dromman character in the story, who is clearly the villain but is nevertheless depicted with considerable empathy, thinks of his people's history of having been the target of "whipped up xenophobia, pogroms and concentration camps", in one of which his own grandfather died. He also thinks of how angry his people were when an off-world philosopher told them: "Unjust treatment is apt to produce paranoia in the victim. Your race has outlived its oppressors, but not the reflexes they built into your society. Your canalised nervous system make you incapable of regarding anyone else as anything but a dangerous enemy."
"Fire Time" gives the detailed history of a prolonged escalating conflict on a planet colonized simultaneously by humans who call it Mundomar and the nonhuman Naqsans who call it Tseyakka: The historical film of the human leader Sigurdsson declaring the independent republic of Eleutheria in the midst of war is clearly reminiscent of David Ben-Gurion declaring Israel's independence in 1948; in a later war, the Eleutherians conquer the Naqsan continent of G'yaaru, rename it Sigurdssonia and establish settlements in it.
There is in this context a short reappearance of Gunnar Heim, the protagonist of "The Star Fox". In the earlier book, Heim personally, as a privateer waging an undeclared war on the Aleriona, forced a reluctant Earth into an all-out war which Heim felt was needed since the Aleriona were ideologically committed to the universal conquest of everybody else (apparently, in this context, the analogue of Communism, though the Aleriona do not resemble Communists in any particular detail). With regard to Mundomar/Tseyakka, however, the same Heim is the voice of moderation, calling for compromise and coexistence between the two warring parties and strongly condemning the uncritical support of Earth for the aggressive Eleutherians (which seems an analogue of U.S. support of Israel).
In a related story, a group of isolated humans had been living for several generations on an alien planet, on extremely good terms with its non-human inhabitants and without the slightest conflict with them. Nevertheless, the captain of an arriving Earth ship forces them at gunpoint to leave the planet, stating: "Can you speak for your grandchildren and for their grandchildren, for generations which will grow more and more numerous and need more and more land? When my ancestors arrived in Palestine, they did not intend to depose the local Arabs and drive them into refugee camps—but in the end, that's what they did." (The captain's family name is Ben Yehuda, the name of the noted Zionist linguist Eliezer Ben Yehuda who had a major role in reviving Hebrew as a vernacular spoken language.)
This is a typical example of Anderson's frequent motif of a tragic conflict: a story with no villains at all, with all protagonists having the best of intentions and still forced into bitter conflict.
Fairness to the adversaries.
In his numerous books and stories depicting conflict in science-fictional or fantasy settings, Anderson takes trouble to make both sides' points of view comprehensible. Even where there can be no doubt as to whose side the author is on, the antagonists are usually not depicted as villains but as honourable on their own terms. The reader is given access to their thoughts and feelings, and they often have a tragic dignity in defeat. Typical examples are "The Winter of the World" and "The People of the Wind".
A common theme in Anderson's works, and one with obvious origins in the Northern European legends, is that doing the "right" (wisest) thing often involves performing actions that, at face value, seem dishonorable, illegal, destructive, or downright evil. "The Man who Counts", Nicholas van Rijn is "The Man" because he is prepared to be tyrannical and callously manipulative so that he and his companions can survive. In "High Treason" the protagonist disobeys orders and betrays his subordinates to prevent a war crime that would bring severe retribution upon Humanity. In "A Knight of Ghosts and Shadows", Dominic Flandry first (effectively) lobotomizes his own son and then bombards the home planet of the Chereionite race in order to do his duty and prop up the Terran Empire. These actions affect their characters in different ways, and dealing with the repercussions of having done the "right" (but unpleasant) thing is often the major focus of his short stories. The general lesson seems to be that guilt is the penalty for action.
In "The Star Fox", a relationship of grudging respect is built up between the hero, space privateer Gunnar Heim, and his enemy Cynbe, an exceptionally gifted Alerione trained from a young age to understand his species' human enemies to the point of being alienated from his own kind. In the final scene, Cynbe challenges Heim to a space battle which only one of them would survive. Heim accepts, whereupon Cynbe says, "I thank you, my brother."
Underestimating "primitives" as a costly mistake.
Anderson set much of his work in the past, often with the addition of magic, or in alternate or future worlds that resemble past eras. A specialty was his ancestral Scandinavia, as in his novel versions of the legends of Hrólf Kraki ("Hrolf Kraki's Saga") and Haddingus ("The War of the Gods"). Frequently he presented such worlds as superior to the dull, over-civilized present. Notable depictions of this superiority are the prehistoric world of "The Long Remembering", the quasi-medieval society of "No Truce with Kings", and the untamed Jupiter of "Call Me Joe" and "Three Worlds to Conquer". He handled the lure and power of atavism satirically in "Pact", critically in "The Queen of Air and Darkness" and "The Night Face", and tragically in "Goat Song".
His 1965 novel, "The Corridors of Time", alternates between the European Stone Age and a repressive future. In this vision of tomorrow, almost everyone is either an agricultural serf or an industrial slave, but the rulers genuinely believe they are creating a better world. Set largely in Denmark, it treats the Neolithic society with knowledge and respect while not hiding its own faults. It is there that the protagonist, having access to literally all periods of the past and future, finally decides to settle down and finds a happy and satisfying life.
In many stories, a representative of a technologically advanced society underestimates "primitives" and pays a high price for it. In "The High Crusade", aliens who land in medieval England in the expectation of an easy conquest find that they are not immune to swords and arrows. In "The Only Game in Town", a Mongol warrior, while not knowing that the two "magicians" he meets are time travelers from the future, correctly guesses their intentions—and captures them with the help of the "magic" flashlight they had given him in an attempt to impress him. In another time-travel tale, "The Shield of Time", a "time policeman" from the Twentieth Century, equipped with information and technologies from much further in the future, is outwitted by a medieval knight and barely escapes with his life. Yet another story, "The Man Who Came Early", features a 20th-century United States Army soldier stationed in Iceland who is transported to the tenth century. Although he is full of ideas, his lack of practical knowledge of how to implement them and his total unfamiliarity with the technology and customs of the period lead to his downfall.
Anderson wrote "Uncleftish Beholding", an introduction to atomic theory, using only Germanic-rooted words. Fitting his love for olden years, this kind of learned writing has been named Ander-Saxon after him.
Tragic conflicts.
The story told in "The Shield of Time" is also an example of a tragic conflict, another common theme in Anderson's writing. The knight tries to do his best in terms of his own society and time, but his actions might bring about a horrible Twentieth Century (even more horrible than the one we know). Therefore, the Time Patrol protagonists, who like the young knight and wish him well (the female protagonist comes close to falling in love with him), have no choice but to fight and ultimately kill him.
In "The Sorrow of Odin the Goth" a time-travelling American anthropologist is assigned to study the culture of an ancient Gothic tribe by regular visits every few decades. Gradually he is drawn into close involvement, feeling protective towards the Goths (many of them his own descendants, following a brief and poignant liaison with a Gothic girl who died in childbirth), and they identify him as the god Odin/Wodan. Then he finds that he must cruelly betray his beloved Goths, since a ballad says that Odin did so; failure to fulfill his prescribed role might change history and bring the whole of the Twentieth Century as we know it crashing down. In the final scene he cries out in anguish: "Not even the gods can defy the Norns!"—giving a new twist to this central aspect of the Norse religion.
In "The Pirate", the hero is duty-bound to deny a band of people from societies blighted by poverty the chance for a new start on a new planet, because their settling the planet would eradicate the remnants of the artistic and articulate beings who lived there before. A similar theme but with much higher stakes appears in "Sister Planet": although terraforming Venus would provide new hope to starving people on the overcrowded Earth, it would exterminate Venus's just-discovered intelligent race, and the hero can avert that genocide only by murdering his best friends.
In "Delenda Est" the stakes are the highest imaginable. Time-travelling outlaws have created a new 20th Century—"not better or worse, just completely different". The hero can fight the outlaws and restore his (and our) familiar history, but only at the price of totally destroying the world which has taken its place. "Risking your neck in order to negate a world full of people like yourself" is how the hero describes what he eventually undertakes.
Fictional appearances.
Philip K. Dick's story "Waterspider" features Poul Anderson as one of the main characters.
In the opening of S.M. Stirling's novel "In the Courts of the Crimson Kings", a group of science fiction authors, including Poul Anderson, watch first contact with the book's Martians while attending an SF convention. Poul supplies the beer.

</doc>
<doc id="23678" url="http://en.wikipedia.org/wiki?curid=23678" title="Panspermia">
Panspermia

Panspermia (from " "πᾶν" (pan)", meaning "all", and " "σπέρμα" (sperma)", meaning "seed") is the hypothesis that life exists throughout the Universe, distributed by meteoroids, asteroids, comets, planetoids, Self-replicating spacecraft, and also by spacecraft in the form of unintended contamination by microorganisms.
Panspermia is a hypothesis proposing that microscopic life forms that can survive the effects of space, such as extremophiles, become trapped in debris that is ejected into space after collisions between planets and small Solar System bodies that harbor life. Some organisms may travel dormant for an extended amount of time before colliding randomly with other planets or intermingling with protoplanetary disks. If met with ideal conditions on a new planet's surfaces, the organisms become active and the process of evolution begins. Panspermia is not meant to address how life began, just the method that may cause its distribution in the Universe.
Pseudo-panspermia (sometimes called "soft panspermia" or "molecular panspermia") argues that the pre-biotic organic building blocks of life originated in space and were incorporated in the solar nebula from which the planets condensed and were further —and continuously— distributed to planetary surfaces where life then emerged (abiogenesis). From the early 1970s it was becoming evident that interstellar dust consisted of a large component of organic molecules. Interstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. The dust plays a critical role of shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars.
Several simulations in laboratories and in low Earth orbit suggest that ejection, entry and impact is survivable for some simple organisms.
History.
The first known mention of the term was in the writings of the 5th century BC Greek philosopher Anaxagoras. Panspermia began to assume a more scientific form through the proposals of Jöns Jacob Berzelius (1834), Hermann E. Richter (1865), Kelvin (1871), Hermann von Helmholtz (1879) and finally reaching the level of a detailed hypothesis through the efforts of the Swedish chemist Svante Arrhenius (1903).
Sir Fred Hoyle (1915–2001) and Chandra Wickramasinghe (born 1939) were influential proponents of panspermia. In 1974 they proposed the hypothesis that some dust in interstellar space was largely organic (containing carbon), which Wickramasinghe later proved to be correct. Hoyle and Wickramasinghe further contended that life forms continue to enter the Earth's atmosphere, and may be responsible for epidemic outbreaks, new diseases, and the genetic novelty necessary for macroevolution.
In a presentation on April 7, 2009, physicist Stephen Hawking stated his opinion about what humans may find when venturing into space, such as the possibility of alien life through the theory of panspermia. 
Proposed mechanisms.
Panspermia can be said to be either interstellar (between star systems) or interplanetary (between planets in the same star system); its transport mechanisms may include comets, radiation pressure and lithopanspermia (microorganisms embedded in rocks). Interplanetary transfer of nonliving material is well documented, as evidenced by meteorites of Martian origin found on Earth. Space probes may also be a viable transport mechanism for interplanetary cross-pollination in our Solar System or even beyond. However, space agencies have implemented planetary protection procedures to reduce the risk of planetary contamination, although, as recently discovered, some microorganisms, such as Tersicoccus phoenicis, may be resistant to procedures used in spacecraft assembly clean room facilities. In 2012, mathematician Edward Belbruno and astronomers Amaya Moro-Martín and Renu Malhotra proposed that gravitational low energy transfer of rocks among the young planets of stars in their birth cluster is commonplace, and not rare in the general galactic stellar population. Deliberate directed panspermia from space to seed Earth or sent from Earth to seed other solar systems have also been proposed. One twist to the hypothesis by engineer Thomas Dehel (2006), proposes that plasmoid magnetic fields ejected from the magnetosphere may move the few spores lifted from the Earth's atmosphere with sufficient speed to cross interstellar space to other systems before the spores can be destroyed.
Radiopanspermia.
In 1903, Svante Arrhenius published in his article "The Distribution of Life in Space", the hypothesis now called radiopanspermia, that microscopic forms of life can be propagated in space, driven by the radiation pressure from stars. Arrhenius argued that particles at a critical size below 1.5 μm would be propagated at high speed by radiation pressure of the Sun. However, because its effectiveness decreases with increasing size of the particle, this mechanism holds for very tiny particles only, such as single bacterial spores. The main criticism of radiopanspermia hypothesis came from Shklovskii and Sagan, who pointed out the proofs of the lethal action of space radiations (UV and X-rays) in the cosmos. Regardless of the evidence, Wallis and Wickramasinghe argued in 2004 that the transport of individual bacteria or clumps of bacteria, is overwhelmingly more important than lithopanspermia in terms of numbers of microbes transferred, even accounting for the death rate of unprotected bacteria in transit.
Then, data gathered by the orbital experiments ERA, BIOPAN, EXOSTACK and EXPOSE, determined that isolated spores, including those of "B. subtilis", were killed by several orders of magnitude if exposed to the full space environment for a mere few seconds, but if shielded against solar UV, the spores were capable of surviving in space for up to 6 years while embedded in clay or meteorite powder (artificial meteorites). Though minimal protection is required to shelter a spore against UV radiation, exposure to solar UV and cosmic ionizing radiation of unprotected DNA, break it up into its bases. Also, exposing DNA to the ultrahigh vacuum of space alone is sufficient to cause DNA damage, so the transport of unprotected DNA or RNA during interplanetary flights is extremely unlikely.
Based on experimental data on radiation effects and DNA stability, it has been concluded that for such long travel times, boulder sized rocks which are greater than or equal to 1 meter in diameter are required to effectively shield resistant microorganisms, such as bacterial spores against galactic cosmic radiation. These results clearly negate the radiopanspermia hypothesis, which requires single spores accelerated by the radiation pressure of the Sun, requiring many years to travel between the planets, and support the likelihood of interplanetary transfer of microorganisms within asteroids or comets, the so-called lithopanspermia hypothesis.
Lithopanspermia.
Lithopanspermia, the transfer of organisms in rocks from one planet to another either through interplanetary or interstellar space, remains speculative. Although there is no evidence that lithopanspermia has occurred in our own Solar System, the various stages have become amenable to experimental testing.
Accidental panspermia.
Thomas Gold, a professor of astronomy, suggested in 1960 the hypothesis of "Cosmic Garbage", that life on Earth might have originated from a pile of waste products accidentally dumped on Earth long ago by extraterrestrial beings.
Directed panspermia.
Directed panspermia concerns the deliberate transport of microorganisms in space, sent to Earth to start life here, or sent from Earth to seed new solar systems with life by introduced species of microorganisms on lifeless planets. The Nobel prize winner Francis Crick, along with Leslie Orgel proposed that life may have been purposely spread by an advanced extraterrestrial civilization, but considering an early "RNA world" Crick noted later that life may have originated on Earth. It has been suggested that 'directed' panspermia was proposed in order to counteract various objections, including the argument that microbes would be inactivated by the space environment and cosmic radiation before they could make a chance encounter with Earth.
Conversely, active directed panspermia has been proposed to secure and expand life in space. This may be motivated by biotic ethics that values, and seeks to propagate, the basic patterns of our organic gene/protein life-form. The panbiotic program would seed new solar systems nearby, and clusters of new stars in interstellar clouds. These young targets, where local life would not have formed yet, avoid any interference with local life.
For example, microbial payloads launched by solar sails at speeds up to 0.0001 "c" (30,000 m/s) would reach targets at 10 to 100 light-years in 0.1 million to 1 million years. Fleets of microbial capsules can be aimed at clusters of new stars in star-forming clouds, where they may land on planets or captured by asteroids and comets and later delivered to planets. Payloads may contain extremophiles for diverse environments and cyanobacteria similar to early microorganisms. Hardy multicellular organisms (rotifer cysts) may be included to induce higher evolution.
The probability of hitting the target zone can be calculated from formula_1 where "A"(target) is the cross-section of the target area, "dy" is the positional uncertainty at arrival; "a" - constant (depending on units), "r"(target) is the radius of the target area; "v" the velocity of the probe; (tp) the targeting precision (arcsec/yr); and "d" the distance to the target, guided by high-resolution astrometry of 1×10−5 arcsec/yr (all units in SIU). These calculations show that relatively near target stars(Alpha PsA, Beta Pictoris) can be seeded by milligrams of launched microbes; while seeding the Rho Ophiochus star-forming cloud requires hundreds of kilograms of dispersed capsules.
Theoretically, unintended panspermia may occur by spacecraft travelling to other celestial bodies. This may concern space researchers who try to prevent contamination. However, directed panspermia may reach a few dozen target systems, leaving billions in the galaxy untouched. In any case, matter is exchanged by meteor impacts in the solar system even without human intervention.
Directed panspermia to secure and expand life in space is becoming possible due to developments in solar sails, precise astrometry, extrasolar planets, extremophiles and microbial genetic engineering. After determining the composition of chosen meteorites, astroecologists performed laboratory experiments that suggest that many colonizing microorganisms and some plants could obtain many of their chemical nutrients from asteroid and cometary materials. However, the scientists noted that phosphate (PO4) and nitrate (NO3–N) critically limit nutrition to many terrestrial lifeforms. With such materials, and energy from long-lived stars, microscopic life planted by directed panspermia could find an immense future in the galaxy.
A number of publications since 1979 have proposed the idea that directed panspermia could be demonstrated to be the origin of all life on Earth if a distinctive 'signature' message were found, deliberately implanted into either the genome or the genetic code of the first microorganisms by our hypothetical progenitor. In 2013 a team of physicists claimed that they had found mathematical and semiotic patterns in the genetic code which, they believe, is evidence for such a signature. Further investigations are needed.
A microscopic ball made of titanium and vanadium was found in Earth's upper atmosphere in early 2015. Milton Wainwright, a UK researcher and astrobiologist at the University of Buckingham claimed in a tabloid that the metal ball "could contain DNA." He speculates that it could be an alien device sent to Earth by extraterrestrials in order to continue seeding the planet with life.
Pseudo-panspermia.
Pseudo-panspermia (sometimes called soft panspermia, molecular panspermia or quasi-panspermia) proposes that the organic molecules used for life originated in space and were incorporated in the solar nebula, from which the planets condensed and were further —and continuously— distributed to planetary surfaces where life then emerged (abiogenesis). From the early 1970s it was becoming evident that interstellar dust consisted of a large component of organic molecules. The first suggestion came from Chandra Wickramasinghe, who proposed a polymeric composition based on the molecule formaldehyde (CH2O). Interstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. Usually this occurs when a molecule becomes ionized, often as the result of an interaction with cosmic rays. This positively charged molecule then draws in a nearby reactant by electrostatic attraction of the neutral molecule's electrons. Molecules can also be generated by reactions between neutral atoms and molecules, although this process is generally slower. The dust plays a critical role of shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars.
A 2008 analysis of 12C/13C isotopic ratios of organic compounds found in the Murchison meteorite indicates a non-terrestrial origin for these molecules rather than terrestrial contamination. Biologically relevant molecules identified so far include uracil, an RNA nucleobase, and xanthine. These results demonstrate that many organic compounds which are components of life on Earth were already present in the early Solar System and may have played a key role in life's origin.
In August 2009, NASA scientists identified one of the fundamental chemical building-blocks of life (the amino acid glycine) in a comet for the first time.
On August 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting building blocks of DNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space. In October 2011, scientists reported that cosmic dust contains complex organic matter ("amorphous organic solids with a mixed aromatic-aliphatic structure") that could be created naturally, and rapidly, by stars. One of the scientists suggested that these complex organic compounds may have been related to the development of life on Earth and said that, "If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life."
On August 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary "IRAS 16293-2422", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.
In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics - "a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons "for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks."
In 2013, the Atacama Large Millimeter Array (ALMA Project) confirmed that researchers have discovered an important pair of prebiotic molecules in the icy particles in interstellar space (ISM). The chemicals, found in a giant cloud of gas about 25,000 light-years from Earth in ISM, may be a precursor to a key component of DNA and the other may have a role in the formation of an important amino acid. Researchers found a molecule called cyanomethanimine, which produces adenine, one of the four nucleobases that form the “rungs” in the ladder-like structure of DNA. The other molecule, called ethanamine, is thought to play a role in forming alanine, one of the twenty amino acids in the genetic code. Previously, scientists thought such processes took place in the very tenuous gas between the stars. The new discoveries, however, suggest that the chemical formation sequences for these molecules occurred not in gas, but on the surfaces of ice grains in interstellar space. NASA ALMA scientist Anthony Remijan stated that finding these molecules in an interstellar gas cloud means that important building blocks for DNA and amino acids can 'seed' newly formed planets with the chemical precursors for life.
In March 2013, a simulation experiment indicate that dipeptides (pairs of amino acids) that can be building blocks of proteins, can be created in interstellar dust.
In February 2014, NASA announced a for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.
In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.
Extraterrestrial life.
The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old. According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe. Nonetheless, Earth is the only place in the universe known to harbor life. The sheer number of planets in the Milky Way galaxy, however, may make it probable that life has arisen somewhere else in the galaxy and the universe. It is generally agreed that the conditions required for the evolution of intelligent life as we know it are probably exceedingly rare in the universe, while simultaneously noting that simple single-celled microorganisms may be more likely.
The extrasolar planet results from the Kepler mission estimate 100–400 billion exoplanets, with over 3,500 as candidates or confirmed exoplanets. On 4 November 2013, astronomers reported, based on Kepler space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of sun-like stars and red dwarf stars within the Milky Way Galaxy. 11 billion of these estimated planets may be orbiting sun-like stars. The nearest such planet may be 12 light-years away, according to the scientists.
It is estimated that space travel over cosmic distances would take an incredibly long time to an outside observer, and with vast amounts of energy required. However, there are reasons to hypothesize that faster-than-light interstellar space travel might be feasible. This has been explored by NASA scientists since at least 1995.
Hypotheses on extraterrestrial sources of illnesses.
Hoyle and Wickramasinghe have speculated that several outbreaks of illnesses on Earth are of extraterrestrial origins, including the 1918 flu pandemic, and certain outbreaks of polio and mad cow disease. For the 1918 flu pandemic they hypothesized that cometary dust brought the virus to Earth simultaneously at multiple locations—a view almost universally dismissed by experts on this pandemic. Hoyle also speculated that HIV came from outer space. After Hoyle's death, "The Lancet" published a letter to the editor from Wickramasinghe and two of his colleagues, in which they hypothesized that the virus that causes severe acute respiratory syndrome (SARS) could be extraterrestrial in origin and not originated from chickens. "The Lancet" subsequently published three responses to this letter, showing that the hypothesis was not evidence-based, and casting doubts on the quality of the experiments referenced by Wickramasinghe in his letter. A 2008 encyclopedia notes that "Like other claims linking terrestrial disease to extraterrestrial pathogens, this proposal was rejected by the greater research community."
Hoaxes.
A separate fragment of the Orgueil meteorite (kept in a sealed glass jar since its discovery) was found in 1965 to have a seed capsule embedded in it, whilst the original glassy layer on the outside remained undisturbed. Despite great initial excitement, the seed was found to be that of a European Juncaceae or Rush plant that had been glued into the fragment and camouflaged using coal dust. The outer "fusion layer" was in fact glue. Whilst the perpetrator of this hoax is unknown, it is thought he sought to influence the 19th century debate on spontaneous generation — rather than panspermia — by demonstrating the transformation of inorganic to biological matter.
Extremophiles.
Until the 1970s, life was believed to depend on its access to sunlight. Even life in the ocean depths, where sunlight cannot reach, was believed to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible "Alvin", scientists discovered colonies of assorted creatures clustered around undersea volcanic features known as black smokers. It was soon determined that the basis for this food chain is a form of bacterium that derives its energy from oxidation of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. This chemosynthesis revolutionized the study of biology by revealing that terrestrial life need not be Sun-dependent; it only requires water and an energy gradient in order to exist.
It is now known that extremophiles, microorganisms with extraordinary capability to thrive in the harshest environments on Earth, can specialize to thrive in the deep-sea, ice, boiling water, acid, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life. Living bacteria found in ice core samples retrieved from 3700 m deep at Lake Vostok in Antarctica, have provided data for extrapolations to the likelihood of microorganisms surviving frozen in extraterrestrial habitats or during interplanetary transport. Also, bacteria have been discovered living within warm rock deep in the Earth's crust.
In order to test some these organism's potential resilience in outer space, plant seeds and spores of bacteria, fungi and ferns have been exposed to the harsh space environment. Spores are produced as part of the normal life cycle of many plants, algae, fungi and some protozoans, and some bacteria produce endospores or cysts during times of stress. These structures may be highly resilient to ultraviolet and gamma radiation, desiccation, lysozyme, temperature, starvation and chemical disinfectants, while metabolically inactive. Spores germinate when favourable conditions are restored after exposure to conditions fatal to the parent organism.
Although computer models suggest that a captured meteoroid would typically take some tens of millions of years before collision with a neighboring solar system planet, there are documented viable Earthly bacterial spores that are 40 million years old that are very resistant to radiation, and others able to resume life after being dormant for 25 million years, suggesting that lithopanspermia life-transfers are possible via meteorites exceeding 1m in size.
The discovery of deep-sea ecosystems, along with advancements in the fields of astrobiology, observational astronomy and discovery of large varieties of extremophiles, opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats and possible transport of hardy microbial life through vast distances.
Research in outer space.
The question of whether certain microorganisms can survive in the harsh environment of outer space has intrigued biologists since the beginning of spaceflight, and opportunities were provided to expose samples to space. The first tests were made in 1966, during the Gemini IX and XII missions, when samples of bacteriophage T1 and spores of "Penicillium roqueforti" were exposed to outer space for 16.8 h and 6.5 h, respectively. Other basic life sciences research in low Earth orbit started in 1966 with the Soviet biosatellite program Bion and the U.S. Biosatellite program. Thus, the plausibility of panspermia can be evaluated by examining life forms on Earth for their capacity to survive in space. The following experiments carried on low Earth orbit specifically tested some aspects of panspermia or lithopanspermia:
ERA.
The Exobiology Radiation Assembly (ERA) was a 1992 experiment on board the European Retrievable Carrier (EURECA) on the biological effects of space radiation. EURECA was an unmanned 4.5 tonne satellite with a payload of 15 experiments. It was an astrobiology mission developed by the European Space Agency (ESA). Spores of different strains of "Bacillus subtilis" and the "Escherichia coli" plasmid pUC19 were exposed to selected conditions of space (space vacuum and/or defined wavebands and intensities of solar ultraviolet radiation). After the approximately 11-month mission, their responses were studied in terms of survival, mutagenesis in the "his" ("B. subtilis") or "lac" locus (pUC19), induction of DNA strand breaks, efficiency of DNA repair systems, and the role of external protective agents. The data were compared with those of a simultaneously running ground control experiment:
BIOPAN.
BIOPAN is a multi-user experimental facility installed on the external surface of the Russian Foton descent capsule. Experiments developed for BIOPAN are designed to investigate the effect of the space environment on biological material after exposure between 13 to 17 days. The experiments in BIOPAN are exposed to solar and cosmic radiation, the space vacuum and weightlessness, or a selection thereof. Of the 6 missions flown so far on BIOPAN between 1992 and 2007, dozens of experiments were conducted, and some analyzed the likelihood of panspermia. Some bacteria, lichens ("Xanthoria elegans", "Rhizocarpon geographicum" and their mycobiont cultures, the black Antarctic microfungi "Cryomyces minteri" and "Cryomyces antarcticus"), spores, and even one animal (tardigrades) were found to have survived the harsh outer space environment and cosmic radiation.
EXOSTACK.
The German EXOSTACK experiment was deployed in 7 April 1984 on board the Long Duration Exposure Facility statellite. 30% of "Bacillus subtilis" spores survived the nearly 6 years exposure when embedded in salt crystals, whereas 80% survived in the presence of glucose, which stabilize the structure of the cellular macromolecules, especially during vacuum-induced dehydration.
If shielded against solar UV, spores of "B. subtilis" were capable of surviving in space for up to 6 years, especially if embedded in clay or meteorite powder (artificial meteorites). The data support the likelihood of interplanetary transfer of microorganisms within meteorites, the so-called lithopanspermia hypothesis.
EXPOSE.
EXPOSE is a multi-user facility mounted outside the International Space Station dedicated to astrobiology experiments. Results from the orbital mission, especially the experiments "SEEDS" and "LiFE", concluded that after an 18-month exposure, some seeds and lichens ("Stichococcus sp." and "Acarospora sp"., a lichenized fungal genus) may be capable to survive interplanetary travel if sheltered inside comets or rocks from cosmic radiation and UV radiation. The survival of some lichen species in space has also been characterized in simulated laboratory experiments.
A separate experiment on EXPOSE called Beer was designed to find microbes that could be used in life-support recycling equipment and future "bio-mining" projects on Mars. It carried group of microbes called OU-20 resembling cyanobacteria genus "Gloeocapsa", and it survived 553 days exposure outside the ISS.
Rosetta.
In 2014, the "Rosetta" spacecraft arrived at COMET 67P/Churyumov–Gerasimenko. A few months after arriving at the comet, "Rosetta" released a small lander, named "Philae", onto its surface. The plan was to investigate Churyumov-Gerasimenko up close for two years. "Philae's" battery has since died; however scientists hope that as the comet travels toward the sun greater solar energy will recharge "Philae" (via its solar panels) and "Philae" will resume operation. Rosetta's Project Scientist, Gerhard Schwehm, stated that sterilization is generally not crucial since comets are usually regarded as objects where prebiotic molecules can be found, but not living microorganisms. Notwithstanding, other scientists think it will be an opportunity to gather evidence for one of panspermia's hypotheses: the possibility of both active and dormant microbes inside comets.
Phobos LIFE.
The "Phobos LIFE" or "Living Interplanetary Flight Experiment", was developed by the Planetary Society and intended to send selected microorganisms on a three-year interplanetary round-trip in a small capsule aboard the Russian Fobos-Grunt spacecraft in 2011. Unfortunately, the spacecraft suffered technical difficulties soon after launch and fell back to Earth, so the experiment was never carried out. The experiment would have tested one aspect of panspermia: lithopanspermia, the hypothesis that life could survive space travel, if protected inside rocks blasted by impact off one planet to land on another.

</doc>
<doc id="23680" url="http://en.wikipedia.org/wiki?curid=23680" title="There's Plenty of Room at the Bottom">
There's Plenty of Room at the Bottom

"There's Plenty of Room at the Bottom" was a lecture given by physicist Richard Feynman at an American Physical Society meeting at Caltech on December 29, 1959. Feynman considered the possibility of direct manipulation of individual atoms as a more powerful form of synthetic chemistry than those used at the time. The talk went unnoticed and it didn't inspire the conceptual beginnings of the field. In the 1990s it was rediscovered and publicised as a seminal event in the field, probably to boost the history of nanotechnology with Feynman's reputation.
Conception.
Feynman considered a number of interesting ramifications of a general ability to manipulate matter on an atomic scale. He was particularly interested in the possibilities of denser computer circuitry, and microscopes which could see things much smaller than is possible with scanning electron microscopes. These ideas were later realized by the use of the scanning tunneling microscope, the atomic force microscope and other examples of scanning probe microscopy and storage systems such as Millipede, created by researchers at IBM.
Feynman also suggested that it should be possible, in principle, to make nanoscale machines that "arrange the atoms the way we want", and do chemical synthesis by mechanical manipulation.
He also presented the "weird possibility" of "swallowing the doctor," an idea which he credited in the essay to his friend and graduate student Albert Hibbs. This concept involved building a tiny, swallowable surgical robot by developing a set of one-quarter-scale manipulator hands slaved to the operator's hands to build one-quarter scale machine tools analogous to those found in any machine shop. This set of small tools would then be used by the small hands to build and operate ten sets of one-sixteenth-scale hands and tools, and so forth, culminating in perhaps a billion tiny factories to achieve massively parallel operations. He uses the analogy of a pantograph as a way of scaling down items. This idea was anticipated in part, down to the microscale, by science fiction author Robert A. Heinlein in his 1942 story "Waldo".
As the sizes got smaller, one would have to redesign some tools, because the relative strength of various forces would change. Although gravity would become unimportant, surface tension would become more important, Van der Waals attraction would become important, etc. Feynman mentioned these scaling issues during his talk. Nobody has yet attempted to implement this thought experiment, although it has been noted that some types of biological enzymes and enzyme complexes (especially ribosomes) function chemically in a way close to Feynman's vision.
Challenges.
At the meeting, Feynman concluded his talk with two challenges, and he offered a prize of $1000 for the first individuals to solve each one. The first challenge involved the construction of a tiny motor, which, to Feynman's surprise, was achieved by November 1960 by William McLellan, a meticulous craftsman, using conventional tools. The motor met the conditions, but did not advance the art. The second challenge involved the possibility of scaling down letters small enough so as to be able to fit the entire Encyclopædia Britannica on the head of a pin, by writing the information from a book page on a surface 1/25,000 smaller in linear scale. In 1985, Tom Newman, a Stanford graduate student, successfully reduced the first paragraph of "A Tale of Two Cities" by 1/25,000, and collected the second Feynman prize.
Impact.
K. Eric Drexler later took the Feynman concept of a billion tiny factories and added the idea that they could make more copies of themselves, via computer control instead of control by a human operator, in his 1986 book "Engines of Creation: The Coming Era of Nanotechnology".
After Feynman's death, scholars studying the historical development of nanotechnology have concluded that his actual role in catalyzing nanotechnology research was limited, based on recollections from many of the people active in the nascent field in the 1980s and 1990s. Chris Toumey, a cultural anthropologist at the University of South Carolina, has reconstructed the history of the publication and republication of Feynman’s talk, along with the record of citations to “Plenty of Room” in the scientific literature. In Toumey's 2008 article, "Reading Feynman into Nanotechnology", he found 11 versions of the publication of “Plenty of Room", plus two instances of a closely related talk by Feynman, “Infinitesimal Machinery,” which Feynman called “Plenty of Room, Revisited.” Also in Toumey’s references are videotapes of that second talk.
Toumey found that the published versions of Feynman’s talk had a negligible influence in the twenty years after it was first published, as measured by citations in the scientific literature, and not much more influence in the decade after the Scanning Tunneling Microscope was invented in 1981. Subsequently, interest in “Plenty of Room” in the scientific literature greatly increased in the early 1990s. This is probably because the term “nanotechnology” gained serious attention just before that time, following its use by Drexler in his 1986 book, "Engines of Creation: The Coming Era of Nanotechnology", which cited Feynman, and in a cover article headlined "Nanotechnology", published later that year in a mass-circulation science-oriented magazine, "OMNI". The journal "Nanotechnology" was launched in 1989; the famous Eigler-Schweizer experiment, precisely manipulating 35 xenon atoms, was published in "Nature" in April 1990; and "Science" had a special issue on nanotechnology in November 1991. These and other developments hint that the retroactive rediscovery of Feynman’s “Plenty of Room” gave nanotechnology a packaged history that provided an early date of December 1959, plus a connection to the charisma and genius of Richard Feynman.
Toumey’s analysis also includes comments from distinguished scientists in nanotechnology who say that “Plenty of Room” did not influence their early work, and in fact most of them had not read it until a later date.
Feynman's stature as a Nobel laureate and as an iconic figure in 20th century science surely helped advocates of nanotechnology and provided a valuable intellectual link to the past. More concretely, his stature and concept of atomically precise fabrication played a role in securing funding for nanotechnology research, illustrated by President Clinton January 2000 speech calling for a Federal program:
My budget supports a major new National Nanotechnology Initiative, worth $500 million. Caltech is no stranger to the idea of nanotechnology the ability to manipulate matter at the atomic and molecular level. Over 40 years ago, Caltech's own Richard Feynman asked, "What would happen if we could arrange the atoms one by one the way we want them?"
References.
</dl>

</doc>
<doc id="23681" url="http://en.wikipedia.org/wiki?curid=23681" title="Philately">
Philately

Philately ( ) is the study of stamps and postal history and other related items. Philately involves more than just stamp collecting, which does not necessarily involve the study of stamps. It is possible to be a philatelist without owning any stamps. For instance, the stamps being studied may be very rare, or reside only in museums.
Etymology.
The word "philately" is the English version of the French word "philatélie", coined by Georges Herpin in 1864. Herpin stated that stamps had been collected and studied for the previous six or seven years and a better name was required for the new hobby than "timbromanie", which was disliked. He took the Greek root word φιλ(ο)- "phil(o)-", meaning "an attraction or affinity for something", and ἀτέλεια "ateleia", meaning "exempt from duties and taxes" to form "philatelie". The introduction of postage stamps meant that the receipt of letters was now free of charge, whereas before stamps it was normal for postal charges to be paid by the recipient of a letter.
The alternative terms "timbromania", "timbrophily" and "timbrology" gradually fell out of use as "philately" gained acceptance during the 1860s.
Origins.
The origins of philately lie in the observation that in a number of apparently similar stamps, closer examination may reveal differences in the printed design, paper, watermark, colour, perforations and other areas of the stamp. Comparison with the records of postal authorities may or may not show that the variations were intentional, which leads to further inquiry as to how the changes could have happened, and why. To make things more interesting, thousands of forgeries have been produced over the years, some of them very good, and only a thorough knowledge of philately gives any hope of detecting the fakes.
Types.
Traditional philately is the study of the technical aspects of stamp production and stamp identification, including:
Tools.
Philately uses a number of tools, including stamp tongs (a specialized form of tweezers) to safely handle the stamps, a strong magnifying glass and a perforation gauge (odontometer) to measure the perforation gauge of the stamp.
The identification of watermarks is important and may be done with the naked eye by turning the stamp over or holding it up to the light. If this fails then "watermark fluid" may be used, which "wets" the stamp to reveal the mark.
Other common tools include stamp catalogues, stamp stock books and stamp hinges.
Organisations.
Philatelic organisations sprang up soon after people started collecting and studying stamps. They include local, national and international clubs and societies where collectors come together to share their hobby.

</doc>
<doc id="23682" url="http://en.wikipedia.org/wiki?curid=23682" title="Puget Sound">
Puget Sound

Puget Sound is a sound along the northwestern coast of the U.S. state of Washington, an inlet of the Pacific Ocean, and part of the Salish Sea. It is a complex estuarine system of interconnected marine waterways and basins, with one major and one minor connection to the open Pacific Ocean via the Strait of Juan de Fuca—Admiralty Inlet being the major connection and Deception Pass being the minor. Flow through Deception Pass is approximately equal to 2% of the total tidal exchange between Puget Sound and the Strait of Juan de Fuca. Puget Sound extends approximately 100 mi from Deception Pass in the north to Olympia, Washington in the south. Its average depth is 450 ft and its maximum depth, off Point Jefferson between Indianola and Kingston, is 930 ft. The depth of the main basin, between the southern tip of Whidbey Island and Tacoma, Washington, is approximately 600 ft.
The term "Puget Sound" is used not just for the body of water but also the Puget Sound region centered on the sound.
Name and definition.
In 1792 George Vancouver gave the name "Puget's Sound" to the waters south of the Tacoma Narrows, in honor of Peter Puget, a Huguenot lieutenant accompanying him on the Vancouver Expedition. This name later came to be used for the waters north of Tacoma Narrows as well.
The USGS defines Puget Sound as all the waters south of three entrances—the main entrance at Admiralty Inlet being a line between Point Wilson, on the Olympic Peninsula, and Point Partridge, on Whidbey Island; a second entrance at Deception Pass being a line from West Point, on Whidbey Island, to Deception Island and Rosario Head, on Fidalgo Island; and a third entrance at the south end of the Swinomish Channel, which connects Skagit Bay and Padilla Bay. Under this definition, Puget Sound includes the waters of Hood Canal, Admiralty Inlet, Possession Sound, Saratoga Passage, and others. It does not include Bellingham Bay, Padilla Bay, the waters of the San Juan Islands or anything farther north.
Another definition, given by NOAA, subdivides Puget Sound into five basins or regions. Four of these correspond to areas within the USGS definition, but the fifth one, called "Northern Puget Sound" includes a large additional region. It is defined as bounded to the north by the international boundary with Canada, and to the west by a line running north from the mouth of the Sekiu River on the Olympic Peninsula. Under this definition significant parts of the Strait of Juan de Fuca and the Strait of Georgia are included in Puget Sound, with the international boundary marking an abrupt and hydrologically arbitrary limit.
According to Arthur Kruckeberg, the term "Puget Sound" is sometimes used for waters north of Admiralty Inlet and Deception Pass, especially for areas along the north coast of Washington and the San Juan Islands, essentially equivalent to NOAA's "Northern Puget Sound" subdivision described above. Kruckeberg uses the term "Puget Sound and adjacent waters".
An alternative term for Puget Sound, still used by some Native Americans and environmental groups, is "Whulge" (or "Whulj"), an Anglicization of the Lushootseed name "'WulcH", which means "Salt Water". Since 2009 the term Salish Sea has been established by the United States Board on Geographic Names as the collective waters of Puget Sound, the Strait of Juan de Fuca, and the Strait of Georgia. Sometimes the terms "Puget Sound" and "Puget Sound and adjacent waters" are used for not only Puget Sound proper but also for waters to the north, such as Bellingham Bay and the San Juan Islands region.
Geology.
Continental ice sheets have repeatedly advanced and retreated from the Puget Sound region. The most recent glacial period, called the Fraser Glaciation, had three phases, or stades. During the third, or Vashon Glaciation, a lobe of the Cordilleran Ice Sheet, called the Puget Lobe, spread south about 15,000 years ago, covering the Puget Sound region with an ice sheet about 3000 ft thick near Seattle, and nearly 6000 ft at the present Canada-US border. Since each new advance and retreat of ice erodes away much of the evidence of previous ice ages, the most recent Vashon phase has left the clearest imprint on the land. At its maximum extent the Vashon ice sheet extended south of Olympia to near Tenino, and covered the lowlands between the Olympic and Cascade mountain range. About 14,000 years ago the ice began to retreat. By 11,000 years ago it survived only north of the Canadian border.
The melting retreat of the Vashon Glaciation eroded the land, creating a drumlin field of hundreds of aligned drumlin hills. Lake Washington and Lake Sammamish (which are ribbon lakes), Hood Canal, and the main Puget Sound basin were altered by glacial forces. These glacial forces are not specifically "carving", as in cutting into the landscape via the mechanics of ice/glaciers, but rather eroding the landscape from melt water of the Vashon Glacier creating the drumlin field. As the ice retreated, vast amounts of glacial till were deposited throughout the Puget Sound region.[1] The soils of the region, less than ten thousand years old, are still characterized as immature.
As the Vashon glacier receded a series of proglacial lakes formed, filling the main trough of Puget Sound and inundating the southern lowlands. Glacial Lake Russell was the first such large recessional lake. From the vicinity of Seattle in the north the lake extended south to the Black Hills, where it drained south into the Chehalis River. Sediments from Lake Russell form the blue-gray clay identified as the Lawton Clay. The second major recessional lake was Glacial Lake Bretz. It also drained to the Chehalis River until the Chimacum Valley, in the northeast Olympic Peninsula, melted, allowing the lake's water to rapidly drain north into the marine waters of the Strait of Juan de Fuca, which was rising as the ice sheet retreated.
The depth of the basins is a result of the Sound being part of the Cascadia subduction zone, where the terranes accreted at the edge of the Juan de Fuca Plate are being subducted under the North American Plate. There has not been a major subduction zone earthquake here since the magnitude nine Cascadia Earthquake; according to Japanese records, it occurred 26 January 1700. Lesser Puget Sound earthquakes with shallow epicenters, caused by the fracturing of stressed oceanic rocks as they are subducted, still cause great damage. The Seattle Fault cuts across Puget Sound, crossing the southern tip of Bainbridge Island and under Elliott Bay. To the south, the existence of a second fault, the Tacoma Fault, has buckled the intervening strata in the Seattle Uplift.
Typical Puget Sound profiles of dense glacial till overlying permeable glacial outwash of gravels above an impermeable bed of silty clay may become unstable after periods of unusually wet weather and slump in landslides.
Hydrology.
The United States Geological Survey (USGS) defines Puget Sound as a bay with numerous channels and branches; more specifically, it is a fjord system of flooded glacial valleys. Puget Sound is part of a larger physiographical structure termed the Puget Trough, which is a physiographic section of the larger Pacific Border province, which in turn is part of the larger Pacific Mountain System.
Puget Sound is a large salt water estuary, or system of many estuaries, fed by highly seasonal freshwater from the Olympic and Cascade Mountain watersheds. The mean annual river discharge into Puget Sound is 41000 cuft/s, with a monthly average maximum of about 367000 cuft/s and minimum of about 14000 cuft/s. Puget Sound's shoreline is 1332 mi long, encompassing a water area of 1020 sqmi and a total volume of 26.5 mi3 at mean high water. The average volume of water flowing in and out of Puget Sound during each tide is 1.26 mi3. The maximum tidal currents, in the range of 9 to 10 knots, occurs at Deception Pass.
The Puget Sound system consists of four deep basins connected by shallower sills. The four basins are Hood Canal, west of the Kitsap Peninsula, Whidbey Basin, east of Whidbey Island, South Sound, south of the Tacoma Narrows, and the Main Basin, which is further subdivided into Admiralty Inlet and the Central Basin. Puget Sound's sills, a kind of submarine terminal moraine, separate the basins from one another, and Puget Sound from the Strait of Juan de Fuca. Three sills are particularly significant—the one at Admiralty Inlet which checks the flow of water between the Strait of Juan de Fuca and Puget sound, the one at the entrance to Hood Canal (about 175 ft below the surface), and the one at the Tacoma Narrows (about 145 ft). Other sills that present less of a barrier include the ones at Blake Island, Agate Pass, Rich Passage, and Hammersley Inlet.
The size of Puget Sound's watershed is 12138 sqmi. "Northern Puget Sound" is frequently considered part of the Puget Sound watershed, which enlarges its size to 13700 sqmi. The USGS uses the name "Puget Sound" for its hydrologic unit subregion 1711, which includes areas draining to the Puget Sound proper as well as the Strait of Juan de Fuca, the Strait of Georgia, and the Fraser River. Significant rivers that drain to "Northern Puget Sound" include the Nooksack, Dungeness, and Elwha Rivers. The Nooksack empties into Bellingham Bay, the Dungeness and Elwha into the Strait of Juan de Fuca. The Chilliwack River flows north to the Fraser River in Canada.
Tides in Puget Sound are of the mixed type with two high and two low tides each tidal day. These are called Higher High Water (HHW), Lower Low Water (LLW), Lower High Water (LHW), and Higher Low Water (HLW). The configuration of basins, sills, and interconnections cause the tidal range to increase within Puget Sound. The difference in height between the Higher High Water and the Lower Low Water averages about 8.3 ft at Port Townsend on Admiralty Inlet, but increases to about 14.4 ft at Olympia, the southern end of Puget Sound.
Puget Sound is generally accepted as the start of the Inside Passage.
Flora and fauna.
Important marine flora of Puget Sound include eelgrass ("Zostera marina") and kelp, especially bull kelp ("Nereocystis luetkeana").
Among the marine mammals species found in Puget Sound are harbor seals ("Phoca vitulina"). Orca ("Orcinus orca") are famous throughout the Sound, and are a large tourist attraction. Although orca are sometimes seen in Puget Sound proper they are far more prevalent around the San Juan Islands north of Puget Sound.
Many fish species occur in Puget Sound. The various salmonid species, including salmon, trout, and char are particularly well-known and studied. Salmonid species of Puget Sound include chinook salmon ("Oncorhynchus tshawytscha"), chum salmon ("O. keta"), coho salmon ("O. kisutch"), pink salmon ("O. gorbuscha"), sockeye salmon ("O. nerka"), sea-run coastal cutthroat trout ("O. clarki clarki"), steelhead ("O. mykiss irideus"), sea-run bull trout ("Salvelinus confluentus"), and Dolly Varden trout ("Salvelinus malma malma").
Common forage fishes found in Puget Sound include Pacific herring ("Clupea pallasii"), surf smelt ("Hypomesus pretiosus"), and Pacific sand lance ("Ammodytes hexapterus"). Important benthopelagic fish of Puget Sound include North Pacific hake ("Merluccius productus"), Pacific cod ("Gadus macrocelhalus"), walleye pollock ("Theragra chalcogramma"), and the spiny dogfish ("Squalus acanthias"). There are about 28 species of Sebastidae (rockfish), of many types, found in Puget Sound. Among those of special interest are copper rockfish ("Sebastes caurinus"), quillback rockfish ("S. maliger"), black rockfish ("S. melanops"), yelloweye rockfish ("S. ruberrimus"), bocaccio rockfish ("S. paucispinis"), canary rockfish ("S. pinniger"), and Puget Sound rockfish ("S. emphaeus").
Many other fish species occur in Puget Sound, such as sturgeons, lampreys, various sharks, rays, and skates.
Puget Sound is home to numerous species of marine invertebrates, including sponges, sea anemones, chitons, clams, sea snails, limpets crabs, barnacles starfish, sea urchins, and sand dollars. Dungeness crabs ("Metacarcinus magister") occur throughout Washington waters, including Puget Sound. Many bivalves occur in Puget Sound, such as Pacific oysters ("Crassostrea gigas") and geoduck clams ("Panopea generosa"). The Olympia oyster ("Ostreola conchaphila"), once common in Puget Sound, was depleted by human activities during the 20th century. There are ongoing efforts to restore Olympia oysters in Puget Sound.
There are many seabird species of Puget Sound. Among these are grebes such as the western grebe ("Aechmophorus occidentalis"); loons such as the common loon ("Gavia immer"); auks such as the pigeon guillemot ("Cepphus columba"), rhinoceros auklet ("Cerorhinca monocerata"), common murre ("Uria aalge"), and marbled murrelet ("Brachyramphus marmoratus"); the brant goose ("Branta bernicla"); seaducks such as the long-tailed duck ("Clangula hyemalis"), harlequin duck ("Histrionicus histrionicus"), and surf scoter ("Melanitta perspicillata"); and cormorants such as the double-crested cormorant ("Phalacrocorax auritus"). Puget Sound is home to a non-migratory and marine-oriented subspecies of great blue herons ("Ardea herodias fannini"). Bald eagles ("Haliaeetus leucocephalus") occur in relative high densities in the Puget Sound region.
History.
George Vancouver explored Puget Sound in 1792. Vancouver claimed it for Great Britain on 4 June 1792, naming it for one of his officers, Lieutenant Peter Puget.
After 1818 Britain and the United States, which both claimed the Oregon Country, agreed to "joint occupancy", deferring resolution of the Oregon boundary dispute until the 1846 Oregon Treaty. Puget Sound was part of the disputed region until 1846, after which it became US territory.
American maritime fur traders visited Puget Sound in the early 19th century.
The first European settlement in the Puget Sound area was Fort Nisqually, a fur trade post of the Hudson's Bay Company (HBC) built in 1833. Fort Nisqually was part of the HBC's Columbia District, headquartered at Fort Vancouver. The Puget Sound Agricultural Company, a subsidiary of the HBC, established farms and ranches near Fort Nisqually. British ships such as the "Beaver", exported foodstuffs and provisions from Fort Nisqually.
The first American settlement on Puget Sound was Tumwater. It was founded in 1845 by Americans who had come via the Oregon Trail. The decision to settle north of the Columbia River was made in part because one of the settlers, George Washington Bush, was considered black and the Provisional Government of Oregon banned the residency of mulattoes but did not actively enforce the restriction north of the river.
In 1853 Washington Territory was formed from part of Oregon Territory. In 1888 the Northern Pacific railroad line reached Puget Sound, linking the region to eastern states.
Transportation.
A unique state-run ferry system, the Washington State Ferries, connects the larger islands to the Washington mainland, as well as both sides of the sound, allowing people and cars to move about the greater Puget Sound region.
View southwest from the Space Needle, overlooking (left to right) Elliott Bay, Duwamish Head, Puget Sound, and Restoration Point.
Environmental issues.
In the past 30 years there has been a large recession in the populations of the species which inhabit the Puget Sound. The decrease has been seen in the populations of: forage fish, salmonids, bottom fish, marine birds, harbor porpoise and orcas. This decline is attributed to the various environmental issues in Puget Sound. Because of this population decline, there have been changes to the fishery practices, and an increase in petitioning to add species to the Endangered Species Act. There has also been an increase in recovery and management plans for many different area species.
The causes of these environmental issues are toxic contamination, eutrophication (low oxygen due to excess nutrients), and near shore habitat changes.
In popular media.
When talking about a crucial piece of evidence in episode 4 of the 4th season of "The Killing", Detective Linden says "It's probably somewhere in the Puget Sound by now."
In the best-selling book "Fifty Shades Of Grey" Puget Sound is referenced many times.
The Puget Sound is mentioned in multiple songs including "Hello Seattle" by Owl City, "This Place is a Prison" by The Postal Service, "Volcano" by The Presidents of the United States of America and "Frances Farmer Will Have Her Revenge On Seattle" by Nirvana among others.
"Snow Falling on Cedars", by David Guterson, is set on the fictional San Piedro Island in the northern Puget Sound region of the state of Washington coast in 1954.
One of the videogame "World at war"'s missions takes place in Puget Sound

</doc>
<doc id="23688" url="http://en.wikipedia.org/wiki?curid=23688" title="Perjury">
Perjury

Perjury, also known as forswearing, is the intentional act of swearing a false oath or of falsifying an affirmation to tell the truth, whether spoken or in writing, concerning matters material to an official proceeding. Contrary to popular misconception, no crime has occurred when a false statement is (intentionally or unintentionally) made while under oath or subject to penalty—instead, criminal culpability only attaches at the instant the declarant falsely asserts the truth of statements (made or to be made) which are material to the outcome of the proceeding. For example, it is not perjury to lie about one's age except where age is a fact material to influencing the legal result, such as eligibility for old age retirement benefits or whether a person was of an age to have legal capacity.
Perjury is considered a serious offence as it can be used to usurp the power of the courts, resulting in miscarriages of justice. In the United States, for example, the general perjury statute under Federal law classifies perjury as a felony and provides for a prison sentence of up to five years. The California Penal Code allows for perjury to be a capital offense in cases causing wrongful execution. However prosecutions for perjury are rare. In some countries such as France and Italy, suspects cannot be heard under oath or affirmation and thus cannot commit perjury, regardless of what they say during their trial.
The rules for perjury also apply when a person has made a statement "under penalty of perjury", even if the person has not been sworn or affirmed as a witness before an appropriate official. An example of this is the United States' income tax return, which, by law, must be signed as true and correct under penalty of perjury (see #redirect ). Federal tax law provides criminal penalties of up to three years in prison for violation of the tax return perjury statute. See: 26 U.S.C. 
Statements which entail an "interpretation" of fact are not perjury because people often draw inaccurate conclusions unwittingly, or make honest mistakes without the intent to deceive. Individuals may have honest but mistaken beliefs about certain facts, or their recollection may be inaccurate, or may have a different perception of what is the accurate way to state the truth. Like most other crimes in the common law system, to be convicted of perjury one must have had the "intention" ("mens rea") to commit the act, and to have "actually committed" the act ("actus reus"). Further, statements that "are facts" cannot be considered perjury, even if they might arguably constitute an omission, and it is not perjury to lie about matters immaterial to the legal proceeding.
Subornation of perjury, attempting to induce another person to commit perjury, is itself a crime.
Canada.
The offence of perjury is created by section 132 of the Criminal Code. It is defined by section 131, which provides:
(1) Subject to subsection (3), every one commits perjury who, with intent to mislead, makes before a person who is authorized by law to permit it to be made before him a false statement under oath or solemn affirmation, by affidavit, solemn declaration or deposition or orally, knowing that the statement is false.
(1.1) Subject to subsection (3), every person who gives evidence under subsection 46(2) of the Canada Evidence Act, or gives evidence or a statement pursuant to an order made under section 22.2 of the Mutual Legal Assistance in Criminal Matters Act, commits perjury who, with intent to mislead, makes a false statement knowing that it is false, whether or not the false statement was made under oath or solemn affirmation in accordance with subsection (1), so long as the false statement was made in accordance with any formalities required by the law of the place outside Canada in which the person is virtually present or heard.
(2) Subsection (1) applies, whether or not a statement referred to in that subsection is made in a judicial proceeding.
(3) Subsections (1) and (1.1) do not apply to a statement referred to in either of those subsections that is made by a person who is not specially permitted, authorized or required by law to make that statement.
This section was amended by section 17 of R.S., 1985, c. 27 (1st Supp.) and by section 92 of the Act 1999 c. 18.
As to corroboration, see section 133.
Mode of trial and sentence
Every one who commits perjury is guilty of an indictable offence and liable to imprisonment for a term not exceeding fourteen years.
Republic of Ireland.
A person who before the Court of Justice of the European Communities swears anything which he knows to be false or does not believe to be true is, whatever his nationality, guilty of perjury. Proceedings for this offence may be taken in any place in the State and the offence may for all incidental purposes be treated as having been committed in that place.
United Kingdom.
England and Wales.
Perjury is a statutory offence in England and Wales. It is created by section 1(1) of the Perjury Act 1911. Section 1 of that Act reads:
(1) If any person lawfully sworn as a witness or as an interpreter in a judicial proceeding wilfully makes a statement material in that proceeding, which he knows to be false or does not believe to be true, he shall be guilty of perjury, and shall, on conviction thereof on indictment, be liable to penal servitude for a term not exceeding seven years, or to imprisonment . . . for a term not exceeding two years, or to a fine or to both such penal servitude or imprisonment and fine.<br>
(2) The expression "judicial proceeding" includes a proceeding before any court, tribunal, or person having by law power to hear, receive, and examine evidence on oath.<br>
(3) Where a statement made for the purposes of a judicial proceeding is not made before the tribunal itself, but is made on oath before a person authorised by law to administer an oath to the person who makes the statement, and to record or authenticate the statement, it shall, for the purposes of this section, be treated as having been made in a judicial proceeding.<br>
(4) A statement made by a person lawfully sworn in England for the purposes of a judicial proceeding-
shall, for the purposes of this section, be treated as a statement made in a judicial proceeding in England.<br>
(5) Where, for the purposes of a judicial proceeding in England, a person is lawfully sworn under the authority of an Act of Parliament-
a statement made by such person so sworn as aforesaid (unless the Act of Parliament under which it was made otherwise specifically provides) shall be treated for the purposes of this section as having been made in the judicial proceeding in England for the purposes whereof it was made.<br>
(6) The question whether a statement on which perjury is assigned was material is a question of law to be determined by the court of trial.
The words omitted from section 1(1) were repealed by section 1(2) of the Criminal Justice Act 1948.
A person guilty of an offence under section 11(1) of the European Communities Act 1972 may be proceeded against and punished in England and Wales as for an offence under section 1(1).
Section 1(4) has effect in relation to proceedings in the Court of Justice of the European Communities as it has effect in relation to a judicial proceeding in a tribunal of a foreign state.
Section 1(4) applies in relation to proceedings before a relevant convention court under the European Patent Convention as it applies to a judicial proceeding in a tribunal of a foreign state.
A statement made on oath by a witness outside the United Kingdom and given in evidence through a live television link by virtue of section 32 of the Criminal Justice Act 1988 must be treated for the purposes of section 1 as having been made in the proceedings in which it is given in evidence.
Section 1 applies in relation to a person acting as an intermediary as it applies in relation to a person lawfully sworn as an interpreter in a judicial proceeding; and for this purpose, where a person acts as an intermediary in any proceeding which is not a judicial proceeding for the purposes of section 1, that proceeding must be taken to be part of the judicial proceeding in which the witness’s evidence is given.
Where any statement made by a person on oath in any proceeding which is not a judicial proceeding for the purposes of section 1 is received in evidence in pursuance of a special measures direction, that proceeding must be taken for the purposes of section 1 to be part of the judicial proceeding in which the statement is so received in evidence.
Judicial proceeding
The definition in section 1(2) is not "comprehensive".
The book "Archbold" said that it appears to be immaterial whether the court, before which the statement is made, has jurisdiction in the particular cause in which the statement is made, because there is no express requirement in the Act that the court be one of "competent jurisdiction" and because the definition in section 1(2) does not appear to require this by implication either.
Actus reus
The actus reus of perjury might be considered to be the making of a statement, whether true or false, on oath in a judicial proceeding.
Perjury is a conduct crime.
Mode of trial
Perjury is triable only on indictment.
Sentence
A person convicted of perjury is liable to imprisonment for a term not exceeding seven years, or to a fine, or to both.
The following cases are relevant:
See also the Crown Prosecution Service sentencing manual.
History.
In Anglo-Saxon legal procedure, the offence of perjury could only be committed by both jurors and by compurgators. With time witnesses began to appear in court they were not so treated despite the fact that their functions were akin to that of modern witnesses. This was due to the fact that their role were not yet differentiated from those of the juror – hence false evidence or perjury by witnesses was not made a crime. Even in the fourteenth century when witnesses started appearing before the jury to testify, perjury by them was not made a punishable offence. The maxim then was that every witness’s evidence on oath was true. Perjury by witnesses began to be punished before the end of fifteenth century by the Star Chamber. The immunity enjoyed by witnesses began also to be whittled down or interfered with by the parliament in England in 1540 with subornation of perjury and, in 1562, with perjury proper. The punishment for the offence then was in the nature of monetary penalty, recoverable in a civil action and, not by penal sanction. In 1613, the Star Chamber declared perjury by a witness to be a punishable offence at common law.
See section 3 of the Maintenance and Embracery Act 1540, the 5 Eliz 1 c 9 (An Act for the Punyshement of suche persones as shall procure or comit any wyllful Perjurye) and the Perjury Act 1728.
Materiality
The requirement that the statement be material can be traced back to, and has been credited to, Coke. He said:
For if it be not material, then though it be false, yet it is no perjury, because it concerneth not the point in suit, and therefore in effect it is extra-judicial. Also this act giveth remedy to the party grieved, and if the deposition be not material, he cannot be grieved thereby.
Northern Ireland.
Perjury is a statutory offence in Northern Ireland. It is created by of the Perjury (Northern Ireland) Order 1979 (S.I. 1979/1714 (N.I. 19)). This replaces the Perjury Act (Northern Ireland) 1946 (c. 13) (N.I.).
United States.
Perjury operates in American law as an inherited principle of the common law of England, which defined the act as the "willful and corrupt giving, upon a lawful oath, or in any form allowed by law to be substituted for an oath, in a judicial proceeding or course of justice, of a false testimony material to the issue or matter of inquiry." William Blackstone touched on the subject in his Commentaries on the Laws of England, establishing perjury as "a crime committed when a lawful oath is administered, in some judicial proceeding, to a person who swears willfully, absolutely, and falsely, in a matter material to the issue or point in question." The punishment for perjury under the common law has varied from death to banishment and has included such grotesque penalties as severing the tongue of the perjurer. The definitional structure of perjury provides an important framework for legal proceedings, as the component parts of this definition have permeated jurisdictional lines, finding a home in American legal constructs. As such, the main tenets of perjury, including mens rea, a lawful oath, occurring during a judicial proceeding, a false testimony have remained necessary pieces of perjury’s definition in the United States.
Statutory definitions.
Perjury’s current position in America’s legal takes the form of state and federal statutes. Most notably, the United States Code prohibits perjury, which is defined in two senses for federal purposes as someone who:
(1) Having taken an oath before a competent tribunal, officer, or person, in any case in which a law of the United States authorizes an oath to be administered, that he will testify, declare, depose, or certify truly, or that any written testimony, declaration, deposition, or certificate by him subscribed, is true, willfully and contrary to such oath states or subscribes any material matter which he does not believe to be true; or
(2) in any declaration, certificate, verification, or statement under penalty of perjury as permitted under section 1746 of title 28, United States Code, willfully subscribes as true any material matter which he does not believe to be true
The above statute provides for a fine and/or up to five years in prison as punishment. Within federal jurisdiction, statements made in two broad categories of judicial proceedings may qualify as perjurious: 1) Federal official proceedings, and 2) Federal Court or Grand Jury proceedings. A third type of perjury entails the procurement of perjurious statements from another person. More generally, the statement must occur in the "course of justice," but this definition leaves room open for interpretation. One particularly precarious aspect of this phrasing is that it entails knowledge of the accused person’s perception of the truthful nature of events and not necessarily the actual truth of those events. It is important to note the distinction here, between giving a false statement under oath and merely misstating a fact accidentally, though this distinction can be especially difficult to discern in court of law.
Precedents.
The development of perjury law in the United States centers on United States v. Dunnigan, a seminal case that set out the parameters of perjury within United States law. The court uses the Dunnigan based legal standard to determine if an accused person, "[T]estifying under oath or affirmation violates this section if she gives false testimony concerning a material matter with the willful intent to provide false testimony, rather than as a result of confusion, mistake, or faulty memory." However, a defendant shown to be willfully ignorant may in fact be eligible for perjury prosecution. The Dunnigan distinction manifests its importance in with regard to the relation between two component parts of perjury’s definition: in willfully giving a false statement, a person must understand that she is giving a false statement to be considered a perjurer under the Dunnigan framework. Deliberation on the part of the defendant is required for a statement to constitute perjury. Jurisprudential developments in the American law of perjury have revolved around the facilitation of "perjury prosecutions and thereby enhance the reliability of testimony before federal courts and grand juries." With this goal in mind, Congress has sometimes expanded the grounds on which an individual may be prosecuted for perjury, with section 1623 of the United States Code recognizing the utterance of two mutually incompatible statements as grounds for perjury indictment even if neither can unequivocally be proven false. However, the two statements must be so mutually incompatible that at least one must necessarily be false; it is irrelevant whether the false statement can be specifically identified from among the two. It thus falls on the government to show that a defendant (a) knowingly made a (b) false (c) material statement (d) under oath (e) in a legal proceeding. These proceedings can be ancillary to normal court proceedings, and thus, even such menial interactions as bail hearings can qualify as protected proceedings under this statute.
Wilfulness is an element of the offense. The mere existence of two mutually exclusive factual statements is not sufficient to prove perjury; the prosecutor nonetheless has the duty to plead and prove statement was willfully made. Mere contradiction will not sustain the charge; there must be strong corroborative evidence of the contradiction.
One significant legal distinction lies in the specific realm of knowledge necessarily possessed by a defendant for her statements to be properly called perjury. Though the defendant must knowingly render a false statement in a legal proceeding or under federal jurisdiction, the defendant need not know that they are speaking under such conditions for the statement to constitute perjury. All tenets of perjury qualification persist- the “knowingly” aspect of telling the false statement simply does not apply to the defendant’s knowledge about the person she intends to deceive.
Materiality.
Perjury law’s evolution in the United States has experienced the most debate with regards to the materiality requirement. Fundamentally, statements that are literally true cannot provide the basis for a perjury charge (as they do not meet the falsehood requirement) just as answers to truly ambiguous statements cannot constitute perjury. However, such fundamental truths of perjury law become muddled when discerning the materiality of a given statement and the way in which it was material to the given case. In "United States v. Brown", the court defined material statements as those with "a natural tendency to influence, or is capable of influencing, the decision of the decision-making body to be addressed," such as a jury or grand jury. While courts have specifically made clear certain instances which have succeeded or failed to meet the nebulous threshold for materiality, the topic remains unresolved in large part, except in certain legal areas where intent manifests itself in an abundantly clear fashion, such as with the so-called perjury trap, a specific situation in which a prosecutor calls a person to testify before a grand jury with the intent of drawing a perjurious statement from the person being questioned.
Defense of recantation.
Despite a tendency of American perjury law toward broad prosecutory power under perjury statutes, American perjury law has afforded potential defendants a new form of defense not found in the British Common Law. This defense requires that an individual admit to making a perjurious statement during that same proceeding and recanting the statement. Though this defensive loophole slightly narrows the types of cases which may be prosecuted for perjury, the effect of this statutory defense is to promote a truthful retelling of facts by witnesses, thus helping to ensure the reliability of American court proceedings just as broadened perjury statutes aimed to do.
Subornation of perjury.
Subornation of perjury stands as a subset of American perjury laws and prohibits an individual from inducing another to commit perjury. Subornation of perjury entails equivalent possible punishments as perjury on the federal level. This crime requires an extra level of satisfactory proof, as prosecutors must show not only that perjury occurred, but also that the defendant positively induced said perjury. Furthermore, the inducing defendant must know that the suborned statement is a false, perjurious statement.
Allegations of perjury.
Notable people who have been accused of perjury include:

</doc>
<doc id="23689" url="http://en.wikipedia.org/wiki?curid=23689" title="Phoenix">
Phoenix

Phoenix most often refers to:
Phoenix or The Phoenix may also refer to:

</doc>
<doc id="23690" url="http://en.wikipedia.org/wiki?curid=23690" title="Phosphate">
Phosphate

A phosphate (PO43−) as an inorganic chemical is a salt of phosphoric acid. In organic chemistry, a phosphate, or organophosphate, is an ester of phosphoric acid. Organic phosphates are important in biochemistry and biogeochemistry or ecology. Inorganic phosphates are mined to obtain phosphorus for use in agriculture and industry. At elevated temperatures in the solid state, phosphates can condense to form pyrophosphates.
Chemical properties.
The phosphate ion is a polyatomic ion with the empirical formula PO43− and a molar mass of 94.97 g/mol. It consists of one central phosphorus atom surrounded by four oxygen atoms in a tetrahedral arrangement. The phosphate ion carries a negative-three formal charge and is the conjugate base of the hydrogen phosphate ion, HPO42−, which is the conjugate base of H2PO4−, the dihydrogen phosphate ion, which in turn is the conjugate base of H3PO4, phosphoric acid. A phosphate salt forms when a positively charged ion attaches to the negatively charged oxygen atoms of the ion, forming an ionic compound. Many phosphates are not soluble in water at standard temperature and pressure. The sodium, potassium, rubidium, caesium, and ammonium phosphates are all water soluble. Most other phosphates are only slightly soluble or are insoluble in water. As a rule, the hydrogen and dihydrogen phosphates are slightly more soluble than the corresponding phosphates. The pyrophosphates are mostly water soluble.
Aqueous phosphate exists in four forms. In strongly basic conditions, the phosphate ion (PO43−) predominates, whereas in weakly basic conditions, the hydrogen phosphate ion (HPO42−) is prevalent. In weakly acid conditions, the dihydrogen phosphate ion (H2PO4−) is most common. In strongly acidic conditions, trihydrogen phosphate (H3PO4) is the main form.
More precisely, considering these three equilibrium reactions:
the corresponding constants at 25 °C (in mol/L) are (see phosphoric acid):
The speciation diagram obtained using these p"K" values shows three distinct regions. In effect, H3PO4, H2PO4− and HPO42− behave as separate weak acids. This is because the successive p"K" values differ by more than 4. For each acid, the pH at half-neutralization is equal to the p"K" value of the acid. The region in which the acid is in equilibrium with its conjugate base is defined by pH ≈ p"K" ± 2. Thus, the three pH regions are approximately 0–4, 5–9 and 10–14. This is idealized, as it assumes constant ionic strength, which will not hold in reality at very low and very high pH values.
For a neutral pH as in the cytosol, pH=7.0
so that only H2PO4− and HPO42− ions are present in significant amounts (62% H2PO4−, 38% HPO42−. Note that in the extracellular fluid (pH=7.4), this proportion is inverted (61% HPO42−, 39% H2PO4−).
Phosphate can form many polymeric ions such as pyrophosphate), P2O74-, and triphosphate, P3O105-. The various metaphosphate ions (which are usually long linear polymers) have an empirical formula of PO3− and are found in many compounds.
Biochemistry of phosphates.
In biological systems, phosphorus is found as a free phosphate ion in solution and is called inorganic phosphate, to distinguish it from phosphates bound in various phosphate esters. Inorganic phosphate is generally denoted Pi and at physiological (neutral) pH primarily consists of a mixture of HPO42− and H2PO4− ions.
Inorganic phosphate can be created by the hydrolysis of pyrophosphate, which is denoted PPi:
However, phosphates are most commonly found in the form of adenosine phosphates, (AMP, ADP, and ATP) and in DNA and RNA, and can be released by the hydrolysis of ATP or ADP. Similar reactions exist for the other nucleoside diphosphates and triphosphates. Phosphoanhydride bonds in ADP and ATP, or other nucleoside diphosphates and triphosphates, contain high amounts of energy which give them their vital role in all living organisms. They are generally referred to as high-energy phosphate, as are the phosphagens in muscle tissue. Compounds such as substituted phosphines have uses in organic chemistry, but do not seem to have any natural counterparts.
The addition and removal of phosphate from proteins in all cells is a pivotal strategy in the regulation of metabolic processes.
Reference ranges for blood tests, showing 'inorganic phosphorus' in purple at right, being almost identical to the molar concentration of phosphate
Phosphate is useful in animal cells as a buffering agent. Phosphate salts that are commonly used for preparing buffer solutions at cell p"H"s include Na2HPO4, NaH2PO4, and the corresponding potassium salts.
An important occurrence of phosphates in biological systems is as the structural material of bone and teeth. These structures are made of crystalline calcium phosphate in the form of hydroxyapatite. The hard dense enamel of mammalian teeth consists of fluoroapatite, an hydroxy calcium phosphate where some of the hydroxyl groups have been replaced by fluoride ions.
Plants take up phosphorus through several pathways: the arbuscular mycorrhizal pathway and the direct uptake pathway.
Occurrence and mining.
Phosphates are the naturally occurring form of the element phosphorus, found in many phosphate minerals. In mineralogy and geology, phosphate refers to a rock or ore containing phosphate ions. Inorganic phosphates are mined to obtain phosphorus for use in agriculture and industry.
The largest phosphorite or rock phosphate deposits in North America lie in the Bone Valley region of central Florida, the Soda Springs region of Idaho, and the coast of North Carolina. Smaller deposits are located in Montana, Tennessee, Georgia, and South Carolina near Charleston along Ashley Phosphate road. The small island nation of Nauru and its neighbor Banaba Island, which used to have massive phosphate deposits of the best quality, have been mined excessively. Rock phosphate can also be found in Egypt, Israel, Morocco, Navassa Island, Tunisia, Togo, and Jordan, countries that have large phosphate-mining industries.
Phosphorite mines are primarily found in:
In 2007, at the current rate of consumption, the supply of phosphorus was estimated to run out in 345 years. However, some scientists believed that a "peak phosphorus" will occur in 30 years and Dana Cordell from Institute for Sustainable Futures said that at "current rates, reserves will be depleted in the next 50 to 100 years." Reserves refer to the amount assumed recoverable at current market prices, and, in 2012, the USGS estimated 71 billion tons of world reserves, while 0.19 billion tons were mined globally in 2011. Phosphorus comprises 0.1% by mass of the average rock (while, for perspective, its typical concentration in vegetation is 0.03% to 0.2%), and consequently there are quadrillions of tons of phosphorus in Earth's 3 * 1019 ton crust, albeit at predominantly lower concentration than the deposits counted as reserves from being inventoried and cheaper to extract; if it is assumed that the phosphate minerals in phosphate rock are hydroxyapatite and fluoroapatite, phosphate minerals contain roughly 18,5 % phosphorus by weight and if phosphate rock contains around 20 % of these minerals, the average phosphate rock has roughly 3,7 % phosphorus by weight.
Some phosphate rock deposits are notable for their inclusion of significant quantities of radioactive uranium isotopes. This syndrome is noteworthy because radioactivity can be released into surface waters in the process of application of the resultant phosphate fertilizer (e.g. in many tobacco farming operations in the southeast US).
In December 2012, Cominco Resources announced an updated JORC compliant resource of their Hinda project in Congo-Brazzaville of 531 Mt, making it the largest measured and indicated phosphate deposit in the world.
Ecology.
In ecological terms, because of its important role in biological systems, phosphate is a highly sought after resource. Once used, it is often a limiting nutrient in environments, and its availability may govern the rate of growth of organisms. This is generally true of freshwater environments, whereas nitrogen is more often the limiting nutrient in marine (seawater) environments. Addition of high levels of phosphate to environments and to micro-environments in which it is typically rare can have significant ecological consequences. For example, blooms in the populations of some organisms at the expense of others, and the collapse of populations deprived of resources such as oxygen (see eutrophication) can occur. In the context of pollution, phosphates are one component of total dissolved solids, a major indicator of water quality, but not all phosphorus is in a molecular form which algae can break down and consume.
Calcium hydroxyapatite and calcite precipitates can be found around bacteria in alluvial topsoil. As clay minerals promote biomineralization, the presence of bacteria and clay minerals resulted in calcium hydroxyapatite and calcite precipitates.
Phosphate deposits can contain significant amounts of naturally occurring heavy metals. Mining operations processing phosphate rock can leave tailings piles containing elevated levels of cadmium, lead, nickel, copper, chromium, and uranium. Unless carefully managed, these waste products can leach heavy metals into groundwater or nearby estuaries. Uptake of these substances by plants and marine life can lead to concentration of toxic heavy metals in food products.
In Germany, the use of uranium-contaminated standard phosphate fertilizers in farming has been linked to significantly raised uranium levels in drinking water. In some areas, it has led to recommendations to use bottled water, instead of tap water, to prepare food for babies and small children.

</doc>
<doc id="23692" url="http://en.wikipedia.org/wiki?curid=23692" title="Prime number theorem">
Prime number theorem

In number theory, the prime number theorem (PNT) describes the asymptotic distribution of the prime numbers among the positive integers. It formalizes the intuitive idea that primes become less common as they become larger.
The first such distribution found is π("N") ~ "N" / ln("N"), where π("N") is the prime-counting function and ln("N") is the natural logarithm of "N". This means that for large enough "N", the probability that a random integer not greater than "N" is prime is very close to 1 / ln("N"). Consequently, a random integer with at most 2"n" digits (for large enough "n") is about half as likely to be prime as a random integer with at most "n" digits. For example, among the positive integers of at most 1000 digits, about one in 2300 is prime (ln 101000 ≈ 2302.6), whereas among positive integers of at most 2000 digits, about one in 4600 is prime (ln 102000 ≈ 4605.2). In other words, the average gap between consecutive prime numbers among the first "N" integers is roughly ln("N").
Statement.
Let π("x") be the prime-counting function that gives the number of primes less than or equal to "x", for any real number "x". For example, π(10) = 4 because there are four prime numbers (2, 3, 5 and 7) less than or equal to 10. The prime number theorem then states that "x" / ln("x") is a good approximation to π("x"), in the sense that the limit of the "quotient" of the two functions π("x") and "x" / ln("x") as "x" increases without bound is 1:
known as the asymptotic law of distribution of prime numbers. Using asymptotic notation this result can be restated as
This notation (and the theorem) does "not" say anything about the limit of the "difference" of the two functions as "x" increases without bound. Instead, the theorem states that "x"/ln("x") approximates π("x") in the sense that the relative error of this approximation approaches 0 as "x" increases without bound.
The prime number theorem is equivalent to the statement that the "n"th prime number "p""n" satisfies
the asymptotic notation meaning, again, that the relative error of this approximation approaches 0 as "n" increases without bound. For example, the 200 · 1015th prime number is 8512677386048191063, and (200 · 1015)ln(200 · 1015) rounds to 7967418752291744388, a relative error of about 6.8%.
The prime number theorem is also equivalent to formula_4, and formula_5.
History of the asymptotic law of distribution of prime numbers and its proof.
Based on the tables by Anton Felkel and Jurij Vega, Adrien-Marie Legendre conjectured in 1797 or 1798 that π("a") is approximated by the function "a"/(A ln("a") + "B"), where "A" and B are unspecified constants. In the second edition of his book on number theory (1808) he then made a more precise conjecture, with "A" = 1 and "B" = −1.08366. Carl Friedrich Gauss considered the same question at age 15 or 16 "ins Jahr 1792 oder 1793", according to his own recollection in 1849. In 1838 Peter Gustav Lejeune Dirichlet came up with his own approximating function, the logarithmic integral li("x") (under the slightly different form of a series, which he communicated to Gauss). Both Legendre's and Dirichlet's formulas imply the same conjectured asymptotic equivalence of π("x") and "x" / ln("x") stated above, although it turned out that Dirichlet's approximation is considerably better if one considers the differences instead of quotients.
In two papers from 1848 and 1850, the Russian mathematician Pafnuty L'vovich Chebyshev attempted to prove the asymptotic law of distribution of prime numbers. His work is notable for the use of the zeta function ζ("s") (for real values of the argument "s", as are works of Leonhard Euler, as early as 1737) predating Riemann's celebrated memoir of 1859, and he succeeded in proving a slightly weaker form of the asymptotic law, namely, that if the limit of π("x")/("x"/ln("x")) as "x" goes to infinity exists at all, then it is necessarily equal to one. He was able to prove unconditionally that this ratio is bounded above and below by two explicitly given constants near 1, for all sufficiently large "x". Although Chebyshev's paper did not prove the Prime Number Theorem, his estimates for π("x") were strong enough for him to prove Bertrand's postulate that there exists a prime number between "n" and 2"n" for any integer "n" ≥ 2.
An important paper concerning the distribution of prime numbers was Riemann's 1859 memoir "On the Number of Primes Less Than a Given Magnitude", the only paper he ever wrote on the subject. Riemann introduced new ideas into the subject, the chief of them being that the distribution of prime numbers is intimately connected with the zeros of the analytically extended Riemann zeta function of a complex variable. In particular, it is in this paper of Riemann that the idea to apply methods of complex analysis to the study of the real function π("x") originates. Extending the ideas of Riemann, two proofs of the asymptotic law of the distribution of prime numbers were obtained independently by Jacques Hadamard and Charles Jean de la Vallée-Poussin and appeared in the same year (1896). Both proofs used methods from complex analysis, establishing as a main step of the proof that the Riemann zeta function ζ("s") is non-zero for all complex values of the variable "s" that have the form "s" = 1 + "it" with "t" > 0.
During the 20th century, the theorem of Hadamard and de la Vallée-Poussin also became known as the Prime Number Theorem. Several different proofs of it were found, including the "elementary" proofs of Atle Selberg and Paul Erdős (1949). While the original proofs of Hadamard and de la Vallée-Poussin are long and elaborate, and later proofs have introduced various simplifications through the use of Tauberian theorems but remained difficult to digest. A short proof was discovered in 1980 by American mathematician Donald J. Newman. Newman's proof is arguably the simplest known proof of the theorem, although it is non-elementary in the sense that it uses Cauchy's integral theorem from complex analysis.
Proof methodology.
In a lecture on prime numbers for a general audience, Fields medalist Terence Tao described one approach to proving the prime number theorem in poetic terms: listening to the "music" of the primes. We start with a "sound wave" that is "noisy" at the prime numbers and silent at other numbers; this is the von Mangoldt function. Then we analyze its notes or frequencies by subjecting it to a process akin to Fourier transform; this is the Mellin transform. The next and most difficult step is to prove that certain "notes" cannot occur in this music. This exclusion of certain notes leads to the statement of the prime number theorem. According to Tao, this proof yields much deeper insights into the distribution of the primes than the "elementary" proofs.
Proof sketch.
Here is a sketch of the proof referred to in Tao's lecture mentioned above. Like most proofs of the PNT, it starts out by reformulating the problem in terms of a less intuitive, but better-behaved, prime-counting function. The idea is to count the primes (or a related set such as the set of prime powers) with "weights" to arrive at a function with smoother asymptotic behavior. The most common such generalized counting function is the Chebyshev function formula_6, defined by
This is sometimes written as formula_8, where formula_9 is the von Mangoldt function, namely
It is now relatively easy to check that the PNT is equivalent to the claim that formula_11. Indeed, this follows from the easy estimates
and (using big O notation) for any formula_13,
The next step is to find a useful representation for formula_6. Let formula_16 be the Riemann zeta function. It can be shown that formula_16 is related to the von Mangoldt function formula_9, and hence to formula_6, via the relation
A delicate analysis of this equation and related properties of the zeta function, using the Mellin transform and Perron's formula, shows that for non-integer "x" the equation
holds, where the sum is over all zeros (trivial and non-trivial) of the zeta function. This striking formula is one of the so-called explicit formulas of number theory, and is already suggestive of the result we wish to prove, since the term "x" (claimed to be the correct asymptotic order of formula_6) appears on the right-hand side, followed by (presumably) lower-order asymptotic terms.
The next step in the proof involves a study of the zeros of the zeta function. The trivial zeros −2, −4, −6, −8, ... can be handled separately:
which vanishes for a large "x". The nontrivial zeros, namely those on the critical strip formula_24, can potentially be of an asymptotic order comparable to the main term "x" if formula_25, so we need to show that all zeros have real part strictly less than 1.
To do this, we take for granted that formula_16 is meromorphic in the half-plane formula_27, and is analytic there except for a simple pole at formula_28, and that there is a product formula formula_29 for formula_30 This product formula follows from the existence of unique prime factorization of integers, and shows that formula_16 is never zero in this region, so that its logarithm is defined there and formula_32 Write formula_33; then
Now observe the identity formula_35 so that
for all formula_37. Suppose now that formula_38. Certainly formula_39 is not zero, since formula_16 has a simple pole at formula_28. Suppose that formula_42 and let formula_43 tend to formula_44 from above. Since formula_16 has a simple pole at formula_28 and formula_47 stays analytic, the left hand side in the previous inequality tends to formula_48, a contradiction.
Finally, we can conclude that the PNT is "morally" true. To rigorously complete the proof there are still serious technicalities to overcome, due to the fact that the summation over zeta zeros in the explicit formula for formula_6 does not converge absolutely but only conditionally and in a "principal value" sense. There are several ways around this problem but many of them require rather delicate complex-analytic estimates that are beyond the scope of this article. Edwards's book provides the details. Another method is to use Ikehara's Tauberian theorem, though this theorem is itself quite hard to prove. D. J. Newman observed that the full strength of Ikehara's theorem is not needed for the prime number theorem, and one can get away with a special case that is much easier to prove.
Prime-counting function in terms of the logarithmic integral.
In a handwritten note on a reprint of his 1838 paper "Sur l'usage des séries infinies dans la théorie des nombres", which he mailed to Carl Friedrich Gauss, Peter Gustav Lejeune Dirichlet conjectured (under a slightly different form appealing to a series rather than an integral) that an even better approximation to π("x") is given by the offset logarithmic integral function Li("x"), defined by
Indeed, this integral is strongly suggestive of the notion that the 'density' of primes around "t" should be 1/ln"t". This function is related to the logarithm by the asymptotic expansion
So, the prime number theorem can also be written as π("x") ~ Li("x"). In fact, in another paper in 1899 La Vallée Poussin proved that
for some positive constant "a", where "O"(...) is the big O notation. This has been improved to
Because of the connection between the Riemann zeta function and π("x"), the Riemann hypothesis has considerable importance in number theory: if established, it would yield a far better estimate of the error involved in the prime number theorem than is available today. More specifically, Helge von Koch showed in 1901 that, if and only if the Riemann hypothesis is true, the error term in the above relation can be improved to
The constant involved in the big O notation was estimated in 1976 by Lowell Schoenfeld: assuming the Riemann hypothesis,
for all "x" ≥ 2657. He also derived a similar bound for the Chebyshev prime-counting function ψ:
for all "x" ≥ 73.2. This latter bound has been shown to express a variance to mean power law (when regarded as a random function over the integers), 1/"f" noise and to also correspond to the Tweedie compound Poisson distribution. Parenthetically, the Tweedie distributions represent a family of scale invariant distributions that serve as foci of convergence for a generalization of the central limit theorem.
The logarithmic integral Li("x") is larger than π("x") for "small" values of "x". This is because it is (in some sense) counting not primes, but prime powers, where a power "p""n" of a prime "p" is counted as 1/"n" of a prime. This suggests that Li("x") should usually be larger than π("x") by roughly Li("x"1/2)/2, and in particular should usually be larger than π("x"). However, in 1914, J. E. Littlewood proved that this is not always the case. The first value of "x" where π("x") exceeds Li("x") is probably around "x" = 10316; see the article on Skewes' number for more details.
Elementary proofs.
In the first half of the twentieth century, some mathematicians (notably G. H. Hardy) believed that there exists a hierarchy of proof methods in mathematics depending on what sorts of numbers (integers, reals, complex) a proof requires, and that the prime number theorem (PNT) is a "deep" theorem by virtue of requiring complex analysis. This belief was somewhat shaken by a proof of the PNT based on Wiener's tauberian theorem, though this could be set aside if Wiener's theorem were deemed to have a "depth" equivalent to that of complex variable methods. There is no rigorous and widely accepted definition of the notion of elementary proof in number theory. One definition is "a proof that can be carried out in first order Peano arithmetic." There are number-theoretic statements (for example, the Paris–Harrington theorem) provable using second order but not first order methods, but such theorems are rare to date.
In March 1948, Atle Selberg established, by elementary means, the asymptotic formula
where
for primes formula_59. By July of that year, Selberg and Paul Erdős had each obtained elementary proofs of the PNT, both using Selberg's asymptotic formula as a starting point. These proofs effectively laid to rest the notion that the PNT was "deep," and showed that technically "elementary" methods (in other words Peano arithmetic) were more powerful than had been believed to be the case. In 1994, Charalambos Cornaros and Costas Dimitracopoulos proved the PNT using only formula_60, a formal system far weaker than Peano arithmetic. On the history of the elementary proofs of the PNT, including the Erdős–Selberg priority dispute, see Dorian Goldfeld.
Computer verifications.
In 2005, Avigad "et al." employed the Isabelle theorem prover to devise a computer-verified variant of the Erdős–Selberg proof of the PNT. This was the first machine-verified proof of the PNT. Avigad chose to formalize the Erdős–Selberg proof rather than an analytic one because while Isabelle's library at the time could implement the notions of limit, derivative, and transcendental function, it had almost no theory of integration to speak of (Avigad et al. p. 19).
In 2009, John Harrison employed HOL Light to formalize a proof employing complex analysis. By developing the necessary analytic machinery, including the Cauchy integral formula, Harrison was able to formalize "a direct, modern and elegant proof instead of the more involved 'elementary' Erdős–Selberg argument".
Prime number theorem for arithmetic progressions.
Let formula_61 denote the number of primes in the arithmetic progression "a", "a" + "n", "a" + 2"n", "a" + 3"n", ... less than "x". Lejeune Dirichlet and Legendre conjectured, and Vallée-Poussin proved, that, if "a" and "n" are coprime, then
where φ is the Euler's totient function. In other words, the primes are distributed evenly among the residue classes ["a"] modulo "n" with gcd("a", "n") = 1. This can be proved using similar methods used by Newman for his proof of the prime number theorem.
The Siegel–Walfisz theorem gives a good estimate for the distribution of primes in residue classes.
Prime number race.
Although we have in particular
empirically the primes congruent to 3 are more numerous and are nearly always ahead in this "prime number race"; the first reversal occurs at "x" = 26,861.:1–2 However Littlewood showed in 1914:2 that there are infinitely many sign changes for the function
so the lead in the race switches back and forth infinitely many times. The phenomenon that π4,3("x") is ahead most of the time is called Chebyshev's bias. The prime number race generalizes to other moduli and is the subject of much research; Pál Turán asked whether it is always the case that π("x";"a","c") and π("x";"b","c") change places when "a" and "b" are coprime to "c". Granville and Martin give a thorough exposition and survey.
Bounds on the prime-counting function.
The prime number theorem is an "asymptotic" result. It gives an ineffective bound on π("x") as a direct consequence of the definition of the limit: for all ε > 0, there is an "S" such that for all "x" > "S",
However, better bounds on π("x") are known, for instance Pierre Dusart's
The first inequality holds for all "x" ≥ 599 and the second one for "x" ≥ 355991.
A weaker but sometimes useful bound for "x" ≥ 55 is
In Dusart's thesis there are stronger versions of this type of inequality that are valid for larger "x".
The proof by de la Vallée-Poussin implies the following.
For every ε > 0, there is an "S" such that for all "x" > "S",
Approximations for the "n"th prime number.
As a consequence of the prime number theorem, one gets an asymptotic expression for the "n"th prime number, denoted by "p""n":
A better approximation is
Again considering the 200 · 1015 prime number 8512677386048191063, this gives an estimate of 8512681315554715386; the first 5 digits match and relative error is about 0.00005%.
Rosser's theorem states that "p""n" is larger than "n" ln "n". This can be improved by the following pair of bounds:
Table of π("x"), "x" / ln "x", and li("x").
The table compares exact values of π("x") to the two approximations "x" / ln "x" and li("x"). The last column, "x" / π("x"), is the average prime gap below "x".
The value for π(1024) was originally computed assuming the Riemann hypothesis; it has since been verified unconditionally.
Analogue for irreducible polynomials over a finite field.
There is an analogue of the prime number theorem that describes the "distribution" of irreducible polynomials over a finite field; the form it takes is strikingly similar to the case of the classical prime number theorem.
To state it precisely, let "F" = GF("q") be the finite field with "q" elements, for some fixed "q", and let "N""n" be the number of monic "irreducible" polynomials over "F" whose degree is equal to "n". That is, we are looking at polynomials with coefficients chosen from "F", which cannot be written as products of polynomials of smaller degree. In this setting, these polynomials play the role of the prime numbers, since all other monic polynomials are built up of products of them. One can then prove that
If we make the substitution "x" = "q""n", then the right hand side is just
which makes the analogy clearer. Since there are precisely "q""n" monic polynomials of degree "n" (including the reducible ones), this can be rephrased as follows: if a monic polynomial of degree "n" is selected randomly, then the probability of it being irreducible is about 1/"n".
One can even prove an analogue of the Riemann hypothesis, namely that
The proofs of these statements are far simpler than in the classical case. It involves a short combinatorial argument, summarised as follows. Every element of the degree "n" extension of "F" is a root of some irreducible polynomial whose degree "d" divides "n"; by counting these roots in two different ways one establishes that
where the sum is over all divisors "d" of "n". Möbius inversion then yields
where μ("k") is the Möbius function. (This formula was known to Gauss.) The main term occurs for "d" = "n", and it is not difficult to bound the remaining terms. The "Riemann hypothesis" statement depends on the fact that the largest proper divisor of "n" can be no larger than "n"/2.

</doc>
<doc id="23693" url="http://en.wikipedia.org/wiki?curid=23693" title="Conflict of laws">
Conflict of laws

Conflict of laws or Private international law (both terms are used interchangeably) concerns relations across different legal jurisdictions between persons, and sometimes also companies, corporations and other legal entities.
Choice of laws.
Courts faced with a choice of law issue have a two-stage process:
Private international law on marriages and legal dissolutions of marriages (divorce).
In divorce cases, when a court is attempting to distribute marital property, if the divorcing couple is local and the property is local, then the court applies its domestic law lex fori. The case becomes even more complicated if foreign elements are thrown into the mix, such as when the place of marriage is different from the territory where divorce was filed; when the parties' nationalities and residences do not match; when there is property in a foreign jurisdiction; or when the parties have changed residence several times during the marriage.
Whereas commercial agreements or prenuptial agreements generally do not require legal formalities to be observed, when married couples enter a property agreement, stringent requirements are imposed, including notarization, witnesses, special acknowledgment forms. In some countries, these must be filed (or docketed) with a domestic court, and the terms must be “so ordered” by a judge. This is done in order to ensure that no undue influence or oppression has been exerted by one spouse against the other. Upon presenting a property agreement between spouses to a court of divorce, that court will generally assure itself of the following factors: signatures, legal formalities, intent, later intent, free will, lack of oppression, reasonableness and fairness, consideration, performance, reliance, later repudiation in writing or by conduct, and whichever other concepts of contractual bargaining apply in the context.
Private international law on unmarried persons.
Unlike marriage which has an international recognised legal status, there are no international treaties on recognition of unmarried couple's legal status. If an unmarried couple change residence to different countries, then the local law on where the couple is last domiciled is applied to them. This covers legal status of the relationship, rights, obligations, and all worldwide movable and immovable property. To otherwise interpret the law would mean if the unmarried couple had assets in several different countries, they would then need separate legal cases in each country to resolve all their movable and immovable property.
In the absence of a valid and enforceable agreement for an unmarried couple, here’s how the conflict of law rules work:
Contracts.
Many contracts and other forms of legally binding agreement include a jurisdiction or arbitration clause specifying the parties' choice of venue for any litigation (called a forum selection clause). Then, choice of law clauses may specify which laws the court or tribunal should apply to each aspect of the dispute. This matches the substantive policy of freedom of contract. Judges have accepted that the principle of party autonomy allows the parties to select the law most appropriate to their transaction. This judicial acceptance of subjective intent excludes the traditional reliance on objective connecting factors; it also harms consumers as vendors often impose one-sided contractual terms selecting a venue far from the buyer's home or workplace.
Harmonization of laws.
To apply one national legal system as against another may never be an entirely satisfactory approach. The parties' interests may always be better protected by applying a law conceived with international realities in mind. The Hague Conference on Private International Law is a treaty organization that oversees conventions designed to develop a uniform system. The deliberations of the conference have recently been the subject of controversy over the extent of cross-border jurisdiction on electronic commerce and defamation issues. There is a general recognition that there is a need for an international law of contracts: for example, many nations have ratified the "Vienna Convention on the International Sale of Goods", the "Rome Convention on the Law Applicable to Contractual Obligations" offers less specialized uniformity, and there is support for the "UNIDROIT Principles of International Commercial Contracts", a private restatement, all of which represent continuing efforts to produce international standards as the internet and other technologies encourage ever more interstate commerce. But other branches of the law are less well served and the dominant trend remains the role of the forum law rather than a supranational system for Conflict purposes. Even the EU, which has institutions capable of creating uniform rules with direct effect, has failed to produce a universal system for the common market. Nevertheless, the Treaty of Amsterdam does confer authority on the Community's institutions to legislate by Council Regulation in this area with supranational effect. Article 177 would give the Court of Justice jurisdiction to interpret and apply their principles so, if the political will arises, uniformity may gradually emerge in letter. Whether the domestic courts of the Member States would be consistent in applying those letters is speculative.

</doc>
<doc id="23696" url="http://en.wikipedia.org/wiki?curid=23696" title="Timeline of programming languages">
Timeline of programming languages

This is a record of historically important programming languages, by decade.
Legend

</doc>
<doc id="23698" url="http://en.wikipedia.org/wiki?curid=23698" title="International Fixed Calendar">
International Fixed Calendar

The International Fixed calendar (also known as the Cotsworth plan, the Eastman plan, the 13 Month calendar or the Equal Month calendar) is a solar calendar proposal for calendar reform designed by Moses B. Cotsworth, who presented it in 1902. It provides for a year of 13 months of 28 days each. It is therefore a perennial calendar, with every date fixed always on the same weekday. Though it was never officially adopted in any country, it was the official calendar of the Eastman Kodak Company from 1928 to 1989.
Rules.
The calendar year has 13 months with 28 days each, divided into exactly 4 weeks (13 × 28 = 364). An extra day added as a holiday at the end of the year (December 29), sometimes called "Year Day", does not belong to any week and brings the total to 365 days. Each year coincides with the corresponding Gregorian year, so January 1 in the Cotsworth calendar always falls on Gregorian January 1. Twelve months are named and ordered the same as those of the Gregorian calendar, except that the extra month is inserted between June and July, and called "Sol". Situated in mid-summer (from the point of view of its Northern Hemisphere authors), the name of the new month was chosen in homage to the sun.
Leap year in the International Fixed Calendar contains 366 days, and its occurrence follows the Gregorian rule. There is a leap year in every year whose number is divisible by 4, but not if the year number is divisible by 100, unless it is also divisible by 400. So although the year 2000 was a leap year, the years 1700, 1800, and 1900 were common years. The International Fixed Calendar inserts the extra day in leap year as June 29 - between Saturday June 28 and Sunday Sol 1.
Each month begins on a Sunday, and ends on a Saturday; consequently, every year begins on Sunday.
Neither Year Day nor Leap Day are considered to be part of any week; they are preceded by a Saturday and are followed by a Sunday.
All the months look like this:
The following shows how the 13 months and extra days of the International Fixed Calendar occur in relation to the dates of the Gregorian calendar:
History.
The simple idea of a 13-month perennial calendar has been around since at least the middle of the 18th century. Versions of the idea differ mainly on how the months are named, and the treatment of the extra day in leap year.
The "Georgian calendar" was proposed in 1745 by an American Colonist from Maryland writing under the pen name, Hirossa Ap-Iccim (=Rev. Hugh Jones). The author named the plan, and the thirteenth month, after King George II of Great Britain. The 365th day each year was to be set aside as Christmas. The treatment of leap year varied from the Gregorian rule, however; and the year would begin closer to the winter solstice. In a later version of the plan, published in 1753, the 13 months were all renamed for Christian saints.
In 1849 the French philosopher Auguste Comte (1798–1857) proposed the 13-month "Positivist Calendar", naming the months: Moses, Homer, Aristotle, Archimedes, Caesar, St. Paul, Charlemagne, Dante, Gutenberg, Shakespeare, Descartes, Frederic and Bichat. The days of the year were likewise dedicated to "saints" in the Positivist Religion of Humanity. Positivist weeks, months, and years begin with Monday instead of Sunday. Comte also reset the year number, beginning the era of his calendar (year 1) with the Gregorian year 1789. For the extra days of the year not belonging to any week or month, Comte followed the pattern of Ap-Iccim (Jones), ending each year with a festival on the 365th day, followed by a subsequent feast day occurring only in leap years.
Whether Moses Cotsworth was familiar with the 13-month plans that preceded his International Fixed Calendar is not known. He did follow Ap-Iccim (Jones) in designating the 365th day of the year as Christmas. His suggestion was that this last day of the year should be designated a Sunday, and hence, because the following day would be New Year's Day and a Sunday also, he called it a Double Sunday. Since Cotsworth's goal was a simplified, more "rational" calendar for business and industry, he would carry over all the features of the Gregorian calendar consistent with this goal, including the traditional month names, the week beginning on Sunday, and the Gregorian leap-year rule.
To promote Cotsworth's calendar reform The International Fixed Calendar League was founded in 1923, just after the plan was selected by the League of Nations as the best of 130 calendar proposals put forward. Sir Sandford Fleming, the inventor and driving force behind worldwide adoption of standard time, became the first president of the IFCL. The League opened offices in London and later in Rochester, New York. George Eastman, of the Eastman Kodak Company, became a fervent supporter of the IFC, and instituted its use at Kodak. The International Fixed Calendar League ceased operations shortly after the calendar plan failed to win final approval of the League of Nations in 1937.
Advantages.
The several advantages of The International Fixed Calendar are mainly related to its organization. 

</doc>
<doc id="23703" url="http://en.wikipedia.org/wiki?curid=23703" title="Potential energy">
Potential energy

In physics, potential energy is the energy that an object has due to its position in a force field or that a system has due to the configuration of its parts. 
Common types include the gravitational potential energy of an object that depends on its vertical position and mass, the elastic potential energy of an extended spring, and the electric potential energy of a charge in an electric field. The SI unit for energy is the joule (symbol J).
The term "potential energy" was introduced by the 19th century Scottish engineer and physicist William Rankine, although it has links to Greek philosopher Aristotle's concept of potentiality.
Potential energy is associated with forces that act on a body in a way that depends only on the body's position in space. These forces can be represented by a vector at every point in space forming what is known as a vector field of forces, or a force field.
If the work of a force field acting on a body that moves from a start to an end position is determined only by these two positions, and does not depend on the trajectory of the body, then there is a function known as "potential energy" that can be evaluated at the two positions to determine this work. Furthermore, the force field is determined by this potential energy and is described as derivable from a potential.
Overview.
Potential energy is the stored or pent-up energy of an object. Potential energy is often associated with restoring forces such as a spring or the force of gravity. The action of stretching the spring or lifting the mass is performed by an external force that works against the force field of the potential. This work is stored in the force field, which is said to be stored as potential energy. If the external force is removed the force field acts on the body to perform the work as it moves the body back to the initial position, reducing the stretch of the spring or causing a body to fall.
The more formal definition is that potential energy is the energy difference between the energy of an object in a given position and its energy at a reference position.
There are various types of potential energy, each associated with a particular type of force. For example, the work of an elastic force is called elastic potential energy; work of the gravitational force is called gravitational potential energy; work of the Coulomb force is called electric potential energy; work of the strong nuclear force or weak nuclear force acting on the baryon charge is called nuclear potential energy; work of intermolecular forces is called intermolecular potential energy. Chemical potential energy, such as the energy stored in fossil fuels, is the work of the Coulomb force during rearrangement of mutual positions of electrons and nuclei in atoms and molecules. Thermal energy usually has two components: the kinetic energy of random motions of particles and the potential energy of their mutual positions.
Forces derivable from a potential are also called conservative forces. The work done by a conservative force is
where formula_2 is the change in the potential energy associated with the force. The negative sign provides the convention that work done against a force field increases potential energy, while work done by the force field decreases potential energy. Common notations for potential energy are "U", "V", and "Ep".
Work and potential energy.
Potential energy is closely linked with forces. If the work done by a force on a body that moves from "A" to "B" does not depend on the path between these points, then the work of this force measured from "A" assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.
If the work for an applied force is independent of the path, then the work done by the force is evaluated at the start and end of the trajectory of the point of application. This means that there is a function "U" (x), called a "potential," that can be evaluated at the two points xA and xB to obtain the work over any trajectory between these two points. It is tradition to define this function with a negative sign so that positive work is a reduction in the potential, that is 
where "C" is the trajectory taken from A to B. Because the work done is independent of the path taken, then this expression is true for any trajectory, "C", from A to B.
The function "U"(x) is called the potential energy associated with the applied force. Examples of forces that have potential energies are gravity and spring forces.
Derivable from a potential.
In this section the relationship between work and potential energy is presented in more detail. The line integral that defines work along curve "C" takes a special form if the force F is related to a scalar field φ(x) so that
In this case, work along the curve is given by
which can be evaluated using the gradient theorem to obtain
This shows that when forces are derivable from a scalar field, the work of those forces along a curve "C" is computed by evaluating the scalar field at the start point "A" and the end point "B" of the curve. This means the work integral does not depend on the path between "A" and "B" and is said to be independent of the path.
Potential energy "U"=-φ(x) is traditionally defined as the negative of this scalar field so that work by the force field decreases potential energy, that is
In this case, the application of the del operator to the work function yields,
and the force F is said to be "derivable from a potential." This also necessarily implies that F must be a conservative vector field. The potential "U" defines a force F at every point x in space, so the set of forces is called a force field.
Computing potential energy.
Given a force field F(x), evaluation of the work integral using the gradient theorem can be used to find the scalar function associated with potential energy. This is done by introducing a parameterized curve γ(t)=r(t) from γ(a)=A to γ(b)=B, and computing,
For the force field F, let v= dr/dt, then the gradient theorem yields,
The power applied to a body by a force field is obtained from the gradient of the work, or potential, in the direction of the velocity v of the point of application, that is
Examples of work that can be computed from potential functions are gravity and spring forces.
Potential energy for near Earth gravity.
In classical physics, gravity exerts a constant downward force F=(0, 0, "Fz") on the center of mass of a body moving near the surface of the Earth. The work of gravity on a body moving along a trajectory r(t) = ("x"(t), "y"(t), "z"(t)), such as the track of a roller coaster is calculated using its velocity, v=("v"x, "v"y, "v"z), to obtain
where the integral of the vertical component of velocity is the vertical distance. Notice that the work of gravity depends only on the vertical movement of the curve r(t).
The function 
is called the potential energy of a near earth gravity field.
Potential energy for a linear spring.
A horizontal spring exerts a force F = (−"kx", 0, 0) that is proportional to its deflection in the "x" direction. The work of this spring on a body moving along the space curve s("t") = ("x"("t"), "y"("t"), "z"("t")), is calculated using its velocity, v = ("v"x, "v"y, "v"z), to obtain
For convenience, consider contact with the spring occurs at "t" = 0, then the integral of the product of the distance "x" and the "x"-velocity, "xvx", is "x"2/2.
The function 
is called the potential energy of a linear spring.
Elastic potential energy is the potential energy of an elastic object (for example a bow or a catapult) that is deformed under tension or compression (or stressed in formal terminology). It arises as a consequence of a force that tries to restore the object to its original shape, which is most often the electromagnetic force between the atoms and molecules that constitute the object. If the stretch is released, the energy is transformed into kinetic energy.
Potential energy for gravitational forces between two bodies.
Gravitational potential energy between two bodies in space is obtained from the force exerted by a mass "M" on another mass "m" is given by
where r is the position vector from "M" to "m".
This can also be expressed as
where formula_18 is a vector of length 1 pointing from "M" to "m".
Let the mass "m" move at the velocity v then the work of gravity on this mass as it moves from position r(t1) to r(t2) is given by
Notice that the position and velocity of the mass "m" are given by
where e"r" and e"t" are the radial and tangential unit vectors directed relative to the vector from "M" to "m". Use this to simplify the formula for work of gravity to,
This calculation uses the fact that
The function
is the gravitational potential function, also known as gravitational potential energy. The negative sign follows the convention that work is gained from a loss of potential energy.
Potential energy for electrostatic forces between two bodies.
The electrostatic force exerted by a charge "Q" on another charge "q" is given by
where r is the position vector from "Q" to "q" and "ε"0 is the vacuum permittivity. This may also be written using Coulomb's constant "k"e = 1 ⁄ 4πε0.
The work "W" required to move "q" from "A" to any point "B" in the electrostatic force field is given by the potential function
Reference level.
The potential energy is a function of the state a system is in, and is defined relative to that for a particular state. This reference state is not always a real state, it may also be a limit, such as with the distances between all bodies tending to infinity, provided that the energy involved in tending to that limit is finite, such as in the case of inverse-square law forces. Any arbitrary reference state could be used, therefore it can be chosen based on convenience.
Typically the potential energy of a system depends on the "relative" positions of its components only, so the reference state can also be expressed in terms of relative positions.
Gravitational potential energy.
Gravitational energy is the potential energy associated with gravitational force, as work is required to elevate objects against Earth's gravity. The potential energy due to elevated positions is called gravitational potential energy, and is evidenced by water in an elevated reservoir or kept behind a dam. If an object falls from one point to another point inside a gravitational field, the force of gravity will do positive work on the object, and the gravitational potential energy will decrease by the same amount.
Consider a book placed on top of a table. As the book is raised from the floor, to the table, some external force works against the gravitational force. If the book falls back to the floor, the "falling" energy the book receives is provided by the gravitational force. Thus, if the book falls off the table, this potential energy goes to accelerate the mass of the book and is converted into kinetic energy. When the book hits the floor this kinetic energy is converted into heat, deformation and sound by the impact.
The factors that affect an object's gravitational potential energy are its height relative to some reference point, its mass, and the strength of the gravitational field it is in. Thus, a book lying on a table has less gravitational potential energy than the same book on top of a taller cupboard, and less gravitational potential energy than a heavier book lying on the same table. An object at a certain height above the Moon's surface has less gravitational potential energy than at the same height above the Earth's surface because the Moon's gravity is weaker. Note that "height" in the common sense of the term cannot be used for gravitational potential energy calculations when gravity is not assumed to be a constant. The following sections provide more detail.
Local approximation.
The strength of a gravitational field varies with location. However, when the change of distance is small in relation to the distances from the center of the source of the gravitational field, this variation in field strength is negligible and we can assume that the force of gravity on a particular object is constant. Near the surface of the Earth, for example, we assume that the acceleration due to gravity is a constant "g" = 9.8 m/s2 ("standard gravity"). In this case, a simple expression for gravitational potential energy can be derived using the "W" = "Fd" equation for work, and the equation
The amount of gravitational potential energy possessed by an elevated object is equal to the work done against gravity in lifting it. The work done equals the force required to move it upward multiplied with the vertical distance it is moved (remember "W = Fd"). The upward force required while moving at a constant velocity is equal to the weight, "mg", of an object, so the work done in lifting it through a height "h" is the product "mgh". Thus, when accounting only for mass, gravity, and altitude, the equation is:
where "U" is the potential energy of the object relative to its being on the Earth's surface, "m" is the mass of the object, "g" is the acceleration due to gravity, and "h" is the altitude of the object. If "m" is expressed in kilograms, "g" in m/s2 and "h" in metres then "U" will be calculated in joules.
Hence, the potential difference is
General formula.
However, over large variations in distance, the approximation that "g" is constant is no longer valid, and we have to use calculus and the general mathematical definition of work to determine gravitational potential energy. For the computation of the potential energy we can integrate the gravitational force, whose magnitude is given by Newton's law of gravitation, with respect to the distance "r" between the two bodies. Using that definition, the gravitational potential energy of a system of masses "m"1 and "M"2 at a distance "r" using gravitational constant "G" is
where "K" is an arbitrary constant dependent on the choice of datum from which potential is measured. Choosing the convention that "K"=0 (i.e. in relation to a point at infinity) makes calculations simpler, albeit at the cost of making "U" negative; for why this is physically reasonable, see below.
Given this formula for "U", the total potential energy of a system of "n" bodies is found by summing, for all formula_30 pairs of two bodies, the potential energy of the system of those two bodies.
Considering the system of bodies as the combined set of small particles the bodies consist of, and applying the previous on the particle level we get the negative gravitational binding energy. This potential energy is more strongly negative than the total potential energy of the system of bodies as such since it also includes the negative gravitational binding energy of each body. The potential energy of the system of bodies as such is the negative of the energy needed to separate the bodies from each other to infinity, while the gravitational binding energy is the energy needed to separate all particles from each other to infinity.
therefore,
Why choose a convention where gravitational energy is negative?
As with all potential energies, only differences in gravitational potential energy matter for most physical purposes, and the choice of zero point is arbitrary. Given that there is no reasonable criterion for preferring one particular finite "r" over another, there seem to be only two reasonable choices for the distance at which "U" becomes zero: formula_33 and formula_34. The choice of formula_35 at infinity may seem peculiar, and the consequence that gravitational energy is always negative may seem counterintuitive, but this choice allows gravitational potential energy values to be finite, albeit negative.
The singularity at formula_33 in the formula for gravitational potential energy means that the only other apparently reasonable alternative choice of convention, with formula_35 for formula_33, would result in potential energy being positive, but infinitely large for all nonzero values of "r", and would make calculations involving sums or differences of potential energies beyond what is possible with the real number system. Since physicists abhor infinities in their calculations, and "r" is always non-zero in practice, the choice of formula_35 at infinity is by far the more preferable choice, even if the idea of negative energy in a gravity well appears to be peculiar at first.
The negative value for gravitational energy also has deeper implications that make it seem more reasonable in cosmological calculations where the total energy of the universe can meaningfully be considered; see inflation theory for more on this.
Uses.
Gravitational potential energy has a number of practical uses, notably the generation of pumped-storage hydroelectricity. For example in Dinorwig, Wales, there are two lakes, one at a higher elevation than the other. At times when surplus electricity is not required (and so is comparatively cheap), water is pumped up to the higher lake, thus converting the electrical energy (running the pump) to gravitational potential energy. At times of peak demand for electricity, the water flows back down through electrical generator turbines, converting the potential energy into kinetic energy and then back into electricity. The process is not completely efficient and some of the original energy from the surplus electricity is in fact lost to friction.
Gravitational potential energy is also used to power clocks in which falling weights operate the mechanism. It's also used by counterweights for lifting up an elevator, crane, or sash window.
Roller coasters are an entertaining way to utilize potential energy - chains are used to move a car up an incline (building up gravitational potential energy), to then have that energy converted into kinetic energy as it falls.
Another practical use is utilizing gravitational potential energy to descend (perhaps coast) downhill in transportation such as the descent of an automobile, truck, railroad train, bicycle, airplane, or fluid in a pipeline. In some cases the kinetic energy obtained from potential energy of descent may be used to start ascending the next grade such as what happens when a road is undulating and has frequent dips. The commercialization of stored energy (in the form of rail cars raised to higher elevations) that is then converted to electrical energy when needed by an electrical grid, is being undertaken in the United States in a system called Advanced Rail Energy Storage (ARES).
Chemical potential energy.
Chemical potential energy is a form of potential energy related to the structural arrangement of atoms or molecules. This arrangement may be the result of chemical bonds within a molecule or otherwise. Chemical energy of a chemical substance can be transformed to other forms of energy by a chemical reaction. As an example, when a fuel is burned the chemical energy is converted to heat, same is the case with digestion of food metabolized in a biological organism. Green plants transform solar energy to chemical energy through the process known as photosynthesis, and electrical energy can be converted to chemical energy through electrochemical reactions.
The similar term chemical potential is used to indicate the potential of a substance to undergo a change of configuration, be it in the form of a chemical reaction, spatial transport, particle exchange with a reservoir, etc.
Electric potential energy.
An object can have potential energy by virtue of its electric charge and several forces related to their presence. There are two main types of this kind of potential energy: electrostatic potential energy, electrodynamic potential energy (also sometimes called magnetic potential energy).
Electrostatic potential energy.
Electrostatic potential energy between two bodies in space is obtained from the force exerted by a charge "Q" on another charge "q" which is given by
where r is the position vector from "Q" to "q" and "ε"0 is the vacuum permittivity. This may also be written using Coulomb's constant "k"e = 1 ⁄ 4πε0.
If the electric charge of an object can be assumed to be at rest, then it has potential energy due to its position relative to other charged objects. The electrostatic potential energy is the energy of an electrically charged particle (at rest) in an electric field. It is defined as the work that must be done to move it from an infinite distance away to its present location, adjusted for non-electrical forces on the object. This energy will generally be non-zero if there is another electrically charged object nearby.
The work "W" required to move "q" from "A" to any point "B" in the electrostatic force field is given by the potential function
A related quantity called "electric potential" (commonly denoted with a "V" for voltage) is equal to the electric potential energy per unit charge.
Magnetic potential energy.
The energy of a magnetic moment m in an externally produced magnetic B-field B has potential energy
The magnetization M in a field is
where the integral can be over all space or, equivalently, where M is nonzero.
Magnetic potential energy is the form of energy related not only to the distance between magnetic materials, but also to the orientation, or alignment, of those materials within the field. For example, the needle of a compass has the lowest magnetic potential energy when it is aligned with the north and south poles of the Earth's magnetic field. If the needle is moved by an outside force, torque is exerted on the magnetic dipole of the needle by the Earth's magnetic field, causing it to move back into alignment. The magnetic potential energy of the needle is highest when its field is in the same direction as the Earth's magnetic field. Two magnets will have potential energy in relation to each other and the distance between them, but this also depends on their orientation. If the opposite poles are held apart, the potential energy will be the highest when they are near the edge of their attraction, and the lowest when they pull together. Conversely, like poles will have the highest potential energy when forced together, and the lowest when they spring apart.
Nuclear potential energy.
Nuclear potential energy is the potential energy of the particles inside an atomic nucleus. The nuclear particles are bound together by the strong nuclear force. Weak nuclear forces provide the potential energy for certain kinds of radioactive decay, such as beta decay.
Nuclear particles like protons and neutrons are not destroyed in fission and fusion processes, but collections of them have less mass than if they were individually free, and this mass difference is liberated as heat and radiation in nuclear reactions (the heat and radiation have the missing mass, but it often escapes from the system, where it is not measured). The energy from the Sun is an example of this form of energy conversion. In the Sun, the process of hydrogen fusion converts about 4 million tonnes of solar matter per second into electromagnetic energy, which is radiated into space.
Forces, potential and potential energy.
Potential energy is closely linked with forces. If the work done by a force on a body that moves from "A" to "B" does not depend on the path between these points, then the work of this force measured from "A" assigns a scalar value to every other point in space and defines a scalar potential field. In this case, the force can be defined as the negative of the vector gradient of the potential field.
For example, gravity is a conservative force. The associated potential is the gravitational potential, often denoted by formula_44 or formula_45, corresponding to the energy per unit mass as a function of position. The gravitational potential energy of two particles of mass "M" and "m" separated by a distance "r" is
The gravitational potential (specific energy) of the two bodies is
where formula_48 is the reduced mass.
The work done against gravity by moving an infinitesimal mass from point A with formula_49 to point B with formula_50 is formula_51 and the work done going back the other way is formula_52 so that the total work done in moving from A to B and returning to A is
If the potential is redefined at A to be formula_54 and the potential at B to be formula_55, where formula_56 is a constant (i.e. formula_56 can be any number, positive or negative, but it must be the same at A as it is at B) then the work done going from A to B is
as before.
In practical terms, this means that one can set the zero of formula_59 and formula_44 anywhere one likes. One may set it to be zero at the surface of the Earth, or may find it more convenient to set zero at infinity (as in the expressions given earlier in this section).
A conservative force can be expressed in the language of differential geometry as a closed form. As Euclidean space is contractible, its de Rham cohomology vanishes, so every closed form is also an exact form, and can be expressed as the gradient of a scalar field. This gives a mathematical justification of the fact that all conservative forces are gradients of a potential field.

</doc>
<doc id="23704" url="http://en.wikipedia.org/wiki?curid=23704" title="Pyramid">
Pyramid

A pyramid (from Greek: πυραμίς "pyramis") is a structure whose outer surfaces are triangular and converge to a single point at the top, making the shape roughly a pyramid in the geometric sense. The base of a pyramid can be trilateral, quadrilateral, or any polygon shape, meaning that a pyramid has at least three outer triangular surfaces (at least four faces including the base). The square pyramid, with square base and four triangular outer surfaces, is a common version.
A pyramid's design, with the majority of the weight closer to the ground, and with the pyramidion on top means that less material higher up on the pyramid will be pushing down from above. This distribution of weight allowed early civilizations to create stable monumental structures.
Pyramids have been built by civilizations in many parts of the world. For thousands of years, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both of Egypt, the latter the only one of the Seven Wonders of the Ancient World still remaining. Khufu's Pyramid is built mainly of limestone (with large red granite blocks used in some interior chambers), and is considered an architectural masterpiece. It contains over 2,300,000 blocks ranging in weight from 2.5 t to 15 t and is built on a square base with sides measuring about 230 m (755 ft), covering 13 acres. Its four sides face the four cardinal points precisely and it has an angle of 52 degrees. The original height of the Pyramid was 146.5 m (488 ft), but today it is only 137 m (455 ft) high, the 9 m (33 ft) that is missing is due to the theft of the fine quality white Tura limestone covering, or casing stones, for construction in Cairo. It is still the tallest pyramid.
The largest pyramid by volume is the Great Pyramid of Cholula, in the Mexican state of Puebla.
Ancient monuments.
Mesopotamia.
The Mesopotamians built the earliest pyramidal structures, called "ziggurats". In ancient times, these were brightly painted in gold/bronze. Since they were constructed of sun-dried mud-brick, little remains of them. Ziggurats were built by the Sumerians, Babylonians, Elamites, Akkadians, and Assyrians for local religions. Each ziggurat was part of a temple complex which included other buildings. The precursors of the ziggurat were raised platforms that date from the Ubaid period during the fourth millennium BC. The earliest ziggurats began near the end of the Early Dynastic Period. The latest Mesopotamian ziggurats date from the 6th century BC. Built in receding tiers upon a rectangular, oval, or square platform, the ziggurat was a pyramidal structure with a flat top. Sun-baked bricks made up the core of the ziggurat with facings of fired bricks on the outside. The facings were often glazed in different colors and may have had astrological significance. Kings sometimes had their names engraved on these glazed bricks. The number of tiers ranged from two to seven. It is assumed that they had shrines at the top, but there is no archaeological evidence for this and the only textual evidence is from Herodotus. Access to the shrine would have been by a series of ramps on one side of the ziggurat or by a spiral ramp from base to summit. The Mesopotamian ziggurats were not places for public worship or ceremonies. They were believed to be dwelling places for the gods and each city had its own patron god. Only priests were permitted on the ziggurat or in the rooms at its base, and it was their responsibility to care for the gods and attend to their needs. The priests were very powerful members of Sumerian society.
Egypt.
The most famous pyramids are the Egyptian pyramids — huge structures built of brick or stone, some of which are among the world's largest constructions. They are shaped as a reference to the rays of the sun. Most pyramids had a polished, highly reflective white limestone surface, in order to give them a shining appearance when viewed from a distance. The capstone was usually made of hard stone - granite or basalt - and could be plated with gold, silver, or electrum and would also be highly reflective.
After 2700 BC, the Egyptians began building pyramids, until about 1700 BC. The first pyramid was built during the Third Dynasty by king Djoser and his architect Imhotep, as a step pyramid by stacking six mastabas. The largest Egyptian pyramids are the pyramids at Giza. "The Egyptian sun god Ra, considered the father of all pharaohs, was said to have created himself from a pyramid-shaped mound of earth before creating all other gods. The pyramid’s shape is thought to have symbolized the sun’s rays" (Donald B. Redford, Ph.D., Penn State).
The age of the pyramids reached its zenith at Giza in 2575–2150 BC. Ancient Egyptian pyramids were in most cases placed west of the river Nile because the divine pharaoh’s soul was meant to join with the sun during its descent before continuing with the sun in its eternal round.
As of 2008, some 135 pyramids have been discovered in Egypt. The Great Pyramid of Giza is the largest in Egypt and one of the largest in the world. It was the tallest building in the world until Lincoln Cathedral was finished in 1311 AD. The base is over 52600 m2 in area. While pyramids are associated with Egypt, the nation of Sudan has 220 extant pyramids, the most numerous in the world.
The Great Pyramid of Giza is one of the Seven Wonders of the Ancient World. It is the only one to survive into modern times. The Ancient Egyptians covered the faces of pyramids with polished white limestone, containing great quantities of fossilized seashells. Many of the facing stones have fallen or have been removed and used for construction in Cairo.
Most pyramids are located near Cairo, with only one royal pyramid being located south of Cairo, at the Abydos temple complex. The pyramid at Abydos, Egypt were commissioned by Ahmose I who founded the 18th Dynasty and the New Kingdom.
The building of pyramids began in the Third Dynasty with the reign of King Djoser. Early kings such as Snefru built several pyramids, with subsequent kings adding to the number of pyramids until the end of the Middle Kingdom. The last king to build royal pyramids was Ahmose, with later kings hiding their tombs in the hills, like in the Valley of the Kings in Luxor's West Bank.
In Medinat Habu, or Deir el-Medina, smaller pyramids were built by individuals. Smaller pyramids were also built by the Nubians who ruled Egypt in the Late Period, though their pyramids had steeper sides.
Sudan.
Nubian pyramids were constructed (roughly 240 of them) at three sites in Sudan to serve as tombs for the kings and queens of Napata and Meroë. The pyramids of Kush, also known as Nubian Pyramids, have different characteristics than the pyramids of Egypt. The Nubian pyramids were constructed at a steeper angle than Egyptian ones. Pyramids were still being built in Sudan as late as 300 AD.
Nigeria.
One of the unique structures of Igbo culture was the Nsude Pyramids, at the Nigerian town of Nsude, northern Igboland. Ten pyramidal structures were built of clay/mud. The first base section was 60 ft. in circumference and 3 ft. in height. The next stack was 45 ft. in circumference. Circular stacks continued, till it reached the top. The structures were temples for the god Ala/Uto, who was believed to reside at the top. A stick was placed at the top to represent the god's residence. The structures were laid in groups of five parallel to each other. Because it was built of clay/mud like the Deffufa of Nubia, time has taken its toll requiring periodic reconstruction.
Greece.
Pausanias (2nd century AD) mentions two buildings resembling pyramids, one, 19 kilometres (12 mi) southwest of the still standing structure at Hellenikon, a common tomb for soldiers who died in a legendary struggle for the throne of Argos and another which he was told was the tomb of Argives killed in a battle around 669/8 BC. Neither of these still survive and there is no evidence that they resembled Egyptian pyramids.
There are also at least two surviving pyramid-like structures still available to study, one at Hellenikon and the other at Ligourio/Ligurio, a village near the ancient theatre Epidaurus. These buildings were not constructed in the same manner as the pyramids in Egypt. They do have inwardly sloping walls but other than those there is no obvious resemblance to Egyptian pyramids. They had large central rooms (unlike Egyptian pyramids) and the Hellenikon structure is rectangular rather than square, 12.5 by which means that the sides could not have met at a point. The stone used to build these structures was limestone quarried locally and was cut to fit, not into freestanding blocks like the Great Pyramid of Giza.
There are no remains or graves in or near the structures. Instead, the rooms that the walls housed were made to be locked from the inside. This coupled with the platform roof, means that one of the functions these structures could have served was as watchtowers. Another possibility for the buildings is that they are shrines to heroes and soldiers of ancient times, but the lock on the inside makes no sense for such a purpose.
The dating of these structures has been made from the pot shards excavated from the floor and on the grounds. The latest dates available from scientific dating have been estimated around the 5th and 4th centuries. Normally this technique is used for dating pottery, but here researchers have used it to try to date stone flakes from the walls of the structures. This has created some debate about whether or not these structures are actually older than Egypt, which is part of the Black Athena controversy. The basis for their use of thermoluminescence in order to date these structures is a new method of collecting samples for testing. Scientists from laboratories hired out by the recent excavators of the site, The Academy of Athens, say that they can use the electrons trapped on the inner surface of the stones to positively identify the date that the stones were quarried and put together.
Mary Lefkowitz has criticised this research. She suggests that some of the research was done not to determine the reliability of the dating method, as was suggested, but to back up an assumption of age and to make certain points about pyramids and Greek civilization. She notes that not only are the results not very precise, but that other structures mentioned in the research are not in fact pyramids, e.g. a tomb alleged to be the tomb of Amphion and Zethus near Thebes, a structure at Stylidha (Thessaly) which is just a long wall, etc. She also notes the possibility that the stones that were dated might have been recycled from earlier constructions. She also notes that earlier research from the 1930s, confirmed in the 1980s by Fracchia was ignored. She argues that they undertook their research using a novel and previously untested methodology in order to confirm a predetermined theory about the age of these structures.
Liritzis responded in a journal article published in 2011, stating that Lefkowitz failed to understand and misinterpreted the methodology.
Spain.
The Pyramids of Güímar refer to six rectangular pyramid-shaped, terraced structures, built from lava stone without the use of mortar. They are located in the district of Chacona, part of the town of Güímar on the island of Tenerife in the Canary Islands. The structures have been dated to the 19th century and their original function explained as a byproduct of contemporary agricultural techniques.
Local traditions as well as surviving images indicate that similar structures (also known as, "Morras", "Majanos", "Molleros", or "Paredones") could once have been found in many locations on the island. However, over time they have been dismantled and used as a cheap building material. In Güímar itself there were nine pyramids, only six of which survive.
China.
There are many square flat-topped mound tombs in China. The First Emperor Qin Shi Huang (circa 221 BC, who unified the 7 pre-Imperial Kingdoms) was buried under a large mound outside modern day Xi'an. In the following centuries about a dozen more Han Dynasty royals were also buried under flat-topped pyramidal earthworks.
Mesoamerica.
A number of Mesoamerican cultures also built pyramid-shaped structures. Mesoamerican pyramids were usually stepped, with temples on top, more similar to the Mesopotamian ziggurat than the Egyptian pyramid.
The largest pyramid by volume is the Great Pyramid of Cholula, in the Mexican state of Puebla. Constructed from the 3rd century BC to the 9th century AD, this pyramid is considered the largest monument ever constructed anywhere in the world, and is still being excavated. The third largest pyramid in the world, the Pyramid of the Sun, at Teotihuacan is also located in Mexico. There is an unusual pyramid with a circular plan at the site of Cuicuilco, now inside Mexico City and mostly covered with lava from an eruption of the Xitle Volcano in the 1st century BC. There are several circular stepped pyramids called Guachimontones in Teuchitlán, Jalisco as well.
Pyramids in Mexico were often used as places of human sacrifice. For the re-consecration of Great Pyramid of Tenochtitlan in 1487, the Aztecs reported that they sacrificed about 80,400 people over the course of four days.
North America.
Many pre-Columbian Native American societies of ancient North America built large pyramidal earth structures known as platform mounds. Among the largest and best-known of these structures is Monks Mound at the site of Cahokia in what became Illinois, completed around 1100 AD, which has a base larger than that of the Great Pyramid at Giza. Many of the mounds underwent multiple episodes of mound construction at periodic intervals, some becoming quite large. They are believed to have played a central role in the mound-building peoples' religious life and documented uses include semi-public chief's house platforms, public temple platforms, mortuary platforms, charnel house platforms, earth lodge/town house platforms, residence platforms, square ground and rotunda platforms, and dance platforms. Cultures who built substructure mounds include the Troyville culture, Coles Creek culture, Plaquemine culture and Mississippian cultures.
Roman Empire.
The 27-meter-high Pyramid of Cestius was built by the end of the 1st century BC and still exists today, close to the Porta San Paolo. Another one, named "Meta Romuli", standing in the "Ager Vaticanus" (today's Borgo), was destroyed at the end of the 15th century.
Medieval Europe.
Pyramids have occasionally been used in Christian architecture of the feudal era, e.g. as the tower of Oviedo's Gothic Cathedral of San Salvador.
India.
Many giant granite temple pyramids were made in South India during the Chola Empire, many of which are still in religious use today. Examples of such pyramid temples include Brihadisvara Temple at Thanjavur, the Temple of Gangaikondacholapuram and the Airavatesvara Temple at Darasuram. However the largest temple pyramid in the area is Sri Rangam in Srirangam, Tamil Nadu. The Thanjavur temple was built by Raja raja Chola in the 11th century. The Brihadisvara Temple was declared by UNESCO as a World Heritage Site in 1987; the Temple of Gangaikondacholapuram and the Airavatesvara Temple at Darasuram were added as extensions to the site in 2004.
Indonesia.
Next to menhir, stone table, and stone statue; Austronesian megalithic culture in Indonesia also featured earth and stone step pyramid structures called "Punden Berundak" as discovered in Pangguyangan, Cisolok and Gunung Padang, West Java. The construction of stone pyramids is based on the native beliefs that mountains and high places are the abode for the spirit of the ancestors.
The step pyramid is the basic design of 8th century Borobudur Buddhist monument in Central Java. However the later temples built in Java were influenced by Indian Hindu architecture, as displayed by the towering spires of Prambanan temple. In the 15th century Java during late Majapahit period saw the revival of Austronesian indigenous elements as displayed by Sukuh temple that somewhat resemble Mesoamerican pyramid.
Peru.
Andean cultures had used pyramids in various architectural structures such as the ones in Caral, Túcume and Chavín de Huantar.

</doc>
<doc id="23705" url="http://en.wikipedia.org/wiki?curid=23705" title="Predestination">
Predestination

Predestination, in theology, is the doctrine that all events have been willed by God, usually with reference to the eventual fate of the individual soul. Explanations of predestination often seek to address the "paradox of free will", whereby God's omniscience seems incompatible with human free will. In this usage, predestination can be regarded as a form of religious determinism; and usually predeterminism.
Contrasted with other kinds of determinism.
Predestination is the Divine foreordaining or foreknowledge of all that will happen; with regard to the salvation of some and not others. It has been particularly associated with the teachings of John Calvin.
"Predestination" may sometimes be used to refer to other, materialistic, spiritualist, non-theistic or polytheistic ideas of determinism, "destiny", "fate", "doom", or "adrsta". Such beliefs or philosophical systems may hold that any outcome is finally determined by the complex interaction of multiple, possibly immanent, possibly impersonal, possibly equal forces, rather than the issue of a Creator's conscious choice.
For example, some may speak of predestination from a purely physical perspective, such as in a discussion of time travel. In this case, rather than referring to the afterlife, predestination refers to any events that will occur in the future. In a predestined universe the future is immutable and only God's ordained set of events can possibly occur; in a non-predestined universe, the future is mutable. In Chinese Buddhism, "predestination" is a translation of "yuanfen", which does not necessarily imply the existence or involvement of a deity. "Predestination" in this sense takes on a very literal meaning: "pre-" (before) and "destiny", in a straightforward way indicating that some events seem bound to happen. The term, however, is often used to describe relationships instead of all events in general.
Finally, antithetical to determinism of any kind are theories of the cosmos that assert that any outcome is ultimately unpredictable. The ludibrium of luck, chance, or chaos theory have determinist implications, as a logical consequence of the idea of predictability. But "predestination" usually refers to a specifically religious type of determinism, especially as found in various monotheistic systems where omniscience is attributed to God, including Christianity and Islam.
Predestination and omniscience.
Discussion of predestination usually involves consideration of whether God is omniscient, or eternal or atemporal (free from limitations of time or even causality). In terms of these ideas, God may see the past, present, and future, so that God effectively knows the future. If God in some sense knows ahead of time what will happen, then events in the universe are effectively predetermined from God's point of view. This is a form of determinism but not predestination since the latter term implies that God has actually determined (rather than simply seen) in advance the destiny of creatures.
Within Christendom, there is considerable disagreement about God's role in setting ultimate destinies (that is, eternal life or eternal damnation). Christians who follow teachers such as John Calvin generally accept that God alone decides the eternal destinations of each person without regard to man's choices, so that their future actions or beliefs follow according to God's choice (Romans 9:14-16). A contrasting Christian view maintains that God is completely sovereign over all things but that he chose to give each individual self-determining free will through prevenient grace. Classically, this view is called Arminianism, which holds that each person is able to accept or reject God's offer of salvation and hence God allows man's choice to determine salvation (John 3:16-18).
Judaism may accept the possibility that God is atemporal; some forms of Jewish theology teach this virtually as a principle of faith, while other forms of Judaism do not. Jews may use the term "omniscience", or "preordination" as a corollary of omniscience, but normally reject the idea of predestination as being incompatible with the free will and responsibility of moral agents, and it therefore has no place in their religion.
Islam traditionally has strong views of predestination similar to some found in Christianity. In Islam, God knows what choices humans are going to make and allows the actualization of the consequences of those choices based on his attributes of justice and mercy. Muslims believe that God is literally atemporal, eternal and omniscient.
In philosophy, the relation between foreknowledge and predestination is a central part of Newcomb's paradox.
Predestination and time.
A number of speculative ideas have appeared that attempt to explain the relationship between time and eternity, which have bearing on the subject of predestination. Some regard all speculation about predestination and its implications as all alike, pernicious, and offensive to God.
A common pre-Kantian idea of time and eternity, describes "eternity" as a trans-temporal mode of being - such that all the moments of time are in some sense present in eternity. God looks into the realm of temporal reality from outside of it, as though it were a surface or a line stretched out: the edges or ends of which are fully "visible" to God, so that He is in a somewhat spatial sense "omnipresent" with regard to time. In such a speculative view, the past, present and future are all in some sense simultaneously present in the eternal perspective of God. From a temporal point of view, the past seems to disappear and the future doesn't yet exist, and God always appears to act from moment to moment. But from an eternal perspective, there is nothing temporal about time. Non-determinism is not possible on such a view, but predestination may be excluded if the belief system does not permit the direct interference of the non-temporal God and the temporal plane of existence.
Some belief systems allow for the possibility that only God and the present moment are the sum of what is "real". The past persists only in its effects, and the future does not yet exist, and thus only the present is directly knowable. Further, the "eternity" of God is presumed by some not to be accessible to understanding, and therefore no speculation can be meaningfully based upon it.
Nevertheless, these belief systems may retain an idea of God's decision eternally determining the present or future, in the sense of God's decision being "logically prior", or "transcendentally necessary" to all existence. Time is not a "thing", but rather, a succession of the intersections of God's manifold purposes being revealed in the creation. Time is the succession of events, identified as moments by an intentional, mental act of setting one event apart from another and noticing their relation to one another - but, otherwise time does not exist as irreducible, discrete moments. Time is coherent, because God consistently acts according to his own character.
Strong predestinarian views are basically undisturbed by these assumptions, because strong predestination is based upon God's knowledge of Himself and of His own purposes. The effect of these new views of time are more clearly seen among those who reject strong predestinarian views, because those views classically share a comparable conception of the relation between time and eternity.
" Predestinarian version":
God, in comparison to temporality, always is. Temporal things however, exist from each fleeting moment of being to the next, only in the present. Such a conception of reality may be thoroughly predestinarian, if God is the personal cause of continued existence and the orchestrator or determiner of the relationship between each present event and each subsequent present event; but, it is only predestination if in this conception God acts with absolute freedom and entire knowledge of Himself. God brings to pass each moment in its turn by a continuous, timeless act of self-revelation. God sustains the effectiveness of all secondary causes and choices, and so on. Thus, each moment is a disclosure of God's character. The meaning of time and experience is disclosed not in the subjective relation of the present to the past and the future, but rather, because of the relation of all created things, in every aspect, to the will of God. As a logical consequence, the meaning of history is known only through the knowledge of God (an idea similar to this can be found in the speculations of Augustine of Hippo and some Calvinist philosophers, such as Herman Dooyeweerd).
" Anti-predestinarian version":
If the idea of absolute freedom and entire self-knowledge is absent from this kind of idea of God's acts in time, then God Himself is (to express the idea anthropomorphically) becoming something new, or discovering something new about Himself with each new moment, just as we are. It's as though God is waking up to the possibilities that are inherent in temporally limited acts, and like an artist developing his ideas in dynamic interaction with an ever-changing medium, He is making new discoveries about himself every day. A summary of such a view might be that, the present is an encounter "in God" with new possibilities (where "God" is sometimes not understood "theistically", in the sense of a "person"), and the past is thus a record or remembrance "by God" of the experiences of existent beings. Or, put another way, the past is what God has thus far become in the process of all experience, and the future is pure possibility. Predestination is completely excluded from such a system, except possibly in the most broad outlines of God's intentions. God's decision, on such a view, is an inventive experience, almost precisely equivalent to the unfolding process of historical events (thinking like this can be found in modern process theology and open theism).
There are other types of Christian or Christian-influenced belief, which exclude the personality, or the volitional aspect of the personality of God, so that even if they express some form of determinism, it is not predestination in a theistic sense.
Types of predestination.
Predestination may be described under two types, with the basis for each found within their definition of free will. Between these poles, there is a complex variety of systematic differences, particularly difficult to describe because the foundational terms are not strictly equivalent between systems. The two poles of predestinarian belief may be usefully described in terms of their doctrinal comparison between the Creator's freedom, and the creature's freedom. These can be contrasted as either univocal, or equivocal conceptions of freedom.
In terms of ultimates, with God's decision to create as the ultimate beginning, and the ultimate outcome, a belief system has a doctrine of predestination if it teaches:
There are numerous ways to describe the spectrum of beliefs concerning predestination in Christian thinking. To some extent, this spectrum has analogies in other monotheistic religions, although in other religions the term "predestination" may not be used. For example, teaching on predestination may vary in terms of three considerations.
Furthermore, the same sort of considerations apply to the freedom of man's will.
Univocal concept of freedom.
The univocal conception of freedom holds that human will is free of cause, even though creaturely in character. These belief systems hold that the Creator (or, in some cases, Nature or Evolution) has fashioned a system of absolute freedom: human volition that features a free and independent nature.
On the other end of the spectrum is the position that the Creator (or a foreign Being, object, etc.) exercises absolute control over human will and/or that all decisions originate with some outside cause, leaving no room for freedom.
Equivocal or analogical concepts of freedom.
At the other end of the spectrum are analogical conceptions of freedom. These versions of predestination hold that individual choice is not excluded from the fashioning work of the Creator. Man's will is free "because" it is determined, boundaried or created by God. In other words, apart from God's will determining man's will in a divine sense, only chaos or enslavement to mindless and impersonal forces is possible. Man's will may be called free and responsible, but not in an absolute sense; the choice of good or of evil must be uncoerced to be free, but it is never uncreated or uncaused. The likeness of creaturely freedom to divine freedom is analogical, not univocal.
It is important to note that among predestinarians there is no significant representation for the idea that human choices are unreal, but merely that they are the direct expression of the Creator's will. The analogy implied here means that however else human and divine freedom may be comparable, there is an unlikeness between the free will of the Creator and human freedom, which depends on the Creator for existence and power. With no significant exception, when predestinarians deny that man has freedom of will, it is to deny that man's will is free in the same sense as the Creator's will, or to affirm that man's choices are entirely subject to divine causation. That men are responsible without being absolutely original is particularly true in these systems, if they acknowledge a doctrine of Original Sin, whereby every person is understood to be born into a condition of helplessness under the power or the effects of sin; for whom, either through inherited guilt, or the inherited consequences of guilt, a purely free choice of the good is not possible without the aid of God's undeserved grace.
Traditional Islam holds to the powerlessness of human will, apart from the aid of Allah, and yet without a doctrine of Original Sin. Thus, Islam has a simpler version of predestination, viewing all that comes to pass as the will of Allah. And yet, the Qur'an affirms human responsibility, saying for example: "Allah changeth not the condition of a people until they change what is in their hearts". There is no significant view of predestination that entirely relieves man of responsibility for his own choices.
Therefore, all significant versions of predestination account for the differences between people (perhaps in life or, in death, or both) by reference to the will of the Creator. Also, all versions of predestination incorporate into the doctrine various concepts of human responsibility, which differ from one another in terms of the kind of volitional freedom possible for the creature.
Christianity.
Christians understand the doctrine of predestination in terms of God's work of salvation in the world, and in terms of the presedination of the individual soul to salvation or damnation. The doctrine is a tension between the divine perspective in which God saves those whom he chooses (the "elect") from eternity apart from human action and the human perspective of free will in which each person is responsible for his or her choice to accept or reject God. The views on predestination within Christianity vary somewhat in emphasis on one of these two perspectives.
History of the doctrine.
Church Fathers on the doctrine.
The early Church Fathers consistently uphold the freedom of human choice. This position was crucial in the Christian confrontation with Cynicism and some of the chief forms of Gnosticism, such as Manichaeism, which taught that man is by nature flawed and therefore not responsible for evil in himself or in the world. At the same time, belief in human responsibility to do good as a precursor to salvation and eternal reward was consistent. The decision to do good along with God's aid pictured a synergism of the human will and God's will. The early Church Fathers taught a doctrine of conditional predestination.
Augustine of Hippo's early writings affirm that God's predestinating grace is granted on the basis of his foreknowledge of the human desire to pursue salvation, this changed after 396. His later position affirmed the necessity of God granting grace in order for the desire for salvation to be awakened. However, Augustine does argue (against the Manicheans) that humans have free will; however, their will is so distorted, and the Fall is so extensive, that in the postlapsarian world they can only choose evil.
Augustine's position raised objections. Julian bishop of Eclanum, expressed the view that Augustine was bringing Manichean thoughts into the church. For Vincent of Lérins, this was a disturbing innovation. This new tension eventually became obvious with the confrontation between Augustine and Pelagius culminating in condemnation of Pelagianism (as interpreted by Augustine) at the Council of Ephesus in 431. The British monk Pelagius denied Augustine's view of "predestination" in order to affirm that salvation is achieved by an act of free will.
The Eastern Orthodox Church tradition has never adopted the Augustinian view of predestination, and formed a doctrine of predestination by another historical route, sometimes called Semi-Pelagianism in the West. The Western Church, including the Catholic and Protestant denominations, are predominantly Augustinian in some form, especially as interpreted by Gregory the Great and the Second Council of Orange (a Western council that anathematized Semi-Pelagianism as represented in some of the writings of John Cassian and his followers). The council explicitly denied double predestination.
In Catholic doctrine, the accepted understanding of predestination most predominantly follows the interpretation of Thomas Aquinas, and can be contrasted with the Jansenist interpretation of Augustinianism, which was condemned by the Catholic Church during the Counter-Reformation. The only important branch of Western Christianity that continues to hold to a "double predestination" interpretation of Augustinianism, is within the Calvinist branch of the Protestant Reformation. The meaning of this term is discussed under the subsection on Calvinism, below.
In broad Christian conversation, "predestination" refers to the view of predestination commonly associated with John Calvin and the Calvinist branch of the Protestant Reformation; and, this is the non-technical sense in which the term is typically used today, when belief in predestination is affirmed or denied.
Augustine's formulation is neither complete nor universally accepted by Christians. But his system laid the foundation onto virgin ground for the then later writers and innovators of the Reformation period.
Various views on Christian predestination.
Conditional predestination.
Conditional Predestination, or more commonly referred to as conditional election, is a theological stance stemming from the writings and teachings of Jacobus Arminius, after whom Arminianism is named. Arminius studied under the staunch Reformed scholar Theodore Beza, whose views of election, Arminius eventually argued, could not reconcile freedom with moral responsibility.
Arminius used a philosophy called Molinism (named for the philosopher Luis de Molina) that attempted to reconcile freedom with God's omniscience. They both saw human freedom in terms of the Libertarian philosophy: man's choice is not decided by God's choice, thus God's choice is "conditional", depending on what man chooses. Arminius saw God "looking down the corridors of time" to see the free choices of man, and choosing those who will respond in faith and love to God's love and promises, revealed in Jesus.
Arminianism sees the choice of Christ as an impossibility, apart from God's grace; and the freedom to choose is given to all, because God's prevenient grace is universal (given to everyone). Therefore, God predestines on the basis of foreknowledge of how some will respond to his universal love ("conditional"). In contrast, Calvinism views universal grace as resistible and not sufficient for leading to salvation—or denies universal grace altogether—and instead supposes grace that leads to salvation to be particular and irresistible, given to some but not to others on the basis of God's predestinating choice ("unconditional"). This is also known as "double-predestination."
Temporal predestination.
Temporal predestination is the view that God only determines temporal matters, and not eternal ones. This Christian view is analogous to the traditional Jewish view, which distinguishes between "preordination" and "predestination". Temporal matters are pre-ordained by God, but eternal matters, being supra-temporal, are subject to absolute freedom of choice.
Infralapsarianism.
Infralapsarianism (also called sublapsarianism) holds that predestination logically coincides with the preordination of Man's fall into sin. That is, God predestined sinful men for salvation. Therefore according to this view, God is the "ultimate cause", but not the "proximate source" or "author" of sin. Infralapsarians often emphasize a difference between God's decree (which is inviolable and inscrutable), and his revealed will (against which man is disobedient). Proponents also typically emphasize the grace and mercy of God toward all men, although teaching also that only some are predestined for salvation.
In common English parlance, the doctrine of predestination often has particular reference to the doctrines of Calvinism. The version of predestination espoused by John Calvin, after whom Calvinism is named, is sometimes referred to as "double predestination" because in it God predestines some people for salvation (i.e. Unconditional election) and some for condemnation (i.e. Reprobation). Calvin himself defines predestination as "the eternal decree of God, by which he determined with himself whatever he wished to happen with regard to every man. Not all are created on equal terms, but some are preordained to eternal life, others to eternal damnation; and, accordingly, as each has been created for one or other of these ends, we say that he has been predestined to life or to death.".
On the spectrum of beliefs concerning predestination, Calvinism is the strongest form among Christians. It teaches that God's predestining decision is based on the knowledge of His own will rather than foreknowledge, concerning every particular person and event; and, God continually acts with entire freedom, in order to bring about his will in completeness, but in such a way that the freedom of the creature is not violated, "but rather, established"
Calvinists who hold the infralapsarian view of predestination usually prefer that term to "sublapsarianism," perhaps with the intent of blocking the inference that they believe predestination is on the basis of foreknowledge ("sublapsarian" meaning, assuming the fall into sin).
The different terminology has the benefit of distinguishing the Calvinist double predestination version of infralapsarianism from Lutheranism's view that predestination is a mystery, which forbids the unprofitable intrusion of prying minds.
Single predestination.
Drawing on Luther's "Bondage of the Will" written in his debate over free will with Erasmus, Lutherans hold doctrinally to a view of single predestination. That is to say, desiring to save all fallen human beings, God sent his Son Jesus Christ to atone for the sins of the "whole world" on the cross. Those God saves have been predestined from eternity in Christ. Those who are condemned are condemned because of their fallen will. While these statements may seem like they contradict each other, this is what Luther saw as the essential and major story-line within scripture and didn't attempt to systematically or logically "fix" it. The underlying question here is, of course, if God wants all to be saved and Jesus died for everyone, why doesn't God convert the fallen will of all? This is a question that Lutherans, following Luther, put into the category of the "hidden God", the God "behind the cross" whom we don't know everything about. The answer to the question lies within God's "hidden counsel" that we are to have nothing to do with. If we doubt our own predestination, we should look for it in the God who has revealed himself in the wounds of Christ on the cross and there see a God who loved us enough to die for us. For Lutherans, systematic treatment of predestination follows the Gospel (What God has done for us in Jesus Christ) rather than being a topic discussed prior to the Gospel. As such, the sole purpose of predestination is to reinforce "Justification by Grace through Faith solely on account of Christ". Believers are reminded "you didn't choose God, God chose you in Christ!"
Supralapsarianism.
Supralapsarianism is the doctrine that God's decree of predestination for salvation and reprobation logically precedes his preordination of the human race's fall into sin. That is, God decided to save, and to damn; he then determined the means by which that would be made possible. It is a matter of controversy whether or not Calvin himself held this view, but most scholars link him with the infralapsarian position. It is known, however, that Calvin's successor in Geneva, Theodore Beza, held to the supralapsarian view.
Open theism.
Advocates of open theism, like most who affirm conditional predestination, understand predestination to be as corporate. In corporate election, God does not choose which individuals he will save prior to creation, but rather God chooses the church as a whole. Or put differently, God chooses what type of individuals he will save. Another way the New Testament puts this is to say that God chose the church in Christ (Eph. 1:4). In other words, God chose from all eternity to save all those who would be found in Christ, by faith in God. This choosing is not primarily about salvation from eternal destruction either but is about God's chosen agency in the world. Thus individuals have full freedom in terms of whether they become members of the church or not. Corporate election is thus consistent with the open view's position on God's omniscience, which states that the outcomes of individual free will cannot be known specifically before they are performed since who becomes a Christian is a matter of free will and not knowable.
Protestantism.
Lutheranism.
Lutherans central idea to all of their beliefs is that they are saved by grace through faith because of Jesus Christ called Justification. Lutherans do not believe that there are certain people elect that are predestined to salvation, but salvation is predestined for those who seek God. Lutherans believe Christians should be assured that they are among the predestined. However, they disagree with those who make predestination the source of salvation rather than Christ's suffering, death, and resurrection. Unlike some Calvinists, Lutherans do not believe in a predestination to damnation. Instead, Lutherans teach eternal damnation is a result of the unbeliever's sins, rejection of the forgiveness of sins, and unbelief. Martin Luther's attitude towards predestination is set out in his On the Bondage of the Will, published in 1525. This publication by Luther was in response to the published treatise by Desiderius Erasmus in 1524 known as "On Free Will". Luther based his views on Ephesians 2:8-10, which says: "For by grace you have been saved through faith, and that not of yourselves; it is the gift of God, not of works, lest anyone should boast. For we are His workmanship, created in Christ Jesus for good works, which God prepared beforehand that we should walk in them."
Calvinism.
The Belgic Confession of 1561 affirmed that God "delivers and preserves" from perdition "all whom he, in his eternal and unchangeable council, of mere goodness hath elected in Christ Jesus our Lord, without respect to their works" (Article XVI).
Calvinists believe that God picked those who he will save and bring with him to heaven before the world was created. They also believe people God does not save will go to Hell. John Calvin thought people who were saved could never lose their salvation and the "elected" (those God saved) would know they were saved because of their actions.
Controversy concerning Calvinism.
In this common, loose sense of the term, to affirm or to deny predestination has particular reference to the Calvinist doctrine of Unconditional Election. In the Calvinist interpretation of the Bible, this doctrine normally has only pastoral value related to the assurance of salvation and the absolution of salvation by grace alone. However, the philosophical implications of the doctrine of election and predestination are sometimes discussed beyond these systematic bounds. Under the topic of the doctrine of God (theology proper), the predestinating decision of God cannot be contingent upon anything outside of Himself, because all other things are dependent upon Him for existence and meaning. Under the topic of the doctrines of salvation (soteriology), the predestinating decision of God is made from God's knowledge of his own will (Romans 9:15), and is therefore not contingent upon human decisions (rather, free human decisions are outworkings of the decision of God, which sets the total reality within which those decisions are made in exhaustive detail: that is, nothing left to chance). Calvinists do not pretend to understand how this works; but they are insistent that the Scriptures teach both the sovereign control of God and the responsibility and freedom of human decisions (see "Equivocal or analogical concepts of freedom" above).
This view is commonly called "double predestination", although within a Calvinist system this term is usually accepted only with qualifications, and many reject the term altogether as being incompatible with the pastoral use of the doctrine of election.
Double predestination is the eternal act of God, whereby the future of every particular person in the human race has been determined beforehand, by God. Whatever the individual wills or does, for good or for evil, is conceived as performing a functional part, or outworking of that ordained purpose. This prior determination applies to both, the elect and the reprobate. This idea is formed on an interpretation of various Scriptures in the Old and New Testaments. Romans 9 is frequently quoted in explanation of the doctrine.
Calvinist groups use the term "Hyper-Calvinism" to describe Calvinistic systems that assert without qualification that God's intention to destroy some is equal to His intention to save others. Some forms of Hyper-Calvinism have racial implications, against which other Calvinists vigorously object (see Afrikaner Calvinism). The Dutch settlers of South Africa claimed that the Blacks were members of the non-elect, because they were the sons of Ham, whom Noah had cursed to be slaves, according to Genesis 9:18-19. The Dutch Calvinist theologian Franciscus Gomarus also argued that Jews, because of their refusal to worship Jesus Christ, were members of the non-elect. According to I John 2:22-23, anyone who refuses to believe that Jesus is the Christ is an antichrist. This is what I John 2: 22-23 says: "Who is a liar but he who denies that Jesus is the Christ? He is antichrist who denies the Father and the Son. Whoever denies the Son does not have the Father either; he who acknowledges the Son has the Father also." Martin Luther published in 1543 "On the Jews and Their Lies", in which he denounced the Jews for their failure to convert to Christianity.
Expressed sympathetically, the Calvinist doctrine is that God has mercy or withholds it, with particular consciousness of who are to be the recipients of mercy in Christ. Therefore, the particular persons are chosen, out of the total number of human beings, who will be rescued from enslavement to sin and the fear of death, and from punishment due to sin, to dwell forever in His presence. Those who are being saved are assured through the gifts of faith, the sacraments, and communion with God through prayer and increase of good works, that their reconciliation with Him through Christ is settled by the sovereign determination of God's will. God also has particular consciousness of those who are passed over by His selection, who are without excuse for their rebellion against Him, and will be judged for their sins.
By implication, and expressed unsympathetically, the number of the elect subtracted from the total number, leaves an exact number of those who are consciously passed over by the mercy of God, who will dwell forever away from His presence, without regard to anything that otherwise distinguishes people from one another. All are believed to be undeserving, whether they are rich or poor, male or female, murderers or philanthropists, or any other difference. In other words, God determines the exact numbers of the damned and the saved, and these numbers are consciously known and indeed, decided upon by God, before any of these individuals have begun to exist.
Thus, Calvinists may acknowledge with qualifications that, "double predestination" is a legitimate position, logically deduced from any form of single predestination that does not include universal salvation.
Calvinists typically divide on the issue of predestination into infralapsarians (sometimes called 'sublapsarians') and supralapsarians. Infralapsarians interpret the biblical election of God to highlight his love (1 John 4:8; Ephesians 1:4b-5a) and chose his elect considering the situation after the Fall, while supralapsarians interpret biblical election to highlight God's sovereignty (Romans 9:16) and that the Fall was ordained by God's decree of election. In infralapsarianism, election is God's response to the Fall, while in supralapsarianism the Fall is part of God's plan for election. In spite of the division, many Calvinist theologians would consider the debate surrounding the infra- and supralapsarian positions one in which scant Scriptural evidence can be mustered in either direction, and that, at any rate, has little effect on the overall doctrine.
Some Calvinists decline from describing the eternal decree of God in terms of a sequence of events or thoughts, and many caution against the simplifications involved in describing any action of God in speculative terms. Most make distinctions between the positive manner in which God chooses some to be recipients of grace, and the manner in which grace is consciously withheld so that some are destined for everlasting punishments.
Debate concerning predestination according to the common usage, concerns the destiny of the damned, whether God is just if that destiny is settled prior to the existence of any actual violition of the individual, and whether the individual is in any meaningful sense responsible for his destiny if it is settled by the eternal action of God.
Arminianism.
Arminians hold that God does not predetermine, but instead infallibly knows who will believe and perseveringly be saved. This view is known as Conditional Election, because it states that election is conditional on the one who wills to have faith in God for salvation. Although God knows from the beginning of the world who will go where, the choice is still with the individual. The Dutch Calvinist theologian Franciscus Gomarus strongly opposed the views of Jacobus Arminius with his doctrine of supralapsarian predestination.
Karl Barth's view.
Barthians espouse a view of predestination that attempts to circumvent the antithesis between Augustinianism and Pelagianism. In the Barthian scheme, predestination only properly applies to God Himself. Thus, humanity is chosen for salvation in Jesus Christ, at the permanent cost of God's self-surrendered hiddenness, or transcendence. Thus, the redemption of all mankind is a devoutly hoped-for possibility, but the only inevitability is that God has predestined Himself, in Jesus Christ, to be revealed and given for human salvation.
Comparison between Protestants.
This table summarizes the classical views of three different Protestant beliefs.
Eastern Orthodoxy.
The Eastern Orthodox view was summarized by Bishop Theophan the Recluse in response to the question, "What is the relationship between the Divine provision and our free will?"
Roman Catholicism.
Roman Catholicism also teaches the doctrine of predestination, while rejecting the classical Calvinist view known as "double predestination." This means that while it is held that those whom God has elected to eternal life will infallibly attain it, and are therefore said to be predestined to salvation by God, those who perish are not predestined to damnation. The Catholic Encyclopedia entry on Predestination says:
Pope John Paul II wrote:
The Catholic Catechism says:
Nevertheless, certain Catholics in the seventeenth and eighteenth century who followed a movement called Jansenism believed in double predestination, which was condemned by the Vatican as a heretical movement.
Additionally, Catholics do not believe that any hints or evidence of the predestined status of individuals is available to humans, and predestination generally plays little or no part in Catholic teaching to the faithful, being a topic adressed in a professional theological context only.
Roman Catholics on Predestination
St. Augustine of Hippo laid the foundation for much of the later Catholic teaching on predestination. His teachings on grace and free will were largely adopted by the Second Council of Orange (529), whose decrees were directed against the Semipelagians. Augustine wrote, "[God] promised not from the power of our will but from His own predestination. For He promised what He Himself would do, not what men would do. Because, although men do those good things which pertain to God’s worship, He Himself makes them to do what He has commanded; it is not they that cause Him to do what He has promised. Otherwise the fulfilment of God’s promises would not be in the power of God, but in that of men" John Calvin, in his "Institutes of the Christian Religion" claims the authority of St. Augustine for his own teaching on predestination. However, while adopting some of St. Augustine's ideas and arguments, Calvin's denial of free will, and hence his adoption of double predestination, is not consistent with Augustine's. For example, in "On Grace and Free Will," (see especially chapters II-IV) St. Augustine states that "He [God] has revealed to us, through His Holy Scriptures, that there is in man a free choice of will," and that "God's precepts themselves would be of no use to a man unless he had free choice of will, so that by performing them he might obtain the promised rewards." (chap. II)
Thomas Aquinas views concerning predestination are largely in agreement with Augustine and can be summarized by many of his writings in his Summa Theologiae:
"God does reprobate some. For it was said above (A[1]) that predestination is a part of providence. To providence, however, it belongs to permit certain defects in those things which are subject to providence, as was said above (Q[22], A[2]). Thus, as men are ordained to eternal life through the providence of God, it likewise is part of that providence to permit some to fall away from that end; this is called reprobation. Thus, as predestination is a part of providence, in regard to those ordained to eternal salvation, so reprobation is a part of providence in regard to those who turn aside from that end. Hence reprobation implies not only foreknowledge, but also something more, as does providence, as was said above (Q[22], A[1]). Therefore, as predestination includes the will to confer grace and glory; so also reprobation includes the will to permit a person to fall into sin, and to impose the punishment of damnation on account of that sin."
The Church of Jesus Christ of Latter-day Saints.
The LDS church rejects the doctrine of predestination, but does believe in foreordination. Foreordination states that valiant spirits in the pre-mortal realm were foreordained to fulfill certain callings or missions during their mortal experience. However, the fulfillment of these callings is dependent upon the choices of the called individuals. Latter-day Saints believe that Jesus Christ was foreordained to perform the Atonement. The LDS church teaches the doctrine of agency, the ability to choose and act for ourselves, and decide whether to accept Christ's Atonement.
Biblical support of predestination.
Some biblical verses often used as sources for Christian beliefs in predestination are below. Note that most of these verses do not distinguish between the conditional election (Arminian) and unconditional election (Calvinist), but are simply evidence of some type of election.
Biblical support of free will.
Some biblical verses often used as sources for Christian beliefs in free will are below:
Islam.
In Islam, "predestination" is the usual English language rendering of a belief that Muslims call al-qada wa al-qadar in Arabic. The phrase means "the divine decree and the predestination". Despite the fact that Free will and predestination have always been conflicting topics in the thought of certain sects that – according to Sunni thought (Ahlul Sunnah Wal Jama'a) – have gone astray from the true Islamic doctrine; Sunni Muslims believe there is no conflict whatsoever between Free will and Predestination.
It is, however, a difficult concept to understand and translate. In Islam, Allah has predetermined, known, ordained, and is constantly creating every event that takes place in the world. This is entailed by His being omnipotent and omniscient. In Sahihul Bukhary, a chapter is dedicated to authenticated Scripture in this regard, under the title "The creation of humans' Deeds". Sunni scholars hold that there is no contradiction in people's deeds (and naturally their choices) being created and predetermined by the creator, since they define free will to be the antonym of compulsion and coercion. People – in the Sunni perspective – do acknowledge that they are free, since they do not see anybody or anything forcing them to do whatever they chose to do. This, however, does not contradict with the belief that everything they do, including the choices they make, are predestined and predetermined by Allah. Consequently, people are already predestined to either heaven or hell at birth, as Sunnis believe; however, they will have no argument on the day of judgment since they never knew in advance what their fate would be, and they do acknowledge that they have choice; which is what moral responsibility comes with.
The concept of human will being predetermined by Allah's will is stated clearly in the Quran:
"Verily this (The Holy Quran) is no less than a Message to (all) the Worlds; (With profit) to whoever among you wills to go straight, "but ye shall not will except as God wills;" the Cherisher of the Worlds."
Islam and Christianity.
Although comparable in broad terms, the differences between Christian and Islamic ideas of predestination are complex. These differences are due to the distinctives of each faith's belief system. In broad terms, the doctrine of predestination refers to inevitability as a general principle, and usually more particularly refers to the exercise of God's will as it relates to the future of members of the human race, considered either as groups or as individuals, with special concern for issues of human responsibility as it relates to the sovereignty of God. Predestination always involves issues of the Creator's personality and will; and consequently, the different versions of the doctrine of predestination go hand in hand with appropriately different conceptions of the contribution any creature is able to make toward its own present condition, or future destiny.
Judaism.
Generally speaking Reform Judaism has no strong doctrine of predestination. Some critics claim that the idea that God is omnipotent and omniscient didn't formally exist in Judaism during the Biblical Era (mostly before the Siege of Jerusalem (587 BC)), but rather was a later development due to the influence of neo-Platonic and neo-Aristotelian philosophy. Some modern Jewish thinkers in the 20th century (for example, Martin Buber) have resolved the dialectical tension by holding that God is simply not omnipotent, in the commonly used sense of that word. These thinkers are primarily not Orthodox Jews. Orthodox Jewish rabbis generally affirm that God must be viewed as omnipotent, but they have varying definitions of what the word "omnipotent" means. Thus one finds that some Modern Orthodox theologians have views that are essentially the same as non-Orthodox theologians, but they use different terminology.
One noted Jewish philosopher, Hasdai Crescas, resolved this dialectical tension by taking the position that free-will doesn't exist. Hence all of a person's actions are pre-determined by the moment of their birth, and thus their judgment in the eyes of God (so to speak) is effectively pre-ordained. However in this scheme this is not a result of God's predetermining one's fate, but rather from the view that the universe is deterministic. Crescas's views on this topic were rejected by Judaism at large. In later centuries this idea independently developed among some in the Chabad (Lubavitch) movement of Hasidic Judaism. Many individuals within Chabad take this view seriously, and hence effectively deny the existence of free will.
However, many Chabad (Lubavitch) Jews attempt to hold both views. They affirm as infallible their rebbe's teachings that God knows and controls the fate of all, yet at the same time affirm the classical Jewish belief in free-will (i.e. no such thing as determinism). The inherent contradiction between the two results in their belief that such contradictions are only "apparent", due to man's inherent lack of ability to understand greater truths and due to the fact that Creator and Created exist in different realities.
One does not have to be a Chabad Hassid to believe in this, however. It is enough to read the statement in Pirkei Avot: "Everything is predetermined but freedom of will is given." The same idea is strongly repeated by Rambam (Mishneh Torah, Laws of Repentance, Chapter 5).
Many other Jews (Orthodox, Conservative, Reform and secular) affirm that since free-will exists, then by definition one's fate is not preordained. It is held as a tenet of faith that whether God is omniscient or not, nothing interferes with mankind's free will. Some Jewish theologians, both during the medieval era and today, have attempted to formulate a philosophy in which free will is preserved, while also affirming that God has knowledge of what decisions people will make in the future. Whether or not these two ideas are mutually compatible, or whether there is a contradiction between the two, is still a matter of great study and interest in philosophy today.
In Rabbinic literature, there is much discussion as to the apparent contradiction between God's omniscience and free will. The representative view is that "Everything is foreseen; yet free will is given" (Rabbi Akiva, "Pirkei Avoth" ). Based on this understanding, the problem is formally described as a paradox, perhaps beyond our understanding.
Zoroastrianism.
Predestination is rejected in Zoroastrian teaching. Humans bear responsibility for all situations they are in, and in the way they act toward one another. Reward, punishment, happiness, and grief all depend on how individuals live their lives.

</doc>
<doc id="23706" url="http://en.wikipedia.org/wiki?curid=23706" title="Primitive notion">
Primitive notion

In mathematics, logic, and formal systems, a primitive notion is an undefined concept. In particular, a primitive notion is not defined in terms of previously defined concepts, but is only motivated informally, usually by an appeal to intuition and everyday experience. In an axiomatic theory or other formal system, the role of a primitive notion is analogous to that of axiom. In axiomatic theories, the primitive notions are sometimes said to be "defined" by one or more axioms, but this can be misleading. Formal theories cannot dispense with primitive notions, under pain of infinite regress.
Alfred Tarski explained the role of primitive notions as follows:
In axiomatic set theory the fundamental concept of "set" is an example of a primitive notion. As Mary Tiles wrote:
As evidence, she quotes Felix Hausdorff: "A set is formed by the grouping together of single objects into a whole. A set is a plurality thought of as a unit."
When an axiomatic system begins with its axioms, the primitive notions may not be explicitly stated. Susan Haak (1978) wrote, "A set of axioms is sometimes said to give an implicit definition of its primitive terms."
An inevitable regress to primitive notions in the theory of knowledge was explained by Gilbert de B. Robinson:
Examples.
The necessity for primitive notions is illustrated in several areas of mathematics:

</doc>
<doc id="23707" url="http://en.wikipedia.org/wiki?curid=23707" title="Priest">
Priest

A priest or priestess (feminine) (from Greek πρεσβύτερος "presbýteros" through Latin "presbyter", "elder"), is a person authorized to perform the sacred rituals of a religion, especially as a mediatory agent between humans and one or more deities. They also have the authority or power to administer religious rites; in particular, rites of sacrifice to, and propitiation of, a deity or deities. Their office or position is the priesthood, a term which also may apply to such persons collectively.
Priests and priestesses have existed since the earliest of times (see Proto-indo-European trifunctional hypothesis) and in the simplest societies, most likely as a result of agricultural surplus and consequent social stratification. The necessity to read sacred texts and keep temple or church records helped foster literacy in many early societies. Priests exist in many religions today, such as all or some branches of Judaism, Christianity, Shintoism, Hinduism. They are generally regarded as having positive contact with the deity or deities of the religion to which they subscribe, often interpreting the meaning of events and performing the rituals of the religion. There is no common definition of the duties of priesthood between faiths; but generally it includes mediating the relationship between one's congregation, worshippers, and other members of the religious body, and its deity or deities, and administering religious rituals and rites. These often include blessing worshipers with prayers of joy at marriages, after a birth, and at consecrations, teaching the wisdom and dogma of the faith at any regular worship service, and mediating and easing the experience of grief and death at funerals - maintaining a spiritual connection to the afterlife in faiths where such a concept exists. Administering religious building grounds and office affairs and papers, including any religious library or collection of sacred texts, is also commonly a responsibility - for example, the modern term for clerical duties in a secular office refers originally to the duties of a cleric. The question of which religions have a "priest" depends on how the titles of leaders are used or translated into English. In some cases, leaders are more like those that other believers will often turn to for advice on spiritual matters, and less of a "person authorized to perform the sacred rituals." For example, clergy in Roman Catholicism and Orthodox Christianity are "priests", but in Protestant Christianity they are typically "minister" and "pastor". The terms "priest" and "priestess" are sufficiently generic that they may be used in an anthropological sense to describe the religious mediators of an unknown or otherwise unspecified religion.
In many religions, being a priest or priestess is a full-time position, ruling out any other career. Many Christian priests and pastors choose or are mandated to dedicate themselves to their churches and receive their living directly from their churches. In other cases it is a part-time role. For example in the early history of Iceland the chieftains were titled "goði", a word meaning "priest". As seen in the saga of Hrafnkell Freysgoði, however, being a priest consisted merely of offering periodic sacrifices to the Norse gods and goddesses; it was not a full-time role, nor did it involve ordination.
In some religions, being a priest or priestess is by human election or human choice. In Judaism the priesthood is inherited in familial lines. In a theocracy a society is governed by its priesthood.
Etymology.
The word "priest", is ultimately derived from Greek, via Latin "presbyter", the term for "elder", especially elders of Jewish or Christian communities in Late Antiquity. It is possible that the Latin word was loaned into Old English, and only from Old English reached other Germanic languages via the Anglo-Saxon mission to the continent, giving Old Icelandic "prestr", Old Swedish "präster", Old High German "priast". Old High German also has the disyllabic "priester, priestar", apparently derived from Latin independently via Old French "presbtre". The Latin "presbyter" ultimately represents Greek "presbyteros", the regular Latin word for "priest" being "sacerdos", corresponding to Greek "hiereus".
That English should have only the single term "priest" to translate "presbyter" and "sacerdos" came to be seen as a problem in English Bible translations. The "presbyter" is the minister who both presides and instructs a Christian congregation, while the "sacerdos", offerer of sacrifices, or in a Christian context the eucharist, performs "mediatorial offices between God and man".
The feminine English noun, "priestess", was coined in the 17th century, to refer to female priests of the pre-Christian religions of classical antiquity. In the 20th century, the word was used in controversies surrounding the ordination of women. In the case of the ordination of women in the Anglican communion, it is more common to speak of "priests", regardless of gender.
Historical religions.
In historical polytheism, a priest administers the sacrifice to a deity, often in highly elaborate ritual. In the Ancient Near East, the priesthood also acted on behalf of the deities in managing their property.
Priestesses in antiquity often performed sacred prostitution, and in Ancient Greece, some priestesses such as Pythia, priestess at Delphi, acted as oracles.
Ancient Egypt.
In Egyptian ideology, the right and obligation to interact with the gods belonged to the pharaoh. He delegated this duty to priests, who were effectively bureaucrats authorized to act on his behalf. Priests staffed temples throughout Egypt, giving offerings to the cult statues in which the gods were believed to take up residence and performing other rituals for their benefit. Little is known about what training may have been required of priests, and the selection of personnel for positions was affected by a tangled set of traditions, although the pharaoh had the final say. In the New Kingdom, when temples owned great estates, the high priests of the most important cult—that of Amun at Karnak—were important political figures.
High-ranking priestly roles were usually held by men. Women were generally relegated to lower positions in the temple hierarchy, although some held specialized and influential positions, especially that of the God's Wife of Amun, whose religious importance overshadowed the High Priests of Amun in the Late Period.
Ancient Rome.
In Ancient Rome and throughout Italy, the ancient sanctuaries of Ceres and Proserpina were invariably led by female "sacerdotes", drawn from women of local and Roman elites. It was the only public priesthood attainable by Roman matrons and was held in great honor.
Abrahamic religions.
Judaism.
In ancient Israel the priests were required by the Law of Moses to be of direct paternal descendency from Aaron, Moses' elder brother. In Exodus 30:22–25 God instructs Moses to make a holy anointing oil to consecrate the priests "for all of eternity." During the times of the two Jewish Temples in Jerusalem, the Aaronic priests were responsible for the daily and special Jewish holiday offerings and sacrifices within the temples, these offerings are known as the "korbanot".
In Hebrew the word "priest" is "kohen" (singular כהן "kohen", plural כּהנִים "kohanim"), hence the family names "Cohen", "Cahn", "Kahn", "Kohn", "Kogan", etc. These families are from the tribe of Levi (Levites) and in twenty-four instances are called by scripture as such (Jerusalem Talmud to Mishnaic tractate Maaser Sheini p. 31a). In Hebrew the word for "priesthood" is kehunnah.
Since the destruction of the Second Temple, and (therefore) the cessation of the daily and seasonal temple ceremonies and sacrifices, Kohanim in traditional Judaism (Orthodox Judaism and to some extent, Conservative Judaism) continue to perform a number of priestly ceremonies and roles such as the Pidyon HaBen (redemption of a first-born son) ceremony and the Priestly Blessing, and have remained subject, particularly in Orthodox Judaism, to a number of restrictions, such as restrictions on certain marriages and ritual purity (see Kohanic disqualifications).
Orthodox Judaism regard the "kohanim" as being held in reserve for a future restored Temple. In all branches of Judaism, Kohanim do not perform roles of propitiation, sacrifice, or sacrament. Rather, a "kohen"'s principal religious function is to perform the Priestly Blessing, and, provided he is rabbinically qualified, to serve as an authoritative judge ("posek") and expositor of Jewish halakha law.
Christianity.
With the spread of Christianity and the formation of parishes, the Greek word "ἱερεύς" (hiereus), and Latin "sacerdos", which Christians had since the 3rd century applied to bishops and only in a secondary sense to presbyters, began in the 6th century to be used of presbyters, and is today commonly used of presbyters, distinguishing them from bishops.
Today the term "priest" is used in Catholicism, Eastern Orthodoxy, Anglicanism, Oriental Orthodoxy, The Church of the East, and some branches of Lutheranism to refer to those who have been ordained to a ministerial position through receiving the sacrament of Holy Orders, although "presbyter" is also used. Since the Protestant Reformation, non-sacramental denominations are more likely to use the term "elder" to refer to their pastors.
However, nowhere in the New Testament is a Christian pastor (besides Christ) titled "hiereus," the distinctive Greek word for "priest," and thus its rendering into English is seen as an etymological corruption of the Greek word "presbuteros," which means "elder," and which is the word for the lead category of Christian leaders in the New Testament church, under the Lord Jesus Christ, the great High Priest (archiereus). In the New Testament, it is taught that as Christ made the perfect sacrifice for the forgiveness of sins, then believers have direct access to the Father through Him, () with the only priesthood that is named under Christ in the church being that which consists of all believers.
The New Testament Epistle to the Hebrews in particular draws a distinction between the Jewish priesthood and the high priesthood of Christ; it teaches that the sacrificial atonement by Jesus Christ on Calvary has made the Jewish priesthood and its prescribed ritual sacrifices redundant, along with the rest of the ceremonial acts of the Mosaic law, see Christian views on the Old Covenant for details. Thus, for Christians, Christ himself is the only high priest, and Christians have no priesthood independent or distinct from participation in the priesthood of Christ, the head of the Church. The one sacrifice of Christ, which he offered "once for all" () on the Cross, provides eternal sanctification and redemption. Catholics, Eastern Orthodox, High Church Anglicans, Lutherans, and some Methodists consider the sacrifice to be "re-presented" in the Eucharist. The Church of Jesus Christ of Latter-day Saints (LDS Church) claims to uphold all priesthood positions of the primitive gospel by the laying on of hands.
The most known form of distinctive clothing for the priest is the easily identifiable clerical collar (or Roman collar), which takes form in either the traditional cassock, or modern day clerical shirt. The typical modern version consists of a white plastic tab, inserted into a specially made collar of a black shirt, although traditional cloth collars are still worn.
Catholicism and Eastern Orthodoxy.
The most significant liturgical acts reserved to priests in these traditions are the administration of the Sacraments, including the celebration of the Holy Mass or Divine Liturgy (the terms for the celebration of the Eucharist in the Latin and Byzantine traditions, respectively), and the Sacrament of Reconciliation, also called Confession. The sacraments of Anointing of the Sick (Extreme Unction) and Confirmation or Chrismation are also administered by priests, though in the Western tradition Confirmation is ordinarily celebrated by a bishop. In the East, Chrismation is performed by the priest (using oil specially consecrated by a bishop) immediately after Baptism, and Unction is normally performed by several priests (ideally seven), but may be performed by one if necessary. In the West, Holy Baptism may be celebrated by anyone. The Vatican catechism states that "According to Latin tradition, the spouses as ministers of Christ's grace mutually confer upon each other the sacrament of Matrimony". Thus marriage is a sacrament administered by the couple to themselves, but may be witnessed and blessed by a deacon, or priest (who usually administers the ceremony). In the East, Holy Baptism and Marriage (which is called "Crowning") may be performed only by a priest. If a person is baptized "in extremis" (i.e., when in fear of immediate death), only the actual threefold immersion together with the scriptural words () may be performed by a layperson or deacon. The remainder of the rite, and Chrismation, must still be performed by a priest, if the person survives. The only sacrament which may be celebrated only by a bishop is that of Ordination ("cheirotonia", "Laying-on of Hands"), or Holy Orders.
In these traditions, only men who meet certain requirements may become priests. In Roman Catholicism the canonical minimum age is twenty-five. Bishops may dispense with this rule and ordain men up to one year younger. Dispensations of more than a year are reserved to the Holy See (Can. 1031 §§1, 4.) A Catholic priest must be incardinated by his bishop or his major religious superior in order to engage in public ministry. In Orthodoxy, the normal minimum age is thirty (Can. 9 of Neocaesarea) but a bishop may dispense with this if needed. In neither tradition may priests marry after ordination. In the Roman Catholic Church, priests in the Latin Rite, which covers the vast majority of Roman Catholicism, must be celibate except under special rules for married clergy converting from certain other Christian confessions. Married men may become priests in Eastern Orthodoxy and the Eastern Catholic Churches, but in neither case may they marry after ordination, even if they become widowed. Candidates for bishop are chosen only from among the celibate. Orthodox priests will either wear a clerical collar similar to the above mentioned, or simply a very loose black robe that does not have a collar.
Anglican or Episcopalian.
The role of a priest in the Anglican Communion is largely the same as within the Roman Catholic Church and Eastern Christianity, except that canon law in almost every Anglican province restricts the administration of confirmation to the bishop, just as with ordination. Whilst Anglican priests who are members of religious orders must remain celibate (although there are exceptions, such as priests in the Anglican Order of Cistercians), the secular clergy – (bishops, priests, and deacons who are not members of religious orders) – are permitted to marry before or after ordination. The Anglican churches, unlike the Roman Catholic or Eastern Christian traditions, have allowed the ordination of women as priests in some provinces since 1971. This practice remains controversial, however; a minority of provinces (ten out of the thirty-eight worldwide) retain an all-male priesthood. Most Continuing Anglican churches do not ordain women to the priesthood.
As Anglicanism represents a broad range of theological opinion, its presbyterate includes priests who consider themselves no different in any respect from those of the Roman Catholic Church, and a minority who prefer to use the title "presbyter" in order to distance themselves from the more sacrificial theological implications which they associate with the word “priest.” While priest is the official title of a member of the presbyterate in every Anglican province worldwide, the ordination rite of certain provinces (including the Church of England) recognizes the breadth of opinion by adopting the title "The Ordination of Priests (also called Presbyters)". Historically, the term “priest” has been more associated with the “High Church” or Anglo-Catholic wing, whereas the term “minister” has been more commonly used in “Low Church” or Evangelical circles.
Protestantism.
The general priesthood or the priesthood of all believers, is a Christian doctrine derived from several passages of the New Testament. It is a foundational concept of Protestantism. It is this doctrine that Martin Luther adduces in his 1520 "To the Christian Nobility of the German Nation" in order to dismiss the medieval Christian belief that Christians were to be divided into two classes: "spiritual" and "temporal" or non-spiritual.
The conservative reforms of Lutherans are reflected in the theological and practical view of the ministry of the Church. Much of European Lutheranism follows the traditional catholic governance of deacon, priest and bishop. The Lutheran archbishops of Finland, Sweden, etc. and Baltic countries are the historic national primates (See the original Catholic Church) and some ancient cathedrals and parishes in the Lutheran church were constructed many centuries before the Reformation. Indeed, ecumenical work within the Anglican communion and among Scandinavian Lutherans mutually recognize the historic apostolic legitimacy and full communion. Likewise in America, Lutherans have embraced the apostolic succession of bishops in the full communion with Episcopalians and most Lutheran ordinations are performed by a bishop. The Catholic Church, however, does not recognise Episcopalians or Lutherans as having legitimate apostolic succession.
Ordained Protestant clergy often have the title of pastor, minister, reverend, etc. In some Lutheran churches, ordained clergy are called priests, while in others the term pastor is preferred.
Latter Day Saints.
In the Latter Day Saint movement, priesthood is the power and authority of God given to man, including the authority to perform ordinances and to act as a leader in the church. A body of priesthood holders is referred to as a quorum. Priesthood denotes elements of both power and authority. The priesthood includes the power Jesus gave his apostles to perform miracles such as the casting out of devils and the healing of sick (Luke 9:1). Latter Day Saints believe that the Biblical miracles performed by prophets and apostles were performed by the power of priesthood, including the miracles of Jesus, who holds all of the keys of the priesthood. The priesthood is formally known as the "Priesthood after the Order of the Son of God", but to avoid the too frequent use of the name of deity, the priesthood is referred to as the Melchizedek priesthood (Melchizedek being the high priest to whom Abraham paid tithes). As an authority, priesthood is the authority by which a bearer may perform ecclesiastical acts of service in the name of God. Latter Day Saints believe that acts (and in particular, ordinances) performed by one with priesthood authority are recognized by God and are binding in heaven, on earth, and in the afterlife. In addition, Latter Day Saints believe that leadership positions within the church are legitimized by the priesthood authority.
Islam.
No single Islamic office encompasses all the meanings of "priest" in the Christian sense, and some priestly functions are not performed by any office. The title mullah, commonly translated "cleric" in the West and thought to be analogous to "priest", is a title of address for any educated or respected figure, not even necessarily (though frequently) religious. The nearest Islamic analogue to the parish priest, or to the "pulpit rabbi" of a synagogue, is the "imam khatib." This compound title is merely a common combination of two elementary offices: leader "(imam)" of the congregational prayer, which in larger mosques is performed at the times of all daily prayers; and preacher "(khatib)" of the sermon or "khutba" at the required congregational prayer on Friday.
Eastern religions.
Hinduism.
Hindu priests historically were members of the Brahmin caste. Priests are ordained and trained as well. There are two types of Hindu priests, "pujaris" and "purohits". A "pujari" performs rituals in a temple. These rituals include bathing the "murtis" (the statues of the gods/goddesses), performing "puja", a ritualistic offering of various items to the Gods, the waving of a "ghee" or oil lamp also called an offering in light, known in Hinduism as "aarti", before the "murtis". "Pujaris" are often married.
A "purohit", on the other hand, performs rituals and "saṃskāras" (sacraments) outside of the temple. There are special "purohits" who perform only funeral rites.
In many cases, a "purohit" also functions as a "pujari". Both women and men are ordained as "purohits" and "pujaris".
Zoroastrianism.
In Zoroastrianism, the priesthood is reserved for men and is a mostly hereditary position. The priests prepare a drink from a sacred plant, which is called the "haoma" ritual. They officiate the "Yasna", pouring libations into the sacred fire to the accompaniment of ritual chants.
Taoism.
The Taoist priest is called a Daoshi (道士 "master of the Dao" p. 488). Daoshi act as interpreters of the principles of Yin-Yang 5 elements (fire, water, soil, wood, and metal p. 53) school of ancient Chinese philosophy, as they relate to marriage, death, festival cycles, and so on. The Daoshi seeks to share the benefits of meditation with his or her community through public ritual and liturgy (p. 326). In the ancient priesthood before the Tang, the priest was called "Jijiu" ("libationer" p. 550), with both male and female practitioners selected by merit. The system gradually changed into a male only hereditary "Daoshi" priesthood until more recent times (p. 550,551).
Indigenous and ethnic religions.
Shintoism.
The shinto priest is called a kannushi (神主, lit. "Master of the kami"), originally pronounced "kamunushi", sometimes referred to as a shinshoku (神職). A Kannushi is the person responsible for the maintenance of a Shinto shrine, or jinja, purificatory rites, and for leading worship and veneration of a certain kami. Additionally, priests are aided by miko (巫女, "shrine maidens") for many rites as a kind of shaman or medium. The maidens may either be family members in training, apprentices, or local volunteers.
Saiin were female relatives of the Japanese emperor (termed saiō) who served as High Priestesses in Kamo Shrine. Saiō also served at Ise Shrine. Saiin priestesses usually were elected from royalty. In principle, Saiin remained unmarried, but there were exceptions. Some Saiin became consorts of the emperor, called Nyōgo in Japanese. The Saiin order of priestesses existed throughout the Heian and Kamakura periods.
Africa.
The Yoruba people of western Nigeria practice an indigenous religion with a chiefly hierarchy of priests and priestesses that dates to AD 800–1000. Ifá priests and priestesses bear the titles Babalawo for men and Iyanifa for women. Priests and priestesses of the varied Orisha are titled Babalorisa for men and Iyalorisa for women. Initiates are also given an Orisa or Ifá name that signifies under which deity they are initiated. For example, a Priestess of Oshun may be named Osunyemi, and a Priest of Ifá may be named Ifáyemi. This ancient culture continues to this day as initiates from all around the world return to Nigeria for initiation into the traditional priesthood, and varied derivative sects in the New World (such as Cuban Santeria and Brazilian Umbanda) use the same titles to refer to their officers as well.
Paganism.
Wicca.
According to traditional Wiccan beliefs, every member of the religion is considered a priestess or priest, as it is believed that no person can stand between another and the Divine. However, in response to the growing number of Wiccan temples and churches, several denominations of the religion have begun to develop a core group of ordained priestesses and priests serving a larger laity. This trend is far from widespread, but is gaining acceptance due to increased interest in the religion.
Dress.
The dress of religious workers in ancient times may be demonstrated in frescoes and artifacts from the cultures. The dress is presumed to be related to the customary clothing of the culture, with some symbol of the deity worn on the head or held by the person. Sometimes special colors, materials, or patterns distinguish celebrants, as the white wool veil draped on the head of the Vestal Virgins.
Occasionally the celebrants at religious ceremonies shed all clothes in a symbolic gesture of purity. This was often the case in ancient times. An example of this is shown to the left on a Kylix dating from c. 500 BC where a priestess is featured. Modern religious groups tend to avoid such symbolism and some may be quite uncomfortable with the concept.
The retention of long skirts and vestments among many ranks of contemporary priests when they officiate may be interpreted to express the ancient traditions of the cultures from which their religious practices arose.
In most Christian traditions, priests wear clerical clothing, a distinctive form of street dress. Even within individual traditions it varies considerably in form, depending on the specific occasion. In Western Christianity, the stiff white clerical collar has become the nearly universal feature of priestly clerical clothing, worn either with a cassock or a clergy shirt. The collar may be either a full collar or a vestigial tab displayed through a square cutout in the shirt collar.
Eastern Christian priests mostly retain the traditional dress of two layers of differently cut cassock: the "rasson" (Greek) or "podriasnik" (Russian) beneath the outer "exorasson" (Greek) or "riasa" (Russian). If a pectoral cross has been awarded it is usually worn with street clothes in the Russian tradition, but not so often in the Greek tradition.
Distinctive clerical clothing is less often worn in modern times than formerly, and in many cases it is rare for a priest to wear it when not acting in a pastoral capacity, especially in countries that view themselves as largely secular in nature. There are frequent exceptions to this however, and many priests rarely if ever go out in public without it, especially in countries where their religion makes up a clear majority of the population. Pope John Paul II often instructed Catholic priests and religious to always wear their distinctive (clerical) clothing, unless wearing it would result in persecution or grave verbal attacks.
Christian traditions that retain the title of priest also retain the tradition of special liturgical vestments worn only during services. Vestments vary widely among the different Christian traditions.
In modern Pagan religions, such as Wicca, there is no one specific form of dress designated for the clergy. If there is, it is a particular of the denomination in question, and not a universal practice. However, there is a traditional form of dress, (usually a floor-length tunic and a knotted cord cincture, known as the "cingulum"), which is often worn by worshipers during religious rites. Among those traditions of Wicca that do dictate a specific form of dress for its clergy, they usually wear the traditional tunic in addition to other articles of clothing (such as an open-fronted robe or a cloak) as a distinctive form of religious dress, similar to a habit.
Assistant priest.
In many religions there are one or more layers of assistant priests.
In the Ancient Near East, hierodules served in temples as assistants to the priestess.
In ancient Judaism, the Priests (Kohanim) had a whole class of Levites as their assistants in making the sacrifices, in singing psalms and in maintaining the Temple. The Priests and the Levites were in turn served by servants called Nethinim. These lowest level of servants were not priests.
An assistant priest is a priest in the Anglican and Episcopal churches who is not the senior member of clergy of the parish to which they are appointed, but is nonetheless in priests' orders; there is no difference in function or theology, merely in 'grade' or 'rank'. Some assistant priests have a "sector ministry", that is to say that they specialize in a certain area of ministry within the local church, for example youth work, hospital work, or ministry to local light industry. They may also hold some diocesan appointment part-time. In most (though not all) cases an assistant priest has the legal status of assistant curate, although it should also be noted that not all assistant curates are priests, as this legal status also applies to many deacons working as assistants in a parochial setting.
The corresponding term in the Catholic Church is "parochial vicar" – an ordained priest assigned to assist the pastor (Latin: "parochus") of a parish in the pastoral care of parishioners. Normally, all pastors are also ordained priests; occasionally an auxiliary bishop will be assigned that role.
In Wicca, the leader of a coven or temple (either a high priestess or high priest) often appoints an assistant. This assistant is often called a 'deputy', but the more traditional terms 'maiden' (when female and assisting a high priestess) and 'summoner' (when male and assisting a high priest) are still used in many denominations.

</doc>
<doc id="23708" url="http://en.wikipedia.org/wiki?curid=23708" title="PL/I">
PL/I

PL/I ("Programming Language One", pronounced ) is a procedural, imperative computer programming language designed for scientific, engineering, business and systems programming applications. It has been used by various academic, commercial and industrial organizations since it was introduced in the 1960s, and continues to be actively used as of 2009[ [update]].
PL/I's principal domains are data processing, numerical computation, scientific computing, and systems programming; it supports recursion, structured programming, linked data structure handling, fixed-point, floating-point, complex, character string handling, and bit string handling. The language syntax is English-like and suited for describing complex data formats, with a wide set of functions available to verify and manipulate them.
Early history.
In the 1950s and early 1960s business and scientific users programmed for different computer hardware using different programming languages. Business users were moving from Autocoders via COMTRAN to COBOL, while scientific users programmed in General Interpretive Programme (GIP), Fortran, ALGOL, GEORGE, and others. The IBM System/360 (announced in 1964 but not delivered until 1966) was designed as a common machine architecture for both groups of users, superseding all existing IBM architectures. Similarly, IBM wanted a single programming language for all users. It hoped that Fortran could be extended to include the features needed by commercial programmers. In October 1963 a committee was formed composed originally of 3 IBMers from New York and 3 members of SHARE, the IBM
scientific users group, to propose these extensions to Fortran. Given the constraints
of Fortran, they were unable to do this and embarked on the design of a “new programming language” based loosely on Algol labeled “NPL". This acronym conflicted with that of the UK’s National Physical Laboratory and was
replaced briefly by MPPL (MultiPurpose Programming Language) and, in 1965, with PL/I (with a Roman numeral “I” ). The first definition appeared in April 1964.
IBM took NPL as a starting point and completed the design to a level that the first compiler could
be written: the NPL definition was incomplete in scope and in detail. Control of the
PL/I language was vested initially in the New York Programming Center and later at the IBM UK Laboratory at Hursley. The SHARE and GUIDE user groups were involved in extending the
language and had a role in IBM’s process for controlling the language through their PL/I Projects.
The experience of defining such a large language showed the need for a formal definition of PL/I. A project was set up in 1967 in IBM Vienna to make an unambiguous and complete specification. This led in turn to one of the first large scale Formal Methods for development, VDM.
The language was first specified in detail in the manual “PL/I Language Specifications. C28-6571” written in New York from 1965 and superseded by “PL/I Language Specifications. GY33-6003” written in Hursley from 1967. IBM continued to develop PL/I in the late sixties and early seventies, publishing it in the GY33-6003 manual. These manuals were used by the Multics group and other early implementers.
The first compiler was delivered in 1966. The Standard for PL/I was approved in 1976.
Goals and principles.
The SHARE 3by3 committee set these goals for NPL:
These goals evolved during the early development of the language. Competitiveness with COBOL’s record handling and report writing capabilities was needed. The “scope of usefulness” of the language grew to include system programming and event-driven programming. The additional goals for PL/I were:
To meet these goals PL/I borrowed ideas from contemporary languages while adding substantial new capabilities and casting it with a distinctive concise and readable syntax. A number of principles and capabilities combined to give the language its character and were key in meeting the goals:
These principles inevitably resulted in a large language which would need compilers substantially more complex than those for COBOL or Fortran. This was not seen as a drawback since though the few—the compiler writers—would have more work, the many—the programmers—would have less.
Language summary.
The language is designed to be all things to all programmers. The summary is extracted from the ANSI PL/I Standard
and the ANSI PL/I General-Purpose Subset Standard.
A PL/I program consists of a set of procedures, each of which is written as a sequence of statements. The codice_3 construct is used to include text from other sources during program translation. All of the statement types are summarized here in groupings which give an overview of the language (the Standard uses this organization).
Names may be declared to represent data of the following types, either as single values, or as aggregates in the form of arrays, with a lower-bound and upper-bound per dimension, or structures (comprising nested structure, array and scalar variables):
The codice_4 type comprises these attributes:
The base, scale, precision and scale factor of the codice_5 type is encoded within the codice_6. The mode is specified separately, with the codice_7 applied to both the real and the imaginary parts.
Values are computed by expressions written using a specific set of operations and builtin functions, most of which may be applied to aggregates as well as to single values, together with user-defined procedures which, likewise, may operate on and return aggregate as well as single values. The assignment statement assigns values to one or more variables.
There are no reserved words in PL/I. A statement is terminated by a semi-colon. The maximum length of a statement is implementation defined. A comment may appear anywhere in a program where a space is permitted and is preceded by the characters forward slash, asterisk and is terminated by the characters asterisk, forward slash (i.e. codice_8). Statements may have a label-prefix introducing an entry name (codice_9 and codice_10 statements) or label name, and a condition prefix enabling or disabling a computational condition - e.g. codice_11). Entry and label names may be single identifiers or identifiers followed by a subscript list of constants (as in codice_12.
A sequence of statements becomes a "group" when preceded by a codice_2 statement and followed by an codice_14 statement. Groups may include nested groups and begin blocks. The codice_15 statement specifies a group or a single statement as the codice_16 part and the codice_17 part (see the sample program). The group is the unit of iteration. The begin "block" (codice_18) may contain declarations for names and internal procedures local to the block. A "procedure" starts with a codice_10 statement and is terminated syntactically by an codice_14 statement. The body of a procedure is a sequence of blocks, groups, and statements and contains declarations for names and procedures local to the procedure or codice_21 to the procedure.
An "on-unit" is a single statement or block of statements written to be executed when one or more of these "conditions" occur:
a "computational condition",
or an "Input/Output" condition,
or one of the conditions:
A declaration of an identifier may contain one or more of the following attributes (but they need to be mutually consistent):
Current compilers from Kednos, Micro Focus, and particularly that from IBM implement many extensions over the standardized version of the language. The IBM extensions are summarised in the Implementation sub-section for the compiler later. Although there are some extensions common to these compilers the lack of a current standard means that compatibility is not guaranteed.
Standardization.
Language standardization began in April 1966 in Europe with ECMA TC10. In 1969 ANSI established a "Composite Language Development Committee", nicknamed "Kludge", which fortunately was renamed X3J1 PL/I. Standardization became a joint effort of ECMA TC/10 and ANSI X3J1. A subset of the GY33-6003 document was offered to the joint effort by IBM and became the base document for standardization. The major features omitted from the base document were multitasking and the attributes for program optimization (e.g. codice_25 and codice_26).
Proposals to change the base document were voted upon by both committees. In the event that the committees disagreed, the chairs, initially Michael Marcotty of General Motors and C.A.R. Hoare representing ICL had to resolve the disagreement. In addition to IBM, Honeywell, CDC, Data General, Digital Equipment, Prime Computer, Burroughs, RCA, and Univac served on X3J1 along with major users Eastman Kodak, MITRE, Union Carbide, Bell Laboratories, and various government and university representatives. Further development of the language occurred in the standards bodies, with continuing improvements in structured programming and internal consistency, and with the omission of the more obscure or contentious features.
As language development neared an end, X3J1/TC10 realized that there were a number of problems with a document written in English text. Discussion of a single item might appear in multiple places which might or might not agree. It was difficult to determine if there were omissions as well as inconsistencies. Consequently, David Beech (IBM), Robert Freiburghouse (Honeywell), Milton Barber (CDC), M. Donald MacLaren (Argonne National Laboratory), Craig Franklin (Data General), Lois Frampton (Digital Equipment), and editor, D.J. Andrews of IBM undertook to rewrite the entire document, each producing one or more complete chapters. The standard is couched as a formal definition using a "PL/I Machine" to specify the semantics. It was the first, and possibly the only, programming language standard to be written as a semi-formal definition.
A "PL/I General-Purpose Subset" ("Subset-G") standard was issued by ANSI in 1981 and a revision published in 1987. The General Purpose subset was widely adopted as the kernel for PL/I implementations.
Implementations.
IBM PL/I F and D compilers.
PL/I was first implemented by IBM, at its Hursley Laboratories in the United Kingdom, as part of the development of System/360. The first production PL/I compiler was the PL/I F compiler for the OS/360 Operating System, built by John Nash's team at Hursley in the UK: the runtime library team was managed by I.M. (Nobby) Clarke. The PL/I F compiler was written entirely in System/360 assembly language. Release 1 shipped in 1966. OS/360 was a real-memory environment and the compiler was designed for systems with as little as 64 kilobytes of real storage – F being 64 kB in S/360 parlance. To fit a large compiler into the 44 kilobytes of memory available on a 64-kilobyte machine, the compiler consisted of a control phase and a large number of compiler phases (approaching 100). The phases were brought into memory from disk, and released, one at a time to handle particular language features and aspects of compilation.
Aspects of the language were still being designed as PL/I F was implemented, so some were omitted until later releases. PL/I RECORD I/O was shipped with PL/I F Release 2. The list processing functions - Based Variables, Pointers, Areas and Offsets and LOCATE-mode I/O - were first shipped in Release 4. In a major attempt to speed up PL/I code to compete with Fortran object code, PL/I F Release 5 did substantial program optimization of DO-loops facilitated by the REORDER option on procedures.
A version of PL/I F was released on the TSS/360 timesharing operating system for the System/360 Model 67, adapted at the IBM Mohansic Lab. The IBM La Gaude Lab in France developed “Language Conversion Programs” to convert Fortran, Cobol, and Algol programs to the PL/I F level of PL/I.
The PL/I D compiler, using 16 kilobytes of memory, was developed by IBM Germany for the DOS/360 low end operating system. It implemented a subset of the PL/I language requiring all strings and arrays to have fixed extents, thus simplifying the run-time environment. Reflecting the underlying operating system it lacked dynamic storage allocation and the "controlled" storage class. It was shipped within a year of PL/I F.
Multics PL/I and derivatives.
Compilers were implemented by several groups in the early 1960s. The Multics project at MIT, one of the first to develop an operating system in a high level language, used Early PL/I (EPL), a subset dialect of PL/I, as their implementation language in 1964. EPL was developed at Bell Labs and MIT by Douglas McIlroy, Robert Morris, and others. The influential Multics PL/I compiler, described on the "Multicians" website, was the source of compiler technology used by a number of manufacturers and software groups.
The Honeywell PL/I compiler (for Series 60) was an implementation of the full ANSI X3J1 standard.
IBM PL/I optimizing and checkout compilers.
The PL/I Optimizer and Checkout compilers produced in Hursley supported a common level of PL/I language and aimed to replace the PL/I F compiler. The checkout compiler was a rewrite of PL/I F in BSL, IBM's PL/I-like proprietary implementation language (later PL/S). The performance objectives set for the compilers are shown in an IBM presentation to the BCS. The compilers had to produce identical results - the Checkout Compiler was used to debug programs that would then be submitted to the Optimizer. Given that the compilers had entirely different designs and were handling the full PL/I language this goal was challenging: it was achieved.
The PL/I optimizing compiler took over from the PL/I F compiler and was IBM’s workhorse compiler from the 1970s to the 1990s. Like PL/I F, it was a multiple pass compiler with a 44 kilobyte design point, but it was an entirely new design. Unlike the F compiler, it had to perform compile time evaluation of constant expressions using the run-time library, reducing the maximum memory for a compiler phase to 28 kilobytes. A second-time around design, it succeeded in eliminating the annoyances of PL/I F such as cascading diagnostics. It was written in S/360 Macro Assembler by a team, led by Tony Burbridge, most of whom had worked on PL/I F. Macros were defined to automate common compiler services and to shield the compiler writers from the task of managing real-mode storage, allowing the compiler to be moved easily to other memory models. The gamut of program optimization techniques developed for the contemporary IBM Fortran H compiler were deployed: the Optimizer equaled Fortran execution speeds in the hands of good programmers. Announced with the IBM S/370 in 1970, it shipped first for the DOS/360 operating system in August 1971, and shortly afterward for OS/360, and the first virtual memory IBM operating systems OS/VS1, MVS, and VM/CMS. (The developers were unaware that while they were shoehorning the code into 28 kb sections, IBM Poughkeepsie was finally ready to ship virtual memory support in OS/360). It supported the batch programming environments and, under TSO and CMS, it could be run interactively. This compiler went through many versions covering all mainframe operating systems including the operating systems of the Japanese PCMs.
The compiler has been superseded by "IBM PL/I for OS/2, AIX, Linux, z/OS" below.
The PL/I checkout compiler, (colloquially "The Checker") announced in August 1970 was designed to speed and improve the debugging of PL/I programs. The team was led by Brian Marks. The three-pass design cut the time to compile a program to 25% of that taken by the F Compiler. It was run from an interactive terminal, converting PL/I programs into an internal format, “H-text”. This format was interpreted by the Checkout compiler at run-time, detecting virtually all types of errors. Pointers were represented in 16 bytes, containing the target address and a description of the referenced item, thus permitting "bad" pointer use to be diagnosed. In a conversational environment when an error was detected, control was passed to the user who could inspect any variables, introduce debugging statements and edit the source program. Over time the debugging capability of mainframe programming environments developed most of the functions offered by this compiler and it was withdrawn (in the 1990s?)
DEC PL/I.
Perhaps the most commercially successful implementation aside from IBM's was Digital Equipment's 1988 release of the ANSI PL/I 1987 subset. The implementation is "a strict superset of the ANSI X3.4-1981 PL/I General Purpose Subset and provides most of the features of the new ANSI X3.74-1987 PL/I General Purpose Subset". The front end was designed by Robert Freiburghouse, and the code generator was implemented by Dave Cutler, who managed the design and implementation of VAX/VMS. It runs on VMS on VAX and ALPHA and on Tru64. UniPrise Systems, Inc., was responsible for the compiler; it is currently supported by Kednos Corporation.
Teaching subset compilers.
In the late 1960s and early 1970s, many US and Canadian Universities were establishing time-sharing services on campus and needed conversational compiler/interpreters for use in teaching science, mathematics, engineering, and computer science. Dartmouth were developing BASIC, but PL/I was a popular choice, as it was concise and easy to teach. As the IBM offerings were unsuitable, a number of schools built their own subsets of PL/I and their own interactive support. Examples are:
A compiler developed at Cornell University for teaching a dialect called PL/C, which had the unusual capability of never failing to compile any program through the use of extensive automatic correction of many syntax errors and by converting any remaining syntax errors to output statements. The language was almost all of PL/I as implemented by IBM. PL/C was a very fast compiler.
PLAGO, created at the Polytechnic Institute of Brooklyn, used a simplified subset of the PL/I language and focused on good diagnostic error messages and fast compilation times.
The Computer Systems Research Group of the University of Toronto produced the SP/k compilers which supported a sequence of subsets of PL/I called SP/1, SP/2, SP/3, ..., SP/8 for teaching programming. Programs that ran without errors under the SP/k compilers produced the same results under other contemporary PL/I compilers such as IBM's PL/I F compiler, IBM's checkout compiler or Cornell University's PL/C compiler.
Other examples are PL0 by P. Grouse at the University of New South Wales, PLUM by Marvin Zelkowitz at the University of Maryland., and PLUTO from the University of Toronto.
IBM PL/I for OS/2, AIX, Linux, z/OS.
In a major revamp of PL/I, IBM Santa Teresa in California launched an entirely new compiler in 1992. The initial shipment was for OS/2 and included most ANSI-G features and many new PL/I features. Subsequent releases covered additional platforms (MVS, VM, OS/390, AIX and Windows) and continued to add functions to make PL/I fully competitive with other languages offered on the PC (particularly C and C++) in areas where it had been overtaken. The corresponding “IBM Language Environment" supports inter-operation of PL/I programs with Database and Transaction systems, and with programs written in C, C++, and COBOL, the compiler supports all the data types needed for intercommunication with these languages.
The PL/I design principles were retained and withstood this major extension comprising several new data types, new statements and statement options, new exception conditions, and new organisations of program source. The resulting language is a compatible super-set of the PL/I Standard and of the earlier IBM compilers. Major topics added to PL/I were:
Object orientation.
codice_28 is a new computational data type. The ordinal facilities are like those in Pascal,
e.g. codice_37
but in addition the name and internal values are accessible via built-in functions. Built-in functions provide access to an ordinal value's predecessor and successor.
The codice_38 (see below) allows additional codice_31s to be declared composed from PL/I's built-in attributes.
The codice_40 locator data type is similar to the codice_41 data type, but strongly typed to bind only to a particular data structure. The codice_42 operator is used to select a data structure using a handle.
The codice_32 attribute (equivalent to codice_44 in early PL/I specifications) permits several scalar variables, arrays, or structures to share the same storage in a unit that occupies the amount of storage needed for the largest alternative.
Competitiveness on PC and with C.
These attributes were added:
New string-handling functions were added - to centre text, to edit using a picture format, and to trim blanks or selected characters from the head or tail of text, codice_61 to codice_62 from the right. and codice_63 and codice_64 functions.
Compound assignment operators a la C e.g. codice_65, codice_66, codice_67, codice_68 were added. codice_69 is equivalent to codice_70.
Additional parameter descriptors and attributes were added for omitted arguments and variable length argument lists.
Program readability – making intentions explicit.
The codice_71 attribute declares an identifier as a constant (derived from a specific literal value or restricted expression).
Parameters can have the codice_72 (pass by address) or codice_35 (pass by value) attributes.
The codice_74 and codice_75 attributes prevent unintended assignments.
codice_76 obviates the need for the contrived construct codice_77.
The codice_38 introduces user-specified names (e.g. codice_79) for combinations of built-in attributes (e.g. codice_80). Thus codice_81 creates the codice_31 name codice_79 as an alias for the set of built-in attributes FIXED BINARY(31.0). codice_29 applies to structures and their members; it provides a codice_31 name for a set of structure attributes and corresponding substructure member declarations for use in a structure declaration (a generalisation of the codice_86 attribute).
Structured programming additions.
A codice_87 statement to exit a loop, and an codice_88 to continue with the next iteration of a loop.
codice_54 and codice_55 options on iterative groups.
The package construct consisting of a set of procedures and declarations for use as a unit. Variables declared outside of the procedures are local to the package, and can use codice_91, codice_92 or codice_93 storage. Procedure names used in the package also are local, but can be made external by means of the codice_94 option of the codice_95.
Interrupt handling.
The codice_96 executed in an ON-unit terminates execution of the ON-unit, and raises the condition again in the procedure that called the current one (thus passing control to the corresponding ON-unit for that procedure).
The codice_97 condition handles invalid operation codes detected by the PC processor, as well as illegal arithmetic operations such as subtraction of two infinite values.
The codice_98 condition is provided to intercept conditions for which no specific ON-unit has been provided in the current procedure.
The codice_99 condition is raised when an codice_100 statement is unable to obtain sufficient storage.
Other mainframe and minicomputer compilers.
A number of vendors produced compilers to compete with IBM PL/I F or Optimizing compiler on mainframes and minicomputers in the 1970s. In the 1980s the target was usually the emerging ANSI-G subset.
Usage.
PL/I implementations were developed for mainframes from the late 1960s, mini computers in the 1970s, and personal computers in the 1980s and 1990s. Although its main use has been on mainframes, there are PL/I versions for DOS, Microsoft Windows, OS/2, AIX, OpenVMS, and Unix.
It has been widely used in business data processing and for system use for authoring operating systems on certain platforms. Very complex and powerful systems have been built with PL/I:
The SAS System was initially written in PL/I; the SAS data step is still modeled on PL/I syntax.
The pioneering online airline reservation system Sabre was originally written for the IBM 7090 in assembler. The S/360 version was largely written using SabreTalk, a purpose built subset PL/I compiler for a dedicated control program.
PL/I was used to write an executable formal definition to interpret IBM's System Network Architecture
PL/I did not fulfill its supporters' hopes that it would displace Fortran and COBOL and become the major player on mainframes. It remained a minority but significant player. There cannot be a definitive explanation for this, but some trends in the 1970s and 1980s militated against its success by progressively reducing the territory on which PL/I enjoyed a competitive advantage.
First, the nature of the mainframe software environment changed. Application subsystems for database and transaction processing (CICS and IMS and Oracle on System 370) and application generators became the focus of mainframe users' application development. Significant parts of the language became irrelevant because of the need to use the corresponding native features of the subsystems (such as tasking and much of input/output). Fortran was not used in these application areas, confining PL/I to COBOL’s territory; most users stayed with COBOL. But as the PC became the dominant environment for program development Fortran, COBOL and PL/I all became minority languages overtaken by C++, Java and the like.
Second, PL/I was overtaken in the systems programming field. The IBM system programming community was not ready to use PL/I; instead, IBM developed and adopted a proprietary dialect of PL/I for system programming. – PL/S. With the success of PL/S inside IBM, and of C outside IBM, the unique PL/I strengths for system programming became less valuable.
Third, the development environments grew capabilities for interactive software development that, again, made the unique PL/I interactive and debugging strengths less valuable.
Fourth, COBOL and Fortran added features such as structured programming, character string operations, and object orientation, that further reduced PL/I's relative advantages.
On mainframes there were substantial business issues at stake too. IBM’s hardware competitors had little to gain and much to lose from success of PL/I. Compiler development was expensive, and the IBM compiler groups had an in-built competitive advantage. Many IBM users wished to avoid being locked into proprietary solutions. With no early support for PL/I by other vendors it was best to avoid PL/I.
Evolution of the PL/I language.
This article uses the PL/I standard as the reference point for language features. But a number of features of significance in the early implementations were not in the Standard; and some were offered by non-IBM compilers. And the de facto language continued to grow after the standard, ultimately driven by developments on the Personal Computer.
Significant features omitted from the standard.
Multi tasking.
"Multi tasking" was implemented by PL/I F, the Optimizer and the newer AIX and Z/OS compilers. It comprised the data types codice_102 and codice_103, the codice_104 on the codice_105 (Fork), the codice_106 (Join), the codice_107, codice_108s on the record I/O statements and the codice_109 statement to unlock locked records on codice_110 files. Event data identify a particular event and indicate whether it is complete ('1'B) or incomplete ('0'B): task data items identify a particular task (or process) and indicate its priority relative to other tasks.
Preprocessor.
The first IBM "Compile time preprocessor" was built by the IBM Boston Advanced Programming Center located in Cambridge, Mass, and shipped with the PL/I F compiler. The codice_3 statement was in the Standard, but the rest of the features were not. The DEC and Kednos PL/I compilers implemented much the same set of features as IBM, with some additions of their own. IBM has continued to add preprocessor features to its compilers. The preprocessor treats the written source program as a sequence of tokens, copying them to an output source file or acting on them. When a % token is encountered the following compile time statement is executed: when an identifier token is encountered and the identifier has been codice_112d, codice_113d, and assigned a compile time value, the identifier is replaced by this value. Tokens are added to the output stream if they do not require action (e.g. codice_114), as are the values of ACTIVATEd compile time expressions. Thus a compile time variable codice_115 could be declared, activated, and assigned using codice_116. Subsequent occurrences of codice_115 would be replaced by codice_118.
The data type supported are codice_119 integers and codice_120 strings of varying length with no maximum length. The structure statements are
and the simple statements, which also may have a [label-list:]
The feature allowed programmers to use identifiers for constants - e.g. product part numbers or mathematical constants - and was superseded in the standard by named constants for computational data. Conditional compiling and iterative generation of source code, possible with compile-time facilities, was not supported by the standard. Several manufacturers implemented these facilities.
Structured programming additions.
Structured programming additions were made to PL/I during standardization but were not accepted into the standard. These features were the codice_130 to exit from an iterative codice_2, the codice_132 and codice_133 added to codice_2, and a case statement of the general form:
codice_135<br>
These features were all included in DEC PL/I.
Debug facilities.
PL/I F had offered some debug facilities that were not put forward for the standard but were implemented by others - notably the CHECK(variable-list) condition prefix, codice_136 on-condition and the codice_137 option. The IBM Optimizing and Checkout compilers added additional features appropriate to the conversational mainframe programming environment (e.g. an codice_138 condition).
Significant features developed since the standard.
Several attempts had been made to design a structure member type that could have one of several datatypes (codice_44 in early IBM). With the growth of classes in programming theory, approaches to this became possible on a PL/I base - codice_32, codice_31 etc. have been added by several compilers.
PL/I had been conceived in a single byte character world. With support for Japanese and Chinese language becoming essential, and the developments on International Code Pages, the character string concept was expanded to accommodate wide non-ASCII/EBCDIC strings.
codice_142 and codice_143 handling were overhauled to deal with the millennium problem.
Criticisms.
Implementation issues.
Though the language was easy to learn and use, implementing a PL/I compiler was difficult and time-consuming. A language as large as PL/I needed subsets that most vendors could produce and most users master. This was not resolved until "ANSI G" was published. The compile time facilities, unique to PL/I, took added implementation effort and additional compiler passes. A PL/I compiler was two to four times as large as comparable Fortran or COBOL compilers, and also that much slower - fortunately offset by gains in programmer productivity. This was anticipated in IBM before the first compilers were written.
Some argued that PL/I was unusually hard to parse. The PL/I "keywords" were not reserved so programmers could use them as variable or procedure names in programs. Because the original PL/I F compiler attempted "auto-correction" when it encountered a keyword used in an incorrect context, it often assumed it was a variable name. This led to "cascading diagnostics", a problem solved by later compilers.
The effort needed to produce good object code was perhaps underestimated during the initial design of the language. Program optimization (needed to compete with the excellent program optimization carried out by available Fortran compilers) was unusually complex due to side effects and pervasive problems with aliasing of variables. Unpredictable modification can occur asynchronously for ABNORMAL data, or in exception handlers, which may be provided by "ON statements" in (unseen) callers. Together, these make it difficult to reliably predict when a program's variables might be modified at runtime.
It contained many rarely used features, such as multitasking support, which added cost and complexity to the compiler, and its co-processing facilities required a multi-programming environment with support for non-blocking multiple threads for processes by the operating system. Compiler writers were free to select whether to implement these features.
An undeclared variable was by default being declared by first occurrence - thus misspelling might lead to unpredictable results.
Programmer issues.
Many programmers were slow to move from COBOL or Fortran due to a perceived complexity of the language and immaturity of the PL/I F compiler. Programmers were sharply divided into scientific programmers (who used Fortran) and business programmers (who used COBOL), with significant tension and even dislike between the groups. PL/I syntax borrowed from both COBOL and Fortran syntax. So instead of noticing features that would make their job easier, Fortran programmers of the time noticed COBOL syntax and had the opinion that it was a business language, while COBOL programmers noticed Fortran syntax and looked on it as a scientific language.
Both COBOL and Fortran programmers viewed it as a "bigger" version of their own language, and both were somewhat intimidated by the language and disinclined to adopt it. Another factor was "pseudo"-similarities to COBOL, Fortran, and ALGOL. These were PL/I elements that looked similar to one of those languages, but worked differently in PL/I. Such frustrations left many experienced programmers with a jaundiced view of PL/I, and often an active dislike for the language. An early UNIX fortune file contained the following tongue-in-cheek description of the language:
Speaking as someone who has delved into the intricacies of PL/I, I am sure that only Real Men could have written such a machine-hogging, cycle-grabbing, all-encompassing monster. Allocate an array and free the middle third? Sure! Why not? Multiply a character string times a bit string and assign the result to a float decimal? Go ahead! Free a controlled variable procedure parameter and reallocate it before passing it back? Overlay three different types of variable on the same memory location? Anything you say! Write a recursive macro? Well, no, but Real Men use rescan. How could a language so obviously designed and written by Real Men not be intended for Real Man use?
On the positive side, full support for pointers to all data types (including pointers to structures), recursion, multitasking, string handling, and extensive built-in functions PL/I was indeed quite a leap forward compared to the programming languages of its time. However, these were not enough to convince a majority of programmers or shops to switch to PL/I.
The PL/I F compiler's compile time preprocessor was unusual (outside the Lisp world) in using its target language's syntax and semantics ("e.g." as compared to the C preprocessor's "#" directives).
Special topics in PL/I.
Storage classes.
PL/I provides several 'storage classes' to indicate how the lifetime of variables' storage is to be managed - codice_144 and codice_92. The simplest to implement is codice_91, which indicates that memory is allocated and initialized at load-time, as is done in COBOL "working-storage" and Fortran IV. But this is only the default for codice_21 variables.
PL/I's default storage class for codice_148 variables is codice_149, similar to that of other block-structured languages influenced by ALGOL, like the "auto" storage class in the C language, and default storage allocation in Pascal and "local-storage" in IBM COBOL. Storage for codice_149 variables is allocated upon entry into the codice_151, procedure, or on-unit in which they are declared. The compiler and runtime system allocate memory for a stack frame to contain them and other housekeeping information. If a variable is declared with an codice_152, code to set it to an initial value is executed at this time. Care is required to manage the use of initialization properly. Large amounts of code can be executed to initialize variables every time a scope is entered, especially if the variable is an array or structure. Storage for codice_149 variables is freed at block exit: codice_154 or codice_92 variables are used to retain variables' contents between invocations of a procedure or block. codice_93 storage is also managed using a stack, but the pushing and popping of allocations on the stack is managed by the programmer, using codice_100 and codice_158 statements. Storage for codice_92 variables is managed using codice_160, but instead of a stack these allocations have independent lifetimes and are addressed through codice_161 or codice_41 variables.
Storage type sharing.
There are several ways of accessing allocated storage through different data declarations. Some of these are well defined and safe, some can be used safely with careful programming, and some are inherently unsafe and/or machine dependent.
Passing a variable as an argument to a parameter by reference allows the argument's allocated storage to be referenced using the parameter. The codice_163 attribute (e.g. codice_164) allows part or all of a variable's storage to be used with a different, but consistent, declaration. These two usages are safe and machine independent.
Record I/O and list processing produce situations where the programmer needs to fit a declaration to the storage of the next record or item, before knowing what type of data structure it has. Based variables and pointers are key to such programs. The data structures must be designed appropriately, typically using fields in a data structure to encode information about its type and size. The fields can be held in the preceding structure or, with some constraints, in the current one. Where the encoding is in the preceding structure, the program needs to allocate a based variable with a declaration that matches the current item (using expressions for extents where needed). Where the type and size information are to be kept in the current structure ("self defining structures") the type-defining fields must be ahead of the type dependent items and in the same place in every version of the data structure. The codice_101-option is used for self-defining extents (e.g. string lengths as in codice_166 - where codice_167 is used to allocate instances of the data structure. For self-defining structures, any typing and codice_168 fields are placed ahead of the "real" data. If the records in a data set, or the items in a list of data structures, are organised this way they can be handled safely in a machine independent way.
PL/I implementations do not (except for the PL/I Checkout compiler) keep track of the data structure used when storage is first allocated. Any codice_92 declaration can be used with a pointer into the storage to access the storage - inherently unsafe and machine dependent. However this usage has become important for "pointer arithmetic" (typically adding a certain amount to a known address). This has been a contentious subject in computer science. In addition to the problem of wild references and buffer overruns, issues arise due to the alignment and length for data types used with particular machines and compilers. Many cases where pointer arithmetic might be needed involve finding a pointer to an element inside a larger data structure. The codice_170 function computes such pointers, safely and machine independently.
Pointer arithmetic may be accomplished by aliasing a binary variable with a pointer as in 
codice_171 
It relies on pointers being the same length as codice_172 integers and aligned on the same boundaries.
With the prevalence of C and its free and easy attitude to pointer arithmetic, recent IBM PL/I compilers allow pointers to be used with the addition and subtraction operators to giving the simplest syntax (but compiler options can disallow these practices where safety and machine independence are paramount).
On-Units and exception handling.
When PL/I was designed, programs only ran in batch mode, with no possible intervention from the programmer at a terminal. An exceptional condition such as division by zero would abort the program yielding only a hexadecimal core dump. PL/I exception handling, via on-units, allowed the program to stay in control in the face of hardware or operating system exceptions and to recover debugging information before closing down more gracefully. As a program became properly debugged most of the exception handling could be removed or disabled: this level of control became less important when conversational execution became commonplace.
Computational exception handling is enabled and disabled per PL/I condition by condition prefixes on statements, blocks(including on-units) and procedures. – e.g. codice_173. Operating system exceptions for Input/Output and storage management are always enabled.
The on-unit is a single statement or codice_174-block introduced by an codice_175 and is established for a particular condition. When the exception for this condition occurs and the condition is enabled, an on-unit for the condition is executed. On-units are inherited down the call chain. When a block, procedure or on-unit is activated, the on-units established by the invoking activation are inherited by the new activation. They may be over-ridden by another codice_175 and can be reestablished by the codice_177. The exception can be simulated using the codice_178 – e.g. to help debug the exception handlers. The dynamic inheritance principle for on-units allows a routine to handle the exceptions occurring within the subroutines it uses.
If no on-unit is in effect when a condition is raised a standard system action is taken (often this is to raise the codice_179 condition). The system action can be reestablished using the codice_180 option of the codice_175. With some conditions it is possible to complete executing an on-unit and return to the point of interrupt (the codice_182 conditions) and resume normal execution. With other conditions codice_183 the codice_179 condition is raised when this is attempted. An on-unit may be terminated with a codice_185 preventing a return to the point of interrupt.
An on-unit needs to be designed to deal with exceptions that occur in the on-unit itself. The codice_186 statement allows a nested error trap; if an error occurs within an on-unit, control passes to the operating system where a system dump might be produced.
The PL/I codice_187 I/O statements have relatively simple syntax as they do not offer options for the many situations from end-of-file to record transmission errors that can occur when a record is read or written. Instead, these complexities are handled in the on-units for the various file conditions. The same approach was adopted for codice_22 sub-allocation and the codice_22 condition.
The existence of exception handling on-units makes the task of optimizing PL/I programs particularly difficult. Variables can be inspected or altered in ON-units, and the flow of control may be very hard to analyze. This is discussed in the section on Implementation Issues above.

</doc>
<doc id="23711" url="http://en.wikipedia.org/wiki?curid=23711" title="Punctuation">
Punctuation

Punctuation is "the use of spacing, conventional signs, and certain typographical devices as aids to the understanding and correct reading, both silently and aloud, of handwritten and printed texts." Another description is: "The practice, action, or system of inserting points or other small marks into texts, in order to aid interpretation; division of text into sentences, clauses, etc., by means of such marks."
In written English, punctuation is vital to disambiguate the meaning of sentences. For example: "woman, without her man, is nothing" (emphasizing the importance of men), and "woman: without her, man is nothing" (emphasizing the importance of women) have very different meanings; as do "eats shoots and leaves" (which means the subject consumes plant growths) and "eats, shoots, and leaves" (which means the subject eats first, then fires a weapon, and then leaves the scene). The sharp differences in meaning are produced by the simple differences in punctuation within the example pairs, especially the latter.
The rules of punctuation vary with language, location, register and time and are constantly evolving. Certain aspects of punctuation are stylistic and are thus the author's (or editor's) choice. Tachygraphic language forms, such as those used in online chat and text messages, may have wildly different rules. For English usage, see the articles on specific punctuation marks.
History.
The first writing systems were either logographic or syllabic—for example, Chinese and Maya script—which do not necessarily require punctuation, especially spacing. This is because the entire morpheme or word is typically clustered within a single glyph, so spacing does not help as much to distinguish where one word ends and the other starts. Disambiguation and emphasis can easily be communicated without punctuation by employing a separate written form distinct from the spoken form of the language that uses slightly different phraseology. Even today, formal written modern English differs subtly from spoken English because not all emphasis and disambiguation is possible to convey in print, even with punctuation.
Ancient Chinese classical texts were transmitted without punctuation. However, many Warring-states-era bamboo texts contain the symbols ⟨└⟩ and ⟨▄⟩ indicating the end of a chapter and full stop, respectively. By the Song dynasty, addition of punctuation to texts by scholars to aid comprehension became common.
The earliest alphabetic writing had no capitalization, no spaces, no vowels and few punctuation marks. This worked as long as the subject matter was restricted to a limited range of topics (e.g., writing used for recording business transactions). Punctuation is historically an aid to reading aloud.
The oldest known document using punctuation is the Mesha Stele (9th century BC). This employs points between the words and horizontal strokes between the sense section as punctuation.
Western Antiquity.
Most texts were still written in "scriptura continua", that is without any separation between words. However, the Greeks were sporadically using punctuation marks consisting of vertically arranged dots—usually two (dicolon) or three (tricolon)—in around the 5th century b.c. as an aid in the oral delivery of texts. Greek playwrights such as Euripides and Aristophanes used symbols to distinguish the ends of phrases in written drama: this essentially helped the play's cast to know when to pause. After 200 b.c., the Greeks used a system (called "théseis") of a single dot ("punctus") placed at varying heights to mark up speeches at rhetorical divisions:
In addition, the Greeks used the paragraphos (or gamma) to mark the beginning of sentences, marginal diples to mark quotations, and a koronis to indicate the end of major sections.
The Romans ("ca". 1st century b.c.) also occasionally used symbols to indicate pauses, but the Greek "théseis"—under the name "distinctiones"—prevailed by the a.d. 4th century as reported by Donatus and Isidore of Seville (7th century). Also, texts were sometimes lain out "per capitula", that is, every sentence had its own separate line. Diples were used, but by the late period these often degenerated into comma-shaped marks.
"On the page, punctuation performs its grammatical function, but in the mind of the reader it does more than that. It tells the reader how to hum the tune."
 Lynn Truss, "Eats, Shoots and Leaves".
Medieval.
Punctuation developed dramatically when large numbers of copies of the Bible started to be produced. These were designed to be read aloud, so the copyists began to introduce a range of marks to aid the reader, including indentation, various punctuation marks (diple, paragraphos, "simplex ductus"), and an early version of initial capitals ("litterae notabiliores"). St. Jerome and his colleagues, who made the Latin Vulgate translation of the Bible ("ca". a.d. 400), employed a layout system based on established practices for teaching the speeches of Demosthenes and Cicero. Under his layout "per cola et commata" every sense-unit was indented and given its own line. This layout was solely used for biblical manuscripts during the 5th-9th centuries but was abandoned in favor of punctuation.
In the 7th-8th centuries Irish and Anglo-Saxon scribes, whose native languages were not derived from Latin, added more visual cues to render texts more intelligible. Irish scribes introduced the practice of word separation. Likewise, insular scribes adopted the "distinctiones" system while adapting it for minuscule script (so as to be more prominent) by using not differing height but rather a differing number of marks—aligned horizontally (or sometimes triangularly)—to signify a pause's value: one mark for a minor pause, two for a medium one, and three for a major. Most common were the "punctus", a comma-shaped mark, and a 7-shaped mark ("comma positura"), often used in combination. The same marks could be used in the margin to mark off quotations.
In the late 8th century a different system emerged in the Carolingian empire. Originally indicating how the voice should be modulated when chanting the liturgy, the "positurae" migrated into any text meant to be read aloud, and then to all manuscripts. "Positurae" first reached England in the late 10th century probably during the Benedictine reform movement, but was not adopted until after the Norman conquest. The original "positurae" were the "punctus", "punctus elevatus", "punctus versus", and "punctus interrogativus", but a fifth symbol, the "punctus flexus", was added in the 10th century to indicate a pause of a value between the "punctus" and "punctus elevatus". In the late 11th/early 12th century the "punctus versus" disappeared and was taken over by the simple "punctus" (now with two distinct values).
The late Middle Ages saw the addition of the "virgula suspensiva" (slash or slash with a midpoint dot) which was often used in conjunction with the "punctus" for different types of pauses. Direct quotations were marked with marginal diples, as in Antiquity, but from at least the 12th century scribes also began entering diples (sometimes double) within the column of text.
Later developments.
From the invention of moveable type in Europe in the 1450s the amount of printed material and a readership for it began to increase. "The rise of printing in the 14th and 15th centuries meant that a standard system of punctuation was urgently required." The introduction of a standard system of punctuation has also been attributed to the Venetian printers Aldus Manutius and his grandson. They have been credited with popularizing the practice of ending sentences with the colon or full stop, inventing the semicolon, making occasional use of parentheses and creating the modern comma by lowering the virgule. By 1566, Aldus Manutius the Younger was able to state that the main object of punctuation was the clarification of syntax.
By the 19th century, punctuation in the western world had evolved "to classify the marks hierarchically, in terms of weight". Cecil Hartley's poem identifies their relative values:
The use of punctuation was not standardised until after the invention of printing. According to the 1885 edition of "The American Printer", the importance of punctuation was noted in various sayings by children such as:
With a semi-colon and a comma added it reads:
In a 19th-century manual of typography, Thomas MacKellar writes:
Shortly after the invention of printing, the necessity of stops or pauses in sentences for the guidance of the reader produced the colon and full point. In process of time, the comma was added, which was then merely a perpendicular line, proportioned to the body of the letter. These three points were the only ones used until the close of the fifteenth century, when Aldo Manuccio gave a better shape to the comma, and added the semicolon; the comma denoting the shortest pause, the semicolon next, then the colon, and the full point terminating the sentence. The marks of interrogation and admiration were introduced many years after.
The standards and limitations of evolving technologies have exercised further pragmatic influences. For example, minimisation of punctuation in typewritten matter became economically desirable in the 1960s and 1970s for the many users of carbon-film ribbons, since a period or comma consumed the same length of expensive non-reusable ribbon as did a capital letter.
Punctuation of English.
There are two major styles of punctuation in English: American or traditional punctuation; and British or logical punctuation. These two styles differ mainly in the way in which they handle quotation marks.
Other languages.
Other European languages use much the same punctuation as English. The similarity is so strong that the few variations may confuse a native English reader. Quotation marks are particularly variable across European languages. For example, in French and Russian, quotes would appear as: « Je suis fatigué. » (in French, each "double punctuation", as the guillemet, requires a non-breaking space; in Russian it does not).
In Greek, the question mark is written as the English semicolon, while the functions of the colon and semicolon are performed by a raised point (·), known as the "ano teleia" (άνω τελεία).
In Georgian, three dots, ⟨჻⟩, were formerly used as a sentence or paragraph divider. It is still sometimes used in calligraphy. 
Spanish uses an inverted question mark at the beginning of a question and the normal question mark at the end, as well as an inverted exclamation mark at the beginning of an exclamation and the normal exclamation mark at the end.
Armenian uses several punctuation marks of its own. The full stop is represented by a colon, and vice versa; the exclamation mark is represented by a diagonal similar to a tilde (~), while the question mark resembles the "at" symbol.
Arabic, Urdu, and Persian languages—written from right to left—use a reversed question mark: ؟, and a reversed comma: ، . This is a modern innovation; pre-modern Arabic did not use punctuation. Hebrew, which is also written from right to left, uses the same characters as in English, "," and "?" .
Originally, Sanskrit had no punctuation. In the 17th century, Sanskrit and Marathi, both written in the Devanagari script, started using the vertical bar (।) to end a line of prose and double vertical bars (॥) in verse.
Punctuation was not used in Chinese, Japanese, and Korean writing until the adoption of punctuation from the West in the late 19th and early 20th century. In unpunctuated texts, the grammatical structure of sentences in classical writing is inferred from context. Most punctuation marks in modern Chinese, Japanese, and Korean have similar functions to their English counterparts; however, they often look different and have different customary rules.
In the Indian Subcontinent (India, Pakistan, ...), :- is sometimes used in place of colon or after a subheading. Its origin is unclear, but could be a remnant of the British Raj. Another punctuation common in the Indian Subcontinent for writing money amounts is the use of /- or /= after the number. For example, Rs. 20/- or Rs. 20/= implies 20 rupees whole. 
"Further information: Armenian punctuation, Chinese punctuation, Hebrew punctuation, Japanese punctuation and Korean punctuation."
Novel punctuation marks.
“Love point” and similar marks.
In 1966, the French author Hervé Bazin proposed a series of six innovative punctuation marks in his book "Plumons l’Oiseau" (“Let's pluck the bird”, 1966). These were:
“question comma”, “exclamation comma”.
An international patent application was filed, and published in 1992 under WO number WO9219458, for two new punctuation marks: the “question comma” and the “exclamation comma”. The "question comma" is a comma in place of the dot underneath the curve of a question mark, while the "exclamation comma" has a comma in place of the point at the bottom of an exclamation mark. These were intended for use as question and exclamation marks within a sentence, a function for which normal question and exclamation marks can also be used, but which may be considered obsolescent. The patent application entered into national phase exclusively with Canada, advertised as lapsing in Australia on 27 January 1994 and in Canada on 6 November 1995.

</doc>
<doc id="23712" url="http://en.wikipedia.org/wiki?curid=23712" title="Pentomino">
Pentomino

A pentomino is a plane geometric figure formed by joining five equal squares edge to edge. It is a polyomino with five cells. There are twelve pentominoes, not counting rotations and reflections as distinct. They are used chiefly in recreational mathematics for puzzles and problems. Pentominoes were formally defined by American professor Solomon W. Golomb starting in 1953 and later in his 1965 book "Polyominoes: Puzzles, Patterns, Problems, and Packings". Golomb coined the term "pentomino" from the Ancient Greek πέντε / "pénte", "five", and the -omino of domino, fancifully interpreting the "d-" of "domino" as if it were a form of the Greek prefix "di-" (two). Golomb named the 12 "free" pentominoes after letters of the Latin alphabet that they resemble.
Ordinarily, the pentomino obtained by reflecting or rotating a pentomino does not count as a different pentomino. The F, L, N, P, Y, and Z pentominoes are chiral; adding their reflections (F', J, N', Q, Y', S) brings the number of "one-sided" pentominoes to 18. Pentominoes I, T, U, V, W, and X, remain the same when reflected. This matters in some video games in which the pieces may not be reflected, such as Tetris imitations and Rampart.
Each of the twelve pentominoes satisfies the Conway criterion; hence every pentomino is capable of tiling the plane. Each chiral pentomino can tile the plane without reflecting it.
John Horton Conway proposed an alternate labeling scheme for pentominoes, using O instead of I, Q instead of L, R instead of F, and S instead of N. The resemblance to the letters is more strained, especially for the O pentomino, but this scheme has the advantage of using 12 consecutive letters of the alphabet. It is used by convention in discussing Conway's Game of Life, where, for example, one speaks of the R-pentomino instead of the F-pentomino.
Symmetry.
Pentominoes have the following categories of symmetry:
If reflections of a pentomino are considered distinct, as they are with one-sided pentominoes, then the first and fourth categories above double in size, resulting in an extra 6 pentominoes for a total of 18. If rotations are also considered distinct, then the pentominoes from the first category count eightfold, the ones from the next three categories (T, U, V, W, Z) count fourfold, I counts twice, and X counts only once. This results in 5×8 + 5×4 + 2 + 1 = 63 "fixed" pentominoes.
For example, the eight possible orientations of the L, F, N, P, and Y pentominoes are as follows:
For 2D figures in general there are two more categories:
Tiling rectangles.
A standard pentomino puzzle is to tile a rectangular box with the pentominoes, i.e. cover it without overlap and without gaps. Each of the 12 pentominoes has an area of 5 unit squares, so the box must have an area of 60 units. Possible sizes are 6×10, 5×12, 4×15 and 3×20. The avid puzzler can probably solve these problems by hand within a few hours. A more challenging task, typically requiring a computer search, is to count the total number of solutions in each case.
The 6×10 case was first solved in 1960 by Colin Brian and Jenifer Haselgrove. There are exactly 2339 solutions, excluding trivial variations obtained by rotation and reflection of the whole rectangle, but including rotation and reflection of a subset of pentominoes (which sometimes provides an additional solution in a simple way). The 5×12 box has 1010 solutions, the 4×15 box has 368 solutions, and the 3×20 box has just 2 solutions (one is shown in the figure, and the other one can be obtained from the solution shown by rotating, as a whole, the block consisting of the L, N, F, T, W, Y, and Z pentominoes).
A somewhat easier (more symmetrical) puzzle, the 8×8 rectangle with a 2×2 hole in the center, was solved by Dana Scott as far back as 1958. There are 65 solutions. Scott's algorithm was one of the first applications of a backtracking computer program. Variations of this puzzle allow the four holes to be placed in any position. One of the external links uses this rule. Most such patterns are solvable, with the exceptions of placing each pair of holes near two corners of the board in such a way that both corners could only be fitted by a P-pentomino, or forcing a T-pentomino or U-pentomino in a corner such that another hole is created.
Efficient algorithms have been described to solve such problems, for instance by Donald Knuth. Running on modern hardware, these pentomino puzzles can now be solved in mere seconds.
Filling boxes.
A pentacube is a polycube of five cubes. Twelve of the 29 pentacubes correspond to the twelve pentominoes extruded to a depth of one square.
A pentacube puzzle or 3D pentomino puzzle, amounts to filling a 3-dimensional box with these 1-layer pentacubes, i.e. cover it without overlap and without gaps. Each of the 12 pentacubes consists of 5 unit cubes, and are like 2D pentominoes but with unit thickness. Clearly the box must have a volume of 60 units. Possible sizes are 2×3×10 (12 solutions), 2×5×6 (264 solutions) and 3×4×5 (3940 solutions). Following are one solution of each case.
Alternatively one could also consider combinations of five cubes that are themselves 3D, i.e., are not part of one layer of cubes. However, in addition to the 12 extruded pentominoes, 6 sets of chiral pairs and 5 pieces make total 29 pieces, resulting 145 cubes, which will not make a 3D box.
Board game.
There are board games of skill based entirely on pentominoes. Such games are often simply called "Pentominoes".
One of the games is played on an 8×8 grid by two or three players. Players take turns in placing pentominoes on the board so that they do not overlap with existing tiles and no tile is used more than once. The objective is to be the last player to place a tile on the board. This version of Pentominoes is called "Golomb's Game".
The two-player version has been weakly solved in 1996 by Hilarie Orman. It was proved to be a first-player win by examining around 22 billion board positions 
Pentominoes, and similar shapes, are also the basis of a number of other tiling games, patterns and puzzles. For example, the French board game "Blokus" is played with 4 opposing color sets of polyominoes. "In Blokus", each color begins with every pentomino (12), as well as every tetromino (5), every triomino (2), every domino (1), and the monomino (1). Like the game "Pentominoes", the goal is to use all of your tiles, and a bonus is given if the monomino is played on the very last move. The player with the fewest blocks remaining wins.
The game of "Cathedral" is also based on polyominoes.
Parker Brothers released a multi-player pentomino board game called "Universe" in 1966. Its theme is based on an outtake from the movie in which the astronaut (seen playing chess in the final version) is playing a two-player pentomino game against a computer. The front of the board game box features scenes from the movie as well as a caption describing it as the "game of the future". The game comes with 4 sets of pentominoes in red, yellow, blue, and white. The board has two playable areas: a base 10x10 area for two players with an additional 25 squares (two more rows of 10 and one offset row of 5) on each side for more than two players.
Game manufacturer Lonpos has a number of games that use the same pentominoes, but on different game planes. Their "101 Game" has a 5 x 11 plane. By changing the shape of the plane, thousands of puzzles can be played, although only a relatively small selection of these puzzles are available in print.
Literature.
The first pentomino problem, written by Henry Dudeney, was published in 1907 in the Canterbury Puzzles.
Pentominoes were featured in a prominent subplot of Arthur C. Clarke's novel "Imperial Earth", published in 1975. Clarke also wrote an essay in which he described the game and how he got hooked on it.
They were also featured in Blue Balliett's "Chasing Vermeer", which was published in 2003 and illustrated by Brett Helquist, as well as its sequels, "The Wright 3" and "The Calder Game".
In the New York Times crossword puzzle for June 27, 2012, the clue for an 11-letter word at 37 across was "Complete set of 12 shapes formed by this puzzle's black squares."
References.
</dl>

</doc>
<doc id="23716" url="http://en.wikipedia.org/wiki?curid=23716" title="Programmer">
Programmer

A programmer, computer programmer, developer, coder, or software engineer is a person who writes computer software. The term "computer programmer" can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (COBOL, C, C++, C#, Java, Lisp, Python, etc.) is often prefixed to these titles, and those who work in a web environment often prefix their titles with "Web". The term "programmer" can be used to refer to a software developer, Web developer, mobile applications developer, embedded firmware developer, software engineer, computer scientist, or software analyst. However, members of these professions possess other software engineering skills, beyond programming; for this reason, the term "programmer", or "code monkey", is sometimes considered an insulting or derogatory oversimplification of these other professions. This has sparked much debate amongst developers, analysts, computer scientists, programmers, and outsiders who continue to be puzzled at the subtle differences in the definitions of these occupations.
British countess and mathematician Ada Lovelace is considered the first computer programmer, as she was the first to write and publish an algorithm intended for implementation on Charles Babbage's analytical engine, in October 1842, intended for the calculation of Bernoulli numbers. Because Babbage's machine was never completed to a functioning standard in her time, she never saw her algorithm run.
The first person to run a program on a functioning modern electronically based computer was computer scientist Konrad Zuse, in 1941.
The ENIAC programming team, consisting of Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas and Ruth Lichterman were the first regularly working programmers.
International Programmers' Day is celebrated annually on 7 January. In 2009, the government of Russia decreed a professional annual holiday known as Programmers' Day to be celebrated on 13 September (12 September in leap years). It had also been an unofficial international holiday before that.
Nature of the work.
Computer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming — advanced computing technologies and sophisticated new languages and programming tools — have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization.
Programmers work in many settings, including corporate information technology ("IT") departments, big software companies, small service firms and government entities of all sizes. Many professional programmers also work for consulting companies at client sites as contractors. Licensing is not typically required to work as a programmer, although professional certifications are commonly held by programmers. Programming is widely considered a profession (although some authorities disagree on the grounds that only careers with legal licensing requirements count as a profession).
Programmers' work varies widely depending on the type of business for which they are writing programs. For example, the instructions involved in updating financial records are very different from those required to duplicate conditions on an aircraft for pilots training in a flight simulator. Simple programs can be written in a few hours, more complex ones may require more than a year of work, while others are never considered 'complete' but rather are continuously improved as long as they stay in use. In most cases, several programmers work together as a team under a senior programmer’s supervision.
Programmers write programs according to the specifications determined primarily by more senior programmers and by systems analysts. After the design process is complete, it is the job of the programmer to convert that design into a logical series of instructions that the computer can follow. The programmer codes these instructions in one of many programming languages. Different programming languages are used depending on the purpose of the program. COBOL, for example, is commonly used for business applications that typically run on mainframe and midrange computers, whereas Fortran is used in science and engineering. C++ is widely used for both scientific and business applications. Java, C#, VB and PHP are popular programming languages for Web and business applications. Programmers generally know more than one programming language and, because many languages are similar, they often can learn new languages relatively easily. In practice, programmers often are referred to by the language they know, e.g. as "Java programmers", or by the type of function they perform or environment in which they work: for example, "database programmers", "mainframe programmers", or Web developers.
When making changes to the source code that programs are made up of, programmers need to make other programmers aware of the task that the routine is to perform. They do this by inserting comments in the source code so that others can understand the program more easily and by documenting their code. To save work, programmers often use libraries of basic code that can be modified or customized for a specific application. This approach yields more reliable and consistent programs and increases programmers' productivity by eliminating some routine steps.
Testing and debugging.
Programmers test a program by running it and looking for bugs (errors). As they are identified, the programmer usually makes the appropriate corrections, then rechecks the program until an acceptably low level and severity of bugs remain. This process is called testing and debugging. These are important parts of every programmer's job. Programmers may continue to fix these problems throughout the life of a program. Updating, repairing, modifying, and expanding existing programs is sometimes called "maintenance programming". Programmers may contribute to user guides and online help, or they may work with technical writers to do such work.
Application versus system programming.
Computer programmers often are grouped into two broad types: application programmers and systems programmers. Application programmers write programs to handle a specific job, such as a program to track inventory within an organization. They also may revise existing packaged software or customize generic applications which are frequently purchased from independent software vendors. Systems programmers, in contrast, write programs to maintain and control computer systems software, such as operating systems and database management systems. These workers make changes in the instructions that determine how the network, workstations, and CPU of the system handle the various jobs they have been given and how they communicate with peripheral equipment such as printers and disk drives.
Types of software.
Programmers in software development companies may work directly with experts from various fields to create software – either programs designed for specific clients or packaged software for general use – ranging from video games to educational software to programs for desktop publishing and financial planning. Programming of packaged software constitutes one of the most rapidly growing segments of the computer services industry. Some companies or organizations – even small ones – have set up their own IT team to ensure the design and development of in-house software to answer to very specific needs from their internal end-users, especially when existing software are not suitable or too expensive. This is for example the case in research laboratories.
In some organizations, particularly small ones, workers commonly known as "programmer analysts" are responsible for both the systems analysis and the actual programming work. The transition from a mainframe environment to one that is based primarily on personal computers (PCs) has blurred the once rigid distinction between the programmer and the user. Increasingly, adept end users are taking over many of the tasks previously performed by programmers. For example, the growing use of packaged software, such as spreadsheet and database management software packages, allows users to write simple programs to access data and perform calculations.
In addition, the rise of the Internet has made web development a huge part of the programming field. Currently more software applications are web applications that can be used by anyone with a web browser. Examples of such applications include the Google search service, the Hotmail e-mail service, and the Flickr photo-sharing service.
Programming editors, also known as source code editors, are text editors that are specifically designed for programmers or developers for writing the source code of an application or a program. Most of these editors include features useful for programmers, which may include color syntax highlighting, auto indentation, auto-complete, bracket matching, syntax check, and allows plug-ins. These features aid the users during coding, debugging and testing.
Globalization.
Market changes in the UK.
According to BBC News, 17% of computer science students could not find work in their field 6 months after graduation in 2009 which was the highest rate of the university subjects surveyed while 0% of medical students were unemployed in the same survey. The UK category system does, however, class such degrees as information technology and game design as 'computer science', industries in which jobs can be extremely difficult to find, somewhat inflating the actual figure.
Market changes in the US.
Computer programming, offshore outsourcing, and Foreign Worker Visas became a controversial topic after the crash of the dot-com bubble left many programmers without work or with lower wages. Programming was even mentioned in the 2004 US Presidential debate on the topic of offshore outsourcing.
Large companies claim there is a skills shortage with regard to programming talent. However, US programmers and unions counter that large companies are exaggerating their case in order to obtain cheaper programmers from developing countries and to avoid paying for training in very specific technologies.
Enrollment in computer-related degrees in US has dropped recently due to lack of general interests in science and mathematics and also out of an apparent fear that programming will be subject to the same pressures as manufacturing and agriculture careers. This situation has resulted in confusion about whether the US economy is entering a "post-information age" and the nature of US comparative advantages. Technology and software jobs were supposed to be the replacement for factory and agriculture jobs lost to cheaper foreign labor, but if those are subject to free trade losses, then the nature of the next generation of replacement careers is not clear at this point.

</doc>
<doc id="23719" url="http://en.wikipedia.org/wiki?curid=23719" title="Periodic table (large cells)">
Periodic table (large cells)

This page shows large-cell versions of the periodic table. For each element name, symbol, atomic number, and mean atomic mass value for the natural isotopic composition of each element are shown. The periodic table of the chemical elements is a tabular method of displaying the chemical elements.
The two layout forms originate from two graphic forms of presentation of the same periodic table. Historically, when the f-block was identified it was drawn below the existing table, with markings for its in-table location (this page uses dots or asterisks). Also, a common presentation is to put all 15 lanthanide and actinide columns below, while the f-block only has 14 columns. The fifteenth (rightmost) lanthanide and actinide are d-block elements, belonging to group 3, with scandium and yttrium.
Although precursors to this table exist, its invention is generally credited to Russian chemist Dmitri Mendeleev in 1869. Mendeleev intended the table to illustrate recurring ("periodic") trends in the properties of the elements. The layout of the table has been refined and extended over time, as new elements have been discovered, and new theoretical models have been developed to explain chemical behavior.
References.
</dl>

</doc>
<doc id="23721" url="http://en.wikipedia.org/wiki?curid=23721" title="Peter Singer">
Peter Singer

Peter Albert David Singer, AC (born 6 July 1946) is an Australian moral philosopher. He is currently the Ira W. DeCamp Professor of Bioethics at Princeton University, and a Laureate Professor at the Centre for Applied Philosophy and Public Ethics at the University of Melbourne. He specializes in applied ethics and approaches ethical issues from a secular, utilitarian perspective. He is known in particular for his book, "Animal Liberation" (1975), a canonical text in animal rights/liberation theory. For most of his career, he supported preference utilitarianism, but in his later years became a classical or hedonistic utilitarian, when co-authoring "The Point of View of the Universe" with Katarzyna de Lazari-Radek.
On two occasions Singer served as chair of the philosophy department at Monash University, where he founded its Centre for Human Bioethics. In 1996 he stood unsuccessfully as a Greens candidate for the Australian Senate. In 2004 he was recognised as the Australian Humanist of the Year by the Council of Australian Humanist Societies, and in June 2012 was named a Companion of the Order of Australia for his services to philosophy and bioethics. He serves on the Advisory Board of Incentives for Global Health, the NGO formed to develop the Health Impact Fund proposal. He was voted one of Australia's ten most influential public intellectuals in 2006. Singer currently serves on the advisory board of Academics Stand Against Poverty (ASAP).
Life and career.
Singer's parents were Viennese Jews who emigrated to Australia from Vienna in 1938, after Austria's annexation by Nazi Germany. They settled in Melbourne, where Singer was born. His grandparents were less fortunate: his paternal grandparents were taken by the Nazis to Łódź, and were never heard from again; his maternal grandfather died in the Theresienstadt concentration camp. He has a sister, Joan (now Joan Dwyer). Singer's grandfather, David Oppenheim, published numerous papers with Sigmund Freud before a falling out between the two in Venice. Singer's father imported tea and coffee, while his mother practiced medicine. He attended Preshil and later Scotch College. After leaving school, Singer studied law, history and philosophy at the University of Melbourne, gaining his BA degree (hons) in 1967. He received an MA for a thesis entitled "Why should I be moral?" in 1969. He was awarded a scholarship to study at the University of Oxford, and obtained from there a B.Phil in 1971, with a thesis on civil disobedience supervised by R. M. Hare and subsequently published as a book in 1973. Singer names Hare and Australian philosopher H. J. McCloskey as his two most important mentors.
After spending two years as a Radcliffe lecturer at University College, Oxford, he was a visiting professor at New York University for 16 months. He returned to Melbourne in 1977, where he spent most of his career, aside from appointments as visiting faculty abroad, until his move to Princeton in 1999. In June 2011 it was announced he would join the professoriate of New College of the Humanities, a private college in London, in addition to his work at Princeton.
According to philosopher Helga Kuhse, Singer is "almost certainly the best-known and most widely read of all contemporary philosophers". Michael Specter wrote that Singer is among the most influential of contemporary philosophers.
"Animal Liberation".
Published in 1975, "Animal Liberation" has been cited as a formative influence on leaders of the modern animal liberation movement. The central argument of the book is an expansion of the utilitarian idea that "the greatest good of the greatest number" is the only measure of good or ethical behaviour. Singer believes that there is no reason not to apply this to other animals, arguing that the boundary between human and ‘animal’ is completely arbitrary. There are more differences between a great ape and an oyster, for example, than between a human and a great ape, and yet the former two are lumped together as ‘animals’ whilst we are ‘human’.
In particular, he argues that while animals show lower intelligence than the average human, many severely intellectually challenged humans show equally diminished, if not lower, mental capacity, and that some animals have displayed signs of intelligence sometimes on par with that of human children. Singer therefore argues intelligence does not provide a basis for providing nonhuman animals any less consideration than such intellectually challenged humans.
He popularized the term "speciesism", which had been coined previously by English writer Richard D. Ryder, to describe the practice of privileging humans over other animals.
Applied ethics.
Singer's most comprehensive work, "Practical Ethics" (1979), analyzes in detail why and how living beings' interests should be weighed. His principle of equal consideration of interests does not dictate equal treatment of all those with interests, since different interests warrant different treatment. All have an interest in avoiding pain, for instance, but relatively few have an interest in cultivating their abilities. Not only does his principle justify different treatment for different interests, but it allows different treatment for the same interest when diminishing marginal utility is a factor. For example, this approach would privilege a starving person's interest in food over the same interest of someone who is only slightly hungry.
Among the more important human interests are those in avoiding pain, in developing one's abilities, in satisfying basic needs for food and shelter, in enjoying warm personal relationships, in being free to pursue one's projects without interference, "and many others". The fundamental interest that entitles a being to equal consideration is the capacity for "suffering and/or enjoyment or happiness". Singer holds that a being's interests should always be weighed according to that being's concrete properties. He favors a 'journey' model of life, which measures the wrongness of taking a life by the degree to which doing so frustrates a life journey's goals. The journey model is tolerant of some frustrated desire and explains why persons who have embarked on their journeys are not replaceable. Only a personal interest in continuing to live brings the journey model into play. This model also explains the priority that Singer attaches to "interests" over trivial desires and pleasures.
Singer's ideas require the concept of an impartial standpoint from which to compare interests. He has wavered about whether the precise aim is the total amount of satisfied interests or the most satisfied interests among those beings who already exist prior to the decision-making. The second edition of "Practical Ethics" disavows the first edition's suggestion that the total and prior-existence views should be combined. The second edition asserts that preference-satisfaction utilitarianism, incorporating the 'journey' model, applies without invoking the first edition's suggestion about the total view. The details are fuzzy, however, and Singer admits that he is "not entirely satisfied" with his treatment.
Ethical conduct is justifiable by reasons that go beyond prudence to "something bigger than the individual," addressing a larger audience. Singer thinks this going-beyond identifies moral reasons as "somehow universal", specifically in the injunction to 'love thy neighbor as thyself', interpreted by him as demanding that one give the same weight to the interests of others as one gives to one's own interests. This universalising step, which Singer traces from Kant to Hare, is crucial and sets him apart from those moral theorists, from Hobbes to David Gauthier, who tie morality to prudence. Universalisation leads directly to utilitarianism, Singer argues, on the strength of the thought that one's own interests cannot count for more than the interests of others. Taking these into account, one must weigh them up and adopt the course of action that is most likely to maximise the interests of those affected; utilitarianism has been arrived at. Singer's universalising step applies to interests without reference to who has them, whereas a Kantian's applies to the judgments of rational agents (in Kant's kingdom of ends, or Rawls's Original Position, etc.). Singer regards Kantian universalisation as unjust to animals. As for the Hobbesians, Singer attempts a response in the final chapter of "Practical Ethics", arguing that self-interested reasons support adoption of the moral point of view, such as 'the paradox of hedonism', which counsels that happiness is best found by not looking for it, and the need most people feel to relate to something larger than their own concerns.
"Practical Ethics" includes a chapter arguing for the redistribution of wealth to ameliorate absolute poverty (Chapter 8, "Rich and Poor"), and another making a case for resettlement of refugees on a large scale in industrialised countries (Chapter 9, "Insiders and Outsiders").
Although the natural, non-sentient environment has no intrinsic value for a utilitarian like Singer, environmental degradation is a profound threat to sentient life, and for this reason he states that environmentalists are right to speak of wilderness as a 'world heritage'.
Euthanasia and infanticide.
Consistent with his general ethical theory, Singer holds that the right to life is essentially tied to a being's capacity to hold preferences, which in turn is essentially tied to a being's capacity to feel pain and pleasure. Critics such as Laing hold that this view is subject to charges of inconsistency, equivocation and contradiction.
Similar to his argument for abortion, Singer argues that newborns lack the essential characteristics of personhood—"rationality, autonomy, and self-consciousness"—and therefore "killing a newborn baby is never equivalent to killing a person, that is, a being who wants to go on living."
Singer classifies euthanasia as voluntary, involuntary, or non-voluntary. Voluntary euthanasia is that to which the subject consents.
Singer's book "Rethinking Life and Death: The Collapse of Our Traditional Ethics" offers further examination of the ethical dilemmas concerning the advances of medicine. He covers the value of human life and quality of life ethics in addition to abortion and other controversial ethical questions.
Religious critics have argued that Singer's ethic ignores and undermines the traditional notion of the sanctity of life; whereas bioethics associated with the Disability Rights and Disability Studies communities have argued that his epistemology is based on ableist conceptions of disability. Singer agrees and believes the notion of the sanctity of life ought to be discarded as outdated, unscientific and irrelevant to understanding problems in contemporary bioethics.
Singer has experienced the complexities of some of these questions in his own life. His mother had Alzheimer's disease. He said, "I think this has made me see how the issues of someone with these kinds of problems are really very difficult". In an interview with Ronald Bailey, published in December 2000, he explained that his sister shares the responsibility of making decisions about his mother. He did say that, if he were solely responsible, his mother might not continue to live.
Abortion.
In Singer's view, the central argument against abortion may be stated as the following syllogism:
 It is wrong to kill an innocent human being. 
 A human fetus is an innocent human being. 
 Therefore it is wrong to kill a human fetus.
In his book "Rethinking Life and Death," as well as in "Practical Ethics," Singer asserts that, if we take the premises at face value, the argument is deductively valid. Singer comments that defenders of abortion attack the second premise, suggesting that the fetus becomes a "human" or "alive" at some point after conception; however, Singer finds this argument flawed in that human development is a gradual process, and it is nearly impossible to mark a particular moment in time as "the" moment at which human life begins.
Singer's argument for abortion differs from those given by many other proponents of abortion; rather than attacking the second premise of the anti-abortion argument, Singer attacks the first premise, denying that it is necessarily wrong to take innocent human life:
[The argument that a fetus is not alive] is a resort to a convenient fiction that turns an evidently living being into one that legally is not alive. Instead of accepting such fictions, we should recognise that the fact that a being is human, and alive, does not in itself tell us whether it is wrong to take that being's life.
Singer states that arguments for or against abortion should be based on utilitarian calculation which compares the preferences of a woman against the preferences of the fetus. In his view a preference is anything sought to be obtained or avoided; all forms of benefit or harm caused to a being correspond directly with the satisfaction or frustration of one or more of its preferences. Since a capacity to experience the sensations of suffering or satisfaction is a prerequisite to having any preferences at all, and a fetus, up to around eighteen weeks, says Singer, has no capacity to suffer or feel satisfaction, it is not possible for such a fetus to hold any preferences at all. In a utilitarian calculation, there is nothing to weigh against a woman's preferences to have an abortion; therefore, abortion is morally permissible.
Singer's book "Rethinking Life and Death: The Collapse of Our Traditional Ethics" offers further examination of the ethical dilemmas concerning the advances of medicine. He covers the value of human life and quality of life ethics in addition to abortion and other controversial ethical questions.
World poverty.
In "Famine, Affluence, and Morality", one of Singer's best-known philosophical essays, he argues that some people living in abundance while others starve is morally indefensible. Singer proposes that anyone able to help the poor should donate part of their income to aid poverty relief and similar efforts. Singer reasons that, when one is already living comfortably, a further purchase to increase comfort will lack the same moral importance as saving another person's life. Singer himself reports that he donates around 33% of his salary to a variety of cost-effective charities. and he is a member of Giving What We Can, an international society for the promotion of poverty relief inspired by Singer's arguments. In "Rich and Poor", the version of the aforementioned article that appears in the second edition of "Practical Ethics", his main argument is presented as follows:
If we can prevent something bad without sacrificing anything of comparable significance, we ought to do it; absolute poverty is bad; there is some poverty we can prevent without sacrificing anything of comparable moral significance; therefore we ought to prevent some absolute poverty.
Singer's 2009 book, "The Life You Can Save", makes the argument that it is a clear-cut moral imperative for citizens of developed countries to give more to charitable causes that help the poor. While Singer acknowledges that there are problems with ensuring that money goes where it is most needed and that it is used effectively, he does not think that these practical difficulties undermine his original conclusion (that people should make a much greater effort to reduce poverty).
Effective altruism.
Singer is an advocate of effective altruism. Consistent with his utilitarian moral outlook, he argues that people should not only try to reduce suffering, but reduce it in the most effective manner possible. While Singer has previously written at length about the moral imperative to eliminate the suffering of nonhuman animals, particularly in the meat industry, and end world poverty, he writes about how the effective altruism movement is doing these things more effectively in his 2015 book, "The Most Good You Can Do". His own organisation, The Life You Can Save, also recommends a selection of charities deemed by charity evaluators such as GiveWell to be the most effective. In addition, he is a board member of Animal Charity Evaluators, a charity evaluator used by many members of the effective altruism community which recommends the most cost-effective vegetarian and animal advocacy charities.
In 2013, Singer presented a TED talk that advocates for the effective altruism movement.
Other views.
Doping in elite sports.
Singer agrees with Julian Savulescu that elite athletes should be allowed to take whatever performance-enhancing drugs they wish, "as long as it is safe for them to do so." The argument is that, "without drugs, those with the best genes have an unfair advantage. . . . Setting a maximum level of red blood cells [for endurance events] actually levels the playing field by reducing the impact of the genetic lottery. Effort then becomes more important than having the right genes."
Evolutionary biology and leftist politics.
In "A Darwinian Left", Singer outlines a plan for the political left to adapt to the lessons of evolutionary biology. He says that evolutionary psychology suggests that humans naturally tend to be self-interested. He further argues that the evidence that selfish tendencies are natural must not be taken as evidence that selfishness is "right." He concludes that game theory (the mathematical study of strategy) and experiments in psychology offer hope that self-interested people will make short-term sacrifices for the good of others, if society provides the right conditions. Essentially Singer claims that although humans possess selfish, competitive tendencies naturally, they have a substantial capacity for cooperation that has also been selected for during human evolution. Singer's writing in "Greater Good" magazine, published by the Greater Good Science Center of the University of California, Berkeley, includes the interpretation of scientific research into the roots of compassion, altruism, and peaceful human relationships.
Nonetheless, he claims not to be anti-capitalist. In an interview with New Left Project in 2010, he said the following:
Capitalism is very far from a perfect system, but so far we have yet to find anything that clearly does a better job of meeting human needs than a regulated capitalist economy coupled with a welfare and health care system that meets the basic needs of those who do not thrive in the capitalist economy.
He then adds that "If we ever do find a better system, I'll be happy to call myself an anti-capitalist."
Personism.
Although he has expressed admiration for many of the values promoted by secular humanism, Singer believes it to be incomplete and promotes a preference utilitarian view he calls "personism" instead.
Surrogacy.
In 1985, Singer wrote a book with the physician Deanne Wells arguing that surrogate motherhood should be allowed and regulated by the state by establishing non-profit 'State Surrogacy Boards', which would ensure fairness between surrogate mothers and surrogacy-seeking parents. Singer and Wells endorsed both the payment of medical expenses endured by surrogate mothers and an extra "fair fee" to compensate the surrogate mother.
Vegetarianism and ethics of food consumption.
In an article for the online publication Chinadialogue, Singer called Western-style meat production cruel, unhealthy and damaging to the ecosystem. He rejected the idea that the method was necessary to meet the population's increasing demand, explaining that animals in factory farms have to eat food grown explicitly for them, and they burn up most of the food's energy just to breathe and keep their bodies warm.
Singer calls himself a vegetarian and a "flexible vegan". In his May 2006 interview in "Mother Jones", he states:
I don't eat meat. I've been a vegetarian since 1971. I've gradually become increasingly vegan. I am largely vegan but I'm a flexible vegan. I don't go to the supermarket and buy non-vegan stuff for myself. But when I'm traveling or going to other people's places I will be quite happy to eat vegetarian rather than vegan.
In addition to his addressing issues concerning the consumption of animal products, Singer's "Can You Do Good by Eating Well?" in "Greater Good" examines the ethics of eating locally grown food.
Bestiality.
In a 2001 review of Midas Dekkers' "Dearest Pet: On Bestiality", Singer argues that sexual activities between humans and animals that result in harm to the animal should remain illegal, but that "sex with animals does not always involve cruelty" and that "mutually satisfying activities" of a sexual nature may sometimes occur between humans and animals, and that writer Otto Soyka would condone such activities. This position is countered by fellow philosopher Tom Regan, who writes that the same argument could be used to justify having sex with children. Regan writes that Singer's position is a consequence of his adapting a utilitarian, or consequentialist, approach to animal rights, rather than a strictly rights-based one, and argues that the rights-based position distances itself from non-consensual sex. The Humane Society of the United States takes the position that all sexual molestation of animals by humans is abusive, whether it involves physical injury or not.
Religion.
Singer is an atheist. He was a speaker at the 2012 Global Atheist Convention. He has debated with Christians such as John Lennox and Dinesh D'Souza. Singer's main objection against the Christian conception of God is the problem of evil. He stated: "The evidence of our own eyes makes it more plausible to believe that the world was not created by any god at all. If, however, we insist on believing in divine creation, we are forced to admit that the god who made the world cannot be all-powerful and all good. He must be either evil or a bungler." In keeping with his considerations of non-human animals, Singer also takes issue with the original sin reply to the problem of evil, saying that, "animals also suffer from floods, fires, and droughts, and, since they are not descended from Adam and Eve, they cannot have inherited original sin."
Death penalty.
Singer is opposed to the death penalty, claiming that it does not effectively deter the crimes for which it is the punitive measure and that he cannot see any other justification for it.
Criticism of Singer.
Singer's positions have been criticised by groups, such as advocates for disabled people and right-to-life supporters, concerned with what they see as his attacks upon human dignity. Singer has replied that many people judge him based on secondhand summaries and short quotations taken out of context, not his books or articles.
Some claim that Singer's utilitarian ideas lead to eugenics. American publisher Steve Forbes ceased his donations to Princeton University in 1999 because of Singer's appointment to a prestigious professorship. Nazi-hunter Simon Wiesenthal wrote to organisers of a Swedish book fair to which Singer was invited that "A professor of morals ... who justifies the right to kill handicapped newborns ... is in my opinion unacceptable for representation at your level." Marc Maurer, President of the National Federation of the Blind, criticised Singer's appointment to the Princeton Faculty in a banquet speech at the organisation's national convention in July 2001, claiming that Singer's support for euthanizing disabled babies could lead to disabled older children and adults being valued less as well. Conservative psychiatrist Theodore Dalrymple wrote in 2010 that Singerian moral universalism is "preposterous—psychologically, theoretically, and practically".
Singer's work has attracted criticism from other philosophers. Bernard Williams, who was a critic of utilitarianism, said of Singer that he "is always so keen to mortify himself and tell everyone how to live". Williams criticised Singer's ethic by saying that he's "always so damn logical" and thus "leaves out an entire dimension of value". Williams claimed that Singer's utilitarianism is impractical as it's impossible to "make these calculations and comparisons in real life".
Williams develops an extended critique of Singer for suggesting that "speciesism" is a prejudice roughly equivalent to sexism or racism by suggesting that we have yet to face the sort of scenarios where species membership would become a morally significant property, but that some science fiction-style thought experiments may provide such examples. He imagines an invasion of aliens that are "very disgusting indeed: their faces, for instance, if those are faces, are seething with what seem to be worms, but if we wait long enough to find out what they are at, we may gather that they are quite benevolent". Said aliens "want to live with us—rather closely with us" even though their "disgustingness is really, truly, unforgettable". Williams also suggests that another sort of alien visitors might have "much more successful experience than we have in running peacable societies" but that they would need to exercise significant control and remove the cultural autonomy of human beings. In both scenarios, Williams argues, it would be perfectly reasonable for human beings to treat their species membership as a reasonable morally significant property. Singer responds to Williams by arguing that the right and courageous thing to do is to make the decision without regards to species.
The aesthetics philosopher Roger Scruton wrote in 2000, "Singer's works, remarkably for a philosophy professor, contain little or no philosophical argument. They derive their radical moral conclusions from a vacuous utilitarianism that counts the pain and pleasure of all living things as equally significant and that ignores just about everything that has been said in our philosophical tradition about the real distinction between persons and animals".
In 2002 disability rights activist Harriet McBryde Johnson debated Singer, challenging his belief that parents ought to be able to euthanize their disabled children. "Unspeakable Conversations," Johnson's account of her encounters with Singer and the pro-euthanasia movement, was published in the "New York Times Magazine" in 2003. It also served as inspiration for "The Thrill", a 2013 play by Judith Thompson partly based on Johnson's life.
Protests.
In 1989 and 1990, Peter Singer's work was the subject of a number of protests in Germany. A course in ethics led by Dr Hartmut Kliemt at the University of Duisburg where the main text used was Singer's "Practical Ethics" was, according to Singer, "subjected to organized and repeated disruption by protesters objecting to the use of the book on the grounds that in one of its ten chapters it advocates active euthanasia for severely disabled newborn infants". The protests led to the course being shut down.
When Singer tried to speak during a lecture at Saarbrücken, he was interrupted by a group of protesters including advocates for the disabled. He offered the protesters the opportunity to explain why he should not be allowed to speak. The protesters indicated that they believed he was opposed to all rights for the disabled. They were unaware that, although Singer believes that some lives are so blighted from the beginning that their parents may decide their lives are not worth living, in other cases, once the decision is made to keep them alive, everything that can be done to improve the quality of their life should, to Singer's mind, be done. The ensuing discussion revealed that there were many misconceptions about his positions, but the revelation did not end the controversy. One of the protesters expressed that entering serious discussions was a tactical error.
The same year, Singer was invited to speak in Marburg at a European symposium on "Bioengineering, Ethics and Mental Disability". The invitation was fiercely attacked by leading intellectuals and organizations in German media, with an article in "Der Spiegel" comparing Singer's positions to Nazism. The symposium was eventually cancelled and Singer's invitation consequently withdrawn.
A lecture at the Zoological Institute of the University of Zurich was also interrupted by two groups of protesters. The first group was a group of disabled people who staged a brief protest at the beginning of the lecture. They objected to inviting an advocate of euthanasia to speak. At the end of this protest, when Singer tried to address their concerns, a second group of protesters rose and began chanting "Singer raus! Singer raus!" ("Singer out!") When Singer attempted to respond, a protester jumped on stage and grabbed his glasses, and the host ended the lecture. The first group of protesters was distressed by this second, more aggressive group. It had not intended to halt the lecture and even had questions to ask Singer. Singer explains "my views are not threatening to anyone, even minimally" and says that some groups play on the anxieties of those who hear only keywords that are understandably worrying (given the constant fears of ever repeating the Holocaust) if taken with any less than the full context of his belief system.
In 1991, Singer was due to speak along with R. M. Hare and Georg Meggle at the 15th International Wittgenstein Symposium in Kirchberg am Wechsel, Austria. Singer has stated that threats were made to Adolf Hübner, then the president of the Austrian Ludwig Wittgenstein Society, that the conference would be disrupted if Singer and Meggle were given a platform. Hübner proposed to the board of the society that Singer's invitation (as well as the invitations of a number of other speakers) be withdrawn. The Society decided to cancel the symposium.
In an article originally published in "The New York Review of Books", Singer argued that the protests dramatically increased the amount of coverage he got: "instead of a few hundred people hearing views at lectures in Marburg and Dortmund, several millions read about them or listened to them on television". Despite this, Singer argues that it has led to a difficult intellectual climate, with professors in Germany unable to teach courses on applied ethics and campaigns demanding the resignation of professors who invited Singer to speak.
Meta-ethics and foundational issues.
Though Singer focuses more than many philosophers on applied ethical questions, he has also written in depth on foundational issues in meta-ethics, including why one ethical system should be chosen over others. In "The Expanding Circle", he argues that the evolution of human society provides support for the utilitarian point of view. On his account, ethical reasoning has existed from the time primitive foraging bands had to cooperate, compromise, and make group decisions to survive. He elaborates: "In a dispute between members of a cohesive group of reasoning beings, the demand for a reason is a demand for a justification that can be accepted by the group as a whole." Thus, consideration of others' interests has long been a necessary part of the human experience. Singer believes that contemplative analysis may now guide one to accept a broader utilitarianism:
If I have seen that from an ethical point of view I am just one person among the many in my society, and my interests are no more important, from the point of view of the whole, than the similar interests of others within my society, I am ready to see that, from a still larger point of view, my society is just one among other societies, and the interests of members of my society are no more important, from that larger perspective, than the similar interests of members of other societies... Taking the impartial element in ethical reasoning to its logical conclusion means, first, accepting that we ought to have equal concern for all human beings.
Singer elaborates that viewing oneself as equal to others in one's society and at the same time viewing one's society as fundamentally superior to other societies may cause an uncomfortable cognitive dissonance. This is the sense in which he means that reason may push people to accept a broader utilitarian stance. Critics like Ken Binmore say that this cognitive dissonance is apparently not very strong, since people often knowingly ignore the interests of faraway societies quite similar to their own, and that the "ought" above only applies if one already accepts Singer's basic premises about the equality of various interests.
An alternative line taken by Singer about the need for ethics is that living the ethical life may be, on the whole, more satisfying than seeking only material gain. He invokes the hedonistic paradox, noting that those who pursue material gain seldom find the happiness they seek. Having a broader purpose in life may lead to more long-term happiness. On this account, impartial (self-sacrificing) behavior in particular matters may be motivated by self-interested considerations from a broader perspective.
Singer has also implicitly argued that an airtight defense of utilitarianism is not crucial to his work. In "Famine, Affluence, and Morality", he begins by saying that he would like to see how far a seemingly innocuous and widely endorsed principle can take us; the principle is that one is morally required to forgo a small pleasure to relieve someone else's immense pain. He then argues that this principle entails radical conclusions—for example, that affluent people are very immoral if they do not give up some luxury goods to donate the money for famine relief. If his reasoning is valid, he goes on to argue, either it is not very immoral to value small luxuries over saving many lives, or such affluent people are very immoral. As Singer argues in the same essay, regardless of the soundness of his fundamental defense of utilitarianism, his argument has value in that it exposes conflicts between many people's stated beliefs and their actions.
Honours.
Singer was inducted into the United States Animal Rights Hall of Fame in 2000.
On 11 June 2012, Singer was named a Companion of the Order of Australia for "eminent service to philosophy and bioethics as a leader of public debate and communicator of ideas in the areas of global poverty, animal welfare and the human condition."
Personal life.
Since 1968 he has been married to Renata née Diamond; they have three children: Ruth, Marion and Esther. Renata Singer is a novelist and author and has also collaborated on publications with her husband.

</doc>
<doc id="23723" url="http://en.wikipedia.org/wiki?curid=23723" title="Poznań">
Poznań

Poznań (; known also by other historical names) is a city on the Warta river in west-central Poland, the region called Wielkopolska (Greater Poland). The city population is about 550,000, while the continuous conurbation with Poznan County and several other communities (Oborniki, Skoki, Szamotuły and Śrem) is inhabited by almost 1.1 million people. The Larger Poznań Metropolitan Area (PMA) is inhabited by 1.3-1.4 million people and extends to such satellite towns as Nowy Tomyśl, Gniezno and Wrzesnia, making it the fourth largest metropolitan area in Poland.
Poznań is among the oldest cities in Poland and was one of the most important centers in the early Polish state in the tenth and eleventh centuries. The first center city was Ostrów Tumski, the natural island on the Warta river-very similar to the Île de la Cité in Paris. The first rulers were buried in Poznań's cathedral on the island. It also served as the capital for a short time in the 13th century, hence the official name: The capital city of Poznan.
Poznań is one of the biggest cities in Poland. It is the historical capital of the Wielkopolska (Greater Poland) region and is currently the administrative capital of the province called Greater Poland Voivodeship. Poznań is today one of the largest Polish centers of trade, industry, sports, education, technology, tourism and culture. It is particularly important academic center, with about 130,000 students and the third biggest Polish university - Adam Mickiewicz University. It is also the residence of the oldest Polish diocese, now being one of the most populous archdioceses in the country.
In 2012, the Poznań's Art and Business Center "Stary Browar" won a competition organized by National Geographic Traveller and was given the first prize as one of the seven "New Polish Wonders". Poznań has been rated highly, often coming first for Poland, as a city with a very high quality of life. The city has also won many times a prize awarded by "Superbrands" for a very high quality brand of city of Poznań. Poznań was classified in 2012 as high sufficiency city by Globalization and World Cities Research Network.
The official patron saints of Poznań are Saint Peter and Paul of Tarsus, the patrons of the cathedral. As a patron of the city is regarded as well the patron of the main street Święty Marcin - Martin of Tours.
Names.
The name Poznań probably comes from a personal name, "Poznan", (from the Polish participle "poznan(y)" – "one who is known/recognized"), and would mean "Poznan's town". It is also possible that the name comes directly from the verb "poznać", which means "to get to know" or "to recognize," so it may simply mean "known town".
The earliest surviving references to the city are found in the chronicles of Thietmar of Merseburg, written between 1012 and 1018: "episcopus Posnaniensis" ("bishop of Poznań", in an entry for 970) and "ab urbe Posnani" ("from the city of Poznań", for 1005). The city's name appears in documents in the Latin nominative case as "Posnania" in 1236 and "Poznania" in 1247. The phrase "in Poznan" appears in 1146 and 1244.
The city's full official name is "Stołeczne Miasto Poznań" ("The Capital City of Poznań"), in reference to its role as a centre of political power in the early Polish state. Poznań is known as "Posen" in German, and was officially called "Haupt- und Residenzstadt Posen" ("Capital and Residence City of Poznań") between 20 August 1910 and 28 November 1918. The Latin names of the city are "Posnania" and "Civitas Posnaniensis". Its Yiddish name is פּױזן, or "Poyzn".
The Russian version of the name, Познань "(Poznan')", is of feminine gender, in contrast to the Polish name, which is masculine.
History.
For centuries before the Christianization of Poland, Poznań (consisting of a fortified stronghold between the Warta and Cybina rivers, on what is now Ostrów Tumski) was an important cultural and political centre of the Polan tribe. Mieszko I, the first historically recorded ruler of the Polans, and of the early Polish state which they dominated, built one of his main stable headquarters in Poznań. Mieszko's baptism of 966, seen as a defining moment in the establishment of the Polish state, may have taken place in Poznań.
Following the baptism, construction began of Poznań's cathedral, the first in Poland. Poznań was probably the main seat of the first missionary bishop sent to Poland, Bishop Jordan. The Congress of Gniezno in 1000 led to the country's first permanent archbishopric being established in Gniezno (which is generally regarded as Poland's capital in that period), although Poznań continued to have independent bishops of its own. Poznań's cathedral was the place of burial of the early Piast monarchs (Mieszko I, Boleslaus I, Mieszko II, Casimir I), and later of Przemysł I and King Przemysł II.
The pagan reaction that followed Mieszko II's death (probably in Poznań) in 1034 left the region weak, and in 1038 Bretislaus I of Bohemia sacked and destroyed both Poznań and Gniezno. Poland was reunited under Casimir I the Restorer in 1039, but the capital was moved to Kraków, which had been relatively unaffected by the troubles.
In 1138, by the testament of Bolesław III, Poland was divided into separate duchies under the late king's sons, and Poznań and its surroundings became the domain of Mieszko III the Old, the first of the Dukes of Greater Poland. This period of fragmentation lasted until 1320. Duchies frequently changed hands; control of Poznań, Gniezno and Kalisz sometimes lay with a single duke, but at other times these constituted separate duchies.
In about 1249, Duke Przemysł I began constructing what would become the Royal Castle on a hill on the left bank of the Warta. Then in 1253 Przemysł issued a charter to Thomas of Guben (Gubin) for the founding of a town under Magdeburg law, between the castle and the river. Thomas brought a large number of German settlers to aid in the building and settlement of the city – this is an example of the German eastern migration "(Ostsiedlung)" characteristic of that period. The city (covering the area of today's Old Town neighbourhood) was surrounded by a defensive wall, integrated with the castle.
In reunited Poland, and later in the Polish–Lithuanian Commonwealth, Poznań was the seat of a voivodeship. The city's importance began to grow in the Jagiellonian period, due to its position on trading routes from Lithuania and Ruthenia to western Europe. It would become a major centre for the fur trade by the late 16th century. Suburban settlements developed around the city walls, on the river islands and on the right bank, with some (Ostrów Tumski, Śródka, Chwaliszewo, Ostrówek) obtaining their own town charters. However the city's development was hampered by regular major fires and floods. On 2 May 1536 a blaze destroyed 175 buildings, including the castle, the town hall, the monastery and the suburban settlement called St. Martin. In 1519 the Lubrański Academy had been established in Poznań as an institution of higher education (but without the right to award degrees, which was reserved to Kraków's Jagiellonian University). However a Jesuits' college, founded in the city in 1571 during the Counter-Reformation, had the right to award degrees from 1611 until 1773, when it was combined with the Academy.
In the second half of the 17th century and most of the 18th, Poznań was severely affected by a series of wars (and attendant military occupations, lootings and destruction) – the Second and Third Northern Wars, the War of the Polish Succession, the Seven Years' War and the Bar Confederation rebellion. It was also hit by frequent outbreaks of plague, and by floods, particularly that of 1736, which destroyed most of the suburban buildings. The population of the conurbation declined (from 20,000 around 1600 to 6,000 around 1730), and Bambergian and Dutch settlers ("Bambrzy" and "Olędrzy") were brought in to rebuild the devastated suburbs. In 1778 a "Committee of Good Order" "(Komisja Dobrego Porządku)" was established in the city, which oversaw rebuilding efforts and reorganized the city's administration. However in 1793, in the Second Partition of Poland, Poznań, came under the control of the Kingdom of Prussia, becoming part of (and initially the seat of) the province of South Prussia.
The Prussian authorities expanded the city boundaries, making the walled city and its closest suburbs into a single administrative unit. Left-bank suburbs were incorporated in 1797, and Ostrów Tumski, Chwaliszewo, Śródka, Ostrówek and Łacina (St. Roch) in 1800. The old city walls were taken down in the early 19th century, and major development took place to the west of the old city, with many of the main streets of today's city centre being laid out.
In the Greater Poland Uprising of 1806, Polish soldiers and civilian volunteers assisted the efforts of Napoleon by driving out Prussian forces from the region. The city became a part of the Duchy of Warsaw in 1807, and was the seat of Poznań Department - a unit of administrative division and local government. However in 1815, following the Congress of Vienna, the region was returned to Prussia, and Poznań became the capital of the semi-autonomous Grand Duchy of Posen.
The city continued to expand, and various projects were funded by Polish philanthropists, such as the Raczyński Library and the Bazar hotel. The city's first railway, running to Stargard in Pommern (now Stargard Szczeciński), opened in 1848. Due to its strategic location, the Prussian authorities intended to make Poznań into a fortress city, building a ring of defensive fortifications around it. Work began on the citadel (Fort Winiary) in 1828, and in subsequent years the entire set of defences "(Festung Posen)" was completed.
A Greater Poland Uprising during the Revolutions of 1848 was ultimately unsuccessful, and the Grand Duchy lost its remaining autonomy, Poznań becoming simply the capital of the Prussian Province of Posen. It would become part of the German Empire with the unification of German states in 1871. Polish patriots continued to form societies (such as the Central Economic Society for the Grand Duchy of Poznań), and a Polish theatre ("Teatr Polski", still functioning) opened in 1875; however the authorities made efforts to Germanize the region, particularly through the Prussian Settlement Commission (founded 1886). Germans accounted for 38% of the city's population in 1867, though this percentage would later decline somewhat, particularly after the region returned to Poland.
Another expansion of "Festung Posen" was planned, with an outer ring of more widely spaced forts around the perimeter of the city. Building of the first nine forts began in 1876, and nine intermediate forts were built from 1887. The inner ring of fortifications was now considered obsolete and came to be mostly taken down by the early 20th century (although the citadel remained in use). This made space for further civilian construction, particularly the Imperial Palace "("Zamek")", completed 1910, and other grand buildings around it (including today's central university buildings and the opera house). The city's boundaries were also significantly extended to take in former suburban villages: Piotrowo and Berdychowo in 1896, Łazarz, Górczyn, Jeżyce and Wilda in 1900, and Sołacz in 1907.
After World War I the Greater Poland Uprising (1918–1919) brought Poznań and most of the region under Polish control, confirmed by the Treaty of Versailles. The local populace had to acquire Polish citizenship or leave the country. This led to a significant decline of ethnic Germans, whose number decreased from 65,321 in 1910 to 5,980 in 1926 and further to 4,387 in 1934. In the interwar Second Polish Republic, the city again became the capital of Poznań Voivodeship. Poznań's university (today called Adam Mickiewicz University) was founded in 1919, and in 1925 the Poznań International Fairs began. In 1929 the fairs site was the venue for a major National Exhibition ("Powszechna Wystawa Krajowa", popularly "PeWuKa") marking the tenth anniversary of independence; it attracted around 4.5 million visitors. The city's boundaries were again expanded in 1925 (to include Główna, Komandoria, Rataje, Starołęka, Dębiec, Szeląg and Winogrady) and 1933 (Golęcin, Podolany).
During the German occupation of 1939–1945, Poznań was incorporated into the Third Reich as the capital of "Reichsgau Wartheland". Many Polish inhabitants were executed, arrested, expelled to the General Government or used as forced labour; at the same time many Germans and Volksdeutsche were settled in the city. The German population increased from around 5,000 in 1939 (some 2% of the inhabitants) to around 95,000 in 1944. The pre-war Jewish population of about 2,000 were mostly murdered in the Holocaust. A concentration camp was set up in Fort VII, one of the 19th-century perimeter forts. The camp was later moved to Żabikowo south of Poznań. The Nazi authorities significantly expanded Poznań's boundaries to include most of the present-day area of the city; these boundaries were retained after the war. Poznań fell to the Red Army, assisted by Polish volunteers, on 23 February 1945 following the Battle of Poznań, in which the German army conducted a last-ditch defence in line with Hitler's designation of the city as a "Festung". The Citadel was the last point to fall, and the fighting left much of the city, particularly the Old Town, in ruins.
Due to the expulsion and flight of German population Poznań's post-war population was almost uniformly Polish. The city again became a voivodeship capital; in 1950 the size of Poznań Voivodeship was reduced, and the city itself was given separate voivodeship status. This status was lost in the 1975 reforms, which also significantly reduced the size of Poznań Voivodeship.
The Poznań 1956 protests are seen as an early expression of resistance to communist rule. In June 1956, a protest by workers at the city's Cegielski locomotive factory developed into a series of strikes and popular protests against the policies of the government. After a protest march on June 28 was fired on, crowds attacked the communist party and secret police headquarters, where they were repulsed by gunfire. Riots continued for two days until being quelled by the army; 67 people were killed according to official figures. A monument to the victims was erected in 1981 at Plac Mickiewicza.
The post-war years had seen much reconstruction work on buildings damaged in the fighting. From the 1960s onwards intensive housing development took place, consisting mainly of pre-fabricated concrete blocks of flats, especially in Rataje and Winogrady, and later (following its incorporation into the city in 1974) Piątkowo. Another infrastructural change (completed in 1968) was the rerouting of the river Warta to follow two straight branches either side of Ostrów Tumski.
The most recent expansion of the city's boundaries took place in 1987, with the addition of new areas mainly to the north, including Morasko, Radojewo and Kiekrz. The first free local elections following the fall of communism took place in 1990. With the Polish local government reforms of 1999, Poznań again became the capital of a larger province (Greater Poland Voivodeship). It also became the seat of a "powiat" ("Poznań County"), with the city itself gaining separate powiat status.
Recent infrastructural developments include the opening of the fast tram route ("Poznański Szybki Tramwaj", popularly "Pestka") in 1997, and Poznań's first motorway connection (part of the A2 "autostrada") in 2003. In 2006 Poland's first F-16 Fighting Falcons came to be stationed at the 31st Air Base in Krzesiny in the south-east of the city.
Poznań continues to host regular trade fairs and international events, including the United Nations Climate Change Conference. It was one of the host cities for UEFA Euro 2012.
Geography.
A panoramic view of Poznań, taken from the city's north-eastern suburbs in Nowe Miasto
Poznań covers an area of 261.3 km2, and has coordinates in the range 52°17'34"–52°30'27"N, 16°44'08"–17°04'28"E. Its highest point, with an altitude of 157 m, is the summit of "Góra Moraska" (Morasko Hill) within the Morasko meteorite nature reserve in the north of the city. The lowest altitude is 60 m, in the Warta valley.
Poznań's main river is the Warta, which flows through the city from south to north. As it approaches the city centre it divides into two branches, flowing west and east of Ostrów Tumski (the cathedral island) and meeting again further north. The smaller Cybina river flows through eastern Poznań to meet the east branch of the Warta (that branch is also called Cybina – its northern section was originally a continuation of that river, while its southern section has been artificially widened to form a main stream of the Warta). Other tributaries of the Warta within Poznań are the Junikowo Stream "(Strumień Junikowski)", which flows through southern Poznań from the west, meeting the Warta just outside the city boundary in Luboń; the Bogdanka and Wierzbak, formerly two separate tributaries flowing from the north-west and along the north side of the city centre, now with their lower sections diverted underground; the Główna, flowing through the neighbourhood of the same name in north-east Poznań; and the Rose Stream "(Strumień Różany)" flowing east from Morasko in the north of the city. The course of the Warta in central Poznań was formerly quite different from today: the main stream ran between Grobla and Chwaliszewo, which were originally both islands. The branch west of Grobla (the "Zgniła Warta" – "rotten Warta") was filled in late in the 19th century, and the former main stream west of Chwaliszewo was diverted and filled in during the 1960s. This was done partly to prevent floods, which did serious damage to Poznań frequently throughout history.
Poznań's largest lake is "Jezioro Kierskie" (Kiekrz Lake) in the extreme north-west of the city (within the city boundaries since 1987). Other large lakes include Malta (an artificial lake on the lower Cybina, formed in 1952), "Jezioro Strzeszyńskie" (Strzeszyn Lake) on the Bogdanka, and Rusałka, an artificial lake further down the Bogdanka, formed in 1943. The latter two are popular bathing places. Kiekrz Lake is much used for sailing, while Malta is a competitive rowing and canoeing venue.
The city centre (including the Old Town, the former islands of Grobla and Chwaliszewo, the main street "Święty Marcin" and many other important buildings and districts) lies on the west side of the Warta. Opposite it between the two branches of the Warta is Ostrów Tumski, containing Poznań Cathedral and other ecclesiastical buildings, as well as housing and industrial facilities. Facing the cathedral on the east bank of the river is the historic district of Śródka. Large areas of apartment blocks, built from the 1960s onwards, include Rataje in the east, and Winogrady and Piątkowo north of the centre. Older residential and commercial districts include those of Wilda, Łazarz and Górczyn to the south, and Jeżyce to the west. There are also significant areas of forest within the city boundaries, particularly in the east adjoining Swarzędz, and around the lakes in the north-west.
For more details on Poznań's geography, see the articles on the five districts: Stare Miasto, Nowe Miasto, Jeżyce, Grunwald and Wilda.
Climate.
The climate of Poznań is within the transition zone between a humid continental and oceanic climate and with relatively cold winters and warm summers. Snow is common in winter, when night-time temperatures are typically below zero. In summer temperatures may often reach 30 °C. Annual rainfall is less than 500 mm, among the lowest in Poland. The rainiest month is July, mainly due to short but intense cloudbursts and thunderstorms. The number of hours of sunshine are among the highest in the country. Climate in this area has mild differences between highs and lows, and there is adequate rainfall year round. The Köppen Climate Classification subtype for this climate is "" (Marine West Coast Climate/Oceanic climate).
Administrative division.
Poznań is divided into 42 neighbourhoods (osiedles), each of which has its own elected council with certain decision-making and spending powers. The first uniform elections for these councils covering the whole area of the city were held on 20 March 2011.
For certain administrative purposes, the old division into five districts (dzielnicas) is used - although these ceased to be governmental units in 1990. These were:
Citizens of Poznań thanks to the strong economy of the city and high salaries started moving to suburbs (Powiat Poznanski) in the 90's. Although the number of inhabitants in Poznań itself was decreasing for the past two decades, the suburbs gained almost twice as much inhabitants. Thus, Poznań urban area has been growing steadily over past years and has already reached 1.0 million inhabitants when student population is included, whereas the entire metropolitan zone may have reached 1.5-1.7 million inhabitants when satellite cities and towns (so-called second Poznań ring counties such as Wrzesnia, Gniezno and Koscian) are included. The complex infrastructure, population density, number of companies and gross product per capita of Poznań suburbs may be only compared to Warsaw suburbs. It is interesting to note that many parts of closer suburbs (for example Tarnowo Podgorne, Komorniki, Suchy Las, Dopiewo) produce more in terms of GDP per capita than the city itself.
Economy.
Poznań has been an important trade centre since the Middle Ages. Starting in the 19th century, local heavy industry began to grow. Several major factories were built, including the Hipolit Cegielski steel mill and railway factory (see H. Cegielski - Poznań S.A.).
Nowadays Poznań is one of the major trade centers in Poland. Poznań is regarded as the second most prosperous city in Poland after Warsaw. The city of Poznań produced PLN 31.8 billion of Poland's gross domestic product in 2006. It boasts a GDP per capita of 200,4% (2008) of Poland's average. Furthermore, Poznań had very low unemployment rate of 2.3% as of May 2009. For comparison, Poland's national unemployment rate was over 10%.
Many Western European companies have established their Polish headquarters in Poznań or in the nearby towns of Tarnowo Podgórne and Swarzędz. Most foreign investors are German and Dutch companies (see List of corporations in Poznań), with a few others. Investors are mostly from the food processing, furniture, automotive and transport and logistics industries. Foreign companies are primarily attracted by low labour costs and by the relatively good road and railway network, good vocational skills of workers and relatively liberal employment laws.
The recently built Stary Browar shopping center contains many high-end shops and is considered one of the best in Europe. It has won an award for the best shopping center in the world in the medium-sized commercial buildings category. Other notable shopping centers in the city include Galeria Malta, one of the largest in Central Europe, and the shops at the Hotel Bazar, a historical hotel and commercial center in the Old Town.
Selected major corporations based in Poznań and the city's vicinity include QXL Poland Sp. z o.o. (Allegro), Poznań, GlaxoSmithKline Pharmaceuticals SA, Poznań, Grupa Raben, near Kórnik, Poznań metro, Kuehne & Nagel sp. z o.o., Gądki near Poznań, H. Cegielski-Poznań SA, Poznań, and Solaris Bus & Coach sp. z. o.o., Bolechowo, Poznań metro. The abbreviation "Sp. z o.o." stands for "Spółka z ograniczoną odpowiedzialnością", or Limited Liability Company, the equivalent of British Ltd. or German GmbH. The abbreviation for Public Limited Company (a stock company or PLC) is "Spółka Akcyjna" or S.A.
Transport.
Poznań has an extensive public transport system, consisting of trams (see Tramways in Poznań, especially Poznański Szybki Tramwaj) and urban and suburban buses. The main railway station is Poznań Central Station to the southwest of the city centre; there is also the smaller Poznań Wschód and Poznań Garbary station northeast of the centre and a number of other stations on the outskirts of the city. The main east-west A2 motorway runs south of the city connecting it with Berlin in the west and Łódż and Warsaw in the east; other main roads run in the direction of Warsaw, Bydgoszcz, Wągrowiec, Oborniki, Katowice, Wrocław, Buk and Berlin. An intensive programme of road building and improvement in and around the city is underway in preparation for the hosting of matches in the Euro 2012 football championships.
Poznań has one of the biggest airports in the west of Poland called Poznań-Ławica Airport. In 2010 it handled approximately 1.5 million passengers.
Culture and sights.
Poznań has many historic buildings and sights, mostly concentrated around the Old Town and other parts of the city centre. Many of these lie on the Royal-Imperial Route in Poznań – a tourist walk leading through the most important parts of the city showing its history, culture and identity. Portions of the city center are listed as one of Poland's official national Historic Monuments ("Pomnik historii"), as designated November 28, 2008, along with other portions of the city's historic core. Its listing is maintained by the National Heritage Board of Poland.
Results of new extensive archaeological research performed on Poznan's Ostrow Tumski by Prof. dr hab. Hanna Kocka-Krec from Instytut Prahistorii UAM indicate that Poznań indeed was a central site of the early Polish State (recent discovery of first Polish ruler, Mieszko I's Palatium). Thus, the Ostrow Tumski Island is more important that it was thought previously and may have been as important as "Gniezno" in the Poland of first "Piasts". Though it is currently under construction, "Ostrow Tumski of Poznan" should soon have a very rich historical exposition and be a very interesting place for visitors. It promises to include many attractions, such as the above-mentioned Cathedral, Church of St. Mary the Virgin, "Lubranski Academy" and the opened in 2012 "Genius Loci Archeological Park" as well as planned to be opened in 2013 Interactive Center of Ostrow Tumski History ("ICHOT") that presents a multimedia museum of the Polish State through many different periods. The "Palatium in Poznan" will be also transformed into a museum, although more funds are needed. When all the expositions are ready, in a couple of years, Ostrow Tumski may be as worth visiting as "Wawel" in Cracow. There is a very famous sentence illustrating the importance of Ostrow Tumski in Poznań by the Pope John Paul II: "Poland began here".
One of the most interesting places in Poznań is Malta with an artificial lake in its center. On one bank of the lake there are ski and sleigh slopes (Malta Ski), on the opposite bank a huge complex of swimming pools including an Olympic-size one (Termy Maltanskie). This whole recreational city "district" is unique in all of Poland or even Europe.
Perhaps the most important cultural event in Poznań is the annual Malta theatre festival, which takes place at many city venues, usually in late June and early July. It hosts mainly modern experimental off-theatre performances, often taking place on squares and other public spaces. It also includes cinema, visual, music and dancing events. Malta Theatre Festival gave birth to many off-theater groups, expressing new ideas in an already rich theatrical background of the city. Thus, Poznań with a great deal of off-theaters and their performances has recently become a new Polish off-theater performance center.
Classical music events include the Henryk Wieniawski Violin Competition (held every 5 years), and classical music concerts by the city's Philharmonic Orchestra held each month in the University "Aula". Especially popular are concerts by the Poznań Nightingales.
Poznan is also home to new forms of music such as rap and hip-hop made by a great deal of bands and performers ("Peja", "Mezo" and others). Poznań is also known for its rock music performers (Muchy, Malgorzata Ostrowska).
Poznan apart from many traditional theaters with a long history ("Teatr Nowy", "Teatr Wielki", "Teatr Polski", "Teatr Muzyczny" and several others) is also home to a growing number of alternative theater groups, some of them stemming from International Malta Festival: "Teatr Strefa Ciszy", "Teatr Porywcze Cial", "Teatr Usta Usta", "Teatr u Przyjaciol", "Teatr Biuro Podrozy", "Teatr Osmego Dnia" and many others – it is believed that even up to 30 more or less known groups may work in the city.
Every year on the 11th of November, Poznanians celebrate The Day of St. Marcin Street. A procession of horses, with St. Marcin at the head, parades along St Marcin Street, in front of The Imperial Castle. Everybody can eat delicious croissants, the regional product of Poznań.
Poznań hosted the 2009 European Young Adults Meeting of the ecumenical Christian Taizé Community.
Poznań also stages the "Ale Kino!" International Young Audience Film Festival in December and "Off Cinema" festival of independent films. Other festival: "Transatlantyk" (film music festival by Jan A.P. Kaczmarek started in 2011), Maski Theater Festival, Dance International Workshops by Polish Dance Theater, Made in Chicago (Jazz Festival), Ethno Port, Festival of Ice Sculpture, Animator, Science and Art Festival, Tzadik (Jewish music festival), Meditations Biennale (Modern Art). The full list of cultural annual events is even longer.
Poznań has several cinemas, including multiplexes and smaller cinemas, an opera house, several other theatres, and museums.
The "Rozbrat" squat serves as a home for squatters and as a centre of independent and open-minded culture. It hosts frequent gigs, an anarchistic library, vernissages, exhibitions, annual birthday festival (each October), poetry evenings and graffiti festivals. The city centre has many clubs, pubs and coffee houses, mainly in the area of the Old Town.
Education.
Poznań is one of the four largest academic centers in Poland. The number of students in the city of Poznań is about 140 000 (fourth/third after Warsaw, Cracow and close to Wrocław student population). Every one of four inhabitants in Poznań is a student. Since Poznań is smaller than Warsaw or Cracow still having a very large number of students it makes the city even more vibrant and dense "academic hub" than both former and current capitals of Poland. The city has many state-owned universities. Adam Mickiewicz University (abbreviated "UAM" in Polish, "AMU" in English) is one of the most influential and biggest universities in Poland:
"Adam Mickiewicz University" is one of the three best universities in Poland after University of Warsaw and Jagiellonian University of Cracow. They all have a very high number of international student and scientist exchange, research grants and top publications.
In northern suburbs of Poznań a very large "Morasko Campus" has been built (Faculty of Biology, Physics, Mathematics, Chemistry, Political Sciences, Geography). The majority of faculties are already open, although a few more facilities will be constructed. The campus infrastructure belongs to the most impressive among Polish universities. Also, there are plans for "Uniwersytecki Park Historii Ziemii" (Earth History Park), one of the reason for the park construction is a "Morasko meteorite nature reserve" situated close by, it is one of the rare sites of Europe where a number of meteorites fell and some traces may be still seen.
There is also a great number of smaller, mostly private-run colleges and institutions of higher education ("Uczelnie w Poznaniu"):
Poznan with its almost 30 colleges and universities has a second richest educational offer in Poland after Warsaw.
Sports.
Poznań is most famous for its football teams, Warta Poznań which are one the most successful clubs in pre-war history and Lech Poznań, who are currently one of the biggest clubs in the country, frequently playing in European cups and have many fans from all over the region. Lech plays at the Municipal Stadium, which hosted the 2012 European Championship group stages as well as the opening game and the final of the U-19 Euro Championship in June 2006.
The city's third professional football team Olimpia Poznań ceased activity in 2004, focusing on other sports, and remains one of the best judo and tennis clubs in the country, the latter hosting the Poznań Open tournament at the Tennis Park. The club is a large sports complex surrounded by Lake Rusałka, and apart from the tennis facilities boasts a large city recreation area: mountain biking facilities including a four-cross track; an athletics stadium (capacity 3000); and an football-speedway stadium (capacity 20 000), which fell into vast disrepair until it was acquired by the city council from the police in 2013 and was renovated. The football-speedway stadium hosts speedway club PSŻ Poznań, rugby union side NKR Chaos, American football team the Poznań Patriots, and football team Poznaniak Poznań.
The city has the largest circuit in Poland, Tor Poznań, located in the suburbs in Przeźmierowo. Lake Malta hosted the World Rowing Championships in 2009 and has previously hosted some regattas in the Rowing World Cup. It also hosted the ICF Canoe Sprint World Championships (sprint canoe) in 1990 and 2001, and again in 2010. Also near the lake the "Malta Ski" year-long skiing complex hosts minor sport competitions, and is also equipped with a toboggan run and a minigolf course. The is also a roller rink with a roller skating club nearby.
The city is also considered to be the hotbed of Polish field hockey, with several top teams: Warta Poznań; Grunwald Poznań; which also has shooting, wrestling, handball and tennis sections; Pocztowiec Poznań; and AZS AWF Poznań, the student club which also fields professional teams in women's volleyball and basketball (AZS Poznań).
Other clubs include: Posnania Poznań, one of the best rugby union clubs in the country; Polonia Poznań, formerly a multi-sports club with many successes in rugby, however today only a football section remains; KKS Wiara Lecha, football club formed by the supporters of Lech Poznań; and Odlew Poznań, arguably the most famous amateur club in the country due to their extensive media coverage and humorous exploits. There are also numerous rhythmic gymnastics and synchronized swimming clubs, as well as numerous less notable amateur football teams.
Poznań bidded for the 2014 Summer Youth Olympics but lost to Nanjing, with the Chinese city getting 47 votes over Poznań's 42.
Politics.
Municipal politics.
Since the end of the communist era in 1989, Poznań municipality and suburban area have invested heavily in infrastructure, especially public transport and administration. That results in a massive investment from foreign companies in Poznań as well as in communities west and south of Poznań (namely, Kórnik and Tarnowo Podgórne). One of the most important values of Poznań is the positive attitude of public administration towards investments, and less bureaucracy than elsewhere in Poland.
City investments into transportation were mostly into public transport. While the number of cars since 1989 has at least doubled, the policy of improving public transport gave good effects. Limiting car access to the city center, building new tram lines (including Poznański Szybki Tramwaj) and investing in new rolling stock (such as modern Combino trams by Siemens and Solaris low-floor buses) actually increased the level of ridership. This is a notable success, even considering the fact that Polish society only possesses about half of the "old EU"'s purchasing power, hence not everybody can afford to own a car.
Future investments into transportation include the construction of a third bypass of Poznań, and the completion of A2 (E30) motorway towards Berlin. New cycle lanes are being built, linking to existing ones, and an attempt is currently being made to develop a Karlsruhe-style light rail system for commuters. All this is made more complicated (and more expensive) by the heavy neglect of transport infrastructure throughout the Communist era.
Constituency.
Members of Sejm elected in 2005 from Poznań constituency:
Members of European Parliament elected from Poznań constituency:
International relations.
Twin towns – Sister cities.
Poznań is twinned with:

</doc>
<doc id="23725" url="http://en.wikipedia.org/wiki?curid=23725" title="Peter Falk">
Peter Falk

Peter Michael Falk (September 16, 1927 – June 23, 2011) was an American actor, best known for his role as Lt. Columbo in the television series "Columbo." He appeared in numerous films such as "The Princess Bride", "The Great Race", "It's a Mad, Mad, Mad, Mad World", "A Woman Under the Influence" and "Murder by Death", as well as many television guest roles. He was nominated for an Academy Award twice (for 1960's "Murder, Inc." and 1961's "Pocketful of Miracles"), and won the Emmy Award on five occasions (four for "Columbo") and the Golden Globe Award once. Director William Friedkin said of Falk's role in his film "The Brink's Job" (1978): "Peter has a great range from comedy to drama. He could break your heart or he could make you laugh."
In 1968, Falk starred with Gene Barry in a ninety-minute television pilot about a highly skilled, laid-back detective. "Columbo" eventually became part of an anthology series titled "The NBC Mystery Movie," along with "McCloud", "McMillan & Wife and Banacek." The detective series stayed on NBC from 1971 to 1978, took a respite, and returned occasionally on ABC from 1989 to 2003. Falk was "everyone's favorite rumpled television detective," wrote historian David Fantle.
In 1996, TV Guide ranked Falk number 21 on its 50 Greatest TV Stars of All Time list.
Early life.
Born in New York City, Falk was the son of Michael Peter Falk, owner of a clothing and dry goods store, and his wife, Madeline (née Hochhauser), an accountant and buyer. Both of his parents were Jewish coming from Poland and Russia on his father's side, and from Hungary and Czech lands on his mother's side.
Falk's right eye was surgically removed when he was three because of a retinoblastoma; he wore an artificial eye for most of his life. The artificial eye was the cause of his trademark squint. Despite this limitation, as a boy he participated in team sports, mainly baseball and basketball. In a 1997 interview in "Cigar Aficionado" magazine with Arthur Marx, Falk said: "I remember once in high school the umpire called me out at third base when I was sure I was safe. I got so mad I took out my glass eye, handed it to him and said, 'Try this.' I got such a laugh you wouldn't believe."
Falk's first stage appearance was at the age of 12 in "The Pirates of Penzance" at Camp High Point in upstate New York, where one of his camp counselors was Ross Martin (they would later act together in "The Great Race" and the "Columbo" episode "Suitable For Framing"). Falk attended Ossining High School in Westchester County, New York, where he was a star athlete and president of his senior class. After graduating from high school in 1945, Falk briefly attended Hamilton College in Clinton, New York, and then tried to join the armed services as World War II was drawing to a close. Rejected because of his missing eye, he joined the United States Merchant Marine, and served as a cook and mess boy. Falk said of the experience in 1997: "There they don't care if you're blind or not. The only one on a ship who has to see is the captain. And in the case of the "Titanic", he couldn't see very well, either." Falk recalls this period in his autobiography: "A year on the water was enough for me, so I returned to college. I didn't stay long. Too itchy. What to do next? I signed up to go to Israel to fight in the war on its attack on Egypt; I wasn't passionate about Israel, I wasn't passionate about Egypt, I just wanted more excitement… I got assigned a ship and departure date but the war was over before the ship ever sailed."
After a year and a half in the Merchant Marine, Falk returned to Hamilton College and also attended the University of Wisconsin. He transferred to the New School for Social Research in New York City, which awarded him a bachelor's degree in literature and political science in 1951. He then traveled in Europe and worked on a railroad in Yugoslavia for six months. He returned to New York, enrolling at Syracuse University, but he recalled in his 2006 memoir, "Just One More Thing", that he was unsure what he wanted to do with his life for years after leaving high school.
Falk obtained a Master of Public Administration degree at the Maxwell School of Syracuse University in 1953. The program was designed to train civil servants for the federal government, a career that Falk said in his memoir he had "no interest in and no aptitude for". He applied for a job with the CIA, but was rejected because of his membership in the Marine Cooks and Stewards Union while serving in the Merchant Marine, even though he was required to join and was not active in the union (which had been under fire for communist leanings). He then became a management analyst with the Connecticut State Budget Bureau in Hartford. In 1997, Falk characterized his Hartford job as "efficiency expert": "I was such an efficiency expert that the first morning on the job, I couldn't find the building where I was to report for work. Naturally, I was late, which I always was in those days, but ironically it was my tendency never to be on time that got me started as a professional actor."
Career.
Stage career.
While working in Hartford, Falk joined a community theater group called the Mark Twain Masquers, where he performed in plays that included "The Caine Mutiny Court-Martial", "The Crucible", and "The Country Girl" by Clifford Odets. Falk also studied with Eva Le Gallienne, who was giving an acting class at the White Barn Theatre in Westport, Connecticut. Falk later recalled how he "lied his way" into the class, which was for professional actors. He drove down to Westport from Hartford every Wednesday, when the classes were held, and was usually late. In his 1997 interview with Arthur Marx in "Cigar Aficionado" Magazine, Falk said of Le Gallienne: "One evening when I arrived late, she looked at me and asked, 'Young man, why are you always late?' and I said, 'I have to drive down from Hartford.'" She looked down her nose and said, "What do you do in Hartford? There's no theater there. How do you make a living acting?" Falk confessed he wasn't a professional actor. According to him Le Gallienne looked at him sternly and said: "Well, you should be." He drove back to Hartford and quit his job. Falk stayed with the Le Gallienne group for a few months more, and obtained a letter of recommendation from Le Galliene to an agent at the William Morris Agency in New York. In 1956, he left his job with the Budget Bureau and moved to Greenwich Village to pursue an acting career.
Falk's first New York stage role was in an Off-Broadway production of Molière's "Dom Juan" at the Fourth Street Theatre that closed after its only performance on January 3, 1956. Falk played the second lead, Sganarelle. His next theater role proved far better for his career. In May, he appeared at Circle in the Square in a revival of "The Iceman Cometh" with Jason Robards playing the bartender.
Later in 1956, Falk made his Broadway debut, appearing in Alexander Ostrovsky's "Diary of a Scoundrel". As the year came to an end, he appeared again on Broadway as an English soldier in Shaw's "Saint Joan" with Siobhán McKenna.
In 1972, Falk appeared in Broadway's "The Prisoner of Second Avenue". According to film historian Ephraim Katz: "His characters derive added authenticity from his squinty gaze, the result of the loss of an eye ..."
Early films.
Despite his stage success, a theatrical agent advised Falk not to expect much film acting work because of his artificial eye. He failed a screen test at Columbia Pictures and was told by studio boss Harry Cohn: "For the same price I can get an actor with two eyes." He also failed to get a role in the film "Marjorie Morningstar," despite a promising interview for the second lead. His first film performances were in small roles in "Wind Across the Everglades" (1958), "The Bloody Brood" (1959) and "Pretty Boy Floyd" (1960). Falk's performance in "Murder, Inc." (1960) was a turning point in his career. He was cast in the supporting role of killer Abe Reles in a film based on the real-life murder gang of that name that terrorized New York in the 1930s. "The New York Times" film critic Bosley Crowther while dismissing the movie as "an average gangster film" singled out Falk's "amusingly vicious performance." Crowther wrote: Mr. Falk, moving as if weary, looking at people out of the corners of his eyes and talking as if he had borrowed Marlon Brando's chewing gum, seems a travesty of a killer, until the water suddenly freezes in his eyes and he whips an icepick from his pocket and starts punching holes in someone's ribs. Then viciousness pours out of him and you get a sense of a felon who is hopelessly cracked and corrupt.
The film turned out to be Falk's breakout role. In his autobiography, "Just One More Thing" (2006), Falk said his selection for the film from thousands of other Off-Broadway actors was a "miracle" that "made my career" and that without it, he would not have gotten the other significant movie roles that he later played. Falk, who played Reles again in the 1960 TV series "The Witness", was nominated for a Best Supporting Actor Academy Award for his performance in the film.
In 1961, multiple Academy Award-winning director Frank Capra cast Falk in the comedy "Pocketful of Miracles". The film was Capra's last feature, and although it was not the commercial success he hoped it would be, he "gushed about Falk's performance". Falk was nominated for an Oscar for the role. In his autobiography, Capra wrote about Falk:
The entire production was agony ... except for Peter Falk. He was my joy, my anchor to reality. Introducing that remarkable talent to the techniques of comedy made me forget pains, tired blood, and maniacal hankerings to murder Glenn Ford (the film's star). Thank you Peter Falk.":480
For his part, Falk says he "never worked with a director who showed greater enjoyment of actors and the acting craft. There is nothing more important to an actor than to know that the one person who represents the audience to you, the director, is responding well to what you are trying to do." Falk recalled one time how Capra reshot a scene even though he yelled "Cut and Print," indicating the scene was finalized. When Falk asked him why he wanted it reshot: "He laughed and said that he loved the scene so much he just wanted to see us do it again. How's that for support!"
For the remainder of the 1960s, Falk had mainly small movie roles and TV guest-starring appearances. Falk turned in a gem of a performance as one of two cabbies who falls victim to greed in the epic 1963 star-studded comedy "It's a Mad, Mad, Mad, Mad World", although he only appears in the last fifth of the movie. His other roles included a comical crook in the Rat Pack musical comedy "Robin and the 7 Hoods" (1964), in which he sings one of the film's numbers, and the spoof "The Great Race" (1965) with Jack Lemmon and Tony Curtis.
Early television roles.
Falk first appeared on television in 1957, in the dramatic anthology programs that later became known as the "Golden Age of Television." In 1957, he appeared in one episode of "Robert Montgomery Presents." He was also cast in "Studio One," "Kraft Television Theater," "New York Confidential," "Naked City," "Have Gun–Will Travel," "The Islanders," and "Decoy" with Beverly Garland cast as the first female police officer in a series lead. On "The Twilight Zone" he portrayed a Castro-type revolutionary complete with beard who, intoxicated with power, kept seeing his would-be assassins in a newly acquired magic mirror. He starred in two of Alfred Hitchcock's television series, as a gangster terrified of death in a 1961 episode of "Alfred Hitchcock Presents" and as a homicidal evangelist in 1962's "The Alfred Hitchcock Hour".
In 1961, Falk was nominated for an "Emmy Award" for his performance in the episode "Cold Turkey" of James Whitmore's short-lived series "The Law and Mr. Jones" on ABC. On September 29, 1961, Falk and Walter Matthau guest-starred in the premiere episode, "The Million Dollar Dump," of ABC's crime drama "," with Stephen McNally and Robert Harland. He won an Emmy for "The Price of Tomatoes," a drama carried in 1962 on "The Dick Powell Show".
In 1963, Falk and Tommy Sands appeared as brothers who disagreed on the route for a railroad in "The Gus Morgan Story" on ABC's "Wagon Train." Falk played the title role of "Gus", and Sands was his younger brother, Ethan Morgan. Ethan accidentally shoots wagonmaster Chris Hale, played by John McIntire, while the brothers are in the mountains looking at possible route options. Gus makes the decision to leave Hale behind even choking him, believing he is dead. Ethan has been overcome with oxygen deprivation and needs Gus' assistance to reach safety down the mountain. Unknown to the Morgans, Hale crawls down the mountain through snow, determined to obtain revenge against Gus. In time, though, Hale comes to understand the difficult choice Morgan had to make, and the brothers reconcile their own differences. This episode is remembered for its examination of how far a man will persist amid adversity to preserve his own life and that of his brother.
Falk's first television series was in the title role of the drama "The Trials of O'Brien," in which he played a lawyer. The show ran in 1965 and 1966 and was cancelled after 22 episodes.
In 1971, Pierre Cossette produced the first Grammy Awards show on television with some help from Falk. Cossette writes in his autobiography, "What meant the most to me, though, is the fact that Peter Falk saved my ass. I love show business, and I love Peter Falk."
"Columbo".
Although Falk appeared in numerous other television roles in the 1960s and 1970s, he is best known as the star of the TV series "Columbo," "everyone's favorite rumpled television detective." His character was a shabby and ostensibly absent-minded police detective lieutenant, who had first appeared in the 1968 film "Prescription: Murder." Rather than a whodunit, the show typically revealed the murderer from the beginning, then showed how the Los Angeles police detective Columbo (first name never disclosed) went about solving the crime. Falk would describe his role to Fantle:
Columbo has a genuine mistiness about him. It seems to hang in the air… [and] he's capable of being distracted… Columbo is an ass-backwards Sherlock Holmes. Holmes had a long neck, Columbo has no neck; Holmes smoked a pipe, Columbo chews up six cigars a day.
Television critic Ben Falk added that Falk "Created an iconic cop… who always got his man (or woman) after a tortuous cat-and-mouse investigation." He also noted the idea for the character was, "Apparently inspired by Dostoyevsky's dogged police inspector, Porfiry Petrovich, in the novel "Crime and Punishment".
Falk tries to analyze the character and notes the correlation between his own personality and Columbo's:
I'm a Virgo Jew, and that means I have an obsessive thoroughness. It's not enough to get most of the details, it's necessary to get them all. I've been accused of perfectionism. When Lew Wasserman (head of Universal Studios) said that Falk is a perfectionist, I don't know whether it was out of affection or because he felt I was a monumental pain in the ass.
With "general amazement", Falk notes: "The show is all over the world. I've been to little villages in Africa with maybe one TV set, and little kids will run up to me shouting, 'Columbo, Columbo!'" Singer Johnny Cash recalled acting in one episode, and although he was not an experienced actor, he writes in his autobiography: "Peter Falk was good to me. I wasn't at all confident about handling a dramatic role, and every day he helped me in all kinds of little ways."
The first episode of "Columbo" as a series was directed in 1971 by a 25-year-old Steven Spielberg in one of his earliest directing jobs. Falk recalled the episode to Spielberg biographer Joseph McBride:
Let's face it, we had some good fortune at the beginning. Our debut episode, in 1971, was directed by this young kid named Steven Spielberg. I told the producers, Link and Levinson: "This guy is too good for "Columbo""… Steven was shooting me with a long lens from across the street. That wasn't common twenty years ago. The comfort level it gave me as an actor, besides its great look artistically — well, it told you that this wasn't any ordinary director."
The character of Columbo had previously been played by Bert Freed in a single television episode and by Thomas Mitchell on Broadway. Falk first played Columbo in "Prescription: Murder", a 1968 TV movie, and a 1971 sequel, "Ransom for a Dead Man". From 1971 to 1978. "Columbo" aired regularly on NBC as part of the umbrella series "NBC Mystery Movie". All episodes were of TV movie length, in a 90 or 120 minutes slot including commercials. In 1989, the show returned on ABC in the form of a less frequent series of TV movies, still starring Falk, airing until 2003. Falk won four Emmys for his role as Columbo.
"Columbo" was so popular, co-creator William Link wrote a series of short stories published as "The Columbo Collection" (Crippen & Landru, 2010) which includes a drawing by Falk of himself as Columbo, and the cover features a caricature of Falk/Columbo by Al Hirschfeld.
Later career.
Falk was a close friend of independent film director John Cassavetes and appeared in his films "Husbands," "A Woman Under the Influence", and, in a cameo, at the end of "Opening Night". He also co-starred with Cassavetes in "Mikey and Nicky". Cassavetes, in turn, guest-starred in the "Columbo" episode "Étude in Black" in 1972. Falk describes his experiences working with Cassavetes specifically remembering his directing strategies: "Shooting an actor when he might be unaware the camera was running." You never knew when the camera might be going. And it was never: 'Stop. Cut. Start again.' John would walk in the middle of a scene and talk, and though you didn't realize it, the camera kept going. So I never knew what the hell he was doing. [Laughs] But he ultimately made me, and I think every actor, less self-conscious, less aware of the camera than anybody I've ever worked with."
In 1978, Falk appeared on the comedy TV show "The Dean Martin Celebrity Roast" with Frank Sinatra the evening's victim.
Falk continued to work in films, including his performance as a questionable ex-CIA agent of dubious sanity in the comedy "The In-Laws". Director Arthur Hiller said during an interview that the "film started out because Alan Arkin and Peter Falk wanted to work together. They went to Warner Brother's and said, 'We'd like to do a picture,' and Warner said fine ... and out came "The In-laws" ... of all the films I've done, "The In-laws" is the one I get the most comments on.":290 Movie critic Roger Ebert compared the film with a later remake:
Peter Falk and Alan Arkin in the earlier film, versus Michael Douglas and Albert Brooks this time ... yet the chemistry is better in the earlier film. Falk goes into his deadpan lecturer mode, slowly and patiently explaining things that sound like utter nonsense. Arkin develops good reasons for suspecting he is in the hands of a madman."
Falk appeared in "The Great Muppet Caper," "The Princess Bride," "Murder By Death," "The Cheap Detective," "Vibes," "Made," and (as himself) in Wim Wenders' 1987 film "Wings of Desire" and its 1993 sequel, "Faraway, So Close!." In 1998, Falk returned to the New York stage to star in an Off-Broadway production of Arthur Miller's "Mr. Peters' Connections." His previous stage work included shady real estate salesman Shelley "the Machine" Levine in the 1986 Boston/Los Angeles production of David Mamet's prizewinning "Glengarry Glen Ross".
Falk starred in a trilogy of holiday television movies – "A Town Without Christmas" (2001), "Finding John Christmas" (2003), and "When Angels Come to Town" (2004) – in which he portrayed Max, a quirky guardian angel who uses disguises and subterfuge to steer his charges onto the right path. In 2005, he starred in "The Thing About My Folks." Although movie critic Roger Ebert was not impressed with most of the other actors, he wrote in his review: "... We discover once again what a warm and engaging actor Peter Falk is. I can't recommend the movie, but I can be grateful that I saw it, for Falk." In 2007, Falk appeared with Nicolas Cage in the thriller "Next".
Personal life.
Falk married Alyce Mayo whom he met when the two were both students at Syracuse University, on April 17, 1960. The couple adopted two daughters, Catherine (who was to become a private investigator) and Jackie. They divorced in 1976. On December 7, 1977, Falk married actress Shera Danese, who guest-starred on the "Columbo" series on numerous occasions.
Falk was an accomplished artist, and in October 2006 he had an exhibition of his artwork at the Butler Institute of American Art. He took classes at the Art Students League of New York for many years. Examples of his sketches can be seen on his official web site.
Falk was a chess aficionado and a spectator at the American Open in Santa Monica, California, in November 1972, and at the U.S. Open in Pasadena, California, in August 1983.
Falk appeared in the video for Ray Parker Jr.'s "Ghostbusters" in 1984.
Of death, Falk once said: "It is just the gateway."
Falk's memoir "Just One More Thing" (ISBN 978-0-78671795-8) was published by Carroll & Graf on August 23, 2006.
Health.
Rumors of Falk's dementia plagued the actor in the final years of his life and were exacerbated when in late April 2008 he was photographed by paparazzi looking disheveled and acting animated in the streets of Beverly Hills. Although the actor said his behavior resulted from his frustration over being unable to remember where he had parked his car, the images of his erratic appearance and behavior were published by the media; Falk was seldom seen in public after the incident.
In December 2008 it was reported that Falk had been diagnosed with Alzheimer's disease. In June 2009, at a two-day conservatorship trial in Los Angeles, one of Falk's personal physicians, Dr. Stephen Read, reported he had rapidly slipped into dementia after a series of dental operations in 2007. Dr. Read said it was unclear whether Falk's condition had worsened as a result of anesthesia or some other reaction to the operations. Falk's decline was not immediate. He appeared fine signing autographs and intermingling with the general public in his last official public appearance at the 2008 Winter Hollywood Collector's Show in February 2008. Shera Danese Falk was appointed as her husband's conservator in 2009, after his decline.
Death.
Falk died at his longtime Roxbury Drive Beverly Hills home on the evening of June 23, 2011 at the age of 83. His death was triggered by cardiorespiratory arrest, with pneumonia and Alzheimer's disease being the underlying causes. Falk was survived by his wife and two daughters. His daughters said they would remember his "wisdom and humor". Falk is buried at Westwood Village Memorial Park Cemetery in Los Angeles, California.
Falk's death was marked by tributes from many film celebrities. Steven Spielberg said, "I learned more about acting from him at that early stage of my career than I had from anyone else." Rob Reiner said: "He was a completely unique actor," and went on to say that Falk's work with Alan Arkin in "The In-Laws" was "one of the most brilliant comedy pairings we've seen on screen."

</doc>
<doc id="23726" url="http://en.wikipedia.org/wiki?curid=23726" title="Pixies">
Pixies

The Pixies are an American rock band formed in Boston, Massachusetts in 1986. The group currently consists of founders Black Francis (lead vocals, rhythm guitar), Joey Santiago (lead guitar), and David Lovering (drums). Co-founder Kim Deal (bass, backing vocals) left in 2013 and was replaced by Kim Shattuck as live bass player for a few months, then by Paz Lenchantin for the band's 2014 tour. The Pixies achieved relatively modest popularity in their home country, but were significantly more successful in the United Kingdom, mainland Europe and Israel. The group disbanded in 1993 in acrimonious circumstances, but reunited in 2004. Despite limited commercial success, their jarring pop sound subsequently influenced bands such as Nirvana, Radiohead, The Strokes, Bush, Blur and Weezer.
The band's style of music contains a range of elements, including psychedelia, noise pop, hard rock, surf pop, and surf rock. Black Francis is the Pixies' primary songwriter and singer. He has written about a number of offbeat subjects in the band's songs, such as extraterrestrials, surrealism, incest, and biblical violence.
The group is credited with having an influence on the alternative rock boom of the 1990s. The Pixies' legacy and popularity grew in the years following their break-up, leading to sold-out world tours following their reunion in 2004. In June 2013, the band released their first new material in almost 10 years.
History.
Formation (1986).
Joey Santiago and Black Francis (born Charles Thompson IV) first met when they lived next to each other in a suite while attending the University of Massachusetts Amherst. Although Santiago was worried about distractions, he noticed Francis played music and the pair began to jam together. Francis then embarked on a student exchange trip to Puerto Rico to study Spanish. After six months, he returned to Amherst and dropped out of the university. Francis and Santiago spent 1984 working in a warehouse, with Francis composing songs on his acoustic guitar and writing lyrics on the subway train.
Black Francis didn’t discover punk until he was 16, saying “it was good I didn’t listen to these hip records.” He started out on a diet of 60’s songs, religious music and Emerson Lake and Palmer, then Iggy Pop, Husker Du, Captain Beefheart and Talking Heads, who he says “weren’t punk either”.
The pair formed a band in January 1986. Bass player Kim Deal joined Santiago and Francis two weeks later after responding to a classified advertisement Francis had placed, seeking a female bass player who liked both folk music icons Peter, Paul and Mary and the band Hüsker Dü. Deal was the only person to respond, but arrived at the audition without a bass as she had never played the instrument before. She was invited to join the band just because she liked the songs Black Francis was showing her. She later obtained a bass, and the trio started rehearsing in Deal's apartment.
After recruiting Deal, the band tried unsuccessfully to get her sister, Kelley Deal, to join as its drummer. Kim's husband suggested they hire drummer David Lovering, whom Kim had met at her wedding reception. The group arrived at a name after Santiago selected the word "pixies" randomly from a dictionary and took a liking to how it looked and its definition as "mischievous little elves". Once the band had settled on a name and stable line-up, they moved rehearsals to Lovering's parents' garage in mid-1986. They began to play shows at bars in and around the Boston area.
Record contract and "Come On Pilgrim" (1987).
While the Pixies were playing a concert with Throwing Muses, they were noticed by producer Gary Smith, manager of Fort Apache Studios. He told the band he "could not sleep until you guys are world famous". The band produced a 17-track demo at Fort Apache soon afterwards, known to fans as "The Purple Tape" because of the tape cover's purple background. Funded by Francis' father at the cost of $1000, the recording session was completed in three days. Local promoter Ken Goes became the band's manager, and he passed the demo to Ivo Watts-Russell of the independent record label 4AD. Watts-Russell nearly passed on the band, finding them too normal, "too rock 'n' roll", but signed them at the persuasion of his girlfriend.
Upon signing with 4AD, eight tracks from the Purple Tape were selected for the "Come On Pilgrim" mini-LP, the band's first release. On it, Francis drew upon his experiences in Puerto Rico, mostly in the songs "Vamos" and "Isla de Encanta"; the album included lyrics describing the poverty in Puerto Rico. The religious lyrics in "Come On Pilgrim" and later albums came from his parents' born-again Christian days in the Pentecostal Church. Critic Heather Phares sees themes such as sexual frustration ("I've Been Tired") and incest ("Nimrod's Son" and "The Holiday Song") on the record.
"Surfer Rosa" and "Doolittle" (1988–89).
"Come On Pilgrim" was followed by the band's first full-length album, "Surfer Rosa". The album was recorded by Steve Albini (who was hired by Watts-Russell on the advice of a 4AD colleague), completed in two weeks, and released in early 1988. "Surfer Rosa" gained the Pixies acclaim in Europe; both "Melody Maker" and "Sounds" gave "Surfer Rosa" their "Album of the Year" award. American critical response was also positive yet more muted, a reaction that persisted for much of the band's career. The album was eventually certified Gold in the U.S. in 2005. After the album was released, the band arrived in England to support Throwing Muses on the European "Sex and Death" tour—beginning at the Mean Fiddler in London. The tour also took them to the Netherlands, where the Pixies had already received enough media attention to be headlining the tour. Francis later recalled: "The first place I made it with the Pixies was in Holland." The tour became notable for the band's in-jokes, such as playing their entire set list in alphabetical order.
Meanwhile, the Pixies signed an American distribution deal with major record label Elektra. Around this time, the Pixies struck up a relationship with the British producer Gil Norton. Norton produced their second full album, "Doolittle", which was recorded in the last six weeks of 1988 and seen as a departure from the raw sound of "Come On Pilgrim" and "Surfer Rosa". "Doolittle" had a much cleaner sound, largely due to Norton and the production budget of US$40,000, which was quadruple that of "Surfer Rosa". "Doolittle" featured the single "Here Comes Your Man", which biographers Josh Frank and Caryn Ganz describe as an unusually jaunty and pop-like song for the band. "Monkey Gone to Heaven" was a Top 10 modern rock radio hit in the U.S., and reached the Top 100 in the U.K. Like "Surfer Rosa", "Doolittle" was acclaimed by fans and music critics alike.
Hiatus (1989–90).
After "Doolittle" tensions between Deal and Francis came to a head (for example, Francis threw a guitar at Deal during a concert in Stuttgart), and Deal was almost fired from the band when she refused to play at a concert in Frankfurt. Santiago, in an interview with "Mojo", described Deal as being "headstrong and want[ing] to include her own songs, to explore her own world" on the band's albums; eventually she accepted that Francis was the singer and had musical control of the band, but after the Frankfurt incident, "they kinda stopped talking". The band became increasingly tired during the post-"Doolittle" "Fuck or Fight" tour of the United States and fighting among members continued. After the tour's final date in New York, the band was too exhausted to attend the end-of-tour party the following night and soon announced a hiatus.
During this time, Santiago and Lovering went on vacation, while Francis performed a short solo tour, made up of a number of concerts to generate gas money as he traveled across the country. Deal formed a new band, The Breeders, with Tanya Donelly of Throwing Muses and bass player Josephine Wiggs of Perfect Disaster. Their debut album, "Pod", was released in 1990.
"Bossanova", "Trompe le Monde", and break-up (1990–2002).
In 1990, all members of the group except for Deal moved to Los Angeles. Lovering stated that he, Santiago, and Francis moved there "because the recording studio was there". Unlike previous recordings, the band had little time to practice beforehand, and Black Francis wrote much of the album in the studio. Featuring the singles "Velouria" and "Dig for Fire", "Bossanova" reached number 70 in the United States. In contrast, the album peaked at number three in the United Kingdom. Also in 1990, the Pixies released a cover of the Paul Butterfield Blues Band's "Born in Chicago" on the compilation album "".
The band continued to tour and released "Trompe le Monde" in 1991, their final album before their break-up. The album included "U-Mass", which has been described as being about college apathy, and whose guitar riff was written years before at the University of Massachusetts before Francis and Santiago dropped out. The album also featured a cover of "Head On" by The Jesus and Mary Chain. Also that year, the band contributed a cover of "I Can't Forget" to the Leonard Cohen tribute album "I'm Your Fan", and began an international tour on which they played stadiums in Europe and smaller venues in the United States. They then embarked on an uncomfortable tour supporting U2 on their Zoo TV Tour in 1992. Tensions rose between band members, and, at the end of the year, the Pixies went on sabbatical and focused on separate projects.
In early 1993, Francis announced in an interview to BBC Radio 5 that the band was finished and offered no explanation at the time, unbeknownst to the other members of the band. He later called Santiago and subsequently notified Deal and Lovering via fax, in January 1993. After the break-up, the four members embarked on separate projects. Black Francis renamed himself Frank Black, and he released several solo albums, including a string of releases with Frank Black and the Catholics. Deal returned to The Breeders, who scored a hit with "Cannonball" from that group's platinum-selling "Last Splash" in 1993, and released two more albums several years later. She also formed and released one album with The Amps. Santiago played lead guitar on multiple Frank Black albums, as well as on other artists' albums. He wrote theme music for the show "Undeclared" on Fox television and for "Crime and Punishment in Suburbia". He formed a band called The Martinis with his wife, Linda Mallari, who released an album in 2004. In 2004, he also played lead guitar on the album "Statecraft" by the novelist and musician Charles Douglas. Lovering went on to become a magician and made occasional appearances as "The Scientific Phenomenalist", performing experiments on stage and occasionally opening for Frank Black and The Breeders. Lovering continued to do some drumming, playing with the band Cracker, as well as on one of Tanya Donelly's solo albums, and on the Martinis' song "Free", which appeared on the "Empire Records" soundtrack.
4AD and Elektra Records continued to release Pixies material in the band's absence. They released the best-of album "Death to the Pixies" (1997), the Peel-session compilation "Pixies at the BBC" (1998), and the "Complete 'B' Sides" compilation (2001). Meanwhile, in 2002 material from the band's original 17-track demo tape was released as an EP, "Pixies", on Cooking Vinyl in the U.K. and SpinArt Records in the U.S.; Frank Black has also used these labels to release solo work and albums with The Catholics.
Reunion (2003–12).
In the 11 years following the break-up, rumors sometimes circulated regarding a reunion. Though Frank Black steadfastly dismissed them, he did begin to incorporate an increasing number of Pixies songs in his sets with The Catholics, and occasionally included Santiago in his solo work and Lovering's magic show as an opening act to concerts.
In 2003, a series of phone calls between band members resulted in some low-key rehearsals, and soon to a decision to reunite. By February 2004, a full tour was announced, and tickets for nearly all the initial tour dates sold out within minutes. The band's four-night run at London's Brixton Academy was the fastest selling in the venue's twenty-year history.
The Pixies played their first reunion concert on April 13, 2004, at The Fine Line Music Cafe in Minneapolis, Minnesota, and a warm-up tour through the U.S. and Canada was followed by an appearance at the Coachella Valley Music and Arts Festival. The band then spent much of 2004 touring throughout Brazil, Europe, Japan, and the U.S. The group won the Act-of-the-Year award in the 2004 "Boston Music Awards". The 2004 reunion tour reportedly grossed over $14 million in ticket sales.
In June 2004, the band released a new song, "Bam Thwok" exclusively on the iTunes Music Store. The song reached number one in the UK Official Download Chart. 4AD released ', along with a companion DVD, entitled "Pixies". The band also contributed a rendition of "Ain't That Pretty At All" to the Warren Zevon tribute album '. "Bam Thwok" and "Ain't That Pretty At All" were both recorded by engineer Ben Mumphrey, the former at Stagg Street Studios in Van Nuys, CA and the latter at Metamorphosis Studio in Vienna, Austria.
In 2005, the band made appearances at festivals including Lollapalooza, "T on the Fringe", and the Newport Folk Festival. They continued to make appearances through 2006 and 2007, culminating in their first-ever appearances in Australia. Since 2005, Francis has at various times stated that the Pixies recording a new studio album was either a possibility, or an unlikelihood, the main obstacle being Deal's reluctance to do so.
To celebrate the 20th anniversary of the release of "Doolittle", the Pixies launched a tour in October 2009 where they performed the album track-for-track, including the associated B-sides. The tour began in Europe, continued in the United States in November, with the Southamerican and Australian tour following in March 2010, then New Zealand, and more European dates in spring 2010, and back to North America in fall 2010 and into spring 2011. In the autumn of 2011 the "Imaginary Cities" tour continued the "Doolittle Tour" as they played many venues for the first time.
Kim Deal's departure, "Bagboy" and new material (2013–present).
On June 14, 2013, it was announced by the band's Twitter profile that Kim Deal had left the band. Deal has since posted new solo music on her website and the remaining Pixies have welcomed her to come back as her schedule with Breeders allows. Two weeks later, the band released a new song, "Bagboy", as a free download via the Pixies website. The song features Jeremy Dubs of Bunnies and formerly of the Bennies on vocals in place of Deal.
On July 1, 2013, the Pixies announced the addition of The Muffs and The Pandoras guitarist and vocalist Kim Shattuck, who would be replacing Deal for the band's 2013 European tour. On September 3, 2013 Pixies released an EP of new songs titled "EP1". On November 29, 2013, Shattuck announced that she had been dismissed from the band that day. In December 2013, Paz Lenchantin was announced as the new bassist for the 2014 tour. More new material surfaced when Pixies released their second EP, titled "EP2", on January 3, 2014. The single released to radio was "Blue Eyed Hexe." Another new EP, called "EP3", was released on March 24, 2014. All the EPs were only available as downloads and limited edition vinyl. The three EPs were collected in LP format and released as the album "Indie Cindy" in April 2014. The album was the first release from the band in over two decades, the last being "Trompe le Monde" in 1991.
In 2015 it was revealed that the Pixies would go on tour with former Led Zeppelin frontman Robert Plant for a series of dates across North America.
Musical style.
The Pixies' musical style has been described as "an unorthodox marriage of surf music and punk rock, … characterized by Black's bristling lyrics and hackle-raising caterwaul, Kim Deal's whispered harmonies and waspy basslines, Joey Santiago's fragile guitar, and the persistent flush of David Lovering's drums." The band's music incorporates extreme dynamic shifts; Francis explained in 1991, "Those are the two basic components of rock music […] the dreamy side and the rockin' side. It's always been either sweaty or laid back and cool. We do try to be dynamic, but it's dumbo dynamics, because we don't know how to do anything else. We can play loud or quiet—that's it".
Influences.
The Pixies draw influence from a range of artists and genres; each member came from a different musical background. When he first started writing songs for the Pixies, Francis says he was listening to nothing but Hüsker Dü, Captain Beefheart, and Iggy Pop; Other influences associated with Francis include The Gun Club and The Cars. During the making of "Doolittle" he was listening heavily to The Beatles' "The White Album". He has cited Buddy Holly as a model for his compressed songwriting.
Santiago listened to 1970s and 1980s punk including Black Flag, as well as David Bowie. Guitarists who influenced him include Jimi Hendrix, Les Paul, Wes Montgomery, and George Harrison. Deal's musical background was folk music and country; she had formed a country-folk band with her sister in her teenage years, and played covers of artists such as The Everly Brothers and Hank Williams. Other artists they listened to included XTC, Gang of Four and Elvis Costello. Lovering is a fan of the band Rush.
Other media such as film has had an impact on the Pixies; Francis cites surrealist films "Eraserhead" and "Un chien andalou" (as mentioned in "Debaser") as influences. He has commented on these influences, saying he "didn't have the patience to sit around reading Surrealist novels", but found it easier to watch twenty-minute films.
Songwriting and vocals.
Most of the Pixies songs were composed and sung by Francis. Critic Stephen Thomas Erlewine has described Francis's writing as containing "bizarre, fragmented lyrics about space, religion, sex, mutilation, and pop culture". Biblical violence is a theme of "Doolittle"‍ '​s "Dead" and "Gouge Away"; Francis told a "Melody Maker" interviewer, "It's all those characters in the Old Testament. I'm obsessed with them. Why it comes out so much I don't know." He has described "Come on Pilgrim‍ '​s" "Caribou" as being about reincarnation, and extraterrestrial themes appear in a number of songs on "Bossanova".
Deal co-wrote "Doolittle"‍ '​s "Silver" with Francis, and they share lead harmony vocals on the track. She also co-wrote and sang lead vocals on "Surfer Rosa"‍ '​s "Gigantic", and is the sole songwriter of the 2004 digital single "Bam Thwok". She was credited as Mrs. John Murphy on the former composition—at the time she was married and she used this name as an ironic feminist joke. She also sang lead vocals on the song "Into the White" and the Neil Young cover "Winterlong", both B-sides. Lovering sang lead vocals on "Doolittle"‍ '​s "La La Love You" and the B-side "Make Believe".
Impact.
Although the Pixies produced relatively few albums, whose sales were modest, they influenced a number of bands associated with the alternative rock boom of the 1990s. Gary Smith, who produced their "Come On Pilgrim", commented on the band's influence on alternative rock and their legacy in 1997:
I've heard it said about The Velvet Underground that while not a lot of people bought their albums, everyone who did started a band. I think this is largely true about the Pixies as well. Charles' secret weapon turned out to be not so secret and, sooner or later, all sorts of bands were exploiting the same strategy of wide dynamics. It became a kind of new pop formula and, within a short while, "Smells Like Teen Spirit" was charging up the charts and even the members of Nirvana said later that it sounded for all the world like a Pixies song.
Sonically, the Pixies are credited with popularizing the extreme dynamics and stop-start timing that would become widespread in alternative rock; Pixies songs typically feature hushed, restrained verses, and explosive, wailing choruses. Artists including David Bowie, Radiohead, PJ Harvey, U2, Nirvana, The Strokes, Weezer, Bush and Pavement have cited admiration of or influence by the Pixies. Bono of U2 has called the Pixies "one of America's greatest bands ever", and Radiohead's Thom Yorke has said that, while at school, "the Pixies had changed my life". Bowie, whose own music had inspired Francis and Santiago while they were at university, has said that the Pixies made "just about the most compelling music of the entire 80s."
One notable citation as an influence was by Kurt Cobain, on influencing Nirvana's "Smells Like Teen Spirit", which he admitted was a conscious attempt to co-opt the Pixies' style. In a January 1994 interview with "Rolling Stone", he said, "I was trying to write the ultimate pop song. I was basically trying to rip off the Pixies. I have to admit it [smiles]. When I heard the Pixies for the first time, I connected with that band so heavily I should have been in that band—or at least in a Pixies cover band. We used their sense of dynamics, being soft and quiet and then loud and hard." Cobain cited "Surfer Rosa" as one of his main musical influences, and particularly admired the album's natural and powerful drum sounds—a result of Steve Albini's influence on the record. Albini later produced Nirvana's 1993 "In Utero" at the request of Cobain.
Music videos and DVDs.
No music videos were released from "Come on Pilgrim" or "Surfer Rosa", but from "Doolittle" onwards, the following videos were made: "Monkey Gone To Heaven", "Here Comes Your Man", "Velouria", "Dig For Fire", "Allison", "Alec Eiffel", "Head On", and "Debaser"; these were later released on the 2004 DVD "Pixies". Furthermore, a music video accompanied the release of their 2013 song, "Bagboy", as well an alternate video released on a later date. Videos have been made for all the songs in EP1. The videos for "Here Comes Your Man" and "Allison" were also released on "The Complete 'B' Sides".
By "Bossanova", the band had developed a severe aversion to recording music videos, and Francis refused to lip-sync to them. For example, in the "Here Comes Your Man" video, both Black and Deal open their mouths wide instead of mouthing their lyrics. According to the record label this became one of the reasons that the Pixies never achieved major coverage on MTV.
With "Bossanova"‍ '​s release, 4AD hoped to get the Pixies chosen to perform their single "Velouria" on the BBC music programme "Top of the Pops". To this end, the band was pressured into producing a video for the song, and they made one cheaply with the band members filmed running down a quarry, shown in slow motion. The group was ultimately not given a spot on the show.
A 90-minute documentary called "loudQUIETloud: a film about the Pixies" directed by Steven Cantor and Matthew Galkin was released in 2006. The film documents their 2004 reunion and tour, and covers the years after the break-up. In addition to "Pixies" and "LoudQUIETloud", four other Pixies' DVDs were released between 2004 and 2006, all featuring concert performances: "Live at the Town and Country Club 1988", "The Pixies—Sell Out", "The Pixies Acoustic: Live in Newport", and "The Pixies Club Date: Live at the Paradise in Boston".

</doc>
<doc id="23731" url="http://en.wikipedia.org/wiki?curid=23731" title="Plasma ashing">
Plasma ashing

In semiconductor manufacturing plasma ashing is the process of removing the photoresist from an etched wafer. Using a plasma source, a monatomic reactive species is generated. Oxygen or fluorine are the most common reactive species. The reactive species combines with the photoresist to form ash which is removed with a vacuum pump.
Typically, monatomic (single atom) oxygen plasma is created by exposing oxygen gas at a low pressure (O2) to high power radio waves, which ionise it. This process is done under vacuum in order to create a plasma. As the plasma is formed, many free radicals are created which could damage the wafer. Newer, smaller circuitry is increasingly susceptible to these particles. Originally, plasma was generated in the process chamber, but as the need to get rid of free radicals has increased, many machines now use a downstream plasma configuration, where plasma is formed remotely and the desired particles are channeled to the wafer. This allows electrically charged particles time to recombine before they reach the wafer surface, and prevents damage to the wafer surface.
Two forms of plasma ashing are typically performed on wafers. High temperature ashing, or stripping, is performed to remove as much photo resist as possible, while the "descum" process is used to remove residual photo resist in trenches. The main difference between the two processes is the temperature the wafer is exposed to while in an ashing chamber.
Monatomic oxygen is electrically neutral and although it does recombine during the channeling, it does so at a slower rate than the positively or negatively charged free radicals, which attract one another. This means that when all of the free radicals have recombined, there is still a portion of the active species available for process. Because a large portion of the active species is lost to recombination, process times may take longer. To some extent, these longer process times can be mitigated by increasing the temperature of the reaction area.

</doc>
<doc id="23732" url="http://en.wikipedia.org/wiki?curid=23732" title="Psychophysiology">
Psychophysiology

Psychophysiology (from Greek ψῡχή, "psȳkhē", "breath, life, soul"; φύσις, "physis", "nature, origin"; and -λογία, "-logia") is the branch of psychology that is concerned with the physiological bases of psychological processes. While psychophysiology was a general broad field of research in the 1960s and 1970s, it has now become quite specialized, and has branched into subspecializations. For example, social psychophysiology, cardiovascular psychophysiology, cognitive psychophysiology, and cognitive neuroscience.
Background.
Some people have difficulty distinguishing a psychophysiologist from a physiological psychologist, two very different perspectives. Psychologists are interested in why we may fear spiders and physiologists may be interested in the input/output system of the amygdala. A psychophysiologist will attempt to link the two. Psychophysiologists generally study the psychological/physiological link in intact human subjects. While early psychophysiologists almost always examined the impact of psychological states on physiological system responses, since the 1970s, psychophysiologists also frequently study the impact of physiological states and systems on psychological states and processes. It is this perspective of studying the interface of mind and body that makes psychophysiologists most distinct.
Historically, most psychophysiologists tended to examine the physiological responses and organ systems innervated by the autonomic nervous system. More recently, psychophysiologists have been equally, or potentially more, interested in the central nervous system, exploring cortical brain potentials such as the many types of event-related potentials (ERPs), brain waves, and utilizing advanced technology such as functional magnetic resonance imaging (fMRI), MRI, PET, MEG, and other neuroimagery techniques.
Continuing the comparison between a psychophysiologist and a physiological psychologist, a psychophysiologist may look at how exposure to a stressful situation will produce a result in the cardiovascular system such as a change in heart rate (HR), vasodilation/vasoconstriction, myocardial contractility, or stroke volume. A physiological psychologist may look at how one cardiovascular event may influence another cardiovascular or endocrine event, or how activation of one neural brain structure exerts excitatory activity in another neural structure which then induces an inhibitory effect in some other system. Often, physiological psychologists examine the effects that they study in infrahuman subjects using surgical or invasive techniques and processes.
Psychophysiology is closely related to the field of Neuroscience and Social neuroscience, which primarily concerns itself with relationships between psychological events and brain responses. Psychophysiology is also related to the medical discipline known as psychosomatics.
While psychophysiology was a discipline off the mainstream of psychological and medical science prior to roughly the 1960 and 1970s, more recently, psychophysiology has found itself positioned at the intersection of psychological and medical science, and its popularity and importance have expanded commensurately with the realization of the inter-relatedness of mind and body.
Commonly used measures.
Many measures are part of modern psychophysiology including measures of brain activity such as ERPs, brain waves (electroencephalography, EEG), fMRI (functional magnetic resonance imaging), measures of skin conductance (skin conductance response, SCR; galvanic skin response, GSR), cardiovascular measures (heart rate, HR; beats per minute, BPM; heart rate variability, HRV; vasomotor activity), muscle activity (electromyography, EMG), electrogastrogram (EGG) changes in pupil diameter with thought and emotion (pupillometry), eye movements, recorded via the electro-oculogram (EOG) and direction-of-gaze methods, and cardiodynamics, recorded via impedance cardiography .
Uses of psychophysiology.
Psychophysiological measures are often used to study emotion and attention responses to stimuli, during exertion,and increasingly, to better understand cognitive processes.
Physiological sensors have been used to detect emotions in schools and intelligent tutoring systems.
Psychophysiological inference and physiological computer games.
Physiological computing represents a category of affective computing that incorporates real-time software adaption to the psychophysiological activity of the user. The main goal of this is to build a computer that responds to user emotion, cognition and motivation. The approach is to enable implicit and symmetrical human-computer communication by granting the software access to a representation of the user's psychological status.
There are several possible methods to represent the psychological state of the user (discussed in the affective computing page). The advantages of using psychophysiological indices are that their changes are continuous, measures are covert and implicit, and only available data source when the user interacts with the computer without any explicit communication or input device. These systems rely upon an assumption that the psychophysiological measure is an accurate one-to-one representation of a relevant psychological dimension such as mental effort, task engagement and frustration.
Physiological computing systems all contain an element that may be termed as an adaptive controller that may be used to represent the player. This adaptive controller represents the decision-making process underlying software adaptation. In their simplest form, adaptive controllers are expressed in Boolean statements. Adaptive controllers encompass not only the decision-making rules, but also the psychophysiological inference that is implicit in the quantification of those trigger points used to activate the rules. The representation of the player using an adaptive controller can become very complex and often only one-dimensional. The loop used to describe this process is known as the biocybernetic loop. The biocybernetic loop describes the closed loop system that receives psychophysiological data from the player, transforms that data into a computerized response, which then shapes the future psychophysiological response from the player. A positive control loop tends towards instability as player-software loop strives towards a higher standard of desirable performance. The physiological computer game may wish to incorporate both positive and negative loops into the adaptive controller.
Sources.
A. Weissman, M. Aranovitch, S. Blazer, and E. Z. Zimmer (2009)
Pediatrics 124, e921-e92
L. P.T. Hua, C. A. Brown, S. J.M. Hains, M. Godwin, and J. L. Parlow (2009)
Biol Res Nurs 11, 129-143

</doc>
<doc id="23733" url="http://en.wikipedia.org/wiki?curid=23733" title="Periodization">
Periodization

Periodization is the process or study of categorizing the past into discrete, quantified named blocks of time in order to make the study and analysis of history easier to facilitate. The result is descriptive abstractions that provide convenient terms for periods of time with relatively stable characteristics. However, determining the precise beginning and ending to any "period" is often arbitrary.
To the extent that history is continuous and ungeneralizable, all systems of periodization are more or less arbitrary. Yet without named periods, however clumsy or imprecise, past time would be nothing more than scattered events without a framework to help us understand them. Nations, cultures, families, and even individuals, each with their different remembered histories, are constantly engaged in imposing overlapping, often unsystematized, schemes of temporal periodization; periodizing labels are continually challenged and redefined, but once established, a period "brand" is so convenient that many are very hard to shake off.
Origins of periodization.
The origins of periodization is very old and first became part of the Western tradition in the myths of Ancient Greece and The Bible. Virgil spoke of a distant Golden Age and recurrent cycles of history. The Bible outlines a narrative of history from Creation to the End of time. One Biblical periodization scheme commonly used in the Middle Ages was Saint Paul's theological division of history into three ages: the first before the age of Moses (under nature); the second under Mosaic law (under law); the third in the age of Christ (under grace). But perhaps the most widely discussed periodization scheme of the Middle Ages was the Six Ages of the World, where every age was a thousand years counting from Adam to the present, with the present time (in the Middle Ages) being the sixth and final stage.
Background.
Not only do periodizing blocks inevitably overlap, they often seemingly conflict with or contradict one another. Some have a cultural usage ("the Gilded Age"), others refer to prominent historical events ("the Inter-War years: 1918–1939"), yet others are defined by decimal numbering systems ("the 1960s", "the 17th century"). Other periods are named from influential or talismanic individuals ("the Victorian Era", "the Edwardian Era", "the Napoleonic Era").
Some of these usages will also be geographically specific. This is especially true of periodizing labels derived from individuals or ruling dynasties, such as the Jacksonian Era in America, the Meiji Era in Japan, or the Merovingian Period in France. Cultural terms may also have a limited reach. Thus the concept of the "Romantic period" is largely meaningless outside the Western world of Europe and European-influenced cultures. Likewise, "the 1960s", though technically applicable to anywhere in the world according to Common Era numbering, has a certain set of specific cultural connotations in certain countries. For this reason it may be possible to say such things as "The 1960s never occurred in Spain". This would mean that the sexual revolution, counterculture, youth rebellion and so on never developed during that decade in Spain's conservative Roman Catholic culture and under Francisco Franco's authoritarian regime. Likewise it is very often said, as the historian Arthur Marwick has, that "the 1960s" began in the late 1950s and ended in the early 1970s. His reason for saying this is that the cultural and economic conditions that define the meaning of the period covers more than the accidental fact of a 10 year block beginning with the number 6. This extended usage is termed the "long 1960s". This usage derives from other historians who have adopted labels such as "the long 19th century" (1789–1914) to reconcile arbitrary decimal chronology with meaningful cultural and social phases. Similarly, an Eighteenth Century may run 1714–1789. Eric Hobsbawm has also argued for what he calls "the short twentieth century", encompassing the period from the First World War through to the end of the Cold War.
Similar problems attend other labels. Is it possible to use the term "Victorian" outside Britain, and even within, does her reign of 1837-1901 usefully constitute a historical period? It sometimes is used when it is thought that its connotations usefully describe the politics, culture and economic conditions characteristic of the last two-thirds of the nineteenth century. Nevertheless periodizing terms often have negative or positive connotations that may affect their usage. This includes "Victorian", which often negatively suggests sexual repression and class conflict. Other labels such as "Renaissance" have strongly positive characteristics. As a result, these terms sometimes extend in meaning. Thus the "English Renaissance" is often used for a period largely identical to the "Elizabethan Period" or reign of Elizabeth I, and begins some 200 years later than the Italian Renaissance. However the "Carolingian Renaissance" is said to have occurred during the reign of the Frankish king Charlemagne, and his immediate successors. Other examples, neither of which constituted a "rebirth" in the sense of revival, are the "American Renaissance" of the 1820s-60s, referring mainly to literature, and the "Harlem Renaissance" of the 1920s, referring mainly to literature but also to music and the visual arts.
Because of these various positive and negative connotations, some periods are luckier than others regarding their names, although this can lead to problems such as the ones outlined above. The conception of a "rebirth" of Classical Latin learning is first credited to the Italian poet Petrarch (1304-1374), the father of Renaissance Humanism, but the conception of a rebirth has been in common use since Petrarch's time. The dominant usage of the word "Renaissance" refers to the cultural changes that occurred in Italy that culminated in the High Renaissance around 1500-1530. This concept applies dominantly to the visual arts, and the work of Michelangelo, Raphael, and Leonardo da Vinci. Secondarily it is applied to other arts, but it is questionable whether it is useful to describe a phase in economic, social and political history. Many professional historians now refer to the historical periods commonly known as the Renaissance and the Reformation as the start of the Early Modern Period, which extends much later. There is a gradual change in the courses taught and books published to correspond to the change in period nomenclature, which in part reflects differences between social history and cultural history. The new nomenclature suggests a broader geographical coverage and a growing attention to the relationships between Europe and the wider world.
In most cases, people living through a period did not identify themselves as belonging to the period that historians may later assign to them. This is partly because they are unable to predict the future, and so will not be able to tell whether they are at the beginning, middle or end of a period. Another reason may be that their own sense of historical development may be determined by religions or ideologies that differ from those used by later historians.
The term Middle Ages also derives from Petrarch. He was comparing his own period to the Ancient or Classical world, seeing his time as a time of rebirth after a dark intermediate period, the Middle Ages. The idea that the Middle Ages was a "middle" phase between two other large scale periodizing concepts, Ancient and Modern, still persists. It can be sub-divided into the Early, High and Late Middle Ages. The term Dark Ages is no longer in common use among modern scholars because of the difficulty of using it neutrally, though some writers have attempted to retain it and divest it of its negative connotations. The term "Middle Ages" and especially the adjective "medieval" can also have a negative ring in colloquial use ("the barbaric treatment of prisoners in such-and-such a prison is almost medieval") but this does not carry over into academic terminology. However, other terms, such as Gothic architecture, used to refer to a style typical of the High Middle Ages have largely lost the negative connotations they initially had, acquiring new meanings over time (see Gothic architecture and Goth subculture).
The Gothic and the Baroque were both named during subsequent stylistic periods when the preceding style was unpopular. The word "Gothic" was applied as a pejorative term to all things Northern European and, hence, barbarian, probably first by Giorgio Vasari. Vasari is also credited with first using the term "Renaissance" ("rinascita") to describe the period during which he was art historian, artist, and architect. Giorgio Vasari coined the term "Gothic" in an effort to describe, particularly architecture, that he found objectionable, supposedly saying "it is as if the Goths built it". The word "baroque"—derived from similar words in Portuguese, Spanish, or French—literally refers to an irregular or misshapen pearl. Its first use outside the field of jewellery manufacture was in the early 18th century, as a criticism of music that was viewed as over-complicated and rough. Later, the term was also used to describe architecture and art. The Baroque period was first designated as such in the 19th century, and is generally considered to have begun around 1600 in all media. Music history places the end of the period in the year 1750 with the death of J. S. Bach, while art historians consider the main period to have ended significantly earlier in most areas.
Marxian periodisation.
The Marxist theory of historical materialism claims society as fundamentally determined by the "material conditions" at any given time – in other words, the relationships which people have with each other in order to fulfil basic needs such as feeding, clothing and housing themselves and their families. Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe.
The theory identifies the following stages of history:
Primitive communism.
The First Stage: is usually called primitive communism. It has the following characteristics.
Slave society.
The Second Stage: may be called slave society, considered to be the beginning of "class society" where private property appears.
Feudalism.
The Third Stage: may be called feudalism; it appears after slave society collapses. This was most obvious during the European Middle Ages when society went from slavery to feudalism.
Capitalism.
Marx pays special attention to this stage in human development. The bulk of his work is devoted to analysing the mechanisms of capitalism, which in western society classically arose "red in tooth and claw" from feudal society in a revolutionary movement. In capitalism, the profit motive rules and people, freed from serfdom, work for the capitalists for wages. The capitalist class are free to spread their laissez faire practices around the world. In the capitalist-controlled parliament, laws are made to protect wealth.
Capitalism may be considered the Fourth Stage in the sequence. It appears after the bourgeois revolution when the capitalists (or their merchant predecessors) overthrow the feudal system. Capitalism is categorized by the following:
But according to Marx, capitalism, like slave society and feudalism, also has critical failings — inner contradictions which will lead to its downfall. The working class, to which the capitalist class gave birth in order to produce commodities and profits, is the "grave digger" of capitalism. The worker is not paid the full value of what he or she produces. The rest is surplus value — the capitalist's profit, which Marx calls the "unpaid labour of the working class." The capitalists are forced by competition to attempt to drive down the wages of the working class to increase their profits, and this creates conflict between the classes, and gives rise to the development of class consciousness in the working class. The working class, through trade union and other struggles, becomes conscious of itself as an exploited class. In the view of classical Marxism, the struggles of the working class against the attacks of the capitalist class will eventually lead the working class to establish its own collective control over production.
Socialism.
After the working class gains class consciousness and mounts a revolution against the capitalists, socialism, which may be considered the Fifth Stage, will be attained, if the workers are successful.
Socialism may be characterised as follows:
Marx explained that, since socialism, the first stage of communism, would be "in every respect, economically, morally, and intellectually, still stamped with the birthmarks of the old society from whose womb it emerges", each worker would naturally expect to be awarded according to the amount of labor he contributes, despite the fact that each worker's ability and family circumstances would differ, so that the results would still be unequal at this stage, although fully supported by social provision.
Prehistoric periodization.
The usual method for periodization of the distant prehistoric past, in archeology is to rely on changes in material culture and technology, such as the Stone Age, Bronze Age and Iron Age and their sub-divisions also based on different styles of material remains. Despite the development over recent decades of the ability through radiocarbon dating and other scientific methods to give actual dates for many sites or artefacts, these long-established schemes seem likely to remain in use. In many cases neighbouring cultures with writing have left some history of cultures without it, which may be used.
By events.
Some events or short periods of change have such a drastic effect on the cultures they affect that they form a natural break in history. These are often marked by the widespread use of both "pre-" and "post-" phrases centred on the event, as in "pre-Reformation" and "post-Reformation", or "pre-colonial" and "post-colonial". Both pre-war and post-war are still understood to refer to World War II, though at some future point the phrases will need to be altered to make that clear.

</doc>
<doc id="23734" url="http://en.wikipedia.org/wiki?curid=23734" title="Petrarch">
Petrarch

Francesco Petrarca (]; July 20, 1304 – July 19, 1374), commonly anglicized as Petrarch (), was an Italian scholar and poet in Renaissance Italy, and one of the earliest humanists. Petrarch's rediscovery of Cicero's letters is often credited for initiating the 14th-century Renaissance. Petrarch is often called the "Father of Humanism". In the 16th century, Pietro Bembo created the model for the modern Italian language based on Petrarch's works, as well as those of Giovanni Boccaccio, and, to a lesser extent, Dante Alighieri. Petrarch would be later endorsed as a model for Italian style by the Accademia della Crusca. Petrarch's sonnets were admired and imitated throughout Europe during the Renaissance and became a model for lyrical poetry. He is also known for being the first to develop the concept of the "Dark Ages."
Biography.
Youth and early career.
Petrarch was born in the Tuscan city of Arezzo in 1304. He was the son of Ser Petracco and his wife Eletta Canigiani. His given name was "Francesco Petracco." The name was Latinized to "Petrarca." Petrarch's younger brother was born in Incisa in Val d'Arno in 1307. Dante was a friend of his father.
Petrarch spent his early childhood in the village of Incisa, near Florence. He spent much of his early life at Avignon and nearby Carpentras, where his family moved to follow Pope Clement V who moved there in 1309 to begin the Avignon Papacy. He studied law at the University of Montpellier (1316–20) and Bologna (1320–23) with a lifelong friend and schoolmate called Guido Sette. Because his father was in the profession of law he insisted that Petrarch and his brother study law also. Petrarch however was primarily interested in writing and Latin literature and considered these seven years wasted. Additionally he proclaimed that through legal manipulation his guardians robbed him of his small property inheritance in Florence, which only reinforced his dislike for the legal system. He protested, "I couldn't face making a merchandise of my mind", as he viewed the legal system as the art of selling justice.
Petrarch was a prolific letter writer and counted Boccaccio among his notable friends to whom he wrote often. After the death of their parents, Petrarch and his brother Gherardo went back to Avignon in 1326, where he worked in numerous clerical offices. This work gave him much time to devote to his writing. With his first large scale work, "Africa", an epic in Latin about the great Roman general Scipio Africanus, Petrarch emerged as a European celebrity. On April 8, 1341, he became the first poet laureate since antiquity and was crowned by Roman "Senatori" Giordano Orsini and Orso dell'Anguillara on the holy grounds of Rome's Capitol.
He traveled widely in Europe and served as an ambassador and has been called "the first tourist" because he traveled just for pleasure, which was the basic reason he climbed Mont Ventoux. During his travels, he collected crumbling Latin manuscripts and was a prime mover in the recovery of knowledge from writers of Rome and Greece. He encouraged and advised Leontius Pilatus's translation of Homer from a manuscript purchased by Boccaccio, although he was severely critical of the result. Petrarch had acquired a copy, which he did not entrust to Leontius, but he knew no Greek; Homer, Petrarch said, "was dumb to him, while he was deaf to Homer". In 1345 he personally discovered a collection of Cicero's letters not previously known to have existed, the collection "ad Atticum".
Disdaining what he believed to be the ignorance of the centuries preceding the era in which he lived, Petrarch is credited or charged with creating the concept of a historical "Dark Ages".
Mount Ventoux.
Petrarch recounts that on April 26, 1336, with his brother and two servants, he climbed to the top of Mont Ventoux (1912 m), a feat which he undertook for recreation rather than necessity. The exploit is described in a celebrated letter addressed to his friend and confessor, the monk Dionigi di Borgo San Sepolcro, composed some time after the fact. In it Petrarch claimed to have been inspired by Philip V of Macedon's ascent of Mount Haemo and that an aged peasant had told him that nobody had ascended Ventoux before or after himself, 50 years before, and warned him against attempting to do so. The nineteenth-century Swiss historian Jacob Burckhardt noted that Jean Buridan had climbed the same mountain a few years before, and ascents accomplished during the Middle Ages have been recorded, including that of Anno II, Archbishop of Cologne.
Scholars note that Petrarch's letter to Dionigi displays a strikingly "modern" attitude of aesthetic gratification in the grandeur of the scenery and is still often cited in books and journals devoted to the sport of mountaineering. In Petrarch, this attitude is coupled with an aspiration for a virtuous Christian life, and on reaching the summit, he took from his pocket a volume by his beloved mentor, Saint Augustine, that he always carried with him.
For pleasure alone he climbed Mount Ventoux, which rises to more than six thousand feet, beyond Vaucluse. It was no great feat, of course; but he was the first recorded Alpinist of modern times, the first to climb a mountain merely for the delight of looking from its top. (Or almost the first; for in a high pasture he met an old shepherd, who said that fifty years before he had attained the summit, and had got nothing from it save toil and repentance and torn clothing.) Petrarch was dazed and stirred by the view of the Alps, the mountains around Lyons, the Rhone, the Bay of Marseilles. He took St. Augustine's "Confessions" from his pocket and reflected that his climb was merely an allegory of aspiration towards a better life.
As the book fell open, Petrarch's eyes were immediately drawn to the following words:
And men go about to wonder at the heights of the mountains, and the mighty waves of the sea, and the wide sweep of rivers, and the circuit of the ocean, and the revolution of the stars, but themselves they consider not.
Petrarch's response was to turn from the outer world of nature to the inner world of "soul":
I closed the book, angry with myself that I should still be admiring earthly things who might long ago have learned from even the pagan philosophers that nothing is wonderful but the soul, which, when great itself, finds nothing great outside itself. Then, in truth, I was satisfied that I had seen enough of the mountain; I turned my inward eye upon myself, and from that time not a syllable fell from my lips until we reached the bottom again. [...] [W]e look about us for what is to be found only within. [...] How many times, think you, did I turn back that day, to glance at the summit of the mountain which seemed scarcely a cubit high compared with the range of human contemplation [...]
James Hillman argues that this rediscovery of the inner world is the real significance of the Ventoux event. The Renaissance begins not with the ascent of Mont Ventoux but with the subsequent descent—the "return [...] to the valley of soul", as Hillman puts it. Arguing against such a singular and hyperbolic periodization, Paul James suggests a different reading:
Later years.
The later part of Petrarch's life he spent in journeying through northern Italy as an international scholar and poet-diplomat. His career in the Church did not allow him to marry, but he is believed to have fathered two children by a woman or women unknown to posterity. A son, Giovanni, was born in 1337, and a daughter, Francesca, was born in 1343. Both he later legitimized.
Giovanni died of the plague in 1361. In the same year Petrarch was named canon in Monselice near Padua. Francesca married Francescuolo da Brossano (who was later named executor of Petrarch's will) that same year. In 1362, shortly after the birth of a daughter, Eletta (the same name as Petrarch's mother), they joined Petrarch in Venice to flee the plague then ravaging parts of Europe. A second grandchild, Francesco, was born in 1366, but died before his second birthday. Francesca and her family lived with Petrarch in Venice for five years from 1362 to 1367 at Palazzo Molina; although Petrarch continued to travel in those years. Between 1361 and 1369 the younger Boccaccio paid the older Petrarch two visits. The first was in Venice, the second was in Padua.
About 1368 Petrarch and his daughter Francesca (with her family) moved to the small town of Arquà in the Euganean Hills near Padua, where he passed his remaining years in religious contemplation. He died in his house in Arquà on July 19, 1374 – one day short of his seventieth birthday. The house hosts now a permanent exhibition of Petrarchian works and curiosities; among others you find the famous tomb of Petrarch's beloved cat who was embalmed. On the marble slab there is a Latin inscription written by Antonio Quarenghi:
 Etruscus gemino vates ardebat amore:
Maximus ignis ego; Laura secundus erat.
Quid rides? divinæ illam si gratia formæ,
Me dignam eximio fecit amante fides.
Si numeros geniumque sacris dedit illa libellis
Causa ego ne sævis muribus esca forent.
Arcebam sacro vivens a limine mures,
Ne domini exitio scripta diserta forent;
Incutio trepidis eadem defuncta pavorem,
Et viget exanimi in corpore prisca fides.
Petrarch's will (dated April 4, 1370) leaves 50 florins to Boccaccio "to buy a warm winter dressing gown"; various legacies (a horse, a silver cup, a lute, a Madonna) to his brother and his friends; his house in Vaucluse to its caretaker; for his soul, and for the poor; and the bulk of his estate to his son-in-law, Francescuolo da Brossano, who is to give half of it to "the person to whom, as he knows, I wish it to go"; presumably his daughter, Francesca, Brossano's wife. The will mentions neither the property in Arquà nor his library; Petrarch's library of notable manuscripts was already promised to Venice, in exchange for the Palazzo Molina. This arrangement was probably cancelled when he moved to Padua, the enemy of Venice, in 1368. The library was seized by the lords of Padua, and his books and manuscripts are now widely scattered over Europe. Nevertheless, the Biblioteca Marciana traditionally claimed this bequest as its founding, although it was in fact founded by Cardinal Bessarion in 1468.
Works.
Petrarch is best known for his Italian poetry, notably the "Canzoniere" ("Songbook") and the "Trionfi" ("Triumphs"). However, Petrarch was an enthusiastic Latin scholar and did most of his writing in this language. His Latin writings include scholarly works, introspective essays, letters, and more poetry. Among them are "Secretum" ("My Secret Book"), an intensely personal, guilt-ridden imaginary dialogue with Augustine of Hippo; "De Viris Illustribus" ("On Famous Men"), a series of moral biographies; "Rerum Memorandarum Libri", an incomplete treatise on the cardinal virtues; "De Otio Religiosorum" ("On Religious Leisure") and "De Vita Solitaria" ("On the Solitary Life"), which praise the contemplative life; "De Remediis Utriusque Fortunae" ("Remedies for Fortune Fair and Foul"), a self-help book which remained popular for hundreds of years; "Itinerarium" ("Petrarch's Guide to the Holy Land"); invectives against opponents such as doctors, scholastics, and the French; the "Carmen Bucolicum", a collection of 12 pastoral poems; and the unfinished epic "Africa".
Petrarch also published many volumes of his letters, including a few written to his long-dead friends from history such as Cicero and Virgil. Cicero, Virgil, and Seneca were his literary models. Most of his Latin writings are difficult to find today, but several of his works are available in English translations. Several of his Latin works are scheduled to appear in the Harvard University Press series "I Tatti". It is difficult to assign any precise dates to his writings because he tended to revise them throughout his life.
Petrarch collected his letters into two major sets of books called "Epistolae familiares" ("") and "Seniles" (""), both of which are available in English translation. The plan for his letters was suggested to him by knowledge of Cicero's letters. These were published "without names" to protect the recipients, all of whom had close relationships to Petrarch. The recipients of these letters included Philippe de Cabassoles, bishop of Cavaillon; Ildebrandino Conti, bishop of Padua; Cola di Rienzo, tribune of Rome; Francesco Nelli, priest of the Prior of the Church of the Holy Apostles in Florence; and Niccolò di Capoccia, a cardinal and priest of Saint Vitalis. His "Letter to Posterity" (the last letter in "Seniles") gives an and a synopsis of his philosophy in life. It was originally written in Latin and was completed in 1371 or 1372 - the first such autobiography in a thousand years (since Saint Augustine).
While Petrarch's poetry was set to music frequently after his death, especially by Italian madrigal composers of the Renaissance in the 16th century, only one musical setting composed during Petrarch's lifetime survives. This is "Non al suo amante" by Jacopo da Bologna, written around 1350.
Laura and poetry.
On April 6, 1327, after Petrarch gave up his vocation as a priest, the sight of a woman called "Laura" in the church of Sainte-Claire d'Avignon awoke in him a lasting passion, celebrated in the "Rime sparse" ("Scattered rhymes"). Later, Renaissance poets who copied Petrarch's style named this collection of 366 poems "Il Canzoniere" ("Song Book"). Laura may have been Laura de Noves, the wife of Count Hugues de Sade (an ancestor of the Marquis de Sade). There is little definite information in Petrarch's work concerning Laura, except that she is lovely to look at, fair-haired, with a modest, dignified bearing. Laura and Petrarch had little or no personal contact. According to his "Secretum", she refused him for the very proper reason that she was already married to another man. He channeled his feelings into love poems that were exclamatory rather than persuasive, and wrote prose that showed his contempt for men who pursue women. Upon her death in 1348, the poet found that his grief was as difficult to live with as was his former despair. Later in his "Letter to Posterity", Petrarch wrote: "In my younger days I struggled constantly with an overwhelming but pure love affair – my only one, and I would have struggled with it longer had not premature death, bitter but salutary for me, extinguished the cooling flames. I certainly wish I could say that I have always been entirely free from desires of the flesh, but I would be lying if I did".
While it is possible she was an idealized or pseudonymous character – particularly since the name "Laura" has a linguistic connection to the poetic "laurels" Petrarch coveted – Petrarch himself always denied it. His frequent use of "l'aura" is also remarkable: for example, the line "Erano i capei d'oro a "l'aura" sparsi" may both mean "her hair was all over Laura's body", and "the wind ("l'aura") blew through her hair". There is psychological realism in the description of Laura, although Petrarch draws heavily on conventionalised descriptions of love and lovers from troubadour songs and other literature of courtly love. Her presence causes him unspeakable joy, but his unrequited love creates unendurable desires, inner conflicts between the ardent lover and the mystic Christian, making it impossible to reconcile the two. Petrarch's quest for love leads to hopelessness and irreconcilable anguish, as he expresses in the series of paradoxes in Rima 134 "Pace non trovo, et non ò da fa guerra": "I find no peace, and yet I make no war:/and fear, and hope: and burn, and I am ice".
Laura is unreachable – the few physical descriptions are vague, almost impalpable as the love he pines for, and such is perhaps the power of his verse, which lives off the melodies it evokes against the fading, diaphanous image that is no more consistent than a ghost. Francesco De Sanctis remarks much the same thing in his "Storia della letteratura italiana", and contemporary critics agree on the powerful music of his verse. Perhaps the poet was inspired by a famous singer he met in Veneto around 1350's. Gianfranco Contini, in a famous essay on Petrarch's language ("Preliminari sulla lingua del Petrarca". Petrarca, Canzoniere. Turin, Einaudi, 1964) has spoken of linguistic indeterminacy – Petrarch never rises above the "bel pié" (her lovely foot): Laura is too holy to be painted; she is an awe-inspiring goddess. Sensuality and passion are suggested rather by the rhythm and music that shape the vague contours of the lady.
Dante.
Petrarch's is a world apart from Dante and his "Divina Commedia". In spite of the metaphysical subject, the "Commedia" is deeply rooted in the cultural and social milieu of turn-of-the-century Florence: Dante's rise to power (1300) and exile (1302), his political passions call for a "violent" use of language, where he uses all the registers, from low and trivial to sublime and philosophical. Petrarch confessed to Boccaccio that he had never read the "Commedia", remarks Contini, wondering whether this was true or Petrarch wanted to distance himself from Dante. Dante's language evolves as he grows old, from the courtly love of his early novelistic "Rime" and "Vita nuova" to the "Convivio" and "Divina Commedia", where Beatrice is sanctified as the goddess of philosophy – the philosophy announced by the Donna Gentile at the death of Beatrice.
In contrast, Petrarch's thought and style are relatively uniform throughout his life – he spent much of it revising the songs and sonnets of the "Canzoniere" rather than moving to new subjects or poetry. Here, poetry alone provides a consolation for personal grief, much less philosophy or politics (as in Dante), for Petrarch fights within himself (sensuality versus mysticism, profane versus Christian literature), not against anything outside of himself. The strong moral and political convictions which had inspired Dante belong to the Middle Ages and the libertarian spirit of the commune; Petrarch's moral dilemmas, his refusal to take a stand in politics, his reclusive life point to a different direction, or time. The free commune, the place that had made Dante an eminent politician and scholar, was being dismantled: the "signoria" was taking its place. Humanism and its spirit of empirical inquiry, however, were making progress – but the papacy (especially after Avignon) and the empire (Henry VII, the last hope of the white Guelphs, died near Siena in 1313) had lost much of their original prestige.
Petrarch polished and perfected the sonnet form inherited from Giacomo da Lentini and which Dante widely used in his "Vita nuova" to popularise the new courtly love of the "Dolce Stil Novo". The tercet benefits from Dante's terza rima (compare the "Divina Commedia"), the quatrains prefer the ABBA-ABBA to the ABAB-ABAB scheme of the Sicilians. The imperfect rhymes of "u" with closed "o" and "i" with closed "e" (inherited from Guittone's mistaken rendering of Sicilian verse) are excluded, but the rhyme of open and closed "o" is kept. Finally, Petrarch's enjambment creates longer semantic units by connecting one line to the following. The vast majority (317) of Petrarch's 366 poems collected in the "Canzoniere" (dedicated to Laura) were "sonnets", and the Petrarchan sonnet still bears his name.
Philosophy.
Petrarch is traditionally called the father of Humanism and considered by many to be the "father of the Renaissance." In his work "Secretum meum" he points out that secular achievements did not necessarily preclude an authentic relationship with God. Petrarch argued instead that God had given humans their vast intellectual and creative potential to be used to their fullest. He inspired humanist philosophy which led to the intellectual flowering of the Renaissance. He believed in the immense moral and practical value of the study of ancient history and literature – that is, the study of human thought and action. Petrarch was a devout Catholic and did not see a conflict between realizing humanity's potential and having religious faith.
A highly introspective man, he shaped the nascent humanist movement a great deal because many of the internal conflicts and musings expressed in his writings were seized upon by Renaissance humanist philosophers and argued continually for the next 200 years. For example, Petrarch struggled with the proper relation between the active and contemplative life, and tended to emphasize the importance of solitude and study. In a clear disagreement with Dante, in 1346 Petrarch argued in his "De vita solitaria" that Pope Celestine V's refusal of the papacy in 1294 was as a virtuous example of solitary life. Later the politician and thinker Leonardo Bruni argued for the active life, or "civic humanism". As a result, a number of political, military, and religious leaders during the Renaissance were inculcated with the notion that their pursuit of personal fulfillment should be grounded in classical example and philosophical contemplation .
Legacy.
Petrarch's influence is evident in the works of Serafino Ciminelli from Aquila (1466-1500) and in the works of Marin Držić (1508-1567) from Dubrovnik.
The Romantic composer Franz Liszt set three of Petrarch's Sonnets (47, 104, and 123) to music for voice, "Tre sonetti del Petrarca", which he later would transcribe for solo piano for inclusion in the suite "Années de Pèlerinage".
While in Avignon in 1991, Modernist composer Elliott Carter completed his solo flute piece "Scrivo in Vento" which is in part inspired by and structured by Petrarch's Sonnet 212, "Beato in sogno". It was premiered on Petrarch's 687th birthday.
In November 2003, it was announced that pathological anatomists would be exhuming Petrarch's body from his casket in Arquà Petrarca, in order to verify 19th-century reports that he had stood 1.83 meters (about six feet), which would have been tall for his period. The team from the University of Padua also hoped to reconstruct his cranium in order to generate a computerized image of his features to coincide with his 700th birthday. The tomb had been opened previously in 1873 by Professor Giovanni Canestrini, also of Padua University. When the tomb was opened, the skull was discovered in fragments and a DNA test revealed that the skull was not Petrarch's, prompting calls for the return of Petrarch's skull.
The researchers are fairly certain that the body in the tomb is Petrarch's due to the fact that the skeleton bears evidence of injuries mentioned by Petrarch in his writings, including a kick from a donkey when he was 42.
Petrarch and his love for Laura are prominently featured in "Muse", a novel by Canadian author Mary Novik published by Doubleday Canada in 2013.
References.
</dl>
English translations.
</dl>
Further reading.
</dl>
External links.
</dl>

</doc>
<doc id="23735" url="http://en.wikipedia.org/wiki?curid=23735" title="PLD">
PLD

PLD may refer to:

</doc>
<doc id="23738" url="http://en.wikipedia.org/wiki?curid=23738" title="Propeller">
Propeller

A propeller is a type of fan that transmits power by converting rotational motion into thrust. A pressure difference is produced between the forward and rear surfaces of the airfoil-shaped blade, and a fluid (such as air or water) is accelerated behind the blade. Propeller dynamics can be modelled by both Bernoulli's principle and Newton's third law. A marine propeller is sometimes colloquially known as a screw propeller or screw.
History.
Early developments.
The principle employed in using a screw propeller is used in sculling. It is part of the skill of propelling a Venetian gondola but was used in a less refined way in other parts of Europe and probably elsewhere. For example, propelling a canoe with a single paddle using a "pitch stroke" or side slipping a canoe with a "scull" involves a similar technique. In China, sculling, called "lu", was also used by the 3rd century AD.
In sculling, a single blade is moved through an arc, from side to side taking care to keep presenting the blade to the water at the effective angle. The innovation introduced with the screw propeller was the extension of that arc through more than 360° by attaching the blade to a rotating shaft. Propellers can have a single blade, but in practice there are nearly always more than one so as to balance the forces involved.
The origin of the screw propeller starts with Archimedes, who used a screw to lift water for irrigation and bailing boats, so famously that it became known as Archimedes' screw. It was probably an application of spiral movement in space (spirals were a special study of Archimedes) to a hollow segmented water-wheel used for irrigation by Egyptians for centuries. Leonardo da Vinci adopted the principle to drive his theoretical helicopter, sketches of which involved a large canvas screw overhead.
In 1784, J. P. Paucton proposed a gyrocopter-like aircraft using similar screws for both lift and propulsion. At about the same time, James Watt proposed using screws to propel boats, although he did not use them for his steam engines. This was not his own invention, though; Toogood and Hays had patented it a century earlier, and it had become a common use as a means of propelling boats since that time.
By 1827, Czech-Austrian inventor Josef Ressel had invented a screw propeller which had multiple blades fastened around a conical base. He had tested his propeller in February 1826 on a small ship that was manually driven. He was successful in using his bronze screw propeller on an adapted steamboat (1829). His ship "Civetta" with 48 gross register tons, reached a speed of about six knots (11 km/h). This was the first ship successfully driven by an Archimedes screw-type propeller. After a new steam engine had an accident (cracked pipe weld) his experiments were banned by the Austro-Hungarian police as dangerous. Josef Ressel was at the time a forestry inspector for the Austrian Empire. But before this he received an Austro-Hungarian patent (license) for his propeller (1827). He died in 1857. This new method of propulsion was an improvement over the paddlewheel as it was not so affected by either ship motions or changes in draft as the vessel burned coal.
John Patch, a mariner in Yarmouth, Nova Scotia developed a two-bladed, fan-shaped propeller in 1832 and publicly demonstrated it in 1833, propelling a row boat across Yarmouth Harbour and a small coastal schooner at Saint John, New Brunswick, but his patent application in the United States was rejected until 1849 because he was not an American citizen. His efficient design drew praise in American scientific circles but by this time there were multiple competing versions of the marine propeller.
Screw propellers.
Although there was much experimentation with screw propulsion until the 1830s, few of these inventions were pursued to the testing stage, and those that were, proved unsatisfactory for one reason or another.
In 1835, two inventors in Britain, John Ericsson and Francis Pettit Smith, began working separately on the problem. Smith was first to take out a screw propeller patent on 31 May, while Ericsson, a gifted Swedish engineer then working in Britain, filed his patent six weeks later. Smith quickly built a small model boat to test his invention, which was demonstrated first on a pond at his Hendon farm, and later at the Royal Adelaide Gallery of Practical Science in London, where it was seen by the Secretary of the Navy, Sir William Barrow. Having secured the patronage of a London banker named Wright, Smith then built a 30-foot, 6-horsepower canal boat of six tons burthen called the "Francis Smith", which was fitted with a wooden propeller of his own design and demonstrated on the Paddington Canal from November 1836 to September 1837. By a fortuitous accident, the wooden propeller of two turns was damaged during a voyage in February 1837, and to Smith's surprise the broken propeller, which now consisted of only a single turn, doubled the boat's previous speed, from about four miles an hour to eight. Smith would subsequently file a revised patent in keeping with this accidental discovery.
In the meantime, Ericsson built a 45-foot screw propelled steamboat, "Francis B. Ogden" in 1837, and demonstrated his boat on the River Thames to senior members of the British Admiralty, including Surveyor of the Navy Sir William Symonds. In spite of the boat achieving a speed of 10 miles an hour, comparable with that of existing paddle steamers, Symonds and his entourage were unimpressed. The Admiralty maintained the view that screw propulsion would be ineffective in ocean-going service, while Symonds himself believed that screw propelled ships could not be steered efficiently. Following this rejection, Ericsson built a second, larger screw-propelled boat, the "Robert F. Stockton", and had her sailed in 1839 to the United States, where he was soon to gain fame as the designer of the U.S. Navy's first screw-propelled warship, USS "Princeton".
Apparently aware of the Navy's view that screw propellers would prove unsuitable for seagoing service, Smith determined to prove this assumption wrong. In September 1837, he took his small vessel (now fitted with an iron propeller of a single turn) to sea, steaming from Blackwall, London to Hythe, Kent, with stops at Ramsgate, Dover and Folkestone. On the way back to London on the 25th, Smith's craft was observed making headway in stormy seas by officers of the Royal Navy. The Admiralty's interest in the technology was revived, and Smith was encouraged to build a full size ship to more conclusively demonstrate the technology's effectiveness.
SS "Archimedes" was built in 1838 by Henry Wimshurst of London, as the world's first steamship to be driven by a screw propeller
"Archimedes" had considerable influence on ship development, encouraging the adoption of screw propulsion by the Royal Navy, in addition to her influence on commercial vessels. Trials with Smith's SS "Archimedes" led to the famous tug-of-war competition in 1845 between the screw-driven HMS "Rattler" and the paddle steamer HMS "Alecto"; the former pulling the latter backward at 2.5 knots (4.6 km/h).
She also had a direct influence on the design of another innovative vessel, Isambard Kingdom Brunel's , then the world's largest ship and the first screw-propelled steamship to cross the Atlantic Ocean in 1845. Propeller design stabilized in the 1880s.
Aircraft propellers.
The twisted aerofoil shape of modern aircraft propellers was pioneered by the Wright brothers. While some earlier engineers had attempted to model air propellers on marine propellers, they realized that a propeller is essentially the same as a wing, and were able to use data from their earlier wind tunnel experiments on wings. They also introduced a twist along the length of the blades. This was necessary to ensure the angle of attack of the blades was kept relatively constant along their length. Their original propeller blades were only about 5% less efficient than the modern equivalent, some 100 years later. The understanding of low speed propeller aerodynamics was fairly complete by the 1920s, but later requirements to handle more power in smaller diameter have made the problem more complex.
Alberto Santos Dumont, another early pioneer, applied the knowledge he gained from experiences with airships to make a propeller with a steel shaft and aluminium blades for his 14 bis biplane. Some of his designs used a bent aluminium sheet for blades, thus creating an airfoil shape. They were heavily undercambered, and this plus the absence of lengthwise twist made them less efficient than the Wright propellers. Even so, this was perhaps the first use of aluminium in the construction of an airscrew.
Propeller theory.
History.
In the second half of the nineteenth century, several theories were developed. The momentum theory or disk actuator theory—a theory describing a mathematical model of an ideal propeller—was developed by W.J.M. Rankine (1865), Alfred George Greenhill (1888) and R.E. Froude (1889). The propeller is modelled as an infinitely thin disc, inducing a constant velocity along the axis of rotation. This disc creates a flow around the propeller. Under certain mathematical premises of the fluid, there can be extracted a mathematical connection between power, radius of the propeller, torque and induced velocity. Friction is not included.
The blade element theory (BET) is a mathematical process originally designed by William Froude (1878), David W. Taylor (1893) and Stefan Drzewiecki to determine the behaviour of propellers. It involves breaking an airfoil down into several small parts then determining the forces on them. These forces are then converted into accelerations, which can be integrated into velocities and positions.
Theory of operation.
A propeller is the most common propulsor on ships, imparting momentum to a fluid which causes a force to act on the ship.
The ideal efficiency of any size propeller (free-tip) is that of an actuator disc in an ideal fluid. An actual marine propeller is made up of sections of helicoidal surfaces which act together 'screwing' through the water (hence the common reference to marine propellers as "screws"). Three, four, or five blades are most common in marine propellers, although designs which are intended to operate at reduced noise will have more blades. The blades are attached to a "boss" (hub), which should be as small as the needs of strength allow - with fixed-pitch propellers the blades and boss are usually a single casting.
An alternative design is the controllable-pitch propeller (CPP, or CRP for controllable-reversible pitch), where the blades are rotated normally to the drive shaft by additional machinery - usually hydraulics - at the hub and control linkages running down the shaft. This allows the drive machinery to operate at a constant speed while the propeller loading is changed to match operating conditions. It also eliminates the need for a reversing gear and allows for more rapid change to thrust, as the revolutions are constant. This type of propeller is most common on ships such as tugs where there can be enormous differences in propeller loading when towing compared to running free, a change which could cause conventional propellers to lock up as insufficient torque is generated. The downsides of a CPP/CRP include: the large hub which decreases the torque required to cause cavitation, the mechanical complexity which limits transmission power and the extra blade shaping requirements forced upon the propeller designer.
For smaller motors there are self-pitching propellers. The blades freely move through an entire circle on an axis at right angles to the shaft. This allows hydrodynamic and centrifugal forces to 'set' the angle the blades reach and so the pitch of the propeller.
A propeller that turns clockwise to produce forward thrust, when viewed from aft, is called right-handed. One that turns anticlockwise is said to be left-handed. Larger vessels often have twin screws to reduce "heeling torque", counter-rotating propellers, the starboard screw is usually right-handed and the port left-handed, this is called outward turning. The opposite case is called inward turning. Another possibility is contra-rotating propellers, where two propellers rotate in opposing directions on a single shaft, or on separate shafts on nearly the same axis. One example of the latter is the by the ABB Group. Contra-rotating propellers offer increased efficiency by capturing the energy lost in the tangential velocities imparted to the fluid by the forward propeller (known as "propeller swirl"). The flow field behind the aft propeller of a contra-rotating set has very little "swirl", and this reduction in energy loss is seen as an increased efficiency of the aft propeller.
An azimuthing propeller is a propeller that turns around the vertical axis. The individual airfoil-shaped blades turn as the propeller moves so that they are always generating lift in the vessel's direction of movement. This type of propeller can reverse or change its direction of thrust very quickly,
Marine propeller cavitation.
Cavitation is the formation of vapor bubbles in water near a moving propeller blade in regions of low pressure due to Bernoulli's principle. It can occur if an attempt is made to transmit too much power through the screw, or if the propeller is operating at a very high speed. Cavitation can waste power, create vibration and wear, and cause damage to the propeller. It can occur in many ways on a propeller. The two most common types of propeller cavitation are suction side surface cavitation and tip vortex cavitation.
Suction side surface cavitation forms when the propeller is operating at high rotational speeds or under heavy load (high blade lift coefficient). The pressure on the upstream surface of the blade (the "suction side") can drop below the vapor pressure of the water, resulting in the formation of a vapor pocket. Under such conditions, the change in pressure between the downstream surface of the blade (the "pressure side") and the suction side is limited, and eventually reduced as the extent of cavitation is increased. When most of the blade surface is covered by cavitation, the pressure difference between the pressure side and suction side of the blade drops considerably, as does the thrust produced by the propeller. This condition is called "thrust breakdown". Operating the propeller under these conditions wastes energy, generates considerable noise, and as the vapor bubbles collapse it rapidly erodes the screw's surface due to localized shock waves against the blade surface.
Tip vortex cavitation is caused by the extremely low pressures formed at the core of the tip vortex. The tip vortex is caused by fluid wrapping around the tip of the propeller; from the pressure side to the suction side. This demonstrates tip vortex cavitation. Tip vortex cavitation typically occurs before suction side surface cavitation and is less damaging to the blade, since this type of cavitation doesn't collapse on the blade, but some distance downstream.
Cavitation can be used as an advantage in design of very high performance propellers, in form of the supercavitating propeller. In this case, the blade section is designed such that the pressure side stays wetted while the suction side is completely covered by cavitation vapor. Because the suction side is covered with vapor instead of water it encounters very low viscous friction, making the supercavitating (SC) propeller comparably efficient at high speed. The shaping of SC blade sections however, make it inefficient at low speeds, when the suction side of the blade is wetted. (See also fluid dynamics).
A similar, but quite separate issue, is "ventilation," which occurs when a propeller operating near the surface draws air into the blades, causing a similar loss of power and shaft vibration, but without the related potential blade surface damage caused by cavitation. Both effects can be mitigated by increasing the submerged depth of the propeller: cavitation is reduced because the hydrostatic pressure increases the margin to the vapor pressure, and ventilation because it is further from surface waves and other air pockets that might be drawn into the slipstream.
The blade profile of propellers designed to be operate in a ventilated condition is often not of an aerofoil section and is a blunt ended taper instead. These are often known as "chopper" type propellers.
Forces acting on a foil.
The force (F) experienced by a foil is determined by its area (A), fluid density (ρ), velocity (V) and the angle of the foil to the fluid flow, called "angle of attack" (formula_1), where:
The force has two parts - that normal to the direction of flow is "lift" (L) and that in the direction of flow is "drag " (D). Both can be expressed mathematically:
where CL and CD are lift coefficient and drag coefficient respectively.
Each coefficient is a function of the angle of attack and Reynolds number. As the angle of attack increases lift rises rapidly from the "no lift angle" before slowing its increase and then decreasing, with a sharp drop as the "stall angle" is reached and flow is disrupted. Drag rises slowly at first and as the rate of increase in lift falls and the angle of attack increases drag increases more sharply.
For a given strength of circulation (formula_5), formula_6. The effect of the flow over and the circulation around the aerofoil is to reduce the velocity over the face and increase it over the back of the blade. If the reduction in pressure is too much in relation to the ambient pressure of the fluid, "cavitation" occurs, bubbles form in the low pressure area and are moved towards the blade's trailing edge where they collapse as the pressure increases, this reduces propeller efficiency and increases noise. The forces generated by the bubble collapse can cause permanent damage to the surfaces of the blade.
Propeller thrust.
Single blade.
Taking an arbitrary radial section of a blade at "r", if revolutions are "N" then the rotational velocity is formula_7. If the blade was a complete screw it would advance through a solid at the rate of "NP", where "P" is the pitch of the blade. In water the advance speed is rather lower, formula_8, the difference, or "slip ratio", is:
where formula_10 is the "advance coefficient", and formula_11 is the "pitch ratio".
The forces of lift and drag on the blade, "dA", where force normal to the surface is "dL":
where:
These forces contribute to thrust, "T", on the blade:
where:
formula_15
As formula_16,
From this total thrust can be obtained by integrating this expression along the blade. The transverse force is found in a similar manner:
Substituting for formula_19 and multiplying by "r", gives torque as:
which can be integrated as before.
The total thrust power of the propeller is proportional to formula_21 and the shaft power to formula_22. So efficiency is formula_23. The blade efficiency is in the ratio between thrust and torque:
showing that the blade efficiency is determined by its momentum and its qualities in the form of angles formula_25 and formula_26, where formula_26 is the ratio of the drag and lift coefficients.
This analysis is simplified and ignores a number of significant factors including interference between the blades and the influence of tip vortices.
Thrust and torque.
The thrust, "T", and torque, "Q", depend on the propeller's diameter, "D", revolutions, "N", and rate of advance, formula_28, together with the character of the fluid in which the propeller is operating and gravity. These factors create the following non-dimensional relationship:
where formula_30 is a function of the advance coefficient, formula_31 is a function of the Reynolds' number, and formula_32 is a function of the Froude number. Both formula_31 and formula_32 are likely to be small in comparison to formula_30 under normal operating conditions, so the expression can be reduced to:
For two identical propellers the expression for both will be the same. So with the propellers formula_37, and using the same subscripts to indicate each propeller:
For both Froude number and advance coefficient:
where formula_40 is the ratio of the linear dimensions.
Thrust and velocity, at the same Froude number, give thrust power:
For torque:
Actual performance.
When a propeller is added to a ship its performance is altered; there is the mechanical losses in the transmission of power; a general increase in total resistance; and the hull also impedes and renders non-uniform the flow through the propeller. The ratio between a propeller's efficiency attached to a ship (formula_44) and in open water (formula_45) is termed "relative rotative efficiency."
The "overall propulsive efficiency" (an extension of "effective power" (formula_46)) is developed from the "propulsive coefficient" (formula_47), which is derived from the installed shaft power (formula_48) modified by the effective power for the hull with appendages (formula_49), the propeller's thrust power (formula_50), and the relative rotative efficiency.
Producing the following:
The terms contained within the brackets are commonly grouped as the "quasi-propulsive coefficient" (formula_63, formula_64). The formula_63 is produced from small-scale experiments and is modified with a load factor for full size ships.
"Wake" is the interaction between the ship and the water with its own velocity relative to the ship. The wake has three parts: the velocity of the water around the hull; the boundary layer between the water dragged by the hull and the surrounding flow; and the waves created by the movement of the ship. The first two parts will reduce the velocity of water into the propeller, the third will either increase or decrease the velocity depending on whether the waves create a crest or trough at the propeller.
Types of marine propellers.
Controllable-pitch propeller.
One type of marine propeller is the controllable-pitch propeller. This propeller has several advantages with ships. These advantages include: the least drag depending on the speed used, the ability to move the sea vessel backwards, and the ability to use the "vane"-stance, which gives the least water resistance when not using the propeller (e.g. when the sails are used instead).
Skewback propeller.
An advanced type of propeller used on German Type 212 submarines is called a skewback propeller. As in the scimitar blades used on some aircraft, the blade tips of a skewback propeller are swept back against the direction of rotation. In addition, the blades are tilted rearward along the longitudinal axis, giving the propeller an overall cup-shaped appearance. This design preserves thrust efficiency while reducing cavitation, and thus makes for a quiet, stealthy design.
A small number of ships use propellers with winglets similar to those on some airplanes, reducing tip vortices and improving efficiency.
Modular propeller.
A modular propeller provides more control over the boat's performance. There is no need to change an entire prop, when there is an opportunity to only change the pitch or the damaged blades. Being able to adjust pitch will allow for boaters to have better performance while in different altitudes, water sports, and/or cruising.
Protection of small engines.
For smaller engines, such as outboards, where the propeller is exposed to the risk of collision with heavy objects, the propeller often includes a device which is designed to fail when over loaded; the device or the whole propeller is sacrificed so that the more expensive transmission and engine are not damaged.
Typically in smaller (less than 10 hp) and older engines, a narrow shear pin through the drive shaft and propeller hub transmits the power of the engine at normal loads. The pin is designed to shear when the propeller is put under a load that could damage the engine. After the pin is sheared the engine is unable to provide propulsive power to the boat until an undamaged shear pin is fitted. Note that some shear pins used to have shear grooves machined into them. Nowadays the grooves tend to be omitted. The result of this oversight is that the torque required to shear the pin rises as the cutting edges of the propeller bushing and shaft become blunted. Eventually the gears will strip instead.
In larger and more modern engines, a rubber bushing transmits the torque of the drive shaft to the propeller's hub. Under a damaging load the friction of the bushing in the hub is overcome and the rotating propeller slips on the shaft preventing overloading of the engine's components. After such an event the rubber bushing itself may be damaged. If so, it may continue to transmit reduced power at low revolutions but may provide no power, due to reduced friction, at high revolutions. Also the rubber bushing may perish over time leading to its failure under loads below its designed failure load.
Whether a rubber bushing can be replaced or repaired depends upon the propeller; some cannot. Some can but need special equipment to insert the oversized bushing for an interference fit. Others can be replaced easily.
The "special equipment" usually consists of a tapered funnel, some kind of press and rubber lubricant (soap). Often the bushing can be drawn into place with nothing more complex than a couple of nuts, washers and "allscrew" (threaded bar). If one does not have access to a lathe an improvised funnel can be made from steel tube and car body filler (as the filler is only subject to compressive forces it is able to do a good job) A more serious problem with this type of propeller is a "frozen-on" spline bushing which makes propeller removal impossible. In such cases the propeller has to be heated in order to deliberately destroy the rubber insert. Once the propeller proper is removed, the splined tube can be cut away with a grinder. A new spline bushing is of course required. To prevent the problem recurring the splines can be coated with anti-seize anti-corrosion compound.
In some modern propellers, a hard polymer insert called a "drive sleeve" replaces the rubber bushing. The splined or other non-circular cross section of the sleeve inserted between the shaft and propeller hub transmits the engine torque to the propeller, rather than friction. The polymer is weaker than the components of the propeller and engine so it fails before they do when the propeller is overloaded. This fails completely under excessive load but can easily be replaced.
See also.
Propeller variations.
Cleaver.
A cleaver is a type of propeller design especially used for boat racing. Its leading edge is formed round, while the trailing edge is cut straight. It provides little bow lift, so that it can be used on boats that do not need much bow lift, for instance hydroplanes, that naturally have enough hydrodynamic bow lift. To compensate for the lack of bow lift, a hydrofoil may be installed on the lower unit. Hydrofoils reduce bow lift and help to get a boat out of the hole and onto plane.

</doc>
<doc id="23739" url="http://en.wikipedia.org/wiki?curid=23739" title="Peter Duesberg">
Peter Duesberg

Peter H. Duesberg (born December 2, 1936 in Münster, Germany) is a German American molecular biologist and a professor of molecular and cell biology at the University of California, Berkeley. He is known for his early research into genetic aspects of cancer, and more recently for his central role in the AIDS denialism movement as a proponent of the belief that HIV is harmless and does not cause AIDS. Duesberg received acclaim early in his career for research on oncogenes and cancer. With Peter Vogt, he reported in 1970 that a cancer-causing virus of birds had extra genetic material compared with non-cancer-causing viruses, hypothesizing that this material contributed to cancer. At the age of 36, Duesberg was awarded tenure at the University of California, Berkeley, and at 49 he was elected to the National Academy of Sciences. He received an Outstanding Investigator Grant (OIG) from the National Institutes of Health (NIH) in 1986, and from 1986 to 1987 was a Fogarty Scholar-in-Residence at the NIH laboratories in Bethesda, Maryland.
Long considered a contrarian by his scientific colleagues, Duesberg began to gain public notoriety with a March 1987 article in "Cancer Research" entitled "Retroviruses as Carcinogens and Pathogens: Expectations and Reality". In this and subsequent writings, Duesberg proposed his hypothesis that AIDS is caused by long-term consumption of recreational drugs or antiretroviral drugs, and that HIV is a harmless passenger virus. In contrast, the scientific consensus is that HIV infection causes AIDS; Duesberg's HIV/AIDS claims have been addressed and rejected as erroneous by the scientific community. Reviews of his opinions in "Nature" and "Science" asserted that they were unpersuasive and based on selective reading of the literature, and that although Duesberg had a right to a dissenting opinion, his failure to fairly review evidence that HIV causes AIDS meant that his opinion lacked credibility.
Duesberg's views are cited as major influences on South African HIV/AIDS policy under the administration of Thabo Mbeki, which embraced AIDS denialism. Duesberg served on an advisory panel to Mbeki, convened in 2000. The Mbeki administration's failure to provide antiretroviral drugs in a timely manner, due in part to the influence of AIDS denialism, is thought to be responsible for hundreds of thousands of preventable AIDS deaths and HIV infections in South Africa. Duesberg disputed these findings in an article in the journal "Medical Hypotheses", but the journal's publisher, Elsevier, later retracted Duesberg's article over accuracy and ethics concerns as well as its rejection during peer review. The incident prompted several complaints to Duesberg's institution, the University of California, Berkeley, which began a misconduct investigation of Duesberg in 2009. The investigation was dropped in 2010, with University officials finding "insufficient evidence...to support a recommendation for disciplinary action."
Work.
Cancer.
Duesberg grew up during World War II, raised as a Catholic in Germany. He moved to Berkeley, California, in 1964 to work at the University of California, Berkeley, following completion of a Ph.D. in chemistry at the University of Frankfurt. In the 1970s Duesberg won international acclaim for his groundbreaking work on cancer. Duesberg's early work on cancer included being the first to identify the oncogene "v-src" from the genome of Rous sarcoma virus, a chicken virus believed to trigger tumor growth.
Duesberg disputes the importance of oncogenes and retroviruses in cancer. He supports the aneuploidy hypothesis of cancer that was first proposed in 1914 by Theodor Heinrich Boveri.
Duesberg rejects the importance of mutations, oncogenes, and anti-oncogenes entirely. Duesberg along with other researchers, in a 1998 paper published in "Proceedings of the National Academy of Sciences", reported a mathematical correlation between chromosome number and the genetic instability of cancer cells, which they dubbed "the ploidy factor," confirming earlier research by other groups that demonstrated an association between degree of aneuploidy and metastasis. Although unwilling to concur with Duesberg in throwing out a role for cancer genes, many researchers do support exploration of alternative hypotheses. Research and debate on this subject is ongoing. In 2007, "Scientific American" published an article by Duesberg on his aneuploidy cancer theory. In an editorial explaining their decision to publish this article, the editors of "Scientific American" stated: "Thus, as wrong as Duesberg surely is about HIV, there is at least a chance that he is significantly right about cancer."
AIDS.
In his 1996 book "Inventing the AIDS Virus" and in numerous journal articles and letters to the editor, Duesberg asserts that HIV is harmless and that recreational and pharmaceutical drug use, especially of zidovudine (AZT, a drug used in the treatment of AIDS) are the causes of AIDS outside Africa (the so-called Duesberg hypothesis). He considers AIDS diseases as markers for drug use, e.g. use of poppers (alkyl nitrites) among some homosexuals, asserting a correlation between AIDS and recreational drug use. This correlation hypothesis has been disproven by evidence showing that only HIV infection, not homosexuality nor recreational/pharmaceutical drug use, predicts who will develop AIDS.
Duesberg asserts that AIDS in Africa is misdiagnosed and the epidemic a "myth", claiming incorrectly that the diagnostic criteria for AIDS are different in Africa than elsewhere and that the breakdown of the immune system in African AIDS patients can be explained exclusively by factors such as malnutrition, tainted drinking water, and various infections that he presumes are common to AIDS patients in Africa. Duesberg also argues that retroviruses like HIV must be harmless to survive, and that the normal mode of retroviral propagation is mother-to-child transmission by infection in utero.
Since Duesberg published his first paper on the subject in 1987, scientists have examined and criticized the accuracy of his hypotheses on AIDS causation. Duesberg sustained a long dispute with John Maddox, then-editor of the scientific journal "Nature", demanding the right to rebut articles that HIV caused AIDS. For several years Maddox consented to this demand but ultimately refused to continue to publish Duesberg's criticisms:
[Duesberg] forfeited the right to expect answers by his rhetorical technique. Questions left unanswered for more than about ten minutes he takes as further proof that HIV is not the cause of AIDS. Evidence that contradicts his alternative drug hypothesis is on the other hand brushed aside...Duesberg will not be alone in protesting that this is merely a recipe for suppressing challenges to received wisdom. So it can be. But Nature will not so use it. Instead, what Duesberg continues to say about the causation of AIDS will be reported in the general interest. When he offers a text for publication that can be authenticated, it will if possible be published.—Maddox, 1993
A number of scientific criticisms of Duesberg's hypothesis were summarized in a review article in the journal "Science" in 1994, which presented the results of a 3-month scientific investigation into some of Duesberg's claims. In the "Science" article, science writer Jon Cohen interviewed both HIV researchers and AIDS denialists (including Duesberg himself) and examined the AIDS literature in addition to review articles written by Duesberg. The article stated:
...although the Berkeley virologist raises provocative questions, few researchers find his basic contention that HIV is not the cause of AIDS persuasive. Mainstream AIDS researchers argue that Duesberg's arguments are constructed by selective reading of the scientific literature, dismissing evidence that contradicts his theses, requiring impossibly definitive proof, and dismissing outright studies marked by inconsequential weaknesses.—Jon Cohen.
The article also stated that although Duesberg and the AIDS denialist movement have garnered support from some prominent scientists, including Nobel Prize winner Kary Mullis, most of this support is related to Duesberg's right to hold a dissenting opinion, rather than support of his specific claim that HIV does not cause AIDS. Duesberg has been described as "the individual who has done the most damage" regarding denialism, due to the apparent scientific legitimacy his scientific credentials give to his statements.
In a 2010 article on conspiracy theories in science, Ted Goertzel highlights Duesberg's opposition to the HIV/AIDS connection as an example in which scientific findings are disputed on irrational grounds, relying on rhetoric, appeal to fairness and the right to a dissenting opinion rather than on evidence. Goertzel stated that Duesberg, along with many other denialists frequently invoke the meme of a "courageous independent scientist resisting orthodoxy", invoking the name of persecuted physicist and astronomer Galileo Galilei. Regarding this comparison, Goertzel stated:
...being a dissenter from orthodoxy is not difficult; the hard part is actually having a better theory. Publishing dissenting theories is important when they are backed by plausible evidence, but this does not mean giving critics 'equal time' to dissent from every finding by a mainstream scientist.—Goertzel, 2010
Duesberg's advocacy of AIDS denialism has, by all accounts, effectively blackballed him from the worldwide scientific community.
Consequences of AIDS denialism.
In 2000, Duesberg was the most prominent AIDS denialist to sit on a 44-member Presidential Advisory Panel on HIV and AIDS convened by then-President Thabo Mbeki of South Africa. The panel was scheduled to meet concurrently with the 2000 International AIDS Conference in Durban and to convey the impression that Mbeki's doubts about HIV/AIDS science were valid and actively discussed in the scientific community. The views of the denialists on the panel, aired during the AIDS conference, received renewed attention. Mbeki later suffered substantial political fallout for his support for AIDS denialism and for opposing the treatment of pregnant HIV-positive South African women with antiretroviral medication. Mbeki partly attenuated his ties with denialists in 2002, asking them to stop associating their names with his.
In response to the inclusion of AIDS denialists on Mbeki's panel, the Durban Declaration was drafted and signed by over 5,000 scientists and physicians, describing the evidence that HIV causes AIDS as "clear-cut, exhaustive and unambiguous".
Two independent studies have concluded that the public health policies of Thabo Mbeki's government, shaped in part by Duesberg's writings and advice, were responsible for over 330,000 excess AIDS deaths and many preventable infections, including those of infants.
A 2008 Discover Magazine feature on Duesberg addresses Duesberg's role in anti-HIV drug-preventable deaths in South Africa. Jeanne Linzer interviews prominent HIV/AIDS expert Max Essex, who suggests that,
...history will judge Duesberg as either "a nut who is just a tease to the scientific community" or an "enabler to mass murder" for the deaths of many AIDS patients in Africa.
Academic misconduct investigation.
In 2009, Duesberg and co-authors including David Rasnick published an article in the journal "Medical Hypotheses", which is not peer reviewed. The article had been rejected previously by the journal "JAIDS", and a peer reviewer had warned that the authors could face scientific misconduct charges if the paper were published.
The reviewers claimed that Duesberg and his co-authors cherry-picked data, cited favorable results while ignoring unfavorable results, and quoted statements out of context. Moreover, they claim that Duesberg "[committed] a serious breach of professional ethics" by failing to state a possible conflict of interest: That co-author Rasnick previously worked for Matthias Rath, a vitamin entrepreneur who sold vitamin pills as AIDS remedies. The article was not revised in response to these criticisms.
In the article, Duesberg questioned research reporting that drugs policies implemented by the South African government on the advice of Duesberg, Rasnick and others had led to excess AIDS deaths. Observing that the overall population of South Africa has increased, Duesberg claimed that HIV must be a harmless "passenger virus" that has not caused deaths in South Africa or elsewhere. Duesberg stated that HIV does not replicate in the body and that antiviral drugs, which he calls "inevitably toxic", do not inhibit HIV. In addition, Duesberg wrote that neither he nor his co-authors had financial conflicts of interest.
Scientists expressed concerns to Elsevier, the publisher of "Medical Hypotheses", about unsupported assertions and incorrect statements by Duesberg. After an internal review and with a unanimous recommendation of rejection by five "Lancet" reviewers, Elsevier stated that the article was flawed and of potential danger to global public health. Elsevier permanently withdrew the Duesberg article and another AIDS denialist publication and asked that the editor of the journal implement a peer review process.
Letters of complaint to the University of California, Berkeley, including one from Nathan Geffen of the South African Treatment Action Campaign (TAC), prompted university officials to open an inquiry into possible academic misconduct related to false statements and failure to disclose potential conflicts of interest. The investigation was dropped in 2010, with University officials finding "insufficient evidence...to support a recommendation for disciplinary action." The investigation did not evaluate the merits of the research but found that publishing the article was protected by the principle of academic freedom.

</doc>
<doc id="23740" url="http://en.wikipedia.org/wiki?curid=23740" title="Toxin">
Toxin

A toxin (from Ancient Greek: τοξικόν "toxikon") is a poisonous substance produced within living cells or organisms; synthetic toxicants created by artificial processes are thus excluded. The term was first used by organic chemist Ludwig Brieger (1849–1919).
Toxins can be small molecules, peptides, or proteins that are capable of causing disease on contact with or absorption by body tissues interacting with biological macromolecules such as enzymes or cellular receptors. Toxins vary greatly in their severity, ranging from usually minor (such as a bee sting) to almost immediately deadly (such as botulinum toxin).
Terminology.
Toxins are often distinguished from other chemical agents by their method of production—the word toxin does not specify method of delivery (compare with venom and the narrower meaning of poison—all substances that can also cause disturbances to organisms). It simply means it is a biologically produced poison.
There was an ongoing terminological dispute between NATO and the Warsaw Pact over whether to call a toxin a biological or chemical agent, in which the NATO opted for biological agent, and the Warsaw Pact, like most other countries in the world, for chemical agent.
According to an International Committee of the Red Cross review of the Biological Weapons Convention, "Toxins are poisonous products of organisms; unlike biological agents, they are inanimate and not capable of reproducing themselves", and "Since the signing of the Convention, there have been no disputes among the parties regarding the definition of biological agents or toxins".
According to Title 18 of the United States Code, "... the term "toxin" means the toxic material or product of plants, animals, microorganisms (including, but not limited to, bacteria, viruses, fungi, rickettsiae or protozoa), or infectious substances, or a recombinant or synthesized molecule, whatever their origin and method of production..."
A rather informal terminology of individual toxins relates them to the anatomical location where their effects are most notable:
On a broader scale, toxins may be classified as either exotoxins, being excreted by an organism, or endotoxins, that are released mainly when bacteria are lysed.
Related terms are:
Biotoxins.
The term "biotoxin" is sometimes used to explicitly confirm the biological origin. Biotoxins are further classified into fungal biotoxins, or short mycotoxins, microbial biotoxins, plant biotoxins, short phytotoxins and animal biotoxins.
Toxins produced by microorganisms are important virulence determinants responsible for microbial pathogenicity and/or evasion of the host immune response.
Biotoxins vary greatly in purpose and mechanism, and can be highly complex (the venom of the cone snail contains dozens of small proteins, each targeting a specific nerve channel or receptor), or relatively small protein.
Biotoxins in nature have two primary functions:
Some of the more well known types of biotoxins include:
Environmental toxins.
The term "environmental toxin" can sometimes explicitly include synthetic contaminants such as industrial pollutants and other artificially made toxic substances. As this contradicts most formal definitions of the term "toxin", it is important to confirm what the researcher means when encountering the term outside of microbiological contexts.
Environmental toxins from food chains that may be dangerous to human health include:
Finding information about toxins.
The Toxicology and Environmental Health Information Program (TEHIP) at the United States National Library of Medicine (NLM) maintains a comprehensive toxicology and environmental health web site that includes access to toxins-related resources produced by TEHIP and by other government agencies and organizations. This web site includes links to databases, bibliographies, tutorials, and other scientific and consumer-oriented resources. TEHIP also is responsible for the Toxicology Data Network (TOXNET), an integrated system of toxicology and environmental health databases that are available free of charge on the web.
TOXMAP is a Geographic Information System (GIS) that is part of TOXNET. TOXMAP uses maps of the United States to help users visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs.
Computational resources for prediction of toxic peptides and proteins.
One of the bottlenecks in peptide/protein-based therapy is their toxicity. Recently, "in silico" models for predicting toxicity of peptides and proteins, developed by Gajendra Pal Singh Raghava's group, predict toxicity with reasonably good accuracy. The prediction models are based on machine learning technique and quantitative matrix using various properties of peptides. The prediction tool is freely accessible to public in the form of web server - ToxinPred at http://crdd.osdd.net/raghava/toxinpred/.
Misuse of the term.
When used non-technically, the term "toxin" is often applied to any toxic substance, even though the term toxicant would be more appropriate. Toxic substances not directly of biological origin are also termed poisons and many non-technical and lifestyle journalists follow this usage to refer to toxic substances in general.
In the context of alternative medicine the term is often used to refer to any substance claimed to cause ill health, ranging anywhere from trace amounts of pesticides to common food items like refined sugar or additives such as monosodium glutamate (MSG).

</doc>
<doc id="23741" url="http://en.wikipedia.org/wiki?curid=23741" title="Philadelphia Phillies">
Philadelphia Phillies

The Philadelphia Phillies are an American professional baseball team based in Philadelphia, Pennsylvania. They are the oldest continuous, one-name, one-city franchise in all of professional American sports, dating to 1883. The Phillies are a member of the Eastern Division of Major League Baseball's National League (NL). Since 2004, the team's home has been Citizens Bank Park which is located in South Philadelphia.
The Phillies have won two World Series championships (against the Kansas City Royals in 1980 and the Tampa Bay Rays in 2008) and seven National League pennants, the first of which came in 1915. The franchise has also experienced long periods of struggle. Since the first modern World Series was played in 1903, the Phillies played 77 consecutive seasons (and 97 seasons from the club's establishment) to win their first World Series—longer than any other of the 16 teams that made up the major leagues for the first half of the 20th century. The 77 season drought is the fourth longest World Series drought in Major League Baseball history. The longevity of the franchise and its history of adversity have earned it the dubious distinction of having lost the most games of any team in the history of American professional sports. Notwithstanding the collectively poor performance over the years, the Phillies have performed much better in recent seasons, winning five consecutive division titles from 2007 through 2011.
The franchise was founded in Philadelphia in 1883, replacing the team from Worcester, Massachusetts in the National League. The team has played at several stadiums in the city, beginning with Recreation Park and continuing at Baker Bowl; Shibe Park, which was later renamed Connie Mack Stadium in honor of the longtime Philadelphia Athletics manager; Veterans Stadium; and now Citizens Bank Park.
The team's spring training facilities are located in Clearwater, Florida, where its Class-A minor league affiliate Clearwater Threshers plays at Bright House Field. Its Double-A affiliate is the Reading Fightin Phils, which plays in Reading, Pennsylvania, and its Triple-A affiliate is the Lehigh Valley IronPigs, which plays in Allentown, Pennsylvania.
History.
Early history.
After being founded in 1883 as the "Quakers", the team changed its name to the "Philadelphias", after the convention of the times. This was soon shortened to "Phillies". "Quakers" continued to be used interchangeably with "Phillies" from 1883 until 1890, when the team officially became known as the "Phillies". Though the Phillies moved into a permanent home at Baker Bowl in 1887, they did not win their first pennant until nearly 30 years later, after the likes of standout players Billy Hamilton, Sam Thompson, and Ed Delahanty had departed. Player defections to the newly formed American League, especially to the cross-town Philadelphia Athletics, would cost the team dearly over the next several years. A bright spot came in 1915, when the Phillies won their first pennant, thanks to the pitching of Grover Cleveland Alexander and the batting prowess of Gavvy Cravath, who set what was then the modern major-league single-season record for home runs with 24. Poor fiscal management after their appearance in the 1915 World Series, however, doomed the Phillies to sink back into relative obscurity; from 1918 to 1948 they only had one winning season. Though Chuck Klein won the Most Valuable Player Award in 1932 and the National League Triple Crown in 1933, the team continued to flounder at the bottom of the standings for years.
Cox, Carpenter, and the "Whiz Kids" era.
After lumber baron William B. Cox purchased the team in 1943, the Phillies began a rapid rise to prominence in the National League, as the team rose out of the standings cellar for the first time in five years. As a result, the fan base and attendance at home games increased. But it soon became clear that not all was right in Cox's front office. Eventually, it was revealed by Cox that he had been betting on the Phillies and he was banned from baseball. The new owner, Bob Carpenter, Jr., scion of the Delaware-based DuPont family, tried to polish the team's image by unofficially changing its name to the "Bluejays". However, the new moniker did not take, and it was quietly dropped by 1949.
Instead, Carpenter turned his attention to the minor league affiliates, continuing an effort begun by Cox a year earlier; prior to Cox's ownership, the Phillies had paid almost no attention to player development. This led to the advent of the "Whiz Kids," led by a lineup of young players developed by the Phillies' farm system that included future Hall of Famers Richie Ashburn and Robin Roberts. Their 1950 season was highlighted by a last-day, pennant-clinching home run by Dick Sisler to lead the Phillies over the Brooklyn Dodgers and into the World Series, where they were swept by the New York Yankees four-games-to-none (although each game was close).
Comparatively, the Philadelphia Athletics finished last in 1950 and long-time manager Connie Mack retired. The team struggled on for four more years with only one winning team, and then abandoned Philadelphia, under the Johnson brothers, who bought out Mack. They began play in Kansas City in 1955. As part of the deal selling the team to the Johnson brothers, the Phillies bought Shibe Park, where both teams had played since 1938.
From lows to highs.
The Phillies sank back to mediocrity during the mid-1950s after the departure of the "Whiz Kids", their competitive futility culminating in a record that still stands: in 1961, the Phillies lost 23 games in a row (a record since 1900). But from this nadir bright spots began to appear. Though Ashburn and Roberts were gone, younger pitchers Art Mahaffey, Chris Short, and rookie Ray Culp; veterans Jim Bunning and screwballer Jack Baldschun; and fan favorites Cookie Rojas, Johnny Callison, and NL Rookie of the Year Richie Allen brought the team within a hairsbreadth of the World Series in 1964 after strong showings in 1962 and 1963. However, the Phillies squandered a six-and-a-half-game lead during the final weeks of the season that year, losing 10 games in a row with 12 games remaining and losing the pennant by one game to the St. Louis Cardinals. The "Phold of '64" is among the most notable collapses in sports history. One highlight of the season occurred on Father's Day, when Jim Bunning pitched a perfect game against the New York Mets, the first in Phillies history.
At the end of the decade, in October 1970, the Phillies played their final game in Connie Mack Stadium and prepared to move into newly built Veterans Stadium, wearing new maroon uniforms to accentuate the change. While some members of the team performed admirably during the 1970s, the Phillies still clung to their position at the bottom of the National League standings. Ten years after "the Phold", they suffered another minor collapse in August and September 1974, missing out on the playoffs yet again. But the futility would not last much longer. After a run of three straight division titles from 1976 to 1978, the Phillies won the NL East in 1980 behind pitcher Steve Carlton, outfielder Greg Luzinski, and infielders Mike Schmidt, Larry Bowa, and Pete Rose. In a memorable NLCS, with four of the five games going into extra innings, they fell behind 2–1 but battled back to squeeze past the Houston Astros on a tenth-inning, game-winning hit by center fielder Garry Maddox, and the city celebrated its first pennant in 30 years.
Facing the Kansas City Royals in the 1980 World Series, the Phillies won their first World Series championship ever in six games thanks to the timely hitting of Mike Schmidt and Pete Rose. Schmidt, who was the National League MVP that 1980 season, also won the World Series MVP award on the strength of his 8-for-21 hitting (.381 average), including game-winning hits in Game 2 and the clinching Game 6. 
This sixth, final game was also significant because it remains "the most-watched game in World Series history" with a television audience of 54.9 million viewers.
Thus, the Phillies became the last of the 16 teams that made up the major leagues from 1901 to 1961 to win a World Series. The Phillies made the playoffs twice more in the 1980s after their Series win, in 1981 and 1983, where they lost to the Baltimore Orioles in the World Series, but they would soon follow these near-misses with a rapid drop back into the basement of the National League. The 1992 season, for example, would end with the Phillies in last place in the National League East. But their fortunes were about to change.
Recent history.
The 1993 Phillies started the season by going 17–5 in April and finishing with a 97–65 season. The Phillies beat the Atlanta Braves in the 1993 National League Championship Series, four games to two, to earn the fifth pennant in franchise history, only to be defeated by the defending world series champion Toronto Blue Jays in the 1993 World Series. Toronto's Joe Carter hit a walk-off home run in Game 6 to clinch another Phillies loss. The 1994–95 Major League Baseball strike was a blow to the Phillies' attendance and on-field success, as was the arrival of the Braves in the division due to league realignment. Several stars came through Philadelphia, though few would stay, and the minor league system continued to develop its young prospects, who would soon rise to Phillies fame.
In 2001, the Phillies had their first winning season in eight years under new manager Larry Bowa, and their season record would not dip below .500 again from the 2003 season onward. In 2004, the Phillies moved to their new home, Citizens Bank Park, across the street from the Vet.
Charlie Manuel took over the reins of the club from Bowa after the 2004 season, and general manager Ed Wade was replaced by Pat Gillick in November 2005. Gillick reshaped the club as his own, sending stars away in trades and allowing the Phillies' young core to develop. After the franchise lost its 10,000th game in 2007, its core of young players, including infielders Chase Utley, Ryan Howard, and Jimmy Rollins and pitcher Cole Hamels, responded by winning the National League East division title, but they were swept by the Colorado Rockies in the Division Series. After the 2007 season, they acquired closer Brad Lidge.
In 2008, the Phillies clinched their second straight division title and defeated the Milwaukee Brewers in the Division Series to record the franchise's first post-season victory since winning the 1993 NLCS. Behind strong pitching from the rotation and stellar offensive production from virtually all members of the starting lineup, the Phillies won the 2008 National League Championship Series against the Los Angeles Dodgers; Hamels was named the series' Most Valuable Player. The Phillies would then go on to defeat the Tampa Bay Rays in 5 games for their second World Series title in their 126-year history. Hamels was named both NLCS MVP as well as World Series MVP after going 4–0 in the postseason that year.
Gillick retired as general manager after the 2008 season and was succeeded by one of his assistants, Ruben Amaro, Jr. After adding outfielder Raúl Ibañez to replace the departed Pat Burrell, the Phillies retained the majority of their core players for the 2009 season. In July, they signed three-time Cy Young Award winner Pedro Martinez and acquired 2008 American League Cy Young winner Cliff Lee before the trade deadline. On September 30, 2009, they clinched a third consecutive National League East Division title for the first time since the 1976–78 seasons. The team continued this run of success with wins over the Colorado Rockies in the NLDS (3 games to 1) and the Los Angeles Dodgers in the NLCS (4 games to 1), to become the first Phillies team to win back-to-back pennants and the first National League team since the 1996 Atlanta Braves to have an opportunity to defend their World Series title. The Phillies were unable to repeat, falling to the New York Yankees, 4 games to 2. Nevertheless, in recognition of the team's recent accomplishments, "Baseball America" named the Phillies as its Organization of the Year.
On December 16, 2009, they acquired starting pitcher Roy Halladay from the Toronto Blue Jays for three minor-league prospects, and traded Cliff Lee to the Seattle Mariners for three prospects. On May 29, 2010, Halladay pitched a perfect game against the Florida Marlins.[d]
In June 2010, the team's scheduled 2010 series against the Toronto Blue Jays at Rogers Centre was moved to Philadelphia, because of security concerns for the G-20 Summit. The Blue Jays wore their home white uniforms and batted last as the home team, and the designated hitter was used. The game was the first occasion of the use of a designated hitter in a National League ballpark in a regular-season game; Ryan Howard was the first player to fill the role.
The 2010 Phillies won their fourth consecutive NL East Division championship despite a rash of significant injuries to key players, including Ryan Howard, Chase Utley, Jimmy Rollins, Shane Victorino, and Carlos Ruiz. After dropping seven games behind the Atlanta Braves on July 21, Philadelphia finished with an MLB-best record of 97–65. The streak included a 20–5 record in September, the Phillies' best September since winning 22 games that month in 1983, and an 11–0 run in the middle of the month. The acquisition of pitcher Roy Oswalt in early August was a key step, as Oswalt won seven consecutive games in just over five weeks from August 11 through September 17. The Phillies clinched the division on September 27, behind a two-hit shutout by Halladay.
In Game 1 of the 2010 National League Division Series, Halladay threw the second no-hitter in Major League baseball postseason history, leading the Phillies over the Cincinnati Reds, 4–0. The first no-hitter in postseason history was New York Yankee pitcher Don Larsen's perfect game in the 1956 World Series. Halladay's no-hitter was the fifth time a pitcher has thrown two no-hitters in the same season, and was also the first time that one of the two occurred in the postseason. The Phillies went on to sweep the Reds in three straight games. In the 2010 National League Championship Series, the Phillies fell to the eventual World Series champion San Francisco Giants in six games.
On September 17, 2011, the Phillies won their fifth consecutive East Division championship, and on September 28, during the final game of the season, the team set a franchise record for victories in a season with 102 by beating the Atlanta Braves in 13 innings, denying their division rivals a potential Wild Card berth. Yet the Phillies lost in the NLDS to the St. Louis Cardinals – the team that won the National League Wild Card as a result of the Phillies beating the Braves. The Cardinals subsequently beat the Brewers in the NLCS and won the 2011 World Series in 7 games.
The 2012 Phillies experienced an up and down season. They played .500 ball through the first two months, but then slumped through a 9–19 stretch in June where they ended up at the bottom of the NL East by midseason. With any hope dimming, the Phillies traded key players Shane Victorino and Joe Blanton to the Los Angeles Dodgers, and Hunter Pence to the San Francisco Giants before the trade deadline. However, a hot start in the second half of the season put the Phillies back on the postseason hunt, though any hope was eventually extinguished with a loss to the Washington Nationals on September 28, thus the Phillies missed the postseason for the first time since 2006.
During the 2013 season, the team struggled again, and was unable to consistently play well for the majority of the season. On August 16, 2013, with the team's record at 53-68, the Phillies fired manager Charlie Manuel, who had managed the team since 2005. Phillies third base coach, Ryne Sandberg, was promoted to Interim manager. Manuel spent over nine years as the manager, leading Philadelphia to its first World Series victory in nearly thirty years. Manuel amassed an overall record of 780-636, making him the winningest manager in the franchise's history. The 2013 Phillies ended up with a record of 73-89, their first losing season since 2002.
One of the few bright spots of the 2014 Phillies season happened on September 1 against division rival Atlanta Braves, when starter Cole Hamels, and relievers Jake Diekman, Ken Giles, and Jonathan Papelbon combined for a no-hitter in Turner Field, for 7-0 victory over Atlanta.
Team uniform.
Current uniform.
The current team colors, uniform, and logo date to 1992. The main team colors are red and white, with blue serving as a prominent accent. The team name is written in red with a blue star serving as the dot over the "i"s, and blue piping is often found in Phillies branded apparel and materials. The team's home uniform is white with red pinstripes, lettering and numbering. The road uniform is traditional grey with red lettering/numbering. Both bear a script-lettered "Phillies" logo, with the aforementioned star dotting the "i"s across the chest, and the player name and number on the back. Hats are red with a single stylized "P". The uniforms and logo are very similar to those used during the "Whiz Kids" era from 1950 to 1969.
Along with its National League compadres, the St. Louis Cardinals, the Phillies are one of two teams in Major League baseball which utilize chain stitching in its chest emblem.
In 2008, the Phillies introduced an alternate, cream-colored uniform during home day games in tribute to their 125th anniversary. The uniforms are similar to those worn from 1946 through 1949, featuring red lettering bordered with blue piping and lacking pinstripes. The accompanying cap is blue with a red bill and a red stylized "P." The uniforms were announced on November 29, 2007, when Phillies shortstop Jimmy Rollins, pitcher Cole Hamels, and Hall of Fame pitcher Robin Roberts modeled the new uniforms.
For the 2009 season the Phillies added black, circular "HK" patches to their uniforms over their hearts in honor of broadcaster Harry Kalas, who died April 13, 2009, just before he was to broadcast a Phillies game. From Opening Day through July 26, 2009, the Phillies wore 2008 World Champions patches on the right sleeve of their home uniforms. In 2010, the Phillies added a black patch with a white "36" on the sleeves of their jerseys to honor Roberts, who died on May 6. Roberts' No. 36 had been previously retired by the team. In 2011, the Phillies added a black circular patch with a 'B' in honor of minority owners Alexander and John Buck, who died in late 2010.
The Phillies are one of four teams in Major League Baseball that do not display the name of their city, state, or region on their road jerseys, joining the Los Angeles Angels of Anaheim, St. Louis Cardinals, and the . The Phillies are the only team that also displays the player's number on one sleeve except on the alternate jersey, in addition to the usual placement on the back of the jersey.
Batting practice.
The Phillies were an early adopter of the batting practice jersey in 1977, wearing a maroon v-necked top with the "Phillies" script name across the chest, as well as the player name and number on the back and a player number on the left sleeve, all in white. Larry Bowa, Pete Rose, and Mike Schmidt wore this maroon batting jersey in place of their road jersey during the 1979 All-Star Game in Seattle. Currently, during spring training, the Phillies wear solid red practice jerseys with pinstriped pants for Grapefruit League home games. The red jerseys are worn with grey pants on the road.
Former uniforms.
From 1970 to 1991, the Phillies sported colors, uniforms, and a logo that were noticeably different from what had come before, or since, but that were widely embraced by even traditionally minded fans. A dark burgundy was adopted as the main team color, with a classic pinstripe style for home uniforms. Blue was almost entirely dropped as part of the team's official color scheme, except in one area; a pale blue (as opposed to traditional grey) was used as the base-color for away game uniforms. Yet the most important aspect of the 1970 uniform change was the adoption of one of the more distinctive logos in sports; a Phillies "P" that, thanks to its unique shape and "baseball stitched" center swirl, remained instantly recognizable and admired, long after its regular use had ended. It was while wearing this uniform style and color motif that the club achieved its most enduring success, including a World Series title in 1980 and another World Series appearance in 1983. Its continued popularity with fans is still evident, as even today Phillies home games can contain many fans sporting caps, shirts, and/or jackets emblazoned with the iconic "P" and burgundy color scheme. The current Phillies team has worn the burgundy and powder blue throwbacks whenever their opponents are wearing throwback uniforms from that era.
Controversial uniform changes.
In 1979, the Phillies front office modified the uniform into an all-burgundy version with white trimmings, to be worn for Saturday games. They were called "Saturday Night Specials" and were worn for the first and last time on May 19, 1979, a 10–5 loss to the Expos. The immediate reaction of the media, fans, and players alike was negative, with many describing the despised uniforms as pajama-like. As such, the idea was hastily abandoned. Mike Schmidt did wear the uniform during the MLB All-Star Tour of Japan following the 1979 season. The final appearance on field (to date) of this uniform was during the closing ceremonies at Veterans Stadium on September 28, 2003. There was a rather large procession of players during the post game ceremony, most in uniform. Former pitcher Larry Christenson, the starting pitcher in the original game, came out wearing this old burgundy uniform, and was the only one to do so.
Another uniform controversy arose in 1994 when the Phillies introduced blue caps on Opening Day which were to be worn for home day games only. The caps were unpopular with the players, who considered them bad luck after two losses and wanted them discontinued. Management wanted to keep using the caps as planned, as they sold well among fans. A compromise was reached as the players agreed to wear them for weekday games while returning to the customary red caps for Sunday afternoon games. In all, the Phillies wore the "unlucky" blue caps for seven games in 1994, losing six (the lone victory a 5-2 triumph over the Florida Marlins on June 29). A different blue cap was introduced in 2008 as part of the alternate home uniform for day games, a throwback to the late 1940s.
Rivalries.
New York Mets.
The rivalry between the New York Mets and the Phillies was said to be among the "hottest" rivalries in the National League. The two National League East divisional rivals have met each other recently in playoff, division, and Wild Card races.
Aside from several brawls in the 1980s, the rivalry remained low-key before the 2006 season, as the teams had seldom been equally good at the same time. Since 2006, the teams have battled for playoff position. The Mets won the division in 2006 and contended in 2007 and 2008, while the Phillies won five consecutive division titles from 2007 to 2011. The Phillies' 2007 Eastern Division Title was won on the last day of the season as the Mets lost a seven-game lead with seventeen games remaining.
Historical rivalries.
City Series: Philadelphia Athletics.
The City Series was the name of a series of baseball games played between the Philadelphia Athletics of the American League and the Phillies that ran from 1903 through 1955. After the A's move to Kansas City, Missouri in 1955, the City Series rivalry came to an end. The teams have since faced each other in Interleague play (since its introduction in 1997) but the rivalry has effectively died in the intervening years since the A's left Philadelphia. In 2014, when the A's faced the Phillies in inter-league play at O.Co Coliseum, the Athletics didn't bother to mark the historical connection, going so far as to have a Connie Mack promotion the day before the series while the Texas Rangers were in Oakland.
The first City Series was held in 1883 between the Phillies and the American Association's Athletics. When the Athletics first joined the American League, the two teams played each other in a spring and fall series. No City Series was held in 1901 and 1902 due to legal warring between the National and American Leagues.
Pittsburgh Pirates.
The rivalry between the Phillies and the Pittsburgh Pirates was considered by some to be one of the best rivalries in the National League. The rivalry started when the Pittsburgh Pirates entered National League play in their fifth season of 1887, four years after the Phillies.
The Phillies and the Pirates had remained together after the National League split into two divisions in 1969. During the period of two-division play (1969 to 1993), the two National League East division rivals won the two highest numbers of division championships, reigning exclusively as NL East champions in the 1970s and again in the early 1990s, the Pirates 9, the Phillies 6; together, the two teams' 15 championships accounted for more than half of the 25 NL East championships during that span.
After the Pirates moved to the National League Central in 1994, the teams face each other only in two series each year and the rivalry has diminished. However, many fans, especially older ones, retain their dislike for the other team and regional differences between Eastern and Western Pennsylvania still fuel the rivalry. The rivalry between the Philadelphia Flyers and the Pittsburgh Penguins in the National Hockey League is also fiercely contested.
Achievements.
Awards.
Five Phillies have won MVP awards during their career with the team. Mike Schmidt leads with three wins, with back-to-back MVPs in 1980 and 1981, and in 1986 as well. Chuck Klein (1932), Jim Konstanty (1950), Ryan Howard (2006), and Jimmy Rollins (2007) all have one. Pitcher Steve Carlton leads the team in Cy Young Award wins with four (1972, 1977, 1980, and 1982), while John Denny (1983), Steve Bedrosian (1987), and Roy Halladay (2010) each have one. Four Phillies have won Rookie of the Year honors as well. Jack Sanford won in 1957, and Dick Allen won in 1964. Third baseman Scott Rolen brought home the honors in 1997, while Howard was the most recent Phillies winner in 2005. In doing so, Howard became only the second player in MLB history to win Rookie of the Year and Most Valuable Player in consecutive years, Cal Ripken, Jr. of the Baltimore Orioles being the first.
Of the fifteen players who have hit four home runs in one game, three were Phillies at the time (more than any other team). Ed Delahanty was the first, hitting his four in Chicago's West Side Park on July 13, 1896. Chuck Klein repeated the feat nearly 40 years later to the day, on July 10, 1936, at Pittsburgh's Forbes Field. Forty years later, on April 17, 1976, Mike Schmidt became the third, also hitting his in Chicago, these coming at Wrigley Field.
Wall of Fame.
From 1978 to 2003, the Phillies inducted one former Phillie and one former member of the Philadelphia Athletics per year. Since 2004 they have inducted one Phillie annually. Players must be retired and must have played at least four years with the Phillies or Athletics. The last six years' inductees to the Wall of Fame are listed below:
The following inductees have also been elected to the Philadelphia Sports Hall of Fame: Richie Ashburn, Steve Carlton, Robin Roberts, Mike Schmidt, broadcaster Harry Kalas, Grover Cleveland Alexander, Del Ennis, Chuck Klein, Ed Delahanty, Larry Bowa, Tug McGraw, Dick Allen, Curt Simmons, Johnny Callison, Greg Luzinski, and Curt Schilling.
Centennial Team.
In 1983, rather than inducting a player into the Wall of Fame, the Phillies selected their Centennial Team, commemorating the best players of the first 100 years in franchise history. See Philadelphia Baseball Wall of Fame#Centennial Team.
Retired numbers.
The Phillies have retired six numbers, and honored two additional players with the letter "P." Grover Cleveland Alexander played with the team in the era before Major League Baseball used uniform numbers, and Chuck Klein wore a variety of numbers with the team during his career. Of the six players with retired numbers, five were retired for their play with the Phillies and one, 42, was universally retired by Major League Baseball when they honored the fiftieth anniversary of Jackie Robinson's breaking the color barrier.
Community.
Charitable contributions.
The Phillies have supported amyotrophic lateral sclerosis research (also known as Lou Gehrig's disease) with the "Phillies Phestival" since 1984. The team raised over US$750,000 for ALS research at their 2008 festival, compared with approximately $4,500 at the inaugural event in 1984; the event has raised a total of over $10 million in its history. The ALS Association of Philadelphia is the Phillies' primary charity, and the hospitals they support include Pennsylvania Hospital, Thomas Jefferson University Hospital, and Hahnemann University Hospital. Former Phillies pitchers Geoff Geary, now with the Houston Astros and who lost a friend to the disease, and Curt Schilling, who retired with the Boston Red Sox, are both still involved with the Phillies' cause.
Phanatic about Education
The Philadelphia Phillies have shown to be a big supporter of reading and overall education. The Phillies want to use baseball in a positive way to help support education for students. The Phillies have a reading incentive program called Phanatic About Reading which is designed to encourage students from kindergarten to eighth grade to read for a minimum of 15 minutes a night. This reading program is to help students with their literacy skills and comprehension. Phillies Phundamentals is another educational program that is designed to make learning fun and support academic skills by using baseball. This program is offered through after school and summer camps.
The Phillies club will celebrate teachers during the 12th Annual Teacher Appreciation Night.
Fan support.
Phillies fans have earned a reputation over the years for their occasional unruly behavior. In the 1960s, radio announcers for visiting teams would frequently report on the numerous fights breaking out in Connie Mack Stadium. Immediately after the final game at the old park, many fans ran onto the field or dislodged parts of the ballpark to take home with them. Later, at Veterans Stadium, the 700 Level gained a reputation for its "hostile taunting, fighting, public urination and general strangeness."
Phillies fans are known for harsh criticism of their own stars such the 1964 Rookie of the Year Richie Allen and Hall of Fame third baseman Mike Schmidt. The fans, however, are just as well known for heckling the visiting team. Los Angeles Dodgers pitcher Burt Hooton's poor performance during game three of the 1977 National League Championship Series has often been attributed to the crowd's taunting. J. D. Drew, the Phillies' first overall draft pick in the amateur draft of 1997, never signed with the Phillies following a contract dispute with the team, instead re-entering the draft the next year to be drafted by the St. Louis Cardinals. Phillies fans were angered over this disrespect and debris, including two D batteries, was hurled at Drew during an August 1999 game. Subsequent visits by Drew to Philadelphia continue to be met with sustained booing from the Phillies fans.
Many sports writers have noted the passionate presence of Phillies fans, including Allen Barra, who wrote that the biggest roar he ever heard from Philadelphia fans was in 1980 when Tug McGraw, in the victory parade after the World Series, told New York fans they could "take this championship and shove it."
When the Phillies moved to Veteran's Stadium, they hired a group of young ladies to serve as ushers. These women wore maroon-colored outfits featuring hot pants and were called the Hot Pants Patrol. The team also introduced a pair of mascots, attired in colonial garb and named Philadelphia Phil and Phyllis. In addition to costumed characters, animated Phil and Phylis figures mounted on the center field facade would "hit" the Liberty Bell after a Phillie home run. This pair of mascots never achieved any significant level of popularity with fans and were eventually discontinued. In 1978, the team introduced a new mascot, the Phillie Phanatic, who has been called "baseball's best mascot", which has been much more successful and has become closely associated with the marketing of the team.
In Phillies fan culture, it is also not unusual to replace an "f" with a "ph" in words, such as the Phillie Phanatic.
The club surpassed 100 consecutive sellouts on August 19, 2010, selling out over 50% of their home games and averaging an annual attendance of over 3.1 million fans since moving to Citizens Bank Park; on April 3, 2011, the team broke the three-game series attendance record at the ballpark, having 136,254 fans attend the opening weekend against the Houston Astros.
In 2011 and 2012, the Phillies led the league in attendance with 3,680,718 and 3,565,718 fans, respectively, coming out to watch Phillies baseball.
Season-by-season records.
The records of the Phillies' last eight seasons in Major League Baseball are listed below.
Team managers.
Over 126 seasons, the Phillies franchise has employed 51 managers. The duties of the team manager include team strategy and leadership on and off the field. Seven managers have taken the Phillies to the postseason, with Danny Ozark and Charlie Manuel each leading the team to three playoff appearances. Manuel and Dallas Green are the only Phillies managers to win a World Series: Green in 1980 against the Kansas City Royals; and Manuel in 2008 against the Tampa Bay Rays. Gene Mauch is the longest-tenured manager in franchise history, with 1,332 games of service in parts of eight seasons (1960–1968). The records and accomplishments of the last five Phillies' managers are shown below.
Radio and television.
As of 2014, the Phillies' flagship radio stations are WIP-FM (94.1 FM) and WPHT (1210 AM), both owned by CBS Radio. Scott Franzke and Jim Jackson provide play-by-play on the radio, with Larry Andersen as the color commentator. Meanwhile, NBCUniversal (a unit of Philadelphia-based Comcast) handles local television broadcasts through its properties Comcast SportsNet, WCAU, and Comcast Network. Tom McCarthy calls play-by-play for the television broadcasts, with Jamie Moyer and Matt Stairs providing color commentary.
Spanish language broadcasts are on WDAS (1480 AM) with Danny Martinez on play-by-play and Bill Kulik and Juan Ramos on color commentary.
Other popular Phillies broadcasters through the years include By Saam from 1939 to 1975, Bill Campbell from 1962 to 1970, Richie Ashburn from 1963 to 1997, and Harry Kalas from 1971 to 2009. Kalas, a 2002 recipient of the Ford Frick Award and an icon in the Philadelphia area, called play-by-play in the first three and last three innings on television and the fourth inning on the radio until his death on April 13, 2009.
At Citizens Bank Park, the restaurant built into the base of the main scoreboard is named "Harry the K's" in Kalas's honor. After Kalas's death, the Phillies' TV-broadcast booth was renamed "The Harry Kalas Broadcast Booth". It is directly next to the radio-broadcast booth, which is named "The Richie 'Whitey' Ashburn Broadcast Booth". When the Phillies win at home, Kalas' rendition of the song "High Hopes", which he would sing when the Phillies had clinched a playoff berth or advanced in the playoffs, is played as fans file out of the stadium. In addition, when a Phillies player hits a home run a recording of Kalas' famous "That ball is outta here!" home run call is played. The only player the call is not played for is Chase Utley, who was once the subject of another famous Kalas call, "Chase Utley, you are The Man!" When Utley hits a home run, that call is played.
In 2011, the Phillies unveiled a statue of Harry Kalas at Citizens Bank Park. The statue was funded by Phillies fans and the statue was designed and constructed by a Phillies fan.
The Phillies' public-address (PA) announcer is Dan Baker, who started in the 1972 season.
In 2011, the Phillies spent $10 million to upgrade the video system at Citizens Bank Park, including a new display screen in left field, the largest in the National League.

</doc>
<doc id="23743" url="http://en.wikipedia.org/wiki?curid=23743" title="Phanerozoic">
Phanerozoic

The Phanerozoic (British English Phanærozoic) is the current geologic eon in the geologic timescale, and the one during which abundant animal and plant life has existed. It covers 541.0 ± 1.0 million years and goes back to the period when diverse hard-shelled animals first appeared. Its name derives from the Ancient Greek words φανερός (fanerós) and ζωή (zo̱í̱), meaning "visible life", since it was once believed that life began in the Cambrian, the first period of this eon. The time before the Phanerozoic, called the "Precambrian" supereon, is now divided into the Hadean, Archaean and Proterozoic eons. Plant life also appeared from early in the Phanerozoic eon.
The time span of the Phanerozoic includes the rapid emergence of a number of animal phyla; the evolution of these phyla into diverse forms; the emergence and development of complex plants; the evolution of fish; the emergence of insects and tetrapods; and the development of modern faunas. During this time span tectonic forces caused the continents to move and eventually collect into a single landmass known as Pangaea, which then separated into the current continental landmasses.
Proterozoic-Phanerozoic boundary.
The Proterozoic-Phanerozoic boundary is at 541.0 ± 1.0 million years ago. In the 19th century, the boundary was set at time of appearance of the first abundant animal (metazoan) fossils. But several hundred groups (taxa) of metazoa of the earlier Proterozoic era have been identified since systematic study of those forms started in the 1950s. Most geologists and paleontologists would probably set the Proterozoic-Phanerozoic boundary either at the classic point where the first trilobites and reef building animals (archaeocyatha) such as corals and others appear; at the first appearance of a complex feeding burrow called "Treptichnus pedum"; or at the first appearance of a group of small, generally disarticulated, armored forms termed 'the small shelly fauna'. The three different dividing points are within a few million years of each other.
In the older literature, the term "Phanerozoic" is generally used as a label for the time period of interest to paleontologists, but that use of the term seems to be falling into disuse in more modern literature.
Eras.
The Phanerozoic is divided into three eras: the Paleozoic, the Mesozoic, and the Cenozoic, and consisting of 12 periods: the Cambrian, the Ordovician, the Silurian, the Devonian, the Carboniferous, the Permian, the Triassic, the Jurassic, the Cretaceous, the Paleogene, the Neogene, and the Quaternary. The Paleozoic features the rise of fish, amphibians, reptiles, and life in general. The Mesozoic is ruled by the reptiles, and features the evolution of mammals, birds and more famously, dinosaurs. The Cenozoic is the time of the mammals, and more recently, humans.
Paleozoic Era.
The Paleozoic is a time in earth's history when complex life forms evolved, took their first breath of oxygen on dry land, and when the forerunners of all life on earth began to diversify. There are six periods in the Paleozoic era: the Cambrian, the Ordovician, the Silurian, the Devonian, the Carboniferous and the Permian.
Cambrian.
The Cambrian spans from 540 million years to 485 million years ago and is the first period of the Paleozoic and of the Phanerozoic Eon. The Cambrian sparks a boom in evolution in an event known as the Cambrian Explosion in which the largest number of creatures evolve in the history of Earth during one period. Creatures like algae evolve, but most of the water is populated by armored arthropods, like trilobites. Almost all marine phyla evolved in this period. During this time, the super-continent Rodinia begins to break up, most of which becomes the super-continent Gondwana.
Ordovician.
The Ordovician spans from 485 million years to 440 million years ago. The Ordovician is a time in earths history in which many species still prevalent today evolved, such as primitive fish, cephalopods, and coral. The most common forms of life, however, were trilobites, snails and shellfish. More importantly, the first arthropods went ashore to colonize the empty continent of Gondwana. By the end of the period, Gondwana was at the south pole, early North America had collided with Europe, closing the Atlantic Ocean. Glaciation of Africa resulted in a major drop in sea level, killing off all life that staked a claim along coastal Gondwana. Glaciation caused a snowball earth, and the Ordovician-Silurian extinction in which 60% of marine invertebrates and 25% of families went extinct, and is considered the first mass extinction and the second deadliest extinction.
Silurian.
The Silurian spans from 440 million years to 415 million years ago. The Silurian saw the healing of the earth that recovered from the snowball earth. This period saw the mass evolution of fish, as jaw-less fish became more numerous, jawed fish evolved, and the first freshwater fish evolved, though arthropods, such as sea scorpions, were still apex predators. Fully terrestrial life evolved, which included early arachnids, fungi, and centipedes. Also, the evolution of vascular plants ("Cooksonia") allowed plants to gain a foothold on land. These early plants are the forerunners of all plant life on land. During this time, there are four continents: Gondwana (Africa, South America, Australia, Antarctica, Siberia), Laurentia (North America), Baltica (Northern Europe), and Avalonia (Western Europe). The recent rise in sea levels provided many new species to thrive in water.
Devonian.
The Devonian spans from 415 million years to 360 million years ago. Also known as "The Age of the Fish", the Devonian features a huge diversification of fish, including armored fish like Dunkleosteus and lobe-finned fish which eventually evolved into the first tetrapods. On land, plant groups diversified incredibly in an event known as the Devonian Explosion where the first trees evolved, as well as seeds. This event also diversified arthropod life. The first amphibians also evolved, and the fish were now at the top of the food chain. Near the end of the Devonian, 70% of all species went extinct in an event known as the Late Devonian extinction and is the second mass extinction event the world has seen.
Carboniferous.
The Carboniferous spans from 360 million to 300 million years ago. During this time, average global temperatures were exceedingly high; the early Carboniferous averaged at about 20 degrees Celsius (but cooled down to 10 degrees during the Middle Carboniferous). Tropical swamps dominated the earth, and the large amounts of trees created much of the carbon for the coal that is used today (hence the name "Carbon-iferous"). Perhaps the most important evolutionary development of the time was the evolution of amniotic eggs, which allowed amphibians to head farther inland and remain the dominant vertebrate throughout the duration of this period. Also, the first reptiles and synapsids evolved in the swamps. Throughout the Carboniferous, there was a cooling pattern, which eventually led to the glaciation of Gondwana as much of it was situated around the south pole in an event known as the Permo-Carboniferous glaciation or the Carboniferous Rainforest Collapse.
Permian.
The Permian spans from 300 million to 250 million years ago and was the last period of the Paleozoic. At the beginning, all continents formed together to form the super-continent Pangaea and had one ocean called Panthalassa. The earth was very dry during this time, with harsh seasons as the climate of the interior of Pangaea wasn't regulated by large bodies of water. Reptiles and synapsids flourished in the new dry climate. Creatures such as Dimetrodon and Edaphosaurus ruled the new continent. The first conifers evolve, and dominate the terrestrial landscape. Nearing the end of the Permian, however, Pangaea got drier and drier. The interior was nothing but dry deserts, and new species such as Scutosaurus and Gorgonopsid filled the empty desert. Eventually, they disappeared, along with 95% of all life on earth in an event simply known as "the Great Dying", and is the third mass extinction event of the world.
Mesozoic Era.
Also known as "the Age of the dinosaurs", the Mesozoic features the rise of reptiles on their 150 million year conquest to rule the earth from the seas, the land, and even in the air. There are 3 periods in the Mesozoic: the Triassic, the Jurassic, and the Cretaceous.
Triassic.
The Triassic ranges from 250 million to 200 million years ago. The Triassic is a desolate transitional state in Earth's history between the Permian Extinction and the lush Jurassic Period. It has three major epochs: the Early Triassic, the Middle Triassic and the Late Triassic.
The Early Triassic lived between 250 million to 247 million years ago and was dominated by deserts as Pangaea had not yet broken up, thus the interior was nothing but arid. The Earth had just witnessed a massive die-off in which 95% of all life went extinct. The most common life on earth were Lystrosaurus, Labyrinthodont, and Euparkeria along with many other creatturesx that managed to survive the Great Dying. Temnospondyli evolved during this time and would be the dominant predator for much of the Triassic. 
The Middle Triassic spans from 247 million to 237 million years ago. The Middle Triassic featured the beginnings of the breakup of Pangaea, and the beginning of the Tethys Sea. The ecosystem had recovered from the devastation that was the Great Dying. Phytoplankton, coral, and crustaceans all had recovered, and the reptiles began to get bigger and bigger. New aquatic reptiles evolved such as Ichthyosaurs and Nothosaurs. Meanwhile on land, Pine forests flourished, bringing along mosquitoes and fruit flies. The first ancient crocodilians evolved, which sparked competition with the large amphibians that had since rule the freshwater world.
The Late Triassic spans from 237 million to 200 million years ago. Following the bloom of the Middle Triassic, the Late Triassic featured frequent heat spells, as well as moderate precipitation (10-20 inches per year). The recent warming led to a boom of reptilian evolution on land as the first true dinosaurs evolve, as well as pterosaurs. All this climactic change, however, resulted in a large die-out known as the Triassic-Jurassic extinction event, in which all archosaurs (excluding ancient crocodiles), synapsids, and almost all large amphibians went extinct, as well as 34% of marine life in the fourth mass extinction event of the world. The cause is debatable.
Jurassic.
The Jurassic ranges from 200 million years to 145 million years ago and features 3 major epochs: The Early Jurassic, the Middle Jurassic, and the Late Jurassic.
The Early Jurassic spans from 200 million years to 175 million years ago. The climate was much more humid than the Triassic, and as a result, the world was very tropical. In the oceans, Plesiosaurs, Ichthyosaurs and Ammonites fill waters as the dominant races of the seas. On land, dinosaurs and other reptiles stake their claim as the dominant race of the land, with species such as Dilophosaurus at the top. The first true crocodiles evolved, pushing out the large amphibians to near extinction. All-in-all, reptiles rise to rule the world. Meanwhile, the first true mammals evolve, but never exceed the height of a shrew.
The Middle Jurassic spans from 175 million to 163 million years ago. During this epoch, reptiles flourished as huge herds of sauropods, such as Brachiosaurus and Diplodicus, filled the fern prairies of the Middle Jurassic. Many other predators rose as well, such as Allosaurus. Conifer forests made up a large portion of the forests. In the oceans, Plesiosaurs were quite common, and Ichthyosaurs were flourishing. This epoch was the peak of the reptiles. 
The Late Jurassic spans from 163 million to 145 million years ago. The Late Jurassic featured a massive extinction of sauropods and Ichthyosaurs due to the separation of Pangaea into Laurasia and Gondwana in an extinction known as the Jurassic-Cretaceous extinction. Sea levels rose, destroying fern prairies and creating shallows in its wake. Ichthyosaurs went extinct whereas sauropods, as a whole, did not die out in the Jurassic; in fact, some species, like the Titanosaurus, lived up to the K-T extinction. The increase in sea-levels opened up the Atlantic sea way which would continue to get larger over time. The divided world would give opportunity for the diversification of new dinosaurs.
Cretaceous.
The Cretaceous is the longest era in the Mesozoic, ranging from 145 million to 65 million years ago, but has only two periods: the Early Cretaceous, and the Late Cretaceous. 
The Early Cretaceous spans from 145 million to 100 million years ago. The Early Cretaceous saw the expansion of seaways, and as a result, the decline and extinction of sauropods (except in South America). Many coastal shallows were created, and that caused Ichthyosaurs to die out. Mosasaurs evolved to replace them as head of the seas. Some island-hopping dinosaurs, like Eustreptospondylus, evolved to cope with the coastal shallows and small islands of ancient Europe. Other dinosaurs rose up to fill the empty space that the Jurassic-Cretaceous extinction left behind, such as Carcharodontosaurus and Spinosaurus. Of the most successful would be the Iguanodon which spread to every continent. Seasons came back into effect an the poles got seasonally colder, but dinosaurs still inhabited this area like the Leaellynasaura which inhabited the polar forests year-round, and many dinosaurs migrated there during summer like Muttaburrasaurus. Since it was too cold for crocodiles, it was the last stronghold for large amphibians, like Koolasuchus. Pterosaurs got larger as species like Tapejara and Ornithocheirus evolved. More importantly, the first true birds evolved which sparked competition between them and the pterosaurs.
The Late Cretaceous spans from 100 million to 65 million years ago. The Late Cretaceous featured a cooling trend that would continue on in the Cenozoic period. Eventually, tropics were restricted to the equator and areas beyond the tropic lines featured extreme seasonal changes in weather. Dinosaurs still thrived as new species such as Tyrannosaurus, Ankylosaurus, Triceratops and Hadrosaurs dominated the food web. Pterosaurs, however, were going into a decline as birds took to the skies. The last pterosaur to die off was Quetzalcoatlus. Giant crocodiles such as Deinosuchus and Sarcosuchus pushed out the last Temnospondyl into extinction. Marsupials evolved within the large conifer forests as scavengers. In the oceans, Mosasaurs ruled the seas to fill the role of the Ichthyosaurs, and huge plesiosaurs, such as Elasmosaurus, evolved. Also, the first flowering plants evolved.
At the end of the Cretaceous, the Deccan traps and other volcanic eruptions were poisoning the atmosphere. As this was continuing, it is thought that a large meteor smashed into earth, creating the Chicxulub Crater in an event known as the K-T Extinction, the fifth and most recent mass extinction event, in which 75% of life on earth went extinct, including all non-avian dinosaurs. Everything over 10 kilograms went extinct. The age of the dinosaurs was officially over.
Cenozoic Era.
The Cenozoic features the rise of mammals on their conquest to rule the land, as the dinosaurs have now left a huge opening as top dog. There are three division of the Cenezoic: the Paleogene, the Neogene and Quaternary.
Paleogene.
The Paleogene spans from the extinction of the dinosaurs, some 65 million years ago, to the dawn of the Neogene twenty three million years ago. It features three epochs: the Paleocene, Eocene and Oligocene. 
The Paleocene ranged from 65 million to 55 million years ago. The Paleocene is a transitional point between the devastation that is the K-T extinction, to the rich jungles environment that is the Early Eocene. The Early Paleocene saw the recovery of the earth. The continents began to take their modern shape, but all continents (and India) were separated from each other. Afro-Eurasia is separated by the Tethys Sea, and the Americas are separated by the strait of Panama, as the isthmus has not yet formed. This epoch features a general warming trend, with jungles eventually reaching the poles. The oceans were dominated by sharks as the large reptiles that had once ruled went extinct. Archaic mammals filled the world such as creodonts and early primates that evolved during the Mesozoic, and as a result, there was nothing over 10 kilograms. Mammals are still quite small.
The Eocene Epoch ranged from 55 million years to 33 million years ago. In the Early-Eocene, life was small and living in cramped jungles, much like the Paleocene. There was nothing over the weight of 10 kilograms. Among them were early primates, whales and horses along with many other early forms of mammals. At the top of the food chains were huge birds, such as Gastornis. It is the only time in recorded history that birds ruled the world (excluding their ancestors, the dinosaurs). The temperature was 30 degrees Celsius with little temperature gradient from pole to pole. In the Mid-Eocene, the circum-Antarctic current between Australia and Antarctica formed which disrupted ocean currents worldwide and as a result caused a global cooling effect, shrinking the jungles. This allowed mammals to grow to mammoth proportions, such as whales which are, by now, almost fully aquatic. Mammals like Andrewsarchus were now at the top of the food-chain and sharks were replaced by whales such as Basilosaurus as rulers of the seas. The Late-Eocene saw the rebirth of seasons, which caused the expansion of savanna-like areas, along with the evolution of grass.
The Oligocene Epoch spans from 33 million to 23 million years ago. The Oligocene feature the expansion of grass which had led to many new species to evolve, including the first elephants, cats, dogs, marsupials and many other species still prevalent today. Many other species of plants evolved in this period too, such as the evergreen trees. A cooling period was still in effect and seasonal rains were as well. Mammals still continued to grow larger and larger. Paraceratherium, the largest land mammal to ever live evolved during this period, along with many other perissodactyls in an event known as the Grand coupre.
Neogene.
The Neogene spans from 23 million to 3 million years ago, and is the shortest geological period in the Phanerozoic Eon. It features 2 epochs: the Miocene, and the Pliocene.
The Miocene spans from 23 to 5 million years ago and is a period in which grass spreads further across, effectively dominating a large portion of the world, diminishing forests in the process. Kelp forests evolved, leading to new species such as sea otters to evolve. During this time, perissodactyls thrived, and evolved into many different varieties. Alongside them were the apes, which evolved into a staggering 30 species. Overall, arid and mountainous land dominated most of the world, as did grazers. The Tethys Sea finally closed with the creation of the Arabian Peninsula and in its wake left the Black, Red, Mediterranean and Caspian Seas. This only increased aridity. Many new plants evolved, and 95% of modern seed plants evolved in the mid-Miocene.
The Pliocene ranges from 5 to 2 million years ago. The Pliocene features dramatic climactic changes, which ultimately leads to modern species and plants. The most dramatic are the formation of Panama, and the accumulation of ice at the poles, leading to a massive die-off, India and Asia collide forming the Himalayas, the Rockies and Appalachian mountain ranges were formed, and the Mediterranean Sea dried up for the next several million years. Along with these major geological events, the Australopithecus evolves in Africa, beginning the human branch. Also, with the isthmus of Panama, animals migrate across North and South America, wreaking havoc on the local ecology. Climactic changes bring along savannas that are still continuing to spread across the world, Indian monsoons, deserts in East Asia, and the beginnings of the Sahara desert. The earth's continents and seas move into their present shapes, and the world map hasn't changed much since.
Quaternary.
The Quaternary ranges from 3 million to present day, and features modern animals, and dramatic climate changes and features two epochs: the Pleistocene and the Holocene. 
The Pleistocene lasted from 3 million to 12,000 years ago. This epoch features the ice ages which is a result from the cooling effect that started in the Mid-Eocene. As the ice progressively migrated towards the equator, the areas north and south of the tropic line featured intense winters yet mild summers. Meanwhile, Africa experienced terrible droughts which resulted in the creation of the Sahara, Namib, and Kalahari deserts. To cope, many animals evolved including Mammoths, Giant ground sloths, Dire wolves and most famously Homo sapiens. 100,000 years ago marked the end of one of he worst droughts of Africa, and the expansion of primitive man. As the Pleistocene draws to a close, one of the largest die-outs causes many mega-fauna to die off, including the last hominid species (excluding Homo sapiens). All continents are effected, but Africa isn't hit quite as hard.
The Holocene ranges from 12,000 years ago to present day. Also known as "the Age of Man", the Holocene features the rise of man on his path to sentience. All recorded history and "the history of the world" lies within the boundaries of the Holocene epoch. Human activity, however, is being blamed for a die-out that has been going on since 10,000 B.C.E. commonly referred to as "the Sixth Extinction" with an estimated extinction rate of 140,000 species per year.
Biodiversity.
It has been demonstrated that changes in biodiversity through the Phanerozoic correlate much better with the hyperbolic model (widely used in demography and macrosociology) than with exponential and logistic models (traditionally used in population biology and extensively applied to fossil biodiversity as well). The latter models imply that changes in diversity are guided by a first-order positive feedback (more ancestors, more descendants) or a negative feedback arising from resource limitation, or both. The hyperbolic model implies a second-order positive feedback. The hyperbolic pattern of the world population growth arises from a second-order positive feedback between the population size and the rate of technological growth. The character of biodiversity growth in the Phanerozoic can be similarly accounted for by a feedback between the diversity and community structure complexity. It is suggested that the similarity between the curves of biodiversity and human population probably comes from the fact that both are derived from the interference of the hyperbolic trend with cyclical and stochastic dynamics.

</doc>
<doc id="23745" url="http://en.wikipedia.org/wiki?curid=23745" title="Pokémon">
Pokémon

Pokémon (ポケモン, Pokemon, ) is a media franchise owned by The Pokémon Company, and created by Satoshi Tajiri in 1995. It is centered on fictional creatures called "Pokémon", which humans capture and train to fight each other for sport.
The franchise began as a pair of video games for the original Game Boy, developed by Game Freak and published by Nintendo. The franchise now spans video games, trading card games, animated television shows and movies, comic books, and toys. Pokémon is the second-most successful and lucrative video game-based media franchise in the world, behind only Nintendo's "Mario" franchise.
The franchise celebrated in 2006. Cumulative sales of the video games (including home console games, such as "Hey You, Pikachu!" for the Nintendo 64) have reached more than 200 million copies. In November 2005, 4Kids Entertainment, which had managed the non-game related licensing of Pokémon, announced that it had agreed not to renew the Pokémon representation agreement. Pokémon USA Inc. (now The Pokémon Company International), a subsidiary of Japan's Pokémon Co., now oversees all Pokémon licensing outside of Asia. As of 2013, the "Pokémon" media franchise has grossed revenues of ¥4 trillion worldwide (equivalent to US$ billion). The brand earned $2 billion in 2014 alone. 
Name.
The name "Pokémon" is the romanized contraction of the Japanese brand Pocket Monsters (ポケットモンスター, Poketto Monsutā). The term "Pokémon", in addition to referring to the Pokémon franchise itself, also collectively refers to the 721 known fictional species that have made appearances in Pokémon media as of the release of the sixth generation titles "Pokémon X" and "Y". "Pokémon" is identical in both the singular and plural, as is each individual species name; it is grammatically correct to say "one Pokémon" and "many Pokémon", as well as "one Pikachu" and "many Pikachu". (However, "Pokémon Red", "Blue", and "Yellow" feature NPCs referring to the plurals of Clefairy and Diglett with an "s" at the end, shown "CLEFAIRYs" and "DIGLETTs", respectively. This was fixed in "FireRed" and "LeafGreen".)
Concept.
Tajiri first thought of Pokémon around 1989 or 1990, when the Game Boy was first released. The concept of the Pokémon universe, in both the video games and the general fictional world of Pokémon, stems from the hobby of insect collecting, a popular pastime which Pokémon executive director Satoshi Tajiri enjoyed as a child. Players of the games are designated as Pokémon Trainers, and in the main series Pokémon games, these trainers have two general goals. These are to complete the Pokédex by collecting all of the available Pokémon species found in the fictional region where that game takes place; and to train a team of powerful Pokémon from those they have caught to compete against teams owned by other Trainers, and eventually win the fictional Pokémon League. These themes of collecting, training, and battling are present in almost every version of the Pokémon franchise, including the video games, the anime and manga series, and the Pokémon Trading Card Game.
In most incarnations of the fictional Pokémon universe, a Trainer that encounters a wild Pokémon is able to capture that Pokémon by throwing a specially designed, mass-producible spherical tool called a Poké Ball at it. If the Pokémon is unable to escape the confines of the Poké Ball, it is officially considered to be under the ownership of that Trainer. Afterwards, it will obey whatever its new master commands, unless the Trainer demonstrates such a lack of experience that the Pokémon would rather act on its own accord. Trainers can send out any of their Pokémon to wage non-lethal battles against other Pokémon; if the opposing Pokémon is wild, the Trainer can capture that Pokémon with a Poké Ball, increasing his or her collection of creatures. Pokémon already owned by other Trainers cannot be captured, except under special circumstances in certain games. If a Pokémon fully defeats an opponent in battle so that the opponent is knocked out (i.e., "faints"), the winning Pokémon gains experience points and may level up. When leveling up, the Pokémon's statistics ("stats") of battling aptitude increase, such as Attack and Speed. From time to time the Pokémon may also learn new moves, which are techniques used in battle. In addition, many species of Pokémon possess the ability to undergo a form of metamorphosis and transform into a similar but stronger species of Pokémon, a process called evolution.
In the main series, each game's single-player mode requires the Trainer to raise a team of Pokémon to defeat many non-player character (NPC) Trainers and their Pokémon. Each game lays out a somewhat linear path through a specific region of the Pokémon world for the Trainer to journey through, completing events and battling opponents along the way. Each game features eight especially powerful Trainers, referred to as Gym Leaders, that the Trainer must defeat in order to progress. As a reward, the Trainer receives a Gym Badge, and once all eight badges are collected, that Trainer is eligible to challenge the region's Pokémon League, where four immensely talented trainers (referred to collectively as the "Elite Four") challenge the Trainer to four Pokémon battles in succession. If the trainer can overcome this gauntlet, he or she must then challenge the Regional Champion, the master Trainer who had previously defeated the Elite Four. Any Trainer who wins this last battle becomes the new champion.
Video games.
Generations.
The original Pokémon games were role-playing games (RPGs) with an element of strategy, and were created by Satoshi Tajiri for the Game Boy. These RPGs, and their sequels, remakes, and English language translations, are still considered the "main" Pokémon games, and the games which most fans of the series are referring to when they use the term "Pokémon games". All of the licensed Pokémon properties overseen by The Pokémon Company International are divided roughly by generation. These generations are roughly chronological divisions by release; every several years, when an official sequel in the main RPG series is released that features new Pokémon, characters, and gameplay concepts, that sequel is considered the start of a new generation of the franchise. The main games and their spin-offs, the anime, manga, and trading card game are all updated with the new Pokémon properties each time a new generation begins. The franchise began the sixth generation on October 12, 2013.
The Pokémon franchise started off in its first generation with its initial release of "Pocket Monsters Aka" and "Midori" ("Red" and "Green", respectively) for the Game Boy in Japan. When these games proved extremely popular, an enhanced "Ao" ("") version was released sometime after, and the "Ao" version was reprogrammed as "Pokémon Red" and "Blue" for international release. The games launched in the United States on September 30, 1998. The original "Aka" and "Midori" versions were never released outside of Japan. Afterwards, a further enhanced version titled "" was released to partially take advantage of the color palette of the Game Boy Color, as well as to feature more elements from the popular Pokémon anime. This first generation of games introduced the original 151 species of Pokémon, in National Pokédex order, encompassing all Pokémon from Bulbasaur to Mew. It also introduced the basic game concepts of capturing, training, battling, and trading Pokémon with both computer and human players. These versions of the games take place within the fictional Kanto region, inspired by the real world Kantō region of Japan, though the name "Kanto" was not used until the second generation.
The second generation of Pokémon began in 1999 with the release of "Pokémon Gold" and "Silver" for Game Boy Color. Like the previous generation, an enhanced version titled "Pokémon Crystal" was later released. The second generation introduced 100 new species of Pokémon, starting with Chikorita and ending with Celebi. It totaled 251 Pokémon to collect, train, and battle, set in Johto, inspired by Japan's Kansai region. The Pokémon mini is a handheld game console released in November 2001 in North America, December 2001 in Japan, and 2002 in Europe.
Pokémon entered its third generation with the 2002 release of "Pokémon Ruby" and "Sapphire" for Game Boy Advance and continued with the Game Boy Advance remakes of "Pokémon Red and Blue", "Pokémon FireRed" and "LeafGreen", and an enhanced version of "Pokémon Ruby and Sapphire" titled "Pokémon Emerald". The third generation introduced 135 new Pokémon, starting with Treecko and ending with Deoxys, for a total of 386 species. It is set in Hoenn, inspired by Japan's Kyushu region. However, this generation also garnered some criticism for leaving out several gameplay features, including the day-and-night system introduced in the previous generation. It was also the first installment that encouraged the player to collect merely a selected assortment of the total number of Pokémon rather than every existing species. By contrast, 202 out of 386 species are catchable in the "Ruby" and "Sapphire" versions.
In 2006, Japan began the fourth generation of the franchise with the release of "Pokémon Diamond" and "Pearl" for Nintendo DS. The fourth generation introduced another 107 new species of Pokémon, starting with Turtwig and ending with Arceus, bringing the total of Pokémon species to 493. The Nintendo DS "touch screen" allows new features to the game such as cooking poffins with the stylus and using the "Pokétch". New gameplay concepts include a restructured move-classification system, online multiplayer trading and battling via Nintendo Wi-Fi Connection, the return and expansion of the second generation's day-and-night system, the expansion of the third generation's Pokémon Contests into "Super Contests", and the new region of Sinnoh. This region was inspired by Japan's Hokkaido region and part of Russia's Sakhalin, and has an underground component for multiplayer gameplay in addition to the main overworld. "Pokémon Platinum", the enhanced version of Diamond and Pearl—much like "Pokémon Yellow", "Crystal", and "Emerald"—was released in September 2008 in Japan, March 2009 in North America, and May 2009 in Australia and Europe. Spin-off titles in the fourth generation include the "Pokémon Stadium" follow-up "Pokémon Battle Revolution" for Wii, which has Wi-Fi connectivity as well. Nintendo announced in May 2009 that enhanced remakes of "Pokémon Gold" and "Silver", entitled "Pokémon HeartGold" and "SoulSilver", would be released for the Nintendo DS system. "HeartGold" and "SoulSilver" are set in the Johto region and were released in September 2009 in Japan.
The fifth generation of "Pokémon" began on September 18, 2010, with the release of "Pokémon Black" and "White" in Japan for Nintendo DS. The games were originally announced by the Pokémon Company on January 29, 2010, with a tentative release later that year. The final release date of September 18 was announced on June 27, 2010. This version is set in the Unova region (イッシュ地方, Isshu-chihō, Isshu region), inspired by New York City, and utilizes the Nintendo DS's 3-D rendering capabilities to a greater extent than "Platinum", "HeartGold", and "SoulSilver", as shown in game footage of the player walking through the Castelia City (ヒウンシティ, Hiun Shiti) metropolis. A total of 156 new Pokémon were introduced, starting with Victini and ending with Genesect, bringing the franchise's total to 649. It also deployed new game mechanics such as the C Gear (Cギア, C Gia) wireless interactivity features and the ability to upload game data to the Internet and to the player's own computer. "Pokémon Black" and "White" was released in Europe on March 4, 2011, in North America on March 6, 2011, and in Australia on March 10, 2011. On June 23, 2012, Nintendo released "Pokémon Black 2" and "Pokémon White 2" in Japan for Nintendo DS, with early October releases in North America and Europe. "Black 2" and "White 2" are sequels to "Black" and "White", with several events in the second games referencing events in the first; they also allow players to link their previous "Black" or "White" with their "Black 2" or "White 2", introducing several events based on how they played their previous game.
Officially announced on January 8, 2013, and released simultaneously worldwide on October 12, 2013, "Pokémon X" and "Y" for the Nintendo 3DS are part of the sixth generation of games. Introducing the France-inspired Kalos region, these are the first Pokémon games rendered in 3D, and the first released worldwide together. On May 7, 2014, Nintendo announced remakes of the third generation games "Pokémon Ruby" and "Sapphire" titled "Pokémon Omega Ruby" and "Alpha Sapphire" which were released in Japan, North America, Australia, and South Korea on November 21, 2014, and in Europe on November 28, 2014.
Game mechanics.
The main staple of the "Pokémon" video game series revolves around the catching and battling of Pokémon. Starting with a starter Pokémon, the player can catch wild Pokémon by weakening them and catching them with Poké Balls. Conversely, they can choose to defeat them in battle in order to gain experience for their Pokémon, raising their levels and teaching them new moves. Certain Pokémon can evolve into more powerful forms by raising their levels or using certain items. Throughout the game, players will have to battle other trainers in order to progress, with the main goal to defeat various Gym Leaders and earn the right to become a tournament champion. Subsequent games in the series have introduced various side games and side quests, including the Battle Frontiers that display unique battle types and the Pokémon Contests where visual appearance is put on display.
Starter Pokémon.
One of the consistent aspects of the Pokémon games—spanning from "Pokémon Red" and "Blue" on the Game Boy to the Nintendo 3DS games "Pokémon Omega Ruby" and "Alpha Sapphire"—is the choice of one of three different Pokémon at the start of the player's adventures; these three are often labeled "starter Pokémon". Players can choose a Grass-type, a Fire-type, or a Water-type. For example, in "Pokémon Red" and "Blue" (and their respective remakes, "Pokémon FireRed" and "Pokémon LeafGreen"), the player has the choice of starting with Bulbasaur, Charmander, or Squirtle. The exception to this rule is "Pokémon Yellow" (a remake of the original games that follows the story of the "Pokémon" anime), where players are given a Pikachu, an Electric-type mouse Pokémon, famous for being the mascot of the Pokémon media franchise; in this game, however, the three starter Pokémon from "Red" and "Blue" can be obtained during the quest by a single player, something that is not possible in any other installment of the franchise. Another consistent aspect is that the player's rival will always choose as his or her starter Pokémon the one that has a type advantage over the player's Pokémon. For instance, if the player picks a Grass-type Pokémon, the rival will always pick the Fire-type starter. An exception to this is again "Pokémon Yellow", in which the rival picks an Eevee, but whether this Eevee evolves into Jolteon, Vaporeon, or Flareon is decided by when the player wins and loses to the rival through the journey. The GameCube games "Pokémon Colosseum" and "" also contain an exception; whereas in most games the player's initial Pokémon starts at Level 5, in these two games the player's initial Pokémon starts at Levels 10 and 25, respectively. In "Colosseum" the player's starter Pokémon are Espeon and Umbreon, while in "Gale of Darkness" the player's starter is Eevee.
Pokédex.
The Pokédex is a fictional electronic device featured in the Pokémon video game and anime series. In the games, whenever a Pokémon is first captured, its data will be added to a player's Pokédex, but in the anime or manga, the Pokédex is a comprehensive electronic reference encyclopedia, usually referred to in order to deliver exposition. "Pokédex" is also used to refer to a list of Pokémon, usually a list of Pokémon by number. In the video games, a Pokémon Trainer is issued a blank device at the start of the journey. A trainer must then attempt to fill the Pokédex by encountering and at least briefly obtaining each of the different species of Pokémon. A player will receive the name and image of a Pokémon after encountering one that was not previously in the Pokédex, typically after battling said Pokémon either in the wild or in a trainer battle (with the exceptions of link battles and tournament battles, such as in the Battle Frontier). In "Pokémon Red" and "Blue", some Pokémon's data is added to the Pokédex simply by viewing the Pokémon, such as in the zoo outside of the Safari Zone. Also, certain NPC characters may add to the Pokédex by explaining what a Pokémon looks like during conversation. More detailed information is available after the player obtains a member of the species, either through capturing the Pokémon in the wild, evolving a previously captured Pokémon, hatching a Pokémon egg (from the second generation onwards), or through a trade with another trainer (either an NPC or another player). This information includes height, weight, species type, and a short description of the Pokémon. Later versions of the Pokédex have more detailed information, like the size of a certain Pokémon compared to the player character, or Pokémon being sorted by their habitat (so far, the latter feature is only in the "FireRed" and "LeafGreen" versions). The most current forms of Pokédex are capable of containing information on all Pokémon currently known. The GameCube games, "Pokémon Colosseum" and "Pokémon XD: Gale of Darkness", have a Pokémon Digital Assistant (P★DA) which is similar to the Pokédex, but also tells what types are effective against a Pokémon and gives a description of its abilities.
In other media.
Anime series.
The Pokémon anime series and films are a meta-series of adventures separate from the canon that most of the Pokémon video games follow (with the exception of "Pokémon Yellow", a game based loosely on the anime storyline). The anime follows the quest of the main character, Ash Ketchum (known as Satoshi in Japan) a Pokémon Master in training, as he and a small group of friends travel around the fictitious world of Pokémon along with their Pokémon partners. The original series, titled "Pocket Monsters", or simply "Pokémon" in Western countries (often referred to as "Pokémon: Gotta Catch 'Em All" to distinguish it from the later series), begins with Ash's first day as a Pokémon trainer. His first (and signature) Pokémon is a Pikachu, differing from the games, where only Bulbasaur, Charmander, or Squirtle could be chosen. The series follows the storyline of the original games, "Pokémon Red" and "Blue", in the region of Kanto. Accompanying Ash on his journeys are Brock, the Pewter City Gym Leader, and Misty, the youngest of the Gym Leader sisters from Cerulean City. "Pokémon: Adventures in the Orange Islands" follows Ash's adventures in the Orange Islands, a place unique to the anime, and replaces Brock with Tracey Sketchit, an artist and "Pokémon watcher". The next series, based on the second generation of games, include "Pokémon: Johto Journeys", "Pokémon: Johto League Champions", and "Pokémon: Master Quest", following the original trio of Ash, Brock, and Misty in the western Johto region.
The saga continues in "Pokémon: Advanced Battle", based on the third generation games. Ash and company travel to Hoenn, a southern region in the Pokémon World. Ash takes on the role of a teacher and mentor for a novice Pokémon trainer named May. Her brother Max accompanies them, and though he isn't a trainer, he knows large amounts of handy information. Brock (from the original series) soon catches up with Ash, but Misty has returned to Cerulean City to tend to her duties as a gym leader (Misty, along with other recurring characters, appears in the spin-off series "Pokémon Chronicles"). The Advanced Battle series concludes with the "Battle Frontier" saga, based on the "Emerald" version and including aspects of "FireRed" and "LeafGreen". The Advanced Generation series ended with Max leaving to pick his starter Pokémon and May going to the Grand Festival in Johto.
In the "Diamond" and "Pearl" series, based on the fourth generation games, Ash, Brock, and a new companion, an aspiring Pokémon coordinator named Dawn, travel through the region of Sinnoh. At the end of the series, Ash and Brock return to Kanto where Brock begins to follow his newfound dream of becoming a Pokémon doctor himself.
"", based on the fifth generation games, features Ash and Pikachu traveling through the region of Unova (Isshu in Japan) alongside two new companions, Iris and Cilan (Dent in Japan) who part ways with them after returning to Kanto.
"Pocket Monsters: XY" (ポケットモンスターXY, Poketo Monsutā Ekkusu Wai), is the current airing series based on the sixth generation games, following Ash and Pikachu's journey through the region of Kalos, accompanied by Ash's childhood friend Serena and the siblings Clemont and Bonnie.
In addition to the TV series, seventeen Pokémon films have been made, with the pair of films, considered together as one. Collectible bonuses, such as promotional trading cards, have been available with some of the films. Various children's books, collectively known as "Pokémon Junior", are also based on the anime.
Films.
Given release years are the original Japanese release years.
Soundtracks.
Pokémon CDs have been released in North America, most of them in conjunction with the theatrical releases of the first three Pokémon films. These releases were commonplace until late 2001. On March 27, 2007, a tenth anniversary CD was released containing 18 tracks from the English dub; this was the first English-language release in over five years. Soundtracks of the Pokémon feature films have been released in Japan each year in conjunction with the theatrical releases.
 The exact date of release is unknown.
</dl>
Pokémon Trading Card Game.
 The Pokémon Trading Card Game is a collectible card game with a goal similar to a Pokémon battle in the video game series. Players use Pokémon cards, with individual strengths and weaknesses, in an attempt to defeat their opponent by "knocking out" his or her Pokémon cards. The game was first published in North America by Wizards of the Coast in 1999. However, with the release of "Pokémon Ruby" and "Sapphire" Game Boy Advance video games, The Pokémon Company took back the card game from Wizards of the Coast and started publishing the cards themselves. The Expedition expansion introduced the "Pokémon-e Trading Card Game", where the cards (for the most part) were compatible with the Nintendo e-Reader. Nintendo discontinued its production of e-Reader compatible cards with the release of EX FireRed & LeafGreen. In 1998, Nintendo released a Game Boy Color version of the trading card game in Japan; Pokémon Trading Card Game was subsequently released to the US and Europe in 2000. The game included digital versions cards from the original set of cards and the first two expansions (Jungle and Fossil), as well as several cards exclusive to the game. A Japan-exclusive sequel was released in 2001.
Manga.
There are various Pokémon manga series, four of which were released in English by Viz Media, and seven of them released in English by Chuang Yi. The manga series vary from game-based series to being based on the anime and the TCG. Original stories have also been published. As there are several series created by different authors most Pokémon manga series differ greatly from each other and other media, such as the anime.
Criticism and controversy.
Morality and religious beliefs.
Pokémon has been criticized by some Christians over perceived occult and violent themes and the concept of "Pokémon evolution", which they feel goes against the Biblical creation account in Genesis. However, Sat2000, a satellite TV station based in Vatican City, has countered that the Pokémon Trading Card Game and video games are "full of inventive imagination" and have no "harmful moral side effects". In the United Kingdom, the "Christian Power Cards" game was introduced in 1999 by David Tate who stated, "Some people aren't happy with Pokémon and want an alternative, others just want Christian games." The game was similar to the Pokémon TCG but used Biblical figures.
In 1999, Nintendo stopped manufacturing the Japanese version of the "Koga's Ninja Trick" trading card because it depicted a manji, a traditionally Buddhist symbol with no negative connotations. The Jewish civil rights group Anti-Defamation League complained because the symbol is the reverse of a swastika, which is considered offensive to Jewish people. The cards were intended for sale in Japan only, but the popularity of Pokémon led to importation into the United States with approval from Nintendo. The Anti-Defamation League understood that the issue symbol was not intended to offend and acknowledged the sensitivity that Nintendo showed by removing the product.
In 1999, two nine-year-old boys sued Nintendo because they claimed the Pokémon Trading Card Game caused their problematic gambling.
In 2001, Saudi Arabia banned Pokémon games and cards, alleging that the franchise promoted Zionism by displaying the Star of David in the trading cards (a six-pointed star is featured in the card game) as well as other religious symbols such as crosses they associated with Christianity and triangles they associated with Freemasonry; the games also involved gambling, which is in violation of Muslim doctrine.
Pokémon has also been accused of promoting materialism.
Health.
On December 16, 1997, more than 635 Japanese children were admitted to hospitals with epileptic seizures. It was determined the seizures were caused by watching an episode of Pokémon "Dennō Senshi Porygon", (most commonly translated "Electric Soldier Porygon", season 1, episode 38); as a result, this episode has not been aired since. In this particular episode, there were bright explosions with rapidly alternating blue and red color patterns. It was determined in subsequent research that these strobing light effects cause some individuals to have epileptic seizures, even if the person had no previous history of epilepsy. This incident is a common focus of Pokémon-related parodies in other media, and was lampooned by the "Simpsons" episode "Thirty Minutes over Tokyo" and the "South Park" episode "Chinpokomon", among others.
"Monster in My Pocket".
In March 2000, Morrison Entertainment Group, a small toy developer based at Manhattan Beach, California, sued Nintendo over claims that Pokémon infringed on its own "Monster in My Pocket" characters. A judge ruled there was no infringement, so Morrison appealed the ruling in November 2001.
Cultural influence.
Pokémon, being a popular franchise, has undoubtedly left its mark on pop culture. The Pokémon characters themselves have become pop culture icons; examples include two different Pikachu balloons in the Macy's Thanksgiving Day Parade, Pokémon Jets operated by All Nippon Airways, thousands of merchandise items, and a traveling theme park that was in Nagoya, Japan in 2005 and in Taipei in 2006. Pokémon also appeared on the cover of the U.S. magazine "Time" in 1999. The Comedy Central show "Drawn Together" has a character named Ling-Ling who is a direct parody of Pikachu. Several other shows such as "ReBoot", "The Simpsons", "South Park", "The Grim Adventures of Billy & Mandy", "Robot Chicken", "All Grown Up!" and "Johnny Test" have made references and spoofs of Pokémon, among other series. Pokémon was also featured on VH1's "". A live action show called "Pokémon Live!" toured the United States in late 2000. It was based on the popular Pokémon anime, but had some continuity errors relating to it. Jim Butcher cites Pokémon as one of the inspirations for the Codex Alera series of novels.
In November 2001, Nintendo opened a store called the Pokémon Center in New York, in New York's Rockefeller Center, modeled after the two other Pokémon Center stores in Tokyo and Osaka and named after a staple of the videogame series; Pokémon Centers are fictional buildings where Trainers take their injured Pokémon to be healed after combat. The store sold Pokémon merchandise on a total of two floors, with items ranging from collectible shirts to stuffed Pokémon plushies. The store also featured a "Pokémon Distributing Machine" in which players would place their game to receive an egg of a Pokémon that was being given out at that time. The store also had tables that were open for players of the Pokémon Trading Card Game to duel each other or an employee. The store was closed and replaced by the Nintendo World Store on May 14, 2005.
Joseph Jay Tobin theorizes that the success of the franchise was mainly due to the long list of names that could be learned by children and repeated in their peer groups. The rich fictional universe provided a lot of opportunities for discussion and demonstration of knowledge in front of their peers. In the French version Nintendo took care to translate the name of the creatures so that they reflected the French culture and language. In all cases the names of the creatures were linked to its characteristics, which converged with the children's belief that names have symbolic power. Children could pick their favourite Pokémon and affirm their individuality while at the same time affirming their conformance to the values of the group, and they could distinguish themselves from other kids by asserting what they liked and what they didn't like from every chapter. Pokémon gained popularity because it provided a sense of identity to a wide variety of children, and lost it quickly when many of those children found that the identity groups were too big and searched for identities that would distinguish them into smaller groups.
"Pokémon"‍ '​s history has been marked at times by rivalry with the "Digimon" media franchise that debuted at a similar time. Described as "the other 'mon'" by IGN's Juan Castro, "Digimon" has not enjoyed "Pokémon"‍ '​s level of international popularity or success, but has maintained a dedicated fanbase. IGN's Lucas M. Thomas stated that "Pokémon" is "Digimon"‍ '​s "constant competition and comparison", attributing the former's relative success to the simplicity of its evolution mechanic as opposed to Digivolution. The two have been noted for conceptual and stylistic similarities by sources such as GameZone. A debate among fans exists over which of the two franchises came first. In actuality, the first "Pokémon" media, "Pokémon Red" and "Green", were released initially on February 27, 1996; whereas the "Digimon" virtual pet was released on June 26, 1997.
Fan community.
In early 2014, an anonymous video streamer on Twitch launched Twitch Plays "Pokémon", an experiment trying to crowdsource playing subsequent Pokémon games starting with "Pokémon Red".
Bulbapedia, a wiki-based encyclopedia associated with longtime fan site Bulbagarden, is the "Internet's most detailed Pokémon database project".

</doc>
<doc id="23746" url="http://en.wikipedia.org/wiki?curid=23746" title="Paul the Deacon">
Paul the Deacon

Paul the Deacon (c. 720s – 13 April probably 799), also known as Paulus Diaconus, Warnefred, Barnefridus and Cassinensis (i.e. "of Monte Cassino"), was a Benedictine monk and historian of the Lombards.
Life.
An ancestor named Leupichis entered Italy in the train of Alboin and received lands at or near "Forum Julii" (Cividale del Friuli). During an invasion, the Avars swept off the five sons of this warrior into Pannonia, but one, his namesake, returned to Italy and restored the ruined fortunes of his house. The grandson of the younger Leupichis was Warnefrid, who by his wife Theodelinda became the father of Paul.
Born between 720 and 735 in the Duchy of Friuli to this possibly noble Lombard family, Paul received an exceptionally good education, probably at the court of the Lombard king Ratchis in Pavia, learning from a teacher named Flavian the rudiments of Greek. It is probable that he was secretary to the Lombard king Desiderius, a successor of Ratchis; it is certain that this king's daughter Adelperga was his pupil. After Adelperga had married Arichis II, duke of Benevento, Paul at her request wrote his continuation of Eutropius.
It is certain that he lived at the court of Benevento, possibly taking refuge when Pavia was taken by Charlemagne in 774; but his residence there may be much more probably dated to several years before that event. Soon he entered a monastery on Lake Como, and before 782 he had become a resident at the great Benedictine house of Monte Cassino, where he made the acquaintance of Charlemagne. About 776 his brother Arichis had been carried as a prisoner to Francia, and when five years later the Frankish king visited Rome, Paul successfully wrote to him on behalf of the captive.
His literary achievements attracted the notice of Charlemagne, and Paul became a potent factor in the Carolingian Renaissance. In 787 he returned to Italy and to Monte Cassino, where he died on April 13 in one of the years between 796 and 799. His surname Diaconus, shows that he took orders as a deacon; and some think he was a monk before the fall of the Lombard kingdom.
Works.
The chief work of Paul is his "Historia Langobardorum". This incomplete history in six books was written after 787 and at any rate no later than 795/96, maybe at Monte Cassino. It covers the story of the Lombards from their legendary origins in the north in 'Scadinavia' and their subsequent migrations, notably to Italy in 568/9 to the death of King Liutprand in 744, and contains much information about the Byzantine empire, the Franks, and others. The story is told from the point of view of a Lombard and is especially valuable for the relations between the Franks and the Lombards. It begins:
The region of the north, in proportion as it is removed from the heat of the sun and is chilled with snow and frost, is so much the more healthful to the bodies of men and fitted for the propagation of nations, just as, on the other hand, every southern region, the nearer it is to the heat of the sun, the more it abounds in diseases and is less fitted for the bringing up of the human race. 
Among his sources, Paul used the document called the "Origo gentis Langobardorum", the "Liber pontificalis", the lost history of Secundus of Trent, and the lost annals of Benevento; he made a free use of Bede, Gregory of Tours and Isidore of Seville. 
Cognate with this work is Paul's "Historia Romana", a continuation of the "Breviarium" of Eutropius. This was compiled between 766 and 771, at Benevento. The story runs that Paul advised Adelperga to read Eutropius. She did so, but complained that this Pagan writer said nothing about ecclesiastical affairs and stopped with the accession of the emperor Valens in 364; consequently Paul interwove extracts from the Scriptures, from the ecclesiastical historians and from other sources with Eutropius, and added six books, thus bringing the history down to 553. This work has value for its early historical presentation of the end of the Roman Empire in the West, although it was very popular during the Middle Ages. It has been edited by H Droysen and published in the "Monumenta Germaniae Historica. Auctores antiquissimi", Band ii. (1879) as well as by A. Crivellucci, in "Fonti per la storia d' Italia", n. 51 (1914).
Paul wrote at the request of Angilram, bishop of Metz (d. 791), a history of the bishops of Metz to 766, the first work of its kind north of the Alps, translated in english in 2013 as "Liber de episcopis Mettensibus". He also wrote many letters, verses and epitaphs, including those of Duke/Prince Arichis II of Benevento and of many members of the Carolingian family. Some of the letters are published with the "Historia Langobardorum" in the "Monumenta"; the poems and epitaphs edited by Ernst Dümmler will be found in the "Poetae latini aevi carolini", Band i. (Berlin, 188f). Fresh material having come to light, a new edition of the poems ("Die Gedichte des Paulus Diaconus") has been edited by Karl Neff (Munich, 1908), who denies, however, the attribution to Paul of the most famous poem in the collection, the "Ut queant laxis", a hymn to St. John the Baptist, which Guido d'Arezzo fitted to a melody which had previously been used for Horace's "Ode" . From the initial syllables of the first verses of the resultant setting he then took the names of the first notes of the musical scale. Paul also wrote an epitome, which has survived, of Sextus Pompeius Festus' "De significatu verborum". It was dedicated to Charlemagne.
While in Francia, Paul was requested by Charlemagne to compile a collection of homilies. He executed this after his return to Monte Cassino, and it was largely used in the Frankish churches. A life of Pope Gregory the Great has also been attributed to him, and he is credited with a Latin translation of the Greek "Life of Saint Mary the Egyptian".

</doc>
<doc id="23747" url="http://en.wikipedia.org/wiki?curid=23747" title="Peroxide">
Peroxide

A peroxide is a compound containing an oxygen–oxygen single bond or the peroxide anion, O. The O−O group is called the peroxide group or peroxo group. In contrast to oxide ions, the oxygen atoms in the peroxide ion have an oxidation state of −1.
The simplest stable peroxide is hydrogen peroxide. Superoxides, dioxygenyls, ozones and ozonides are considered separately. Peroxide compounds can be roughly classified into organic and inorganic. Whereas the inorganic peroxides have an ionic, salt-like character, the organic peroxides are dominated by the covalent bonds. The oxygen-oxygen chemical bond of peroxide is unstable and easily split into reactive radicals via homolytic cleavage. For this reason, peroxides are found in nature only in small quantities, in water, atmosphere, plants, and animals. Peroxide ion formation has recently been highlighted as one of the main mechanisms by which oxides accommodate excess oxygen in ionic crystals and may have a large impact on a range of industrial applications including solid oxide fuel cells.
Peroxides have a bleaching effect on organic substances and therefore are added to some detergents and hair colorants. Other large-scale applications include medicine and chemical industry, where peroxides are used in various synthesis reactions or occur as intermediate products. With an annual production of over 2 million tonnes, hydrogen peroxide is the most economically important peroxide. Many peroxides are unstable and hazardous substances; they cannot be stored and therefore are synthesized "in situ" and used immediately.
In biochemistry.
Peroxides are usually very reactive and thus occur in nature only in a few forms. These include, in addition to hydrogen peroxide, a few vegetable products such as ascaridole and a peroxide derivative of prostaglandin. Hydrogen peroxide occurs in surface water, groundwater and in the atmosphere. It forms upon illumination or natural catalytic action by substances containing in water. Sea water contains 0.5 to 14 ug/L of hydrogen peroxide, freshwater 1 to 30 ug/L and air 0.1 to 1 parts per billion.
Hydrogen peroxide is formed in human and animal organisms as a short-lived product in biochemical processes and is toxic to cells. The toxicity is due to oxidation of proteins, membrane lipids and DNA by the peroxide ions. The class of biological enzymes called SOD (superoxide dismutase) is developed in nearly all living cells as an important antioxidant agent. They promote the disproportionation of superoxide into oxygen and hydrogen peroxide, which is then rapidly decomposed by the enzyme catalase to oxygen and water.
Peroxisomes are organelles found in virtually all eukaryotic cells. They are involved in the catabolism of very long chain fatty acids, branched chain fatty acids, D-amino acids, polyamines, and biosynthesis of plasmalogens, etherphospholipids critical for the normal function of mammalian brains and lungs. Upon oxidation, they produce hydrogen peroxide in the following process:
Catalase, another peroxisomal enzyme, uses this H2O2 to oxidize other substrates, including phenols, formic acid, formaldehyde, and alcohol, by means of the peroxidation reaction: 
This reaction is important in liver and kidney cells, where the peroxisomes neutralize various toxic substances that enter the blood. Some of the ethanol humans drink is oxidized to acetaldehyde in this way. In addition, when excess H2O2 accumulates in the cell, catalase converts it to H2O through this reaction:
Another origin of hydrogen peroxide is the degradation of adenosine monophosphate which yields hypoxanthine. Hypoxanthine is then oxidatively catabolized first to xanthine and then to uric acid, and the reaction is catalyzed by the enzyme xanthine oxidase:
The degradation of guanosine monophosphate yields xanthine as an intermediate product which is then converted in the same way to uric acid with the formation of hydrogen peroxide.
Eggs of sea urchin, shortly after fertilization by a sperm, produce hydrogen peroxide. It is then quickly dissociated to OH· radicals. The radicals serve as initiator of radical polymerization, which surrounds the eggs with a protective layer of polymer.
The bombardier beetle has a device which allows it to shoot corrosive and foul-smelling bubbles at its enemies. The beetle produces and stores hydroquinone and hydrogen peroxide, in two separate reservoirs in the rear tip of its abdomen. When threatened, the beetle contracts muscles that force the two reactants through valved tubes into a mixing chamber containing water and a mixture of catalytic enzymes. When combined, the reactants undergo a violent exothermic chemical reaction, raising the temperature to near the boiling point of water. The boiling, foul-smelling liquid partially becomes a gas (flash evaporation) and is expelled through an outlet valve with a loud popping sound.
Hydrogen peroxide is a signaling molecule of plant defense against pathogens.
In firefly, oxidation of luciferins, which is catalyzed by luciferases, yields a peroxy compound 1,2-dioxetane. The dioxetane is unstable and decays spontaneously to carbon dioxide and excited ketones, which release excess energy by emitting light (bioluminescence).
Hydrogen peroxide.
The most widely used synthesis method of hydrogen peroxide is the anthraquinone process. There, anthraquinone is first hydrogenated to anthrahydroquinone. This reduced compound is oxidized with molecular oxygen, regenerating anthraquinone and releasing hydrogen peroxide. Direct synthesis of hydrogen peroxide from hydrogen and oxygen is inefficient and currently is not practiced industrially.
Many peroxides of mineral acids, such as peroxodisulfates and percarbonates, can be obtained by anodic oxidation of the respective acids. The anode material must be stable to the required high potentials of a few volts and therefore is either platinum or its alloys.
Peroxydisulfuric acid was historically used for the production of hydrogen peroxide in a method developed in the early 20th century:
This process requires relatively high concentration of peroxydisulfuric acid as its more dilute solutions evolve oxygen gas instead of peroxide.
Inorganic peroxides (aside from hydrogen peroxide).
Inorganic peroxides are divided into ionic and covalent peroxide. The first class mostly contains the peroxides of the alkali and alkaline earth metals whereas the covalent peroxides are represented by such compounds as hydrogen peroxide and peroxymonosulfuric acid (H2SO5). In contrast to the purely ionic character of alkali metal peroxides, peroxides of transition metals have a more covalent character.
It should be noted that in some older literature some high-valence metal oxides are incorrectly named peroxides (e.g. PbO2, MnO2, Ag4O4) even though they do not contain the peroxide ion.
Defect Formation in Fluorite Dioxides.
Both CeO2 and ThO2 accommodate excess oxygen in their lattice by formaing peroxide molecules. This is in contrast to UO2, which takes in excess oxygen by forming charged interstitials. The accommodation of excess oxygen in these systems changes their thermal and diffusion properties. This will have implications in the oxides' use in nuclear fuels and solid oxide fuel cells. Adding oxygen to the CeO2 system, for example, will increase the diffusivity of the oxygen allowing for more efficient solid oxide fuel cells.
Peroxide salts.
Bonding in O22−.
The peroxide ion is composed of two oxygen atoms that are linked by a single bond. The molecular orbital diagram of the peroxide dianion predicts a doubly occupied antibonding π* orbital and a bond order of one. The bond length is 149 pm, which is larger than in the ground state (triplet oxygen) of the oxygen molecule (3O2, 121 pm). This translates into the smaller force constant of the bond (2.8 N/cm vs. 11.4 N/cm for 3O2) and the lower frequency of the molecular vibration (770 cm−1 vs. 1555 cm−1 for 3O2).
The peroxide ion can be compared with other molecular oxygen ions superoxide O2− and ozonide O3−, but contrary to them, the peroxide is not a radical and not paramagnetic. Owing to the weak bonding between the oxygen atoms, peroxide easily undergoes homolytic cleavage yielding two highly reactive radicals. This cleavage is accelerated by temperature, illumination or chemical reactions.
Preparation of peroxide salts.
Most alkali metal peroxides can be synthesized directly by oxygenation of the elements. Lithium peroxide is formed upon treating lithium hydroxide with hydrogen peroxide:
Historically, barium peroxide is prepared by oxygenation of barium oxide at elevated temperature and pressure.
Barium peroxide was once used to produce pure oxygen from air. This process relies on the temperature-dependent chemical balance between barium oxide and peroxide: the reaction of barium oxide with air at 500 °C results in barium peroxide, which upon heating to above 700 °C in oxygen decomposes back to barium oxide releasing pure oxygen.
Properties of peroxide salts.
Few reactions are generally formulated for peroxide salt. In excess of dilute acids or water they release hydrogen peroxide.
Upon heating, the reaction with water leads to the release of oxygen instead
The peroxide anion is a stronger nucleophile than hydroxide and displaces hydroxyl from oxyanions e.g. forming perborates and percarbonates. Sodium perborate and sodium percarbonate are important consumer and industrial bleaching agents; they stabilize hydrogen peroxide and limit side reactions (e.g. reduction and decomposition note below). The peroxide anion forms an adduct with urea, hydrogen peroxide - urea.
Hydrogen peroxide is both an oxidizing agent and reducing agent. The oxidation of hydrogen peroxide by sodium hypochlorite yields singlet oxygen. The net reaction of a ferric ion with hydrogen peroxide is a ferrous ion and oxygen. This proceeds via single electron oxidation and hydroxyl radicals. This is used in some organic chemistry oxidations, e.g. in the Fenton's reagent. Only catalytic quantities of iron ion is needed since peroxide also oxidizes ferrous to ferric ion. The net reaction of hydrogen peroxide and permanganate or manganese dioxide is manganous ion; however, until the peroxide is spent some manganous ions are reoxidized to make the reaction catalytic. This forms the basis for common monopropellant rockets.
As a ligand in coordination chemistry.
Peroxide functions as a bidentate ligand in a variety of coordination complex. Some complexes have only peroxide ligands, e.g., chromium(VI) oxide peroxide (Cr(O2)42−). Similarly, molybdate reacts in alkaline media with peroxide to form red peroxomolybdate {Mo(O2)4}2−. The reaction of hydrogen peroxide with aqueous titanium(IV) gives a brightly colored peroxy complex that is a useful test for titanium as well as hydrogen peroxide. Many transition metal dioxygen complexes are best described as adducts of peroxide.
Applications.
Many inorganic peroxides are used for bleaching textiles and paper and as a bleaching additive to detergents and cleaning products. The increasing environmental concerns resulted in the preference of peroxides over chlorine-based compounds and a sharp increase in the peroxide production. The past use of perborates as additives to detergents and cleaning products has been largely replaced by percarbonates in order to decrease the emission of boron to the environment. Sodium percarbonate is used in such products as OxiClean and Tide laundry detergent. When dissolved in water, it releases hydrogen peroxide and soda ash (sodium carbonate):
The use of peroxide compounds in detergents is often reflected in their trade names; for example, Persil is a combination of the words "per"borate and "sil"icate.
Some peroxide salts release oxygen upon reaction with carbon dioxide. This reaction is used in generation of oxygen from exhaled carbon dioxide on submarines and spaceships. Sodium or lithium peroxides are preferred in space applications because of their lower molar mass and therefore higher oxygen yield per unit weight.
Alkali metal peroxides can be used for the synthesis of organic peroxides. One example is the conversion of benzoyl chloride with sodium peroxide to dibenzoyl peroxide.
Organic peroxides.
Organic peroxides can be divided into two major classes, peroxy acids and organic hydroperoxides. The first class is derived from the carboxylic acid and the second from ethers or alcohols.
Preparation.
Most peroxy acids can be obtained by the reaction of hydrogen peroxide with the corresponding carboxylic acid:
Another route employs acyl halides instead of the carboxylic acid. This method is used primarily with aromatic compounds under basic conditions in order to neutralize the resulting hydrogen chloride.
Aromatic aldehydes can be auto-oxidized into peroxycarboxylic acids:
The products, however, react with the initial aldehyde forming the carboxylic acid:
Several synthesis routes are known for aliphatic peroxides, such as the reaction of dialkylsulfates with alkaline hydrogen peroxide solution. In this method, the alkyl sulfate donates the alkyl group and the sulfate ion forms the leaving group.
This method can also yield cyclic peroxides. The four-membered dioxetanes can be obtained by 2+2 cycloaddition of oxygen to alkenes.
he selective synthesis of hydroperoxides can be carried out by free-radical oxidation of alkanes with oxygen. Here the active site formed by a radical initiator reacts with oxygen to form a hydroperoxyl. The addition of oxygen results in a more active radical which can further extract hydrogen atoms and release the hydroperoxide, leaving a new radical. This process is used industrially for the synthesis of phenol from benzene and is called the Cumene process or Hock process for its cumene and cumene hydroperoxide intermediates.
This auto-oxidation reaction can be used with common solvents from the group of ethers, such as diethyl ether, diisopropyl ether, tetrahydrofuran or 1,4-dioxane. It yields a volatile hydroperoxide ether that upon heating can result in a serious explosion.
Peroxides are formed by living organisms through ene reactions or Diels–Alder reactions between alkenes and oxygen. Unsaturated fatty acids can serve as the olefinic substrates for the ene reaction and unsaturated amino acids like histidine can be the reactant for the Diels-Alder cyclization. Rancidification (decomposition) of fats is partly caused by the formation of peroxides.
Properties and applications of organic peroxides.
Peroxycarboxylic acids are generally weaker acids than the parent carboxylic acids. Like most peroxides, they are strong oxidants and tend to explode at high concentrations and higher temperatures.
Organic peracids are used in the synthesis of epoxies via the Prilezhaev reaction. Another important application is the synthesis of lactones of cyclic ketones in the Baeyer–Villiger oxidation process. In both cases, electron-poor peroxycarboxylic acids are especially efficient, such as "meta"-chloroperoxybenzoic acid (mCPBA).
"Tert"-butyl hydroperoxide is a common oxidant in the Sharpless epoxidation, which is used for the stereoselective synthesis of epoxides. Karl Barry Sharpless was awarded the 2001 Nobel prize in Chemistry for this reaction.
Peracetic acid is a popular disinfectant in the medical field and food industry. Various peroxide solutions are commercially produced for the cleaning and disinfection of contact lenses.
Dibenzoyl peroxide is used as a radical initiator. Its weak peroxide bond cleaves easily yielding reactive benzoxy radicals, which assist polymerization leading to plastics like polyethylene. One of the synthesis methods of the commercially important plastic caprolactam—the precursor to Nylon 6 (polycaprolactam)—is a Baeyer-Villiger rearrangement of cyclohexanone with peracetic acid. This yields caprolactone, which is then converted to caprolactam by reacting it with ammonia.
Industrial resins based on acrylic and/or methacrylic acid esters are invariably produced by radical polymerization with organic peroxides at elevated temperatures. The polymerization rate is adjusted by suitable choice of temperature and type of peroxide. 
Some peroxides are drugs, whose action is based on the formation of radicals at desired locations in the organism. For example, artemisinin and its derivatives, such as such artesunate, possess the most rapid action of all current drugs against falciparum malaria. Artesunate is also efficient in reducing egg production in "Schistosoma haematobium" infection.
Many organic peroxides can initiate explosive polymerization in materials with unsaturated chemical bonds, and specifically triacetone triperoxide (TATP) and hexamethylene triperoxide diamine (HMTD) are powerful explosives. TATP is an inexpensive compound and is relatively easy to make. Whereas most other potent explosives, such as trinitrotoluene (TNT) or RDX (the major component of C4 mixtures), contain nitrogen, which is relatively easy to trace by sniffing techniques, TATP is nitrogen free and therefore is very difficult to detect by conventional screening methods. For this reason, it is an explosive favored by terrorists. TATP and HMTD were used in several executed or planned terrorist acts of the early 2000s, most notably in the 2001 shoe bomb plot and the 2005 London Underground bombings. Several detection devices have been designed since those events. One, for example, releases a chemical mixture which changes color when interacting with traces of TATP.
Laboratory identification.
Several analytical methods are used for qualitative and quantitative determination of peroxides. A simple qualitative detection of peroxides is carried out with the iodine-starch reaction. Here peroxides, hydroperoxides or peracids oxidize the added potassium iodide into iodine, which reacts with starch producing a deep-blue color. Commercial paper indicators using this reaction are available. This method is also suitable for quantitative evaluation, but it can not distinguish between different types of peroxide compounds. Discoloration of various indigo dyes in presence of peroxides is used instead for this purpose. For example, the loss of blue color in leuco-methylene blue is selective for hydrogen peroxide.
Quantitative analysis of hydroperoxides is performed using potentiometric titration with lithium aluminium hydride. Another way to evaluate the content of peracids and peroxides is the volumetric titration with alkoxides such as sodium ethoxide.
Safety.
Organic peroxides can accidentally initiate explosive polymerization in materials with unsaturated chemical bonds. Most notably, TATP and HMTD are high explosives, and TATP, because of its high susceptibility to accidental detonation by shock, friction, or sparks, has earned the nickname "Mother of Satan" among certain Islamic militant groups. TATP can accidentally form as by-products in many commonly used reactions. These reactions range from synthesis of MDMA, where TATP is formed via isosafrole oxidation in acetone, to industrial production of phenol, where the second product of the cumene process, acetone, is partially oxidized to peroxide on the second reaction step. Accidental preparation of organic peroxides can occur by mixing ketone solvents (most commonly acetone) with waste materials containing hydrogen peroxide or other oxidizers and leaving the mixture standing for several hours. In addition, many liquid ethers in the presence of air, light and metals (which act as catalysts) slowly – over a period of months – form highly unstable ether peroxides such as diethyl ether peroxide. Therefore, ethers are often stored over potassium hydroxide, which not only destroys peroxides but also acts as a powerful desiccant.
Peroxides are also strong oxidizers and easily react with skin, cotton and wood pulp. For safety reasons, peroxidic compounds are stored in a cool, opaque container, as heating and illumination accelerate their chemical reactions. Small amounts of peroxides, which emerge from storage or reaction vessels are neutralized using reducing agents such as iron(II) sulfate. The safety measures in industrial plants producing large amounts of peroxides include the following. The equipment is located within reinforced concrete structures with foil windows, which would relieve pressure and not shatter in case of explosion. The products are bottled in small containers and are moved to a cold place promptly after the synthesis. The containers are made of non-reactive materials such as stainless steel, some aluminium alloys or dark glass.
History.
One of the first synthetic peroxides, barium peroxide, was synthesized by Alexander von Humboldt in 1799 as a by-product of his attempts to decompose air. Nineteen years later Louis Jacques Thénard recognized that this compound could be used for the preparation of a previously unknown compound, which he described as "oxidized water" – now known as hydrogen peroxide. Sodium peroxide was synthesized in 1811 by Thénard and Joseph Louis Gay-Lussac. The bleaching effect of peroxides and their salts on natural dyes became known around that time, but early attempts of industrial production of peroxides failed, and the first plant producing hydrogen peroxide was built only in 1873 in Berlin. The discovery of the synthesis of hydrogen peroxide by electrolysis with sulfuric acid had brought the more efficient electrochemical method. It was first implemented into industry in 1908 in Weißenstein, Carinthia, Austria. The anthraquinone process, which is still used, was developed during the 1930s by the German chemical manufacturer IG Farben in Ludwigshafen. The increased demand and improvements in the synthesis methods resulted in the rise of the annual production of hydrogen peroxide from 35,000 tonnes in 1950, to over 100,000 tonnes in 1960, to 300,000 tonnes by 1970, and by 1998, it reached 2.7 million tonnes.

</doc>
<doc id="23748" url="http://en.wikipedia.org/wiki?curid=23748" title="Photolithography">
Photolithography

Photolithography, also termed optical lithography or UV lithography, is a process used in microfabrication to pattern parts of a thin film or the bulk of a substrate. It uses light to transfer a geometric pattern from a photomask to a light-sensitive chemical "photoresist", or simply "resist," on the substrate. A series of chemical treatments then either engraves the exposure pattern into, or enables deposition of a new material in the desired pattern upon, the material underneath the photo resist. For example, in complex integrated circuits, a modern CMOS wafer will go through the photolithographic cycle up to 50 times.
"Photolithography" shares some fundamental principles with photography in that the pattern in the etching resist is created by exposing it to light, either directly (without using a mask) or with a projected image using an optical mask. This procedure is comparable to a high precision version of the method used to make printed circuit boards. Subsequent stages in the process have more in common with etching than with lithographic printing. It is used because it can create extremely small patterns (down to a few tens of nanometers in size), it affords exact control over the shape and size of the objects it creates, and because it can create patterns over an entire surface cost-effectively. Its main disadvantages are that it requires a flat substrate to start with, it is not very effective at creating shapes that are not flat, and it can require extremely clean operating conditions.
History.
The words 'photo', 'litho', and 'graphy' all have Greek origins with the meanings 'light', 'stone' and 'writing'. As suggested by the name, lithography is a printing method where photo suggests the method uses the properties of light to create the prints. Joseph Nicephore Niepce was one of the first people to produce a photograph using such properties. In 1826, Niepce used Bitumen of Judea (a form of asphalt) on a pewter plate to create the image and then mixed lavender oil and minerals to keep the image in place. Years later in 1935, Louis Minsk developed the first negative photoresist, which relied on the solubility of chemicals to determine what part of the surface would remain raised and what parts would dissolve away where the dissolved areas created the image. Five years later, Oskar Süß developed the positive photoresist, which worked in the opposite manner - all the remaining raised surfaces created the image, by using diazonaphthoquinone. It wasn't until 1954, when Louis Plambeck Jr. developed the Dycryl polymeric letterpress plate, that the process of photo making became quicker.
Basic procedure.
A single iteration of photolithography combines several steps in sequence. Modern cleanrooms use automated, robotic wafer track systems to coordinate the process. The procedure described here omits some advanced treatments, such as thinning agents or edge-bead removal.
Cleaning.
If organic or inorganic contaminations are present on the wafer surface, they are usually removed by wet chemical treatment, e.g. the RCA clean procedure based on solutions containing hydrogen peroxide. Other solutions made with trichloroethylene, acetone or methanol can also be used to clean.
Preparation.
The wafer is initially heated to a temperature sufficient to drive off any moisture that may be present on the wafer surface, 150 °C for ten minutes is sufficient. Wafers that have been in storage must be chemically cleaned to remove contamination. A liquid or gaseous "adhesion promoter", such as Bis(trimethylsilyl)amine ("hexamethyldisilazane", HMDS), is applied to promote adhesion of the photoresist to the wafer. The surface layer of silicon dioxide on the wafer reacts with HMDS to form tri-methylated silicon-dioxide, a highly water repellent layer not unlike the layer of wax on a car's paint. This water repellent layer prevents the aqueous developer from penetrating between the photoresist layer and the wafer's surface, thus preventing so-called lifting of small photoresist structures in the (developing) pattern. In order to ensure the development of the image, it is best covered and placed over a hot plate and let it dry while stabilizing the temperature at 120 °C.
Photoresist application.
The wafer is covered with photoresist by spin coating. A viscous, liquid solution of photoresist is dispensed onto the wafer, and the wafer is spun rapidly to produce a uniformly thick layer. The spin coating typically runs at 1200 to 4800 rpm for 30 to 60 seconds, and produces a layer between 0.5 and 2.5 micrometres thick. The spin coating process results in a uniform thin layer, usually with uniformity of within 5 to 10 nanometres. This uniformity can be explained by detailed fluid-mechanical modelling, which shows that the resist moves much faster at the top of the layer than at the bottom, where viscous forces bind the resist to the wafer surface. Thus, the top layer of resist is quickly ejected from the wafer's edge while the bottom layer still creeps slowly radially along the wafer. In this way, any 'bump' or 'ridge' of resist is removed, leaving a very flat layer. Final thickness is also determined by the evaporation of liquid solvents from the resist. For very small, dense features (< 125 or so nm), lower resist thicknesses (< 0.5 micrometres) are needed to overcome collapse effects at high aspect ratios; typical aspect ratios are < 4:1.
The photo resist-coated wafer is then prebaked to drive off excess photoresist solvent, typically at 90 to 100 °C for 30 to 60 seconds on a hotplate.
Exposure and developing.
After prebaking, the photoresist is exposed to a pattern of intense light. The exposure to light causes a chemical change that allows some of the photoresist to be removed by a special solution, called "developer" by analogy with photographic developer. Positive photoresist, the most common type, becomes soluble in the developer when exposed; with negative photoresist, unexposed regions are soluble in the developer.
A post-exposure bake (PEB) is performed before developing, typically to help reduce standing wave phenomena caused by the destructive and constructive interference patterns of the incident light. In deep ultraviolet lithography, chemically amplified resist (CAR) chemistry is used. This process is much more sensitive to PEB time, temperature, and delay, as most of the "exposure" reaction (creating acid, making the polymer soluble in the basic developer) actually occurs in the PEB.
The develop chemistry is delivered on a spinner, much like photoresist. Developers originally often contained sodium hydroxide (NaOH). However, sodium is considered an extremely undesirable contaminant in MOSFET fabrication because it degrades the insulating properties of gate oxides (specifically, sodium ions can migrate in and out of the gate, changing the threshold voltage of the transistor and making it harder or easier to turn the transistor on over time). Metal-ion-free developers such as tetramethylammonium hydroxide (TMAH) are now used.
The resulting wafer is then "hard-baked" if a non-chemically amplified resist was used, typically at 120 to 180 °C for 20 to 30 minutes. The hard bake solidifies the remaining photoresist, to make a more durable protecting layer in future ion implantation, wet chemical etching, or plasma etching.
Etching.
In etching, a liquid ("wet") or plasma ("dry") chemical agent removes the uppermost layer of the substrate in the areas that are not protected by photoresist. In semiconductor fabrication, dry etching techniques are generally used, as they can be made anisotropic, in order to avoid significant undercutting of the photoresist pattern. This is essential when the width of the features to be defined is similar to or less than the thickness of the material being etched (i.e. when the aspect ratio approaches unity). Wet etch processes are generally isotropic in nature, which is often indispensable for microelectromechanical systems, where suspended structures must be "released" from the underlying layer.
The development of low-defectivity anisotropic dry-etch process has enabled the ever-smaller features defined photolithographically in the resist to be transferred to the substrate material.
Photoresist removal.
After a photoresist is no longer needed, it must be removed from the substrate. This usually requires a liquid "resist stripper", which chemically alters the resist so that it no longer adheres to the substrate. Alternatively, photoresist may be removed by a plasma containing oxygen, which oxidizes it. This process is called ashing, and resembles dry etching. 1-Methyl-2-pyrrolidon (NMP) solvent is another method used to remove an image. NMP is soluble with photoresist and has a high boiling point, thus when the resist has dissolved off the solution and wafer can be heated up to 80 °C without leaving any residue.
Exposure ("printing") systems.
Exposure systems typically produce an image on the wafer using a photomask. The light shines through the photomask, which blocks it in some areas and lets it pass in others. (Maskless lithography projects a precise beam directly onto the wafer without using a mask, but it is not widely used in commercial processes.) Exposure systems may be classified by the optics that transfer the image from the mask to the wafer.
Contact and proximity.
A contact printer, the simplest exposure system, puts a photomask in direct contact with the wafer and exposes it to a uniform light. A proximity printer puts a small gap between the photomask and wafer. In both cases, the mask covers the entire wafer, and simultaneously patterns every die.
Contact printing is liable to damage both the mask and the wafer, and this was the primary reason it was abandoned for high volume production. Both contact and proximity lithography require the light intensity to be uniform across an entire wafer, and the mask to align precisely to features already on the wafer. As modern processes use increasingly large wafers, these conditions become increasingly difficult.
Research and prototyping processes often use contact or proximity lithography, because it uses inexpensive hardware and can achieve high optical resolution. The resolution in proximity lithography is approximately the square root of the product of the wavelength and the gap distance. Hence, except for projection lithography (see below), contact printing offers the best resolution, because its gap distance is approximately zero (neglecting the thickness of the photoresist itself). In addition, nanoimprint lithography may revive interest in this familiar technique, especially since the cost of ownership is expected to be low; however, the shortcomings of contact printing discussed above remain as challenges.
Projection.
Very-large-scale integration (VLSI) lithography uses projection systems. Unlike contact or proximity masks, which cover an entire wafer, projection masks (known as "reticles") show only one die or an array of dice (known as a "field"). Projection exposure systems (steppers or scanners) project the mask onto the wafer many times to create the complete pattern.
Photomasks.
The image for the mask originates from a computerized data file. This data file is converted to a series of polygons and written onto a square fused quartz substrate covered with a layer of chromium using a photolithographic process. A laser beam (laser writer) or a beam of electrons (e-beam writer) is used to expose the pattern defined by the data file and travels over the surface of the substrate in either a vector or raster scan manner. Where the photoresist on the mask is exposed, the chrome can be etched away, leaving a clear path for the illumination light in the stepper/scanner system to travel through.
Resolution in projection systems.
The ability to project a clear image of a small feature onto the wafer is limited by the wavelength of the light that is used, and the ability of the reduction lens system to capture enough diffraction orders from the illuminated mask. Current state-of-the-art photolithography tools use deep ultraviolet (DUV) light from excimer lasers with wavelengths of 248 and 193 nm (the dominant lithography technology today is thus also called "excimer laser lithography"), which allow minimum feature sizes down to 50 nm. Excimer laser lithography has thus played a critical role in the continued advance of the so-called Moore’s Law for the last 20 years (see below).
The minimum feature size that a projection system can print is given approximately by:
where
formula_2 is the minimum feature size (also called the critical dimension, "target design rule"). It is also common to write 2 "times" the "half-pitch".
formula_3 (commonly called "k1 factor") is a coefficient that encapsulates process-related factors, and typically equals 0.4 for production. The minimum feature size can be reduced by decreasing this coefficient through Computational lithography.
formula_4 is the wavelength of light used
formula_5 is the numerical aperture of the lens as seen from the wafer
According to this equation, minimum feature sizes can be decreased by decreasing the wavelength, and increasing the numerical aperture (to achieve a tighter focused beam and a smaller spot size). However, this design method runs into a competing constraint. In modern systems, the depth of focus is also a concern:
Here, formula_7 is another process-related coefficient. The depth of focus restricts the thickness of the photoresist and the depth of the topography on the wafer. Chemical mechanical polishing is often used to flatten topography before high-resolution lithographic steps.
Light sources.
Historically, photolithography has used ultraviolet light from gas-discharge lamps using mercury, sometimes in combination with noble gases such as xenon. These lamps produce light across a broad spectrum with several strong peaks in the ultraviolet range. This spectrum is filtered to select a single spectral line. From the early 1960s through the mid-1980s, Hg lamps had been used in lithography for their spectral lines at 436 nm ("g-line"), 405 nm ("h-line") and 365 nm ("i-line"). However, with the semiconductor industry’s need for both higher resolution (to produce denser and faster chips) and higher throughput (for lower costs), the lamp-based lithography tools were no longer able to meet the industry’s requirements.
This challenge was overcome when in a pioneering development in 1982, excimer laser lithography was proposed and demonstrated at I.B.M. by Kanti Jain, and now excimer laser lithography machines (steppers and scanners) are the primary tools used worldwide in microelectronics production. With phenomenal advances made in tool technology in the last two decades, it is the semiconductor industry view that excimer laser lithography has been a crucial factor in the continued advance of Moore’s Law, enabling minimum features sizes in chip manufacturing to shrink from 0.5 micrometer in 1990 to 45 nanometers and below in 2010. This trend is expected to continue into this decade for even denser chips, with minimum features approaching 10 nanometers. From an even broader scientific and technological perspective, in the 50-year history of the laser since its first demonstration in 1960, the invention and development of excimer laser lithography has been highlighted as one of the major milestones.
The commonly used deep ultraviolet excimer lasers in lithography systems are the krypton fluoride laser at 248 nm wavelength and the argon fluoride laser at 193 nm wavelength. The primary manufacturers of excimer laser light sources in the 1980s were Lambda Physik (now part of Coherent, Inc.) and Lumonics. Since the mid-1990s Cymer Inc. has become the dominant supplier of excimer laser sources to the lithography equipment manufacturers, with Gigaphoton Inc. as their closest rival. Generally, an excimer laser is designed to operate with a specific gas mixture; therefore, changing wavelength is not a trivial matter, as the method of generating the new wavelength is completely different, and the absorption characteristics of materials change. For example, air begins to absorb significantly around the 193 nm wavelength; moving to sub-193 nm wavelengths would require installing vacuum pump and purge equipment on the lithography tools (a significant challenge). Furthermore, insulating materials such as silicon dioxide, when exposed to photons with energy greater than the band gap, release free electrons and holes which subsequently cause adverse charging.
Optical lithography has been extended to feature sizes below 50 nm using the 193 nm ArF excimer laser and liquid immersion techniques. Also termed immersion lithography, this enables the use of optics with numerical apertures exceeding 1.0. The liquid used is typically ultra-pure, deionised water, which provides for a refractive index above that of the usual air gap between the lens and the wafer surface. The water is continually circulated to eliminate thermally-induced distortions. Water will only allow "NA"'s of up to ~1.4, but materials with higher refractive indices will allow the effective "NA" to be increased further.
Experimental tools using the 157 nm wavelength from the F2 excimer laser in a manner similar to current exposure systems have been built. These were once targeted to succeed 193 nm lithography at the 65 nm feature size node but have now all but been eliminated by the introduction of immersion lithography. This was due to persistent technical problems with the 157 nm technology and economic considerations that provided strong incentives for the continued use of 193 nm excimer laser lithography technology. High-index immersion lithography is the newest extension of 193 nm lithography to be considered. In 2006, features less than 30 nm were demonstrated by IBM using this technique.
UV excimer lasers have been demonstrated to about 126 nm (for Ar2*). Excimer lasers are generally preferred to more than the mercury arc lamps because they have a higher resolution. Mercury arc lamps are designed to maintain a steady DC current of 50 to 150 Volts, however the resolution is not optimal. Excimer lasers are gas-based light systems that are usually filled with inert and halide gases (Kr, Ar, Xe, F and Cl) that are charged by an electric field. The faster the frequency the greater the resolution of the image. KrF lasers are able to function at a frequency of 4 kHz which is why they are so optimal. In addition to running at a higher frequency, excimer lasers are compatible with more advanced machines than mercury arc lamps are. They are also able to operate from greater distances (up to 25 meters) and are able to maintain their accuracy with a series of mirrors and antireflective-coated lenses. By setting up multiple lasers and mirrors, the amount of energy loss is minimized, also since the lenses are coated with antireflective material, the light intensity remains relatively the same from when it left the laser to when it hits the wafer.
Lasers have been used to indirectly generate non-coherent extreme UV (EUV) light at 13.5 nm for extreme ultraviolet lithography. The EUV light is not emitted by the laser, but rather by a tin or xenon plasma which is excited by an eximer laser. Fabrication of feature sizes of 10 nm has been demonstrated in production environments, but not yet at rates needed for commercialization. However, this is expected by 2016. This technique does not require a synchrotron and EUV sources, as noted, do not produce coherent light. However vacuum systems and a number of novel technologies (including much higher EUV energies than are now produced) are needed to work with UV at the edge of the X-ray spectrum (which begins at 10 nm).
An option, especially if and when wavelengths continue to decrease to extreme UV or X-ray, is the free-electron laser (or one might say xaser for an X-ray device). These can produce high quality beams at arbitrary wavelengths.
Experimental methods.
Photolithography has been defeating predictions of its demise for many years. For instance, by the early 1980s, many in the semiconductor industry had come to believe that features smaller than 1 micrometer could not be printed optically. Modern techniques using excimer laser lithography already print features with dimensions a fraction of the wavelength of light used – an amazing optical feat. New tricks such as immersion lithography, dual-tone resist and multiple patterning continue to improve the resolution of 193 nm lithography. Meanwhile, current research is exploring alternatives to conventional UV, such as electron beam lithography, X-ray lithography, extreme ultraviolet lithography and ion projection lithography.

</doc>
<doc id="23749" url="http://en.wikipedia.org/wiki?curid=23749" title="Platypus">
Platypus

The platypus ("Ornithorhynchus anatinus") also known as the duck-billed platypus is a semiaquatic egg-laying mammal endemic to eastern Australia, including Tasmania. Together with the four species of echidna, it is one of the five extant species of monotremes, the only mammals that lay eggs instead of giving birth. It is the sole living representative of its family (Ornithorhynchidae) and genus ("Ornithorhynchus"), though a number of related species have been found in the fossil record.
The unusual appearance of this egg-laying, duck-billed, beaver-tailed, otter-footed mammal baffled European naturalists when they first encountered it, with some considering it an elaborate hoax. It is one of the few venomous mammals, the male platypus having a spur on the hind foot that delivers a venom capable of causing severe pain to humans. The unique features of the platypus make it an important subject in the study of evolutionary biology and a recognisable and iconic symbol of Australia; it has appeared as a mascot at national events and is featured on the reverse of its 20-cent coin. The platypus is the animal emblem of the state of New South Wales.
Until the early 20th century, it was hunted for its fur, but it is now protected throughout its range. Although captive breeding programs have had only limited success and the platypus is vulnerable to the effects of pollution, it is not under any immediate threat.
Taxonomy and etymology.
When the platypus was first encountered by Europeans in 1798, a pelt and sketch were sent back to Great Britain by Captain John Hunter, the second Governor of New South Wales. British scientists' initial hunch was that the attributes were a hoax. George Shaw, who produced the first description of the animal in the "Naturalist's Miscellany" in 1799, stated it was impossible not to entertain doubts as to its genuine nature, and Robert Knox believed it might have been produced by some Asian taxidermist. It was thought that somebody had sewn a duck's beak onto the body of a beaver-like animal. Shaw even took a pair of scissors to the dried skin to check for stitches.
The common name "platypus" is the latinisation of the Greek word πλατύπους ("platupous"), "flat-footed", from πλατύς ("platus"), "broad, wide, flat" and πούς ("pous"), "foot". Shaw assigned it as a Linnaean genus name when he initially described it, but the term was quickly discovered to belong already to the wood-boring ambrosia beetle genus "Platypus". It was independently described as "Ornithorhynchus paradoxus" by Johann Blumenbach in 1800 (from a specimen given to him by Sir Joseph Banks) and following the rules of priority of nomenclature, it was later officially recognised as "Ornithorhynchus anatinus".
The scientific name "Ornithorhynchus anatinus" is derived from ορνιθόρυνχος ("ornithorhynkhos"), which literally means "bird snout" in Greek; and "anatinus", which means "duck-like" in Latin.
There is no universally agreed plural of "platypus" in the English language. Scientists generally use "platypuses" or simply "platypus". Colloquially, the term "platypi" is also used for the plural, although this is technically incorrect and a form of pseudo-Latin; the correct Greek plural would be "platypodes". Early British settlers called it by many names, such as "watermole", "duckbill", and "duckmole". The name "platypus" is often prefixed with the adjective "duck-billed" to form "duck-billed platypus", which distinguishes the modern platypus from the extinct Riversleigh platypuses.
Description.
In David Collins's account of the new colony 1788 – 1801, he describes coming across "an amphibious, mole like" animal. His account includes a drawing of the animal.
The body and the broad, flat tail of the platypus are covered with dense, brown fur that traps a layer of insulating air to keep the animal warm. The fur is waterproof, and the texture is akin to that of a mole. The platypus uses its tail for storage of fat reserves (an adaptation also found in animals such as the Tasmanian devil and fat-tailed sheep). It has webbed feet and a large, rubbery snout; these features appear closer to those of a duck than to those of any known mammal. The webbing is more significant on the front feet and is folded back when walking on land. Unlike a bird's beak (in which the upper and lower parts separate to reveal the mouth), the snout of the platypus is a sensory organ with the mouth on the underside. The nostrils are located on the dorsal surface of the snout, while the eyes and ears are located in a groove set just back from it; this groove is closed when swimming. Platypuses have been heard to emit a low growl when disturbed and a range of other vocalizations have been reported in captive specimens.
Weight varies considerably from 0.7 to, with males being larger than females; males average 50 cm in total length, while females average 43 cm, with substantial variation in average size from one region to another, and this pattern does not seem to follow any particular climatic rule and may be due to other environmental factors, such as predation and human encroachment.
The platypus has an average body temperature of about 32 °C (90 °F) rather than the 37 °C (99 °F) typical of placental mammals. Research suggests this has been a gradual adaptation to harsh environmental conditions on the part of the small number of surviving monotreme species rather than a historical characteristic of monotremes.
Modern platypus young have three teeth in each of the maxillae (one premolar and two molars) and dentaries (three molars), which they lose before or just after leaving the breeding burrow; adults have heavily keratinised pads in their place. The first upper and third lower cheek teeth of platypus nestlings are small, each having one principal cusp, while the other teeth have two main cusps. The platypus jaw is constructed differently from that of other mammals, and the jaw-opening muscle is different. As in all true mammals, the tiny bones that conduct sound in the middle ear are fully incorporated into the skull, rather than lying in the jaw as in cynodonts and other premammalian synapsids. However, the external opening of the ear still lies at the base of the jaw. The platypus has extra bones in the shoulder girdle, including an interclavicle, which is not found in other mammals. As in many other aquatic and semiaquatic vertebrates, the bones show osteosclerosis, increasing their density to provide ballast. It has a reptilian gait, with the legs on the sides of the body, rather than underneath. When on land, it engages in knuckle-walking on its front feet, to protect the webbing between the toes.
Venom.
While both male and female platypuses are born with ankle spurs, only the male's spurs produce venom,
composed largely of defensin-like proteins (DLPs), three of which are unique to the platypus. The DLPs are produced by the immune system of the platypus. The function of defensins is to blow holes in pathogenic bacteria and viruses, but in platypuses they also are formed into venom for defense. Although powerful enough to kill smaller animals such as dogs, the venom is not lethal to humans, but the pain is so excruciating that the victim may be incapacitated. Oedema rapidly develops around the wound and gradually spreads throughout the affected limb. Information obtained from case histories and anecdotal evidence indicates the pain develops into a long-lasting hyperalgesia (a heightened sensitivity to pain) that persists for days or even months. Venom is produced in the crural glands of the male, which are kidney-shaped alveolar glands connected by a thin-walled duct to a calcaneus spur on each hind limb. The female platypus, in common with echidnas, has rudimentary spur buds that do not develop (dropping off before the end of their first year) and lack functional crural glands.
The venom appears to have a different function from those produced by nonmammalian species; its effects are not life-threatening to humans, but nevertheless powerful enough to seriously impair the victim. Since only males produce venom and production rises during the breeding season, it may be used as an offensive weapon to assert dominance during this period.
Electrolocation.
Monotremes (for the other species, see Echidna) are the only mammals (apart from at least one species of dolphin) known to have a sense of electroreception: they locate their prey in part by detecting electric fields generated by muscular contractions. The platypus' electroreception is the most sensitive of any monotreme.
The electroreceptors are located in rostrocaudal rows in the skin of the bill, while mechanoreceptors (which detect touch) are uniformly distributed across the bill. The electrosensory area of the cerebral cortex is contained within the tactile somatosensory area, and some cortical cells receive input from both electroreceptors and mechanoreceptors, suggesting a close association between the tactile and electric senses. Both electroreceptors and mechanoreceptors in the bill dominate the somatotopic map of the platypus brain, in the same way human hands dominate the Penfield homunculus map.
The platypus can determine the direction of an electric source, perhaps by comparing differences in signal strength across the sheet of electroreceptors. This would explain the characteristic side-to-side motion of the animal's head while hunting. The cortical convergence of electrosensory and tactile inputs suggests a mechanism that determines the distance of prey that, when they move, emit both electrical signals and mechanical pressure pulses. The platypus uses the difference between arrival times of the two signals to sense distance.
The platypus feeds by neither sight nor smell, closing its eyes, ears, and nose each time it dives. Rather, when it digs in the bottom of streams with its bill, its electroreceptors detect tiny electrical currents generated by muscular contractions of its prey, so enabling it to distinguish between animate and inanimate objects, which continuously stimulate its mechanoreceptors. Experiments have shown the platypus will even react to an "artificial shrimp" if a small electrical current is passed through it.
Eyes.
Recent studies say that the eyes of the platypus could possibly be more similar to those of Pacific hagfish or Northern Hemisphere lampreys than to those of most tetrapods. Also it contains double cones, which most mammals do not have.
Although the platypus' eyes are small and not used under water, several features indicate that vision played an important role in its ancestors. The corneal surface and the adjacent surface of the lens is flat while the posterior surface of the lens is steeply curved, similar to the eyes of other aquatic mammals such as otters and sea-lions. A temporal (ear side) concentration of retinal ganglion cells, important for binocular vision, indicates a role in predation, while the accompanying visual acuity is insufficient for such activities. Furthermore, this limited acuity is matched by a low cortical magnification, a small lateral geniculate nucleus and a large optic tectum, suggesting that the visual midbrain plays a more important role than the visual cortex like in some rodents. These features suggest that the platypus has adapted to an aquatic and nocturnal lifestyle, developing its electrosensory system at the cost of its visual system; an evolutionary process paralleled by the small number of electroreceptors in the short-beaked echidna, who dwells in dry environments, whilst the long-beaked echidna, who lives in moist environments, is intermediate between the other two monotremes.
Ecology and behaviour.
The platypus is semiaquatic, inhabiting small streams and rivers over an extensive range from the cold highlands of Tasmania and the Australian Alps to the tropical rainforests of coastal Queensland as far north as the base of the Cape York Peninsula. Inland, its distribution is not well known; it is extinct in South Australia (apart from an introduced population on Kangaroo Island) and is no longer found in the main part of the Murray-Darling Basin, possibly due to the declining water quality brought about by extensive land clearing and irrigation schemes. Along the coastal river systems, its distribution is unpredictable; it appears to be absent from some relatively healthy rivers, and yet maintains a presence in others that are quite degraded (the lower Maribyrnong, for example).
In captivity, platypuses have survived to 17 years of age, and wild specimens have been recaptured when 11 years old. Mortality rates for adults in the wild appear to be low. Natural predators include snakes, water rats, goannas, hawks, owls, and eagles. Low platypus numbers in northern Australia are possibly due to predation by crocodiles. The introduction of red foxes in 1845 for hunting may have had some impact on its numbers on the mainland. The platypus is generally regarded as nocturnal and crepuscular, but individuals are also active during the day, particularly when the sky is overcast. Its habitat bridges rivers and the riparian zone for both a food supply of prey species, and banks where it can dig resting and nesting burrows. It may have a range of up to 7 km, with a male's home range overlapping those of three or four females.
The platypus is an excellent swimmer and spends much of its time in the water foraging for food. When swimming, it can be distinguished from other Australian mammals by the absence of visible ears. Uniquely among mammals, it propels itself when swimming by an alternate rowing motion of the front feet; although all four feet of the platypus are webbed, the hind feet (which are held against the body) do not assist in propulsion, but are used for steering in combination with the tail. The species is endothermic, maintaining its body temperature at about 32 °C (90 °F), lower than most mammals, even while foraging for hours in water below 5 °C (41 °F).
Dives normally last around 30 seconds, but can last longer, although few exceed the estimated aerobic limit of 40 seconds. Recovery at the surface between dives commonly takes from 10 to 20 seconds.
When not in the water, the platypus retires to a short, straight resting burrow of oval cross-section, nearly always in the riverbank not far above water level, and often hidden under a protective tangle of roots.
The average sleep time of a platypus is said to be as long as 14 hours per day, possibly because it eats crustaceans, which provide a high level of calories.
Diet.
The platypus is a carnivore: it feeds on annelid worms, insect larvae, freshwater shrimps, and freshwater yabby that it digs out of the riverbed with its snout or catches while swimming. It uses cheek-pouches to carry prey to the surface, where it is eaten. The platypus needs to eat about 20% of its own weight each day, which requires it to spend an average of 12 hours daily looking for food.
Reproduction.
When the platypus was first encountered by European naturalists, they were divided over whether the female laid eggs. This was not confirmed until 1884, when W. H. Caldwell was sent to Australia, where, after extensive searching assisted by a team of 150 Aborigines, he managed to discover a few eggs. Mindful of the high cost per word, Caldwell famously but tersely wired London, "Monotremes oviparous, ovum meroblastic." That is, monotremes lay eggs, and the eggs are similar to those of reptiles in that only part of the egg divides as it develops. 
The species exhibits a single breeding season; mating occurs between June and October, with some local variation taking place between different populations across its range. Historical observation, mark-and-recapture studies, and preliminary investigations of population genetics indicate the possibility of both resident and transient members of populations, and suggest a polygynous mating system. Females are thought likely to become sexually mature in their second year, with breeding confirmed still to take place in animals over 9 years old.
Outside the mating season, the platypus lives in a simple ground burrow, the entrance of which is about 30 cm above the water level. After mating, the female constructs a deeper, more elaborate burrow up to 20 m long and blocked at intervals with plugs (which may act as a safeguard against rising waters or predators, or as a method of regulating humidity and temperature). The male takes no part in caring for its young, and retreats to his year-long burrow. The female softens the ground in the burrow with dead, folded, wet leaves, and she fills the nest at the end of the tunnel with fallen leaves and reeds for bedding material. This material is dragged to the nest by tucking it underneath her curled tail.
The female platypus has a pair of ovaries, but only the left one is functional. The platypus' genes are a possible evolutionary link between the mammalian XY and bird/reptile ZW sex-determination systems because one of the platypus' five X chromosomes contains the DMRT1 gene, which birds possess on their Z chromosome. It lays one to three (usually two) small, leathery eggs (similar to those of reptiles), about 11 mm in diameter and slightly rounder than bird eggs. The eggs develop "in utero" for about 28 days, with only about 10 days of external incubation (in contrast to a chicken egg, which spends about one day in tract and 21 days externally). After laying her eggs, the female curls around them. The incubation period is divided into three phases. In the first phase, the embryo has no functional organs and relies on the yolk sac for sustenance. The yolk is absorbed by the developing young. During the second phase, the digits develop, and in the last phase, the egg tooth appears.
Most mammal zygotes go though holoblastic cleavage, meaning that following fertilization the ovum is split due to cell divisions into multiple, divisible daughter cells. This is in comparison to meroplastic division in birds and platypuses, which causes the ovum to split but not completely. This causes the cells at the edge of the yolk to be cytoplasmically continuous with the egg’s cytoplasm. This allows the yolk, which contains the embryo, to exchange waste and nutrients with the cytoplasm.
The newly hatched young are vulnerable, blind, and hairless, and are fed by the mother's milk. Although possessing mammary glands, the platypus lacks teats. Instead, milk is released through pores in the skin. The milk pools in grooves on her abdomen, allowing the young to lap it up. After they hatch, the offspring are suckled for three to four months. During incubation and weaning, the mother initially leaves the burrow only for short periods, to forage. When doing so, she creates a number of thin soil plugs along the length of the burrow, possibly to protect the young from predators; pushing past these on her return forces water from her fur and allows the burrow to remain dry. After about five weeks, the mother begins to spend more time away from her young and, at around four months, the young emerge from the burrow. A platypus is born with teeth, but these drop out at a very early age, leaving the horny plates it uses to grind food.
Evolution.
The platypus and other monotremes were very poorly understood, and some of the 19th century myths that grew up around them—for example, that the monotremes were "inferior" or quasireptilian—still endure. In 1947, William King Gregory theorised that placental mammals and marsupials may have diverged earlier, and a subsequent branching divided the monotremes and marsupials, but later research and fossil discoveries have suggested this is incorrect. In fact, modern monotremes are the survivors of an early branching of the mammal tree, and a later branching is thought to have led to the marsupial and placental groups. Molecular clock and fossil dating suggest platypuses split from echidnas around 19–48 million years ago.
The oldest discovered fossil of the modern platypus dates back to about 100,000 years ago, during the Quaternary period. The extinct monotremes "Teinolophos" and "Steropodon" were closely related to the modern platypus. The fossilised "Steropodon" was discovered in New South Wales and is composed of an opalised lower jawbone with three molar teeth (whereas the adult contemporary platypus is toothless). The molar teeth were initially thought to be tribosphenic, which would have supported a variation of Gregory's theory, but later research has suggested, while they have three cusps, they evolved under a separate process. The fossil is thought to be about 110 million years old, which means the platypus-like animal was alive during the Cretaceous period, making it the oldest mammal fossil found in Australia. "Monotrematum sudamericanum", another fossil relative of the platypus, has been found in Argentina, indicating monotremes were present in the supercontinent of Gondwana when the continents of South America and Australia were joined via Antarctica (up to about 167 million years ago). A fossilized tooth of a giant platypus species, "Obdurodon tharalkooschild", was dated 5–15 million years ago. Judging by the tooth, the animal measured 1.3 meters long, making it the largest platypus on record.
Because of the early divergence from the therian mammals and the low numbers of extant monotreme species, the platypus is a frequent subject of research in evolutionary biology. In 2004, researchers at the Australian National University discovered the platypus has ten sex chromosomes, compared with two (XY) in most other mammals (for instance, a male platypus is always XYXYXYXYXY), although given the XY designation of mammals, the sex chromosomes of the platypus are more similar to the ZZ/ZW sex chromosomes found in birds. The platypus genome also has both reptilian and mammalian genes associated with egg fertilisation. Though the platypus lacks the mammalian sex-determining gene SRY, a study found that the mechanism of sex determination is the AMH gene on the oldest Y chromosome. A draft version of the platypus genome sequence was published in "Nature" on 8 May 2008, revealing both reptilian and mammalian elements, as well as two genes found previously only in birds, amphibians, and fish. More than 80% of the platypus' genes are common to the other mammals whose genomes have been sequenced.
Conservation status.
Except for its loss from the state of South Australia, the platypus occupies the same general distribution as it did prior to European settlement of Australia. However, local changes and fragmentation of distribution due to human modification of its habitat are documented. Its current and historical abundance, however, are less well-known and it has probably declined in numbers, although still being considered as common over most of its current range. The species was extensively hunted for its fur until the early years of the 20th century and, although protected throughout Australia since 1905, until about 1950 it was still at risk of drowning in the nets of inland fisheries. The platypus does not appear to be in immediate danger of extinction, because conservation measures have been successful, but it could be impacted by habitat disruption caused by dams, irrigation, pollution, netting, and trapping. The IUCN lists the platypus on its Red List as Least Concern.
Platypuses generally suffer from few diseases in the wild; however, public concern in Tasmania is widespread about the potential impacts of a disease caused by the fungus "Mucor amphibiorum". The disease (termed mucormycosis) affects only Tasmanian platypuses, and has not been observed in platypuses in mainland Australia. Affected platypuses can develop skin lesions or ulcers on various parts of their bodies, including their backs, tails, and legs. Mucormycosis can kill platypuses, death arising from secondary infection and by affecting the animals' ability to maintain body temperature and forage efficiently. The Biodiversity Conservation Branch at the Department of Primary Industries and Water are collaborating with NRM north and University of Tasmania researchers to determine the impacts of the disease on Tasmanian platypuses, as well as the mechanism of transmission and current spread of the disease. Until recently, the introduced Red Fox ("Vulpes vulpes") was confined to mainland Australia, but growing evidence now indicates it is present in low numbers in Tasmania.
Much of the world was introduced to the platypus in 1939 when "National Geographic Magazine" published an article on the platypus and the efforts to study and raise it in captivity. The latter is a difficult task, and only a few young have been successfully raised since, notably at Healesville Sanctuary in Victoria. The leading figure in these efforts was David Fleay, who established a platypusary—a simulated stream in a tank—at the Healesville Sanctuary, where breeding was successful in 1943. In 1972, he found a dead baby of about 50 days old, which had presumably been born in captivity, at his wildlife park at Burleigh Heads on the Gold Coast, Queensland. Healesville repeated its success in 1998 and again in 2000 with a similar stream tank. Taronga Zoo in Sydney bred twins in 2003, and breeding was again successful there in 2006.
Platypus in wildlife sanctuaries.
The platypus is kept, for conservation purposes, in special aquariums at the following Australian wildlife sanctuaries:
International.
s of 2013[ [update]], there is no platypus in captivity outside of Australia. Three attempts were made to bring the animals to the Bronx Zoo, in 1922, 1947, and 1958; of these, only two of the three animals introduced in 1947 lived longer than eighteen months.
Cultural references.
The platypus has been featured in the Dreamtime stories of indigenous Australians, who believed the animal was a hybrid of a duck and a water rat.:57–60 According to one story, the major animal groups, the land animals, water animals and birds, all competed for the platypus to join their respective groups, but the platypus ultimately decided to not join any of them, feeling that he did not need to be part of a group to be special.:83–85
The platypus has been used several times as a mascot: "Syd" the platypus was one of the three mascots chosen for the Sydney 2000 Olympics along with an echidna and a kookaburra, "Expo Oz" the platypus was the mascot for World Expo 88, which was held in Brisbane in 1988, and Hexley the platypus is the mascot for Apple Computer's BSD-based Darwin operating system, Mac OS X.
The platypus is also the mascot for the currently inactive Wenatchee Valley Venom arena football team located in Wenatchee, Washington.
The Platypus Trophy was made as an award for the winner of the college rivalry between the Oregon Ducks and the Oregon State Beavers.
The platypus has also been featured in songs, such as Green Day's "Platypus (I Hate You)" and Mr. Bungle's "Platypus". It is the subject of a children's poem by Banjo Paterson.
The platypus has frequently appeared in Australian postage stamps and coins. The earliest appearance is the 9d Australian stamp from 1937. The platypus re-appeared in the 1960–64 Australian Native Animal Series. Souvenir sheet of "from" Laos and Equatorial Guinea has also featured the animal. The platypus has appeared on a 1987 36 cent stamp and an Australian 1996 95 cent stamp. The 2006 Australian Bush Babies stamp series features a $4.65AUD stamp of a young platypus. A 5 cent stamp also produced in 2006 features the platypus also. Since the introduction of decimal currency to Australia in 1966, the embossed image of a platypus, designed and sculpted by Stuart Devlin, has appeared on the reverse (tails) side of the 20-cent coin., making it a most notable depiction of the animal.
The Platypus also frequently appears as a character in children's television programmes, for example, the Platypus Family on "Mister Rogers' Neighborhood", as well as Perry the Platypus on the show "Phineas and Ferb", and Ovide, the star of the cartoon "Ovide and the Gang".
An Australian series of books written by Dorothy Wall in the 1930s features Flap the Platypus as a friend of Blinky Bill.
The platypus also features as the mascot for the ICOCA public transport smart card, used across the JR West rail network in Japan.

</doc>
<doc id="23750" url="http://en.wikipedia.org/wiki?curid=23750" title="Paramagnetism">
Paramagnetism

Paramagnetism is a form of magnetism whereby certain materials are attracted by an externally applied magnetic field, and form internal, induced magnetic fields in the direction of the applied magnetic field. In contrast with this behavior, diamagnetic materials are repelled by magnetic fields and form induced magnetic fields in the direction opposite to that of the applied magnetic field. Paramagnetic materials include most chemical elements and some compounds; they have a relative magnetic permeability greater than or equal to 1 (i.e., a positive magnetic susceptibility) and hence are attracted to magnetic fields. The magnetic moment induced by the applied field is linear in the field strength and rather weak. It typically requires a sensitive analytical balance to detect the effect and modern measurements on paramagnetic materials are often conducted with a SQUID magnetometer.
Paramagnetic materials have a small, positive susceptibility to magnetic fields. These materials are slightly attracted by a magnetic field and the material does not retain the magnetic properties when the external field is removed. Paramagnetic properties are due to the presence of some unpaired electrons, and from the realignment of the electron paths caused by the external magnetic field. Paramagnetic materials include magnesium, molybdenum, lithium, and tantalum.
Unlike ferromagnets, paramagnets do not retain any magnetization in the absence of an externally applied magnetic field because thermal motion randomizes the spin orientations. (Some paramagnetic materials retain spin disorder even at absolute zero, meaning they are paramagnetic in the ground state, i.e. in the absence of thermal motion.) Thus the total magnetization drops to zero when the applied field is removed. Even in the presence of the field there is only a small induced magnetization because only a small fraction of the spins will be oriented by the field. This fraction is proportional to the field strength and this explains the linear dependency. The attraction experienced by ferromagnetic materials is non-linear and much stronger, so that it is easily observed, for instance, in the attraction between a refrigerator magnet and the iron of the refrigerator itself.
Relation to electron spins.
Constituent atoms or molecules of paramagnetic materials have permanent magnetic moments (dipoles), even in the absence of an applied field. The permanent moment generally is due to the spin of unpaired electrons in atomic or molecular electron orbitals (see Magnetic moment). In pure paramagnetism, the dipoles do not interact with one another and are randomly oriented in the absence of an external field due to thermal agitation, resulting in zero net magnetic moment. When a magnetic field is applied, the dipoles will tend to align with the applied field, resulting in a net magnetic moment in the direction of the applied field. In the classical description, this alignment can be understood to occur due to a torque being provided on the magnetic moments by an applied field, which tries to align the dipoles parallel to the applied field. However, the true origins of the alignment can only be understood via the quantum-mechanical properties of spin and angular momentum.
If there is sufficient energy exchange between neighbouring dipoles they will interact, and may spontaneously align or anti-align and form magnetic domains, resulting in ferromagnetism (permanent magnets) or antiferromagnetism, respectively. Paramagnetic behavior can also be observed in ferromagnetic materials that are above their Curie temperature, and in antiferromagnets above their Néel temperature. At these temperatures, the available thermal energy simply overcomes the interaction energy between the spins.
In general, paramagnetic effects are quite small: the magnetic susceptibility is of the order of 10−3 to 10−5 for most paramagnets, but may be as high as 10−1 for synthetic paramagnets such as ferrofluids.
Delocalization.
In conductive materials the electrons are delocalized, that is, they travel through the solid more or less as an electron gas. Conductivity can be understood in a band structure picture as arising from the incomplete filling of energy bands.
In an ordinary nonmagnetic conductor the conduction band is identical for both spin-up and spin-down electrons. When a magnetic field is applied, the conduction band splits apart into a spin-up and a spin-down band due to the difference in magnetic potential energy for spin-up and spin-down electrons.
Since the Fermi level must be identical for both bands, this means that there will be a small surplus of the type of spin in the band that moved downwards. This effect is a weak form of paramagnetism known as "Pauli paramagnetism".
The effect always competes with a diamagnetic response of opposite sign due to all the core electrons of the atoms. Stronger forms of magnetism usually require localized rather than itinerant electrons. However in some cases a band structure can result in which there are two delocalized sub-bands with states of opposite spins that have different energies. If one subband is preferentially filled over the other, one can have itinerant ferromagnetic order. This situation usually only occurs in relatively narrow (d-)bands, which are poorly delocalized.
s and p electrons.
Generally, strong delocalization in a solid due to large overlap with neighboring wave functions means that there will be a large Fermi velocity; this means that the number of electrons in a band is less sensitive to shifts in that band's energy, implying a weak magnetism. This is why s- and p-type metals are typically either Pauli-paramagnetic or as in the case of gold even diamagnetic. In the latter case the diamagnetic contribution from the closed shell inner electrons simply wins from the weak paramagnetic term of the almost free electrons.
d and f electrons.
Stronger magnetic effects are typically only observed when d- or f-electrons are involved. Particularly the latter are usually strongly localized. Moreover the size of the magnetic moment on a lanthanide atom can be quite large as it can carry up to 7 unpaired electrons in the case of gadolinium(III) (hence its use in MRI). The high magnetic moments associated with lanthanides is one reason why superstrong magnets are typically based on elements like neodymium or samarium.
Molecular localization.
Of course the above picture is a "generalization" as it pertains to materials with an extended lattice rather than a molecular structure. Molecular structure can also lead to localization of electrons. Although there are usually energetic reasons why a molecular structure results such that it does not exhibit partly filled orbitals (i.e. unpaired spins), some non-closed shell moieties do occur in nature. Molecular oxygen is a good example. Even in the frozen solid it contains di-radical molecules resulting in paramagnetic behavior. The unpaired spins reside in orbitals derived from oxygen p wave functions, but the overlap is limited to the one neighbor in the O2 molecules. The distances to other oxygen atoms in the lattice remain too large to lead to delocalization and the magnetic moments remain unpaired.
Curie's law.
For low levels of magnetization, the magnetization of paramagnets follows what is known as Curie's law, at least approximately. This law indicates that the susceptibility formula_1 of paramagnetic materials is inversely proportional to their temperature, i.e. that materials become more magnetic at lower temperatures. The mathematical expression is:
where:
Curie's law is valid under the commonly encountered conditions of low magnetization (μBH ≲ kBT), but does not apply in the high-field/low-temperature regime where saturation of magnetization occurs (μBH ≳ kBT) and magnetic dipoles are all aligned with the applied field. When the dipoles are aligned, increasing the external field will not increase the total magnetization since there can be no further alignment.
For a paramagnetic ion with noninteracting magnetic moments with angular momentum J, the Curie constant is related the individual ions' magnetic moments,
The parameter μeff is interpreted as the effective magnetic moment per paramagnetic ion. If one uses a classical treatment with molecular magnetic moments represented as discrete magnetic dipoles, μ, a Curie Law expression of the same form will emerge with μ appearing in place of μeff.
When orbital angular momentum contributions to the magnetic moment are small, as occurs for most organic radicals or for octahedral transition metal complexes with d3 or high-spin d5 configurations, the effective magnetic moment takes the form (ge = 2.0023... ≈ 2),
Examples of paramagnets.
Materials that are called "paramagnets" are most often those that exhibit, at least over an appreciable temperature range, magnetic susceptibilities that adhere to the Curie or Curie–Weiss laws. In principle any system that contains atoms, ions, or molecules with unpaired spins can be called a paramagnet, but the interactions between them need to be carefully considered.
Systems with minimal interactions.
The narrowest definition would be: a system with unpaired spins that "do not interact" with each other. In this narrowest sense, the only pure paramagnet is a dilute gas of monatomic hydrogen atoms. Each atom has one non-interacting unpaired electron. Of course, the latter could be said about a gas of lithium atoms but these already possess two paired core electrons that produce a diamagnetic response of opposite sign. Strictly speaking Li is a mixed system therefore, although admittedly the diamagnetic component is weak and often neglected. In the case of heavier elements the diamagnetic contribution becomes more important and in the case of metallic gold it dominates the properties. Of course, the element hydrogen is virtually never called 'paramagnetic' because the monatomic gas is stable only at extremely high temperature; H atoms combine to form molecular H2 and in so doing, the magnetic moments are lost ("quenched"), because the spins pair. Hydrogen is therefore "diamagnetic" and the same holds true for many other elements. Although the electronic configuration of the individual atoms (and ions) of most elements contain unpaired spins, they are not necessarily paramagnetic, because at ambient temperature quenching is very much the rule rather than the exception. The quenching tendency is weakest for f-electrons because "f" (especially 4"f") orbitals are radially contracted and they overlap only weakly with orbitals on adjacent atoms. Consequently, the lanthanide elements with incompletely filled 4f-orbitals are paramagnetic or magnetically ordered.
Thus, condensed phase paramagnets are only possible if the interactions of the spins that lead either to quenching or to ordering are kept at bay by structural isolation of the magnetic centers. There are two classes of materials for which this holds:
Systems with interactions.
As stated above, many materials that contain d- or f-elements do retain unquenched spins. Salts of such elements often show paramagnetic behavior but at low enough temperatures the magnetic moments may order. It is not uncommon to call such materials 'paramagnets', when referring to their paramagnetic behavior above their Curie or Néel-points, particularly if such temperatures are very low or have never been properly measured. Even for iron it is not uncommon to say that "iron becomes a paramagnet" above its relatively high Curie-point. In that case the Curie-point is seen as a phase transition between a ferromagnet and a 'paramagnet'. The word paramagnet now merely refers to the linear response of the system to an applied field, the temperature dependence of which requires an amended version of Curie's law, known as the Curie–Weiss law:
This amended law includes a term θ that describes the exchange interaction that is present albeit overcome by thermal motion. The sign of θ depends on whether ferro- or antiferromagnetic interactions dominate and it is seldom exactly zero, except in the dilute, isolated cases mentioned above.
Obviously, the paramagnetic Curie–Weiss description above TN or TC is a rather different interpretation of the word "paramagnet" as it does "not" imply the "absence" of interactions, but rather that the magnetic structure is random in the absence of an external field at these sufficiently high temperatures. Even if θ is close to zero this does not mean that there are no interactions, just that the aligning ferro- and the anti-aligning antiferromagnetic ones cancel. An additional complication is that the interactions are often different in different directions of the crystalline lattice (anisotropy), leading to complicated magnetic structures once ordered.
Randomness of the structure also applies to the many metals that show a net paramagnetic response over a broad temperature range. They do not follow a Curie type law as function of temperature however, often they are more or less temperature independent. This type of behavior is of an itinerant nature and better called Pauli-paramagnetism, but it is not unusual to see e.g. the metal aluminium called a "paramagnet", even though interactions are strong enough to give this element very good electrical conductivity.
Superparamagnets.
Some materials show induced magnetic behavior that follows a Curie type law but with exceptionally large values for the Curie constants. These materials are known as superparamagnets. They are characterized by a strong ferromagnetic or ferrimagnetic type of coupling into domains of a limited size that behave independently from one another. The bulk properties of such a system resembles that of a paramagnet, but on a microscopic level they are ordered. The materials do show an ordering temperature above which the behavior reverts to ordinary paramagnetism (with interaction). Ferrofluids are a good example, but the phenomenon can also occur inside solids, e.g., when dilute paramagnetic centers are introduced in a strong itinerant medium of ferromagnetic coupling such as when Fe is substituted in TlCu2Se2 or the alloy AuFe. Such systems contain ferromagnetically coupled clusters that freeze out at lower temperatures. They are also called mictomagnets.

</doc>
