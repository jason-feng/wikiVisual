<doc id="20347" url="http://en.wikipedia.org/wiki?curid=20347" title="Meaning of life">
Meaning of life

The meaning of life is a philosophical and spiritual question concerning the significance of living or existence in general. It can also be expressed in different forms, such as "What should I do?", "Why are we here?", "What is life all about?", and "What is the purpose of existence?" or even "Does life exist at all?" It has been the subject of much philosophical, scientific, and theological speculation throughout history. There have been a large number of proposed answers to these questions from many different cultural and ideological backgrounds.
The meaning of life is in the philosophical and religious conceptions of existence, social ties, consciousness, and happiness, and borders on many other issues, such as symbolic meaning, ontology, value, purpose, ethics, good and evil, free will, the existence of one or multiple gods, conceptions of God, the soul, and the afterlife. Scientific contributions focus primarily on describing related empirical facts about the universe, exploring the context and parameters concerning the 'how' of life. Science also studies and can provide recommendations for the pursuit of well-being and a related conception of morality. An alternative, humanistic approach poses the question "What is the meaning of "my" life?" The value of the question pertaining to the purpose of life may coincide with the achievement of ultimate reality, or a feeling of oneness, or even a feeling of fearness. 
Questions.
Questions about the meaning of life have been expressed in a broad variety of ways, including the following:
These questions have resulted in a wide range of competing answers and arguments, from scientific theories, to philosophical, theological, and spiritual explanations.
Scientific inquiry and perspectives.
Many members of the scientific community and philosophy of science communities think that science can provide the relevant context, and set of parameters necessary for dealing with topics related to the meaning of life. In their view, science can offer a wide range of insights on topics ranging from the science of happiness to death anxiety. Scientific inquiry facilitates this through nomological investigation into various aspects of life and reality, such as the Big Bang, the origin of life, and evolution, and by studying the objective factors which correlate with the subjective experience of meaning and happiness.
Psychological significance and value in life.
Researchers in positive psychology study empirical factors that lead to life satisfaction, full engagement in activities, making a fuller contribution by utilizing one's personal strengths, and meaning based on investing in something larger than the self. Large-data studies of flow experiences have consistently suggested that humans experience meaning and fulfillment when mastering challenging tasks, and that the experience comes from the way tasks are approached and performed rather than the particular choice of task. For example, flow experiences can be obtained by prisoners in concentration camps with minimal facilities, and occur only slightly more often in billionaires. A classic example is of two workers on an apparently boring production line in a factory. One treats the work as a tedious chore while the other turns it into a game to see how fast she can make each unit, and achieves flow in the process.
Neuroscience describes reward, pleasure, and motivation in terms of neurotransmitter activity, especially in the limbic system and the ventral tegmental area in particular. If one believes that the meaning of life is to maximize pleasure and to ease general life, then this allows normative predictions about how to act to achieve this. Likewise, some ethical naturalists advocate a science of morality – the empirical pursuit of flourishing for all conscious creatures.
Experimental philosophy and neuroethics research collects data about human ethical decisions in controlled scenarios such as trolley problems. It has shown that many types of ethical judgment are universal across cultures, suggesting that they may be innate, whilst others are culture specific. The findings show actual human ethical reasoning to be at odds with most logical philosophical theories, for example consistently showing distinctions between action by cause and action by omission which would be absent from utility based theories. Cognitive science has theorized about differences between conservative and liberal ethics and how they may be based on different metaphors from family life such as strong fathers vs nurturing mother models.
Neurotheology is a controversial field which tries to find neural correlates and mechanisms of religious experience. Some researchers have suggested that the human brain has innate mechanisms for such experiences and that living without using them for their evolved purposes may be a cause of imbalance. Studies have reported conflicted results on correlating happiness with religious belief and it is difficult to find unbiased meta-analyses.
Sociology examines value at a social level using theoretical constructs such as value theory, norms, anomie, etc. One value system suggested by social psychologists, broadly called Terror Management Theory, states that human meaning is derived from a fundamental fear of death, and values are selected when they allow us to escape the mental reminder of death.
Emerging research shows that meaning in life predicts better physical health outcomes. Greater meaning has been associated with a reduced risk of Alzheimer's disease, reduced risk of heart attack among individuals with coronary heart disease, reduced risk of stroke, and increased longevity in both American and Japanese samples. In 2014, the British National Health Service began recommending a five step plan for mental well-being based on meaningful lives, whose steps are: (1) Connect with community and family; (2) Physical exercise; (3) Lifelong learning; (4) Giving to others; (5) Mindfulness of the world around you.
Origin and nature of biological life.
The exact mechanisms of abiogenesis are unknown: notable hypotheses include the RNA world hypothesis (RNA-based replicators) and the iron-sulfur world theory (metabolism without genetics). The process by which different lifeforms have developed throughout history via genetic mutation and natural selection is explained by evolution. At the end of the 20th century, based upon insight gleaned from the gene-centered view of evolution, biologists George C. Williams, Richard Dawkins, David Haig, among others, concluded that if there is a primary function to life, it is the replication of DNA and the survival of one's genes. This view has not achieved universal agreement; Jeremy Griffith is a notable exception, maintaining that the meaning of life is to be integrative. Responding to an interview question from Richard Dawkins about "what it is all for", James Watson stated "I don't think we're "for" anything. We're just the products of evolution."
Though scientists have intensively studied life on Earth, defining life in unequivocal terms is still a challenge. Physically, one may say that life "feeds on negative entropy" which refers to the process by which living entities decrease their internal entropy at the expense of some form of energy taken in from the environment. Biologists generally agree that lifeforms are self-organizing systems regulating the internal environment as to maintain this organized state, metabolism serves to provide energy, and reproduction causes life to continue over a span of multiple generations. Typically, organisms are responsive to stimuli and genetic information changes from generation to generation, resulting in adaptation through evolution; this optimizes the chances of survival for the individual organism and its descendants respectively.
Non-cellular replicating agents, notably viruses, are generally not considered to be organisms because they are incapable of independent reproduction or metabolism. This classification is problematic, though, since some parasites and endosymbionts are also incapable of independent life. Astrobiology studies the possibility of different forms of life on other worlds, including replicating structures made from materials other than DNA.
Origins and ultimate fate of the universe.
Though the Big Bang theory was met with much skepticism when first introduced, it has become well-supported by several independent observations. However, current physics can only describe the early universe from 10−43 seconds after the Big Bang (where zero time corresponds to infinite temperature); a theory of quantum gravity would be required to understand events before that time. Nevertheless, many physicists have speculated about what would have preceded this limit, and how the universe came into being. For example, one interpretation is that the Big Bang occurred coincidentally, and when considering the anthropic principle, it is sometimes interpreted as implying the existence of a multiverse.
The ultimate fate of the universe, and implicitly humanity, is hypothesized as one in which biological life will eventually become unsustainable, such as through a Big Freeze, Big Rip, or Big Crunch.
Theoretical cosmology studies many alternative speculative models for the origin and fate of the universe beyond the big bang theory. A recent trend has been models of the creation of 'baby universes' inside black holes, with our own big bang resulting from the formation of a black hole in another parent universe. Multiverse theories claim that every possibility of quantum mechanics is played out in parallel universes.
Scientific questions about the mind.
The nature and origin of consciousness and the mind itself are also widely debated in science. The explanatory gap is generally equated with the hard problem of consciousness, and the question of free will is also considered to be of fundamental importance. These subjects are mostly addressed in the fields of cognitive science, neuroscience (e.g. the neuroscience of free will) and philosophy of mind, though some evolutionary biologists and theoretical physicists have also made several allusions to the subject.
Reductionistic and eliminative materialistic approaches, for example the Multiple Drafts Model, hold that consciousness can be wholly explained by neuroscience through the workings of the brain and its neurons, thus adhering to biological naturalism.
On the other hand, some scientists, like Andrei Linde, have considered that consciousness, like spacetime, might have its own intrinsic degrees of freedom, and that one's perceptions may be as real as (or even more real than) material objects. Hypotheses of consciousness and spacetime explain consciousness in describing a "space of conscious elements", often encompassing a number of extra dimensions. Electromagnetic theories of consciousness solve the binding problem of consciousness in saying that the electromagnetic field generated by the brain is the actual carrier of conscious experience, there is however disagreement about the implementations of such a theory relating to other workings of the mind. Quantum mind theories use quantum theory in explaining certain properties of the mind. Explaining the process of free will through quantum phenomena is a popular alternative to determinism.
Parapsychology.
Based on the premises of non-materialistic explanations of the mind, some have suggested the existence of a cosmic consciousness, asserting that consciousness is actually the "ground of all being". Proponents of this view cite accounts of paranormal phenomena, primarily extrasensory perceptions and psychic powers, as evidence for an incorporeal higher consciousness. In hopes of proving the existence of these phenomena, parapsychologists have orchestrated various experiments, but apparently unsuccessful results are more likely due to sloppy procedures, poorly trained researchers, or methodological flaws than to actual effects.
Western philosophical perspectives.
The philosophical perspectives on the meaning of life are those ideologies which explain life in terms of ideals or abstractions defined by humans.
Ancient Greek philosophy.
Platonism.
Plato, a pupil of Socrates, was one of the earliest, most influential philosophers. His reputation comes from his idealism of believing in the existence of universals. His Theory of Forms proposes that universals do not physically exist, like objects, but as heavenly forms. In the dialogue of "The Republic", the character of Socrates describes the Form of the Good.
In Platonism, the meaning of life is in attaining the highest form of knowledge, which is the Idea (Form) of the Good, from which all good and just things derive utility and value.
Aristotelianism.
Aristotle, an apprentice of Plato, was another early and influential philosopher, who argued that ethical knowledge is not "certain" knowledge (such as metaphysics and epistemology), but is "general" knowledge. Because it is not a theoretical discipline, a person had to study and practice in order to become "good"; thus if the person were to become virtuous, he could not simply study what virtue "is", he had to "be" virtuous, via virtuous activities. To do this, Aristotle established what is virtuous:
Every skill and every inquiry, and similarly, every action and choice of action, is thought to have some good as its object. This is why the good has rightly been defined as the object of all endeavor [...]<br>Everything is done with a goal, and that goal is "good".—Nicomachean Ethics 1.1
Yet, if action A is done towards achieving goal B, then goal B also would have a goal, goal C, and goal C also would have a goal, and so would continue this pattern, until something stopped its infinite regression. Aristotle's solution is the "Highest Good", which is desirable for its own sake. It is its own goal. The Highest Good is not desirable for the sake of achieving some other good, and all other "goods" desirable for its sake. This involves achieving "eudaemonia", usually translated as "happiness", "well-being", "flourishing", and "excellence".
What is the highest good in all matters of action? To the name, there is almost complete agreement; for uneducated and educated alike call it happiness, and make happiness identical with the good life and successful living. They disagree, however, about the meaning of happiness.—Nicomachean Ethics 1.4
Cynicism.
Antisthenes, a pupil of Socrates, first outlined the themes of Cynicism, stating that the purpose of life is living a life of Virtue which agrees with Nature. Happiness depends upon being self-sufficient and master of one's mental attitude; suffering is the consequence of false judgments of value, which cause negative emotions and a concomitant vicious character.
The Cynical life rejects conventional desires for wealth, power, health, and fame, by being free of the possessions acquired in pursuing the conventional. As reasoning creatures, people could achieve happiness via rigorous training, by living in a way natural to human beings. The world equally belongs to everyone, so suffering is caused by false judgments of what is valuable and what is worthless per the customs and conventions of society.
Cyrenaicism.
Aristippus of Cyrene, a pupil of Socrates, founded an early Socratic school that emphasized only one side of Socrates's teachings - that happiness is one of the ends of moral action and that pleasure is the supreme good; thus a hedonistic world view, wherein bodily gratification is more intense than mental pleasure. Cyrenaics prefer immediate gratification to the long-term gain of delayed gratification; denial is unpleasant unhappiness.
Epicureanism.
Epicurus, a pupil of the Platonist Pamphilus of Samos, taught that the greatest good is in seeking modest pleasures, to attain tranquility and freedom from fear (ataraxia) via knowledge, friendship, and virtuous, temperate living; bodily pain (aponia) is absent through one's knowledge of the workings of the world and of the limits of one's desires. Combined, freedom from pain and freedom from fear are happiness in its highest form. Epicurus' lauded enjoyment of simple pleasures is quasi-ascetic "abstention" from sex and the appetites:
"When we say ... that pleasure is the end and aim, we do not mean the pleasures of the prodigal or the pleasures of sensuality, as we are understood to do, by some, through ignorance, prejudice or wilful misrepresentation. By pleasure we mean the absence of pain in the body and of trouble in the soul. It is not by an unbroken succession of drinking bouts and of revelry, not by sexual lust, nor the enjoyment of fish, and other delicacies of a luxurious table, which produce a pleasant life; it is sober reasoning, searching out the grounds of every choice and avoidance, and banishing those beliefs through which the greatest tumults take possession of the soul."
The Epicurean meaning of life rejects immortality and mysticism; there is a soul, but it is as mortal as the body. There is no afterlife, yet, one need not fear death, because "Death is nothing to us; for that which is dissolved, is without sensation, and that which lacks sensation is nothing to us."
Stoicism.
Zeno of Citium, a pupil of Crates of Thebes, established the school which teaches that living according to reason and virtue is to be in harmony with the universe's divine order, entailed by one's recognition of the universal "logos", or reason, an essential value of all people. The meaning of life is "freedom from suffering" through "apatheia" (Gr: απαθεια), that is, being objective and having "clear judgement", "not" indifference.
Stoicism's prime directives are virtue, reason, and natural law, abided to develop personal self-control and mental fortitude as means of overcoming destructive emotions. The Stoic does not seek to extinguish emotions, only to avoid emotional troubles, by developing clear judgement and inner calm through diligently practiced logic, reflection, and concentration.
The Stoic ethical foundation is that "good lies in the state of the soul", itself, exemplified in wisdom and self-control, thus improving one's spiritual well-being: ""Virtue" consists in a "will" which is in agreement with Nature." The principle applies to one's personal relations thus: "to be free from anger, envy, and jealousy".
Enlightenment philosophy.
The Enlightenment and the colonial era both changed the nature of European philosophy and exported it worldwide. Devotion and subservience to God were largely replaced by notions of inalienable natural rights and the potentialities of reason, and universal ideals of love and compassion gave way to civic notions of freedom, equality, and citizenship. The meaning of life changed as well, focusing less on humankind's relationship to God and more on the relationship between individuals and their society. This era is filled with theories that equate meaningful existence with the social order.
Classical liberalism.
Classical liberalism is a set of ideas that arose in the 17th and 18th centuries, out of conflicts between a growing, wealthy, propertied class and the established aristocratic and religious orders that dominated Europe. Liberalism cast humans as beings with inalienable natural rights (including the right to retain the wealth generated by one's own work), and sought out means to balance rights across society. Broadly speaking, it considers individual liberty to be the most important goal, because only through ensured liberty are the other inherent rights protected.
There are many forms and derivations of liberalism, but their central conceptions of the meaning of life trace back to three main ideas. Early thinkers such as John Locke, Jean-Jacques Rousseau and Adam Smith saw humankind beginning in the state of nature, then finding meaning for existence through labor and property, and using social contracts to create an environment that supports those efforts.
Kantianism.
Kantianism is a philosophy based on the ethical, epistemological, and metaphysical works of Immanuel Kant. Kant is known for his deontological theory where there is a single moral obligation, the "Categorical Imperative", derived from the concept of duty. Kantians believe all actions are performed in accordance with some underlying maxim or principle, and for actions to be ethical, they must adhere to the categorical imperative.
Simply put, the test is that one must universalize the maxim (imagine that all people acted in this way) and then see if it would still be possible to perform the maxim in the world without contradiction. In "Groundwork", Kant gives the example of a person who seeks to borrow money without intending to pay it back. This is a contradiction because if it were a universal action, no person would lend money anymore as he knows that he will never be paid back. The maxim of this action, says Kant, results in a contradiction in conceivability (and thus contradicts perfect duty).
Kant also denied that the consequences of an act in any way contribute to the moral worth of that act, his reasoning being that the physical world is outside one's full control and thus one cannot be held accountable for the events that occur in it.
19th century philosophy.
Utilitarianism.
The origins of utilitarianism can be traced back as far as Epicurus, but, as a school of thought, it is credited to Jeremy Bentham, who found that "nature has placed mankind under the governance of two sovereign masters, pain and pleasure", then, from that moral insight, deriving the "Rule of Utility": "that the good is whatever brings the greatest happiness to the greatest number of people". He defined the meaning of life as the "greatest happiness principle".
Jeremy Bentham's foremost proponent was James Mill, a significant philosopher in his day, and father of John Stuart Mill. The younger Mill was educated per Bentham's principles, including transcribing and summarizing much of his father's work.
Nihilism.
Nihilism suggests that life is without objective meaning.
Friedrich Nietzsche characterized nihilism as emptying the world, and especially human existence, of meaning, purpose, comprehensible truth, and essential value; succinctly, nihilism is the process of "the devaluing of the highest values". Seeing the nihilist as a natural result of the idea that God is dead, and insisting it was something to overcome, his questioning of the nihilist's life-negating values returned meaning to the Earth.
To Martin Heidegger, nihilism is the movement whereby "being" is forgotten, and is transformed into value, in other words, the reduction of being to exchange value. Heidegger, in accordance with Nietzsche, saw in the so-called "death of God" a potential source for nihilism:
"If God, as the supra-sensory ground and goal, of all reality, is dead; if the supra-sensory world of the Ideas has suffered the loss of its obligatory, and above it, its vitalizing and up-building power, then nothing more remains to which Man can cling, and by which he can orient himself."
The French philosopher Albert Camus asserts that the absurdity of the human condition is that people search for external values and meaning in a world which has none, and is indifferent to them. Camus writes of value-nihilists such as Meursault, but also of values in a nihilistic world, that people can instead strive to be "heroic nihilists", living with dignity in the face of absurdity, living with "secular saintliness", fraternal solidarity, and rebelling against and transcending the world's indifference.
20th-century philosophy.
The current era has seen radical changes in both formal and popular conceptions of human nature. The knowledge disclosed by modern science has effectively rewritten the relationship of humankind to the natural world. Advances in medicine and technology have freed humans from significant limitations and ailments of previous eras; and philosophy—particularly following the linguistic turn—has altered how the relationships people have with themselves and each other are conceived. Questions about the meaning of life have also seen radical changes, from attempts to reevaluate human existence in biological and scientific terms (as in pragmatism and logical positivism) to efforts to meta-theorize about meaning-making as a personal, individual-driven activity (existentialism, secular humanism).
Pragmatism.
Pragmatism, originated in the late-19th-century U.S., to concern itself (mostly) with truth, positing that "only in struggling with the environment" do data, and derived theories, have meaning, and that "consequences", like utility and practicality, are also components of truth. Moreover, pragmatism posits that "anything" useful and practical is not always true, arguing that what most contributes to the most human good in the long course is true. In practice, theoretical claims must be "practically verifiable", i.e. one should be able to predict and test claims, and, that, ultimately, the needs of mankind should guide human intellectual inquiry.
Pragmatic philosophers suggest that the practical, useful understanding of life is more important than searching for an impractical abstract truth about life. William James argued that truth could be made, but not sought. To a pragmatist, the meaning of life is discoverable only via experience.
Theism.
Theists believe God created the universe and that God had a purpose in doing so. Many theists, including the former atheist Antony Flew, have been persuaded that God created because of the scientific evidence for a low entropy Big Bang more than 13 billion years ago. Theists also hold the view that humans find their meaning and purpose for life in God's purpose in creating. Theists further hold that if there were no God to give life ultimate meaning, value and purpose, then life would be absurd.
Existentialism.
According to existentialism, each man and each woman creates the essence (meaning) of his and her life; life is not determined by a supernatural god or an earthly authority, one is free. As such, one's ethical prime directives are "action", "freedom", and "decision", thus, existentialism opposes rationalism and positivism. In seeking meaning to life, the existentialist looks to where people find meaning in life, in course of which using only reason as a source of meaning is insufficient; this gives rise to the emotions of anxiety and dread, felt in considering one's free will, and the concomitant awareness of death. According to Jean-Paul Sartre, existence precedes essence; the (essence) of one's life arises "only" after one comes to existence.
Søren Kierkegaard spoke about a "", arguing that life is full of absurdity, and one must make his and her own values in an indifferent world. One can live meaningfully (free of despair and anxiety) in an unconditional commitment to something finite, and devotes that meaningful life to the commitment, despite the vulnerability inherent to doing so.
Arthur Schopenhauer answered: "What is the meaning of life?" by stating that one's life reflects one's will, and that the will (life) is an aimless, irrational, and painful drive. Salvation, deliverance, and escape from suffering are in aesthetic contemplation, sympathy for others, and asceticism.
For Friedrich Nietzsche, life is worth living only if there are goals inspiring one to live. Accordingly, he saw nihilism ("all that happens is meaningless") as without goals. He stated that asceticism denies one's living in the world; stated that values are not objective facts, that are rationally necessary, universally binding commitments: our evaluations are interpretations, and not reflections of the world, as it is, in itself, and, therefore, all ideations take place from a particular perspective.
Absurdism.
"... in spite of or in defiance of the whole of existence he wills to be himself with it, to take it along, almost defying his torment. For to hope in the possibility of help, not to speak of help by virtue of the absurd, that for God all things are possible – no, that he will not do. And as for seeking help from any other – no, that he will not do for all the world; rather than seek help he would prefer to be himself – with all the tortures of hell, if so it must be."
Søren Kierkegaard, "The Sickness Unto Death" 
In absurdist philosophy, the Absurd arises out of the fundamental disharmony between the individual's search for meaning and the apparent meaninglessness of the universe. As beings looking for meaning in a meaningless world, humans have three ways of resolving the dilemma. Kierkegaard and Camus describe the solutions in their works, "The Sickness Unto Death" (1849) and "The Myth of Sisyphus" (1942):
Secular humanism.
Per secular humanism, the human species came to be by reproducing successive generations in a progression of unguided evolution as an integral expression of nature, which is self-existing. Human knowledge comes from human observation, experimentation, and rational analysis (the scientific method), and not from supernatural sources; the nature of the universe is what people discern it to be. Likewise, "values and realities" are determined "by means of intelligent inquiry" and "are derived from human need and interest as tested by experience", that is, by critical intelligence. "As far as we know, the total personality is [a function] of the biological organism transacting in a social and cultural context."
People determine human purpose without supernatural influence; it is the human personality (general sense) that is the purpose of a human being's life. Humanism seeks to develop and fulfill: "Humanism affirms our ability and responsibility to lead ethical lives of personal fulfillment that aspire to the greater good of humanity". Humanism aims to promote enlightened self-interest and the common good for all people. It is based on the premises that the happiness of the individual person is inextricably linked to the well-being of all humanity, in part because humans are social animals who find meaning in personal relations and because cultural progress benefits everybody living in the culture.
The philosophical subgenres posthumanism and transhumanism (sometimes used synonymously) are extensions of humanistic values. One should seek the advancement of humanity and of all life to the greatest degree feasible and seek to reconcile Renaissance humanism with the 21st century's technoscientific culture. In this light, every living creature has the right to determine its personal and social "meaning of life".
From a humanism-psychotherapeutic point of view, the question of the meaning of life could be reinterpreted as "What is the meaning of "my" life?" This approach emphasizes that the question is personal—and avoids focusing on cosmic or religious questions about overarching purpose. There are many therapeutic responses to this question. For example Viktor Frankl argues for "Dereflection", which translates largely as: cease endlessly reflecting on the self; instead, engage in life. On the whole, the therapeutic response is that the question itself—what is the meaning of life?—evaporates when one is fully engaged in life. (The question then morphs into more specific worries such as "What delusions am I under?"; "What is blocking my ability to enjoy things?"; "Why do I neglect loved-ones?".) "See also: Existential Therapy and Irvin Yalom"
Logical positivism.
Logical positivists ask: "What is the meaning of life?", "What is the meaning in asking?" and "If there are no objective values, then, is life meaningless?" Ludwig Wittgenstein and the logical positivists said: "Expressed in language, the question is meaningless"; because, "in" life the statement the "meaning of x", usually denotes the "consequences" of x, or the "significance" of x, or "what is notable" about x, etc., thus, when the meaning of life concept equals "x", in the statement the "meaning of x", the statement becomes recursive, and, therefore, nonsensical, or it might refer to the fact that biological life is essential to having a meaning in life.
The things (people, events) in the life of a person can have meaning (importance) as parts of a whole, but a discrete meaning of (the) life, itself, aside from those things, cannot be discerned. A person's life has meaning (for himself, others) as the life events resulting from his achievements, legacy, family, etc., but, to say that life, itself, has meaning, is a misuse of language, since any note of significance, or of consequence, is relevant only "in" life (to the living), so rendering the statement erroneous. Bertrand Russell wrote that although he found that his distaste for torture was not like his distaste for broccoli, he found no satisfactory, empirical method of proving this:
When we try to be definite, as to what we mean when we say that this or that is "the Good," we find ourselves involved in very great difficulties. Bentham's creed, that pleasure is the Good, roused furious opposition, and was said to be a pig's philosophy. Neither he nor his opponents could advance any argument. In a scientific question, evidence can be adduced on both sides, and, in the end, one side is seen to have the better case — or, if this does not happen, the question is left undecided. But in a question, as to whether this, or that, is the ultimate Good, there is no evidence, either way; each disputant can only appeal to his own emotions, and employ such rhetorical devices as shall rouse similar emotions in others ... Questions as to "values" — that is to say, as to what is good or bad on its own account, independently of its effects — lie outside the domain of science, as the defenders of religion emphatically assert. I think that, in this, they are right, but, I draw the further conclusion, which they do not draw, that questions as to "values" lie wholly outside the domain of knowledge. That is to say, when we assert that this, or that, has "value", we are giving expression to our own emotions, not to a fact, which would still be true if our personal feelings were different.
Postmodernism.
Postmodernist thought—broadly speaking—sees human nature as constructed by language, or by structures and institutions of human society. Unlike other forms of philosophy, postmodernism rarely seeks out "a priori" or innate meanings in human existence, but instead focuses on analyzing or critiquing "given" meanings in order to rationalize or reconstruct them. Anything resembling a "meaning of life", in postmodernist terms, can only be understood within a social and linguistic framework, and must be pursued as an escape from the power structures that are already embedded in all forms of speech and interaction. As a rule, postmodernists see awareness of the constraints of language as necessary to escaping those constraints, but different theorists take different views on the nature of this process: from radical reconstruction of meaning by individuals (as in deconstructionism) to theories in which individuals are primarily extensions of language and society, without real autonomy (as in poststructuralism). In general, postmodernism seeks meaning by looking at the underlying structures that create or impose meaning, rather than the epiphenomenal appearances of the world.
Naturalistic pantheism.
According to naturalistic pantheism, the meaning of life is to care for and look after nature and the environment.
East Asian philosophy.
Mohism.
The Mohist philosophers believed that the purpose of life was universal, impartial love. Mohism promoted a philosophy of impartial caring - a person should care equally for all other individuals, regardless of their actual relationship to him or her. The expression of this indiscriminate caring is what makes man a righteous being in Mohist thought. This advocacy of impartiality was a target of attack by the other Chinese philosophical schools, most notably the Confucians who believed that while love should be unconditional, it should not be indiscriminate. For example, children should hold a greater love for their parents than for random strangers.
Confucianism.
Confucianism recognizes human nature in accordance with the need for discipline and education. Because mankind is driven by both positive and negative influences, Confucianists see a goal in achieving virtue through strong relationships and reasoning as well as minimizing the negative. This emphasis on normal living is seen in the Confucianist scholar Tu Wei-Ming's quote, "we can realize the ultimate meaning of life in ordinary human existence."
Legalism.
The Legalists believed that finding the purpose of life was a meaningless effort. To the Legalists, only practical knowledge was valuable, especially as it related to the function and performance of the state.
Religious perspectives.
The religious perspectives on the meaning of life are those ideologies which explain life in terms of an implicit purpose not defined by humans.
According to the Charter for Compassion signed by many of the world's leading religious and secular organizations, the core of religion is the golden rule of `treat others as you would have them treat you'. The Charter's founder, Karen Armstrong, quotes an ancient Rabbi who suggested that `the rest is commentary'. This is not to reduce the commentary's importance, and Armstrong considers that its study, interpretation and ritual are the means by which religious people internalize and live the golden rule.
Western religions.
Judaism.
In the Judaic world view, the meaning of life is to elevate the physical world ('Olam HaZeh') and prepare it for the world to come ('Olam HaBa'), the messianic era. This is called Tikkun Olam ("Fixing the World"). Olam HaBa can also mean the spiritual afterlife, and there is debate concerning the eschatological order. However, Judaism is not focused on personal salvation, but on communal (between man and man) and individual (between man and God) spiritualised actions in this world.
Judaism's most important feature is the worship of a single, incomprehensible, transcendent, one, indivisible, absolute Being, who created and governs the universe. Closeness with the God of Israel is through study of His Torah, and adherence to its mitzvot (divine laws). In traditional Judaism, God established a special covenant with a people, the people of Israel, at Mount Sinai, giving the Jewish commandments. Torah comprises the written Pentateuch and the transcribed oral tradition, further developed through the generations. The Jewish people are intended as "a kingdom of priests and a holy nation" and a "light to the Nations", influencing the other peoples to keep their own religio-ethical Seven Laws of Noah. The messianic era is seen as the perfection of this dual path to God.
Jewish observances involve ethical and ritual, affirmative and prohibitive injunctions. Modern Jewish denominations differ over the nature, relevance and emphases of mitzvot. Jewish philosophy emphasises that God is not affected or benefited, but the individual and society benefit by drawing close to God. The rationalist Maimonides sees the ethical and ritual divine commandments as a necessary, but insufficient preparation for philosophical understanding of God, with its love and awe. Among fundamental values in the Torah are pursuit of justice, compassion, peace, kindness, hard work, prosperity, humility, and education. The world to come, prepared in the present, elevates man to an everlasting connection with God. Simeon the Righteous says, "the world stands on three things: on Torah, on worship, and on acts of loving kindness." The prayer book relates, "blessed is our God who created us for his honor...and planted within us everlasting life." Of this context, the Talmud states, "everything that God does is for the good," including suffering.
The Jewish mystical Kabbalah gives complimentary esoteric meanings of life. As well as Judaism providing an immanent relationship with God (personal theism), in Kabbalah the spiritual and physical creation is a paradoxical manifestation of the immanent aspects of God's Being (panentheism), related to the Shekhinah (Divine feminine). Jewish observance unites the sephirot (Divine attributes) on high, restoring harmony to creation. In Lurianic Kabbalah, the meaning of life is the messianic rectification of the shattered sparks of God's persona, exiled in physical existence (the Kelipot shells), through the actions of Jewish observance. Through this, in Hasidic Judaism the ultimate essential "desire" of God is the revelation of the Omnipresent Divine essence through materiality, achieved by man from within his limited physical realm, when the body will give life to the soul.
Christianity.
Christianity has its roots in Judaism, and shares much of the latter faith's ontology, its central beliefs derive from the teachings of Jesus Christ, as presented in the New Testament. Life's purpose in Christianity is to seek divine salvation through the grace of God and intercession of Christ. (cf. John 11:26) The New Testament speaks of God wanting to have a relationship with humans both in this life and the life to come, which can happen only if one's sins are forgiven (John 3:16–21; 2 Peter 3:9).
In the Christian view, humankind was made in the Image of God and perfect, but the Fall of Man caused the progeny of the first Parents to inherit Original Sin. The sacrifice of Christ's passion, death and resurrection provide the means for transcending that impure state (Romans 6:23). The means for doing so varies between different groups of Christians, but all rely on belief in Jesus, his work on the cross and his resurrection as the fundamental starting point for a relationship with God. Faith in God is found in Ephesians 2:8–9 – "[8]For by grace you have been saved through faith; and that not of yourselves, it is the gift of God; [9]not as a result of works, that no one should boast." (New American Standard Bible; 1973). A recent alternative Christian theological discourse interprets Jesus as revealing that the purpose of life is to elevate our compassionate response to human suffering. Nonetheless the conventional Christian position is that people are justified by belief in the propitiatory sacrifice of Jesus' death on the cross. The Gospel maintains that through this belief, the barrier that sin has created between man and God is destroyed, and allows God to change people and instill in them a new heart after his own will, and the ability to do it. This is what the terms "reborn" or "saved" almost always refer to.
In the "Westminster Shorter Catechism", the first question is: "What is the chief end of Man?", that is, "What is Man's main purpose?". The answer is: "Man's chief end is to glorify God, and enjoy him forever". God requires one to obey the revealed moral law saying: "love the Lord your God with all your heart, with all your soul, with all your strength, and with all your mind; and your neighbour as yourself". The "Baltimore Catechism" answers the question "Why did God make you?" by saying "God made me to know Him, to love Him, and to serve Him in this world, and to be happy with Him forever in heaven."
The Apostle Paul also answers this question in his speech on the Areopagus in Athens: "And He has made from one blood every nation of men to dwell on all the face of the earth, and has determined their preappointed times and the boundaries of their dwellings, so that they should seek the Lord, in the hope that they might grope for Him and find Him, though He is not far from each one of us."
Catholicism's way of thinking is better expressed through the Principle and Foundation of St. Ignatius of Loyola: "The human person is created to praise, reverence, and serve God Our Lord, and by doing so, to save his or her soul. All other things on the face of the earth are created for human beings in order to help them pursue the end for which they are created. It follows from this that one must use other created things, in so far as they help towards one's end, and free oneself from them, in so far as they are obstacles to one's end. To do this, we need to make ourselves indifferent to all created things, provided the matter is subject to our free choice and there is no other prohibition. Thus, as far as we are concerned, we should not want health more than illness, wealth more than poverty, fame more than disgrace, a long life more than a short one, and similarly for all the rest, but we should desire and choose only what helps us more towards the end for which we are created."
The Church of Jesus Christ of Latter-day Saints (LDS Church) teaches that the purpose of life on Earth is to gain knowledge and experience. Mormons believe that humans are literally the spirit children of God the Father (Acts 17:29, Heb. 12:9), and thus have the potential to progress to become like Him (Matt 5:48). Mormons teach that God provided his children the choice to come to Earth, which is considered a crucial stage in their development — wherein a mortal body, coupled with the freedom to choose, makes for an ideal environment to learn and grow. The Fall of Adam is not viewed as an unfortunate or unplanned cancellation of God's original plan for a paradise, rather the opposition found in mortality is an essential element of God's plan because the process of enduring/overcoming challenges, difficulties, temptations, etc. provides exclusive opportunities to gain wisdom and strength—which is centered on learning to appreciate and choose the good, and reject evil (Gen. 3:22; Book of Mormon, 2 Nephi 2:11; Pearl of Great Price, Moses 6:55). Physical separation from God is an integral part of this mortal learning experience, without which humans would never have the opportunity to learn to live by faith—which as Christ taught in the New Testament, is the key to invoking the powers of heaven (Mark 11:22-23). Despite this physical separation, God doesn't leave humans in darkness. From the beginning, God has followed a pattern of revealing knowledge through chosen prophets. This instruction from God includes the concept of repentance as a lifelong growth process through which humankind continuously learns to make better choices by forsaking sin and learning from mistakes. Throughout this process, baptized members can regularly invoke the cleansing power of Christ's atonement through the weekly ordinance of the sacrament (Luke 22:17-20). It is through the atonement that mortals are made worthy to return to the presence of the Father, where they can continue to build upon the wisdom gained during mortality (Doctrine and Covenants 130:18-19) and ultimately fulfill their end purpose, which is to inherit a fullness of God's glory (Rom. 8:16-17, Gal. 4:7)—that is to say, his intelligence (Doctrine and Covenants 93:36; 50:24). Because God is just, he allows those who weren't taught the gospel during mortality to receive it after death in the spirit world (1 Pet. 3:18-20, 1 Pet. 4:6, Doctrine and Covenants 138), so that all of his children have the opportunity to return to live with God, and reach their full potential.
Islam.
In Islam, man's ultimate life objective is to worship the creator Allah (English: God) by abiding by the Divine guidelines revealed in the Qur'an and the Tradition of the Prophet. Earthly life is merely a test, determining one's afterlife, either in "Jannah" (Paradise) or in "Jahannam" (Hell).
For Allah's satisfaction, via the Qur'an, all Muslims must believe in God, his revelations, his angels, his messengers, and in the "Day of Judgment". The Qur'an describes the purpose of creation as follows: "Blessed be he in whose hand is the kingdom, he is powerful over all things, who created death and life that he might examine which of you is best in deeds, and he is the almighty, the forgiving" (Qur'an 67:1–2) and "And I (Allâh) created not the jinn and mankind except that they should be obedient (to Allah)." (Qur'an 51:56). Obedience testifies to the oneness of God in his lordship, his names, and his attributes. Terrenal life is a test; how one "acts" (behaves) determines whether one's soul goes to Jannat (Heaven) or to Jahannam (Hell). However on the day of Judgement the final decision is of Allah alone. Allah may coverup short comings and allow some people to go to heaven even though they may have some sins in the record.
The Five Pillars of Islam are duties incumbent to every Muslim; they are: Shahadah (profession of faith); salat (ritual prayer); Zakah (charity); Sawm (fasting during Ramadan), and Hajj (pilgrimage to Mecca). They derive from the Hadith works, notably of Sahih Al-Bukhari and Sahih Muslim. The five pillars are not mentioned directly in the Quran.
Beliefs differ among the Kalam. The Sunni and the Ahmadiyya concept of pre-destination is divine decree; likewise, the Shi'a concept of pre-destination is divine justice; in the esoteric view of the Sufis, the universe exists only for God's pleasure; Creation is a grand game, wherein Allah is the greatest prize.
The Sufi view of the meaning of life stems from the hadith qudsi that states "I (God) was a Hidden Treasure and loved to be known. Therefore I created the Creation that I might be known." One possible interpretation of this view is that the meaning of life for an individual is to know the nature of God, and the purpose of all of creation is to reveal that nature, and to prove its value as the ultimate treasure, that is God. However, this hadith is stated in various forms and interpreted in various ways by people, such, as 'Abdu'l-Bahá of the Bahá'í Faith, and in Ibn'Arabī's Fuṣūṣ al-Ḥikam.
Bahá'í Faith.
The Bahá'í Faith emphasizes the unity of humanity. To Bahá'ís, the purpose of life is focused on spiritual growth and service to humanity. Human beings are viewed as intrinsically spiritual beings. People's lives in this material world provide extended opportunities to grow, to develop divine qualities and virtues, and the prophets were sent by God to facilitate this.
Zoroastrianism.
Zoroastrianism is the religion and philosophy named after its prophet Zoroaster, which is believed to have influenced the beliefs of Judaism and its descendant religions. Zoroastrians believe in a universe created by a transcendental God, Ahura Mazda, to whom all worship is ultimately directed. Ahura Mazda's creation is "asha", truth and order, and it is in conflict with its antithesis, "druj", falsehood and disorder. (See also Zoroastrian eschatology).
Since humanity possesses free will, people must be responsible for their moral choices. By using free will, people must take an active role in the universal conflict, with good thoughts, good words and good deeds to ensure happiness and to keep chaos at bay.
South Asian religions.
Hindu philosophies.
Hinduism is a religious category including many beliefs and traditions. Since Hinduism was the way of expressing meaningful living for a long time, before there was a need for naming it as a separate religion, Hindu doctrines are supplementary and complementary in nature, generally non-exclusive, suggestive and tolerant in content. Most believe that the ātman (spirit, soul)—the person's true "self"—is eternal. In part, this stems from Hindu beliefs that spiritual development occurs across many lifetimes, and goals should match the state of development of the individual. There are four possible aims to human life, known as the "purusharthas" (ordered from least to greatest): "Kāma" (wish, desire, love and sensual pleasure), "Artha" (wealth, prosperity, glory), "Dharma" (righteousness, duty, morality, virtue, ethics), encompassing notions such as "ahimsa" (non-violence) and satya (truth) and "Moksha" (liberation, i.e. liberation from Saṃsāra, the cycle of reincarnation).
In all schools of Hinduism, the meaning of life is tied up in the concepts of karma (causal action), sansara (the cycle of birth and rebirth), and moksha (liberation). Existence is conceived as the progression of the ātman (similar to the western concept of a soul) across numerous lifetimes, and its ultimate progression towards liberation from karma. Particular goals for life are generally subsumed under broader yogas (practices) or dharma (correct living) which are intended to create more favorable reincarnations, though they are generally positive acts in this life as well. Traditional schools of Hinduism often worship Devas which are manifestations of Ishvara (a personal or chosen God); these Devas are taken as ideal forms to be identified with, as a form of spiritual improvement.
In short, the goal is to realize the fundamental truth about oneself. This thought is conveyed in the Mahāvākyas ("Tat Tvam Asi" (thou art that), "Aham Brahmāsmi", "Prajñānam Brahma" and "Ayam Ātmā Brahma" (the soul and the world are one)).
Advaita and Dvaita Hinduism.
Later schools reinterpreted the vedas to focus on Brahman, "The One Without a Second", as a central God-like figure.
In monist Advaita Vedanta, ātman is ultimately indistinguishable from Brahman, and the goal of life is to know or realize that one's ātman (soul) is identical to Brahman. To the Upanishads, whoever becomes fully aware of the ātman, as one's core of self, realizes identity with Brahman, and, thereby, achieves Moksha (liberation, freedom).
Dualist Dvaita Vedanta and other bhakti schools have a dualist interpretation. Brahman is seen as a supreme being with a personality and manifest qualities. The ātman depends upon Brahman for its existence; the meaning of life is achieving Moksha through love of God and upon His grace.
Vaishnavism.
Vaishnavism is a branch of Hinduism in which the principal belief is the identification of Vishnu or Narayana as the one supreme God. This belief contrasts with the Krishna-centered traditions, such as Vallabha, Nimbaraka and Gaudiya, in which Krishna is considered to be the One and only Supreme God and the source of all avataras.
Vaishnava theology includes the central beliefs of Hinduism such as monotheism, reincarnation, samsara, karma, and the various Yoga systems, but with a particular emphasis on devotion (bhakti) to Vishnu through the process of Bhakti yoga, often including singing Vishnu's name's (bhajan), meditating upon his form (dharana) and performing deity worship (puja). The practices of deity worship are primarily based on texts such as Pañcaratra and various Samhitas.
One popular school of thought, Gaudiya Vaishnavism, teaches the concept of Achintya Bheda Abheda. In this, Krishna is worshipped as the single true God, and all living entities are eternal parts and the Supreme Personality of the Godhead Krishna. Thus the constitutional position of a living entity is to serve the Lord with love and devotion. The purpose of human life especially is to think beyond the animalistic way of eating, sleeping, mating and defending and engage the higher intelligence to revive the lost relationship with Krishna.
Jainism.
Jainism is a religion originating in ancient India, its ethical system promotes self-discipline above all else. Through following the ascetic teachings of Jina, a human achieves enlightenment (perfect knowledge). Jainism divides the universe into living and non-living beings. Only when the living become attached to the non-living does suffering result. Therefore, happiness is the result of self-conquest and freedom from external objects. The meaning of life may then be said to be to use the physical body to achieve self-realization and bliss.
Jains believe that every human is responsible for his or her actions and all living beings have an eternal soul, "jiva". Jains believe all souls are equal because they all possess the potential of being liberated and attaining Moksha. The Jain view of karma is that every action, every word, every thought produces, besides its visible, an invisible, transcendental effect on the soul.
Jainism includes strict adherence to ahimsa (or "ahinsā"), a form of nonviolence that goes far beyond vegetarianism. Jains refuse food obtained with unnecessary cruelty. Many practice a lifestyle similar to veganism due to the violence of modern dairy farms, and others exclude root vegetables from their diets in order to preserve the lives of the plants from which they eat.
Buddhism.
Early Buddhism.
Buddhists practice to embrace with mindfulness the ill-being (suffering) and well-being that is present in life. Buddhists practice to see the causes of ill-being and well-being in life. For example, one of the causes of suffering is unhealthy attachment to objects material or non-material. The Buddhist sūtras and tantras do not speak about "the meaning of life" or "the purpose of life", but about the potential of human life to end suffering, for example through embracing (not suppressing or denying) cravings and conceptual attachments. Attaining and perfecting dispassion is a process of many levels that ultimately results in the state of Nirvana. Nirvana means freedom from both suffering and rebirth.
Theravada Buddhism is generally considered to be close to the early Buddhist practice. It promotes the concept of Vibhajjavada (Pali), literally "Teaching of Analysis", which says that insight must come from the aspirant's experience, critical investigation, and reasoning instead of by blind faith. However, the Theravadin tradition also emphasizes heeding the advice of the wise, considering such advice and evaluation of one's own experiences to be the two tests by which practices should be judged. The Theravadin goal is liberation (or freedom) from suffering, according to the Four Noble Truths. This is attained in the achievement of Nirvana, or Unbinding which also ends the repeated cycle of birth, old age, sickness and death.
Mahayana Buddhism.
Mahayana Buddhist schools de-emphasize the traditional view (still practiced in Theravada) of the release from individual Suffering (Dukkha) and attainment of Awakening (Nirvana). In Mahayana, the Buddha is seen as an eternal, immutable, inconceivable, omnipresent being. The fundamental principles of Mahayana doctrine are based on the possibility of universal liberation from suffering for all beings, and the existence of the transcendent Buddha-nature, which is the eternal Buddha essence present, but hidden and unrecognised, in all living beings.
Philosophical schools of Mahayana Buddhism, such as Chan/Zen and the vajrayana Tibetan and Shingon schools, explicitly teach that bodhisattvas should refrain from full liberation, allowing themselves to be reincarnated into the world until all beings achieve enlightenment. Devotional schools such as Pure Land Buddhism seek the aid of celestial buddhas—individuals who have spent lifetimes accumulating positive karma, and use that accumulation to aid all.
Sikhism.
The monotheistic Sikh religion was founded by Guru Nanak Dev, the term "Sikh" means student, which denotes that followers will lead their lives forever learning. This system of religious philosophy and expression has been traditionally known as the Gurmat (literally "the counsel of the gurus") or the Sikh Dharma. The followers of Sikhism are ordained to follow the teachings of the ten Sikh Gurus, or enlightened leaders, as well as the holy scripture entitled the "Gurū Granth Sāhib", which includes selected works of many philosophers from diverse socio-economic and religious backgrounds.
The Sikh Gurus say that salvation can be obtained by following various spiritual paths, so Sikhs do not have a monopoly on salvation: "The Lord dwells in every heart, and every heart has its own way to reach Him." Sikhs believe that all people are equally important before God. Sikhs balance their moral and spiritual values with the quest for knowledge, and they aim to promote a life of peace and equality but also of positive action.
A key distinctive feature of Sikhism is a non-anthropomorphic concept of God, to the extent that one can interpret God as the Universe itself (pantheism). Sikhism thus sees life as an opportunity to understand this God as well as to discover the divinity which lies in each individual. While a full understanding of God is beyond human beings, Nanak described God as not wholly unknowable, and stressed that God must be seen from "the inward eye", or the "heart", of a human being: devotees must meditate to progress towards enlightenment and the ultimate destination of a Sikh is to lose the ego completely in the love of the lord and finally merge into the almighty creator. Nanak emphasized the revelation through meditation, as its rigorous application permits the existence of communication between God and human beings.
East Asian religions.
Taoism.
Taoist cosmogony emphasizes the need for all sentient beings and all man to return to the "primordial" or to rejoin with the "Oneness" of the Universe by way of self-cultivation and self-realization. All adherents should understand and be in tune with the ultimate truth.
Taoists believe all things were originally from Taiji and Tao, and the meaning in life for the adherents is to realize the temporal nature of the existence. "Only introspection can then help us to find our innermost reasons for living ... the simple answer is here within ourselves."
Shinto.
Shinto is the native religion of Japan. Shinto means "the path of the kami", but more specifically, it can be taken to mean "the divine crossroad where the kami chooses his way". The "divine" crossroad signifies that all the universe is divine spirit. This foundation of free will, choosing one's way, means that life is a creative process.
Shinto wants life to live, not to die. Shinto sees death as pollution and regards life as the realm where the divine spirit seeks to purify itself by rightful self-development. Shinto wants individual human life to be prolonged forever on earth as a victory of the divine spirit in preserving its objective personality in its highest forms. The presence of evil in the world, as conceived by Shinto, does not stultify the divine nature by imposing on divinity responsibility for being able to relieve human suffering while refusing to do so. The sufferings of life are the sufferings of the divine spirit in search of progress in the objective world.
New religions.
There are many new religious movements in East Asia, and some with millions of followers: Chondogyo, Tenrikyo, Cao Đài, and Seicho-No-Ie. New religions typically have unique explanations for the meaning of life. For example, in Tenrikyo, one is expected to live a Joyous Life by participating in practices that create happiness for oneself and others.
In popular culture.
The mystery of life and its true meaning is an often recurring subject in popular culture, featured in entertainment media and various forms of art.
In Douglas Adams' popular comedy book, movie, television, and radio series "The Hitchhiker's Guide to the Galaxy", the Answer to the Ultimate Question of Life, the Universe, and Everything is given the numeric solution "42", after seven and a half million years of calculation by a giant supercomputer called Deep Thought. When this answer is met with confusion and anger from its constructors, Deep Thought explains that "I think the problem, to be quite honest with you, is that you've never actually known what the question is."
In the continuation of the book, the question is proposed to be "How many roads must a man walk down, before you can call him a man" from Bob Dylan's "Blowin' in the Wind." In the sequel, "The Restaurant at the End of the Universe", it states that the question is 6x9. While 6 x 9 = 54 in base 10, it does equal 42 in base 13, which author Adams claimed was completely serendipitous.
In "Monty Python's The Meaning of Life", there are several allusions to the meaning of life. At the end of the film, a character played by Michael Palin is handed an envelope containing "the meaning of life", which he opens and reads out to the audience: "Well, it's nothing very special. Uh, try to be nice to people, avoid eating fat, read a good book every now and then, get some walking in, and try to live together in peace and harmony with people of all creeds and nations." Many other Python sketches and songs are also existential in nature, questioning the importance we place on life ("Always Look on the Bright Side of Life") and other meaning-of-life related questioning. John Cleese also had his sit-com character Basil Fawlty contemplating the futility of his own existence in Fawlty Towers.
In "The Simpsons" episode "Homer the Heretic", a representation of God agrees to tell Homer what the meaning of life is, but the show's credits begin to roll just as he starts to say what it is.
In Bill and Ted's Excellent Adventure, the characters are asked how we should live our lives, and reply with a version of the golden rule `be excellent to each other' followed by 'party on, dudes!'.
Popular views.
"What is the meaning of life?" is a question many people ask themselves at some point during their lives, most in the context "What is the purpose of life?". Some popular answers include:

</doc>
<doc id="20348" url="http://en.wikipedia.org/wiki?curid=20348" title="Margaret River, Western Australia">
Margaret River, Western Australia

Margaret River is a town in the South West of Western Australia, located in the valley of the eponymous Margaret River, 277 km south of Perth, the state capital. Its Local Government Area is the Shire of Augusta-Margaret River.
Margaret River's coast to the west of the town is a renowned surfing location, with world wide notoriety for its surf breaks. Colloquially, the area is referred to as "Margs" 
The surrounding area is the Margaret River Wine Region and is known for its wine production and tourism, attracting an estimated 500,000 visitors annually. In earlier days the area was better known for hardwood timber and agricultural production.
History.
The town is named after the river, which is presumed to be named after Margaret Whicher, cousin of John Garrett Bussell (founder of Busselton) in 1831. The name is first shown on a map of the region published in 1839. European migrants lived in the area as early as 1850, with timber logging commencing in around 1870. By 1910, the town had a hotel which also operated as a post office.
After World War I, an attempt by the Government of Western Australia to attract migrants to Western Australia (known as the Group Settlement Scheme) and establish farms in the region attracted new settlers to the town. In 1922 over 100 settlers moved into the district.
In the early 1920s the Busselton to Margaret River Railway was built and in 1925 the Margaret River to Flinders Bay line opened.
Geography and climate.
Margaret River is located 9 km inland from the Indian Ocean at a point about halfway between Cape Naturaliste and Cape Leeuwin in Western Australia's South West region.
The climate is humid Mediterranean, with an average annual rainfall of around 1130 mm. Most rain falls between May and August, when around two days in three record measurable rainfall and around one in ten over 10 mm. On occasions, as in August 1955, the town has had measurable rain on every day of a month in this period. During the summer, the weather is very warm, though there are usually sea breezes, and frequently sunny. The hot dry summers, coupled with strong winds, creates an environment where there is always a high risk of bush fires.
Wine region.
Margaret River is the foremost Geographical Indication wine region in the South West Australia Zone, with nearly 55 km2 under vine and over 138 wineries as at 2008. The region is made up predominantly of boutique-size wine producers, although winery operations range from the smallest, crushing 3.5 t per year, to the largest at around 7000 t. The region produces just three percent of total Australian grape production, but commands over 20 percent of the Australian premium wine market.
Stretching some 100 km from north to south and about 27 km wide in parts, the region is bounded to the east by the Leeuwin-Naturaliste Ridge, between Cape Naturaliste and Cape Leeuwin, and to the west by the Indian Ocean. A Mediterranean-style climate, lacking extreme summer and winter temperatures, provides ideal growing conditions. The climate is described as similar to that of Bordeaux in a dry vintage.
Humidity levels are ideal during the growing period and the combination of climate, soil and viticulture practices leads to consistently high quality fruit of intense flavour. Consequently, annual vintage results continue to exceed expectations and reinforce Margaret River's reputation as one of the premium wine-producing regions of the world.
The principal grape varieties in the region are fairly evenly split between red and white; Cabernet Sauvignon, Chardonnay, Sauvignon blanc, Shiraz, Merlot, Chenin blanc and Verdelho.
Caves.
Several hundred caves are located near Margaret River, all of them within Leeuwin-Naturaliste National Park. Six of these are open to the public.
The most famous of these is the multi-chambered Mammoth Cave, which lies 21 km south of the town and contains fossils dating back over 35,000 years. The cave was first discovered by European settlers in 1850 and has been open to the public since 1904. The cave can be explored by a self-guided audio tour, and is one of the few caves in Australia offering partial disabled access. 
The other five caves open to the public in the area are Jewel Cave, Lake Cave, Ngilgi Cave, Calgardup Cave and Giants Cave. Many other caves can be accessed with a permit by experienced cavers.
In the media.
Arte-TV produced an episode of "Nouveaux paradis" about Margaret River. The 2008 documentary shows interviews with (amongst others) tourist officials, surfers, and dolphin watchers.
Margaret River was also visited in the 1966 documentary film "The Endless Summer". On 25 April 2009, on Sky television's Soccer AM, Hugh Jackman called Margaret River the best place he's ever been to, citing the surf, the beaches, the food, the wine, the people and the air as his reasons for thinking so. In 2013, many locals featured in the film Drift, starring Sam Worthington, as well as many surfing scenes being shot on location at local surf breaks. Surfing locations included popular breaks such as Grunters and Main Break. 
Surfing breaks.
The Margaret River area has acquired a range of synonyms for the collection of surf breaks nearby Usually significant surfing competitions concentrate their locale to "Margarets Main Break" (aka Surfers Point) which breaks in the vicinity of Prevelly at the mouth of Margaret River.
The actual range of surf breaks range from the eastern side of Cape Naturaliste down to just south of Cape Hamelin, and despite web sites and online sources calling the whole Cape Naturaliste to Cape Leeuwin region the "Margaret River" surfing area, conditions and break types vary along the coast.
Perimeter road.
In December 2014, construction of the Margaret River Perimeter Road began. This is a bypass to take traffic, including heavy vehicles, from Bussell Highway, to the east of the town, and also connect to a new access road to the nearby airport.
References.
Bibliography.
</dl>
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="20351" url="http://en.wikipedia.org/wiki?curid=20351" title="Maginot Line">
Maginot Line

The Maginot Line (French: "Ligne Maginot", ]), named after the French Minister of War André Maginot, was a line of concrete fortifications, obstacles, and weapons installations that France constructed just before the border with Switzerland and the borders with Germany and Luxembourg during the 1930s. The Line did not extend through to the English Channel because the French military did not want to compromise Belgium's neutrality. The line was a response to France's experience in World War I and was constructed during the run-up to World War II, shortly after the Locarno Conference that gave rise to a fanciful and optimistic "Locarno spirit".
The French established the fortification to provide time for their army to mobilize in the event of attack, allowing French forces to move into Belgium for a decisive confrontation with Germany. The success of static, defensive combat in World War I was a key influence on French thinking. French military experts extolled the Maginot Line as a work of genius, believing it would prevent any further invasions from the east.
While the fortification system did prevent a direct attack, it was strategically ineffective, as the Germans invaded through Belgium, going around the Maginot Line. The German army came through the Ardennes forest and the Low Countries, completely sweeping by the line, causing the French army to surrender and conquering France in about six weeks. As such, reference to the Maginot Line is used to recall a strategy or object that people hope will prove effective but instead fails miserably, giving way to the "Maginot Line mentality".
The Maginot Line was impervious to most forms of attack (including aerial bombings and tank fire), and had state-of-the-art living conditions for garrisoned troops, air conditioning, comfortable eating areas and underground railways. However, it proved costly to maintain and subsequently led to parts of the French Armed Forces being underfunded and not provided with the troops, equipment and communications needed for the war.
Planning and construction.
The defences were first proposed by Marshal Joffre. He was opposed by modernists such as Paul Reynaud and Charles de Gaulle who favoured investment in armour and aircraft. Joffre had support from Henri Philippe Pétain, and there were a number of reports and commissions organised by the government. It was André Maginot who finally convinced the government to invest in the scheme. Maginot was another veteran of World War I; he became the French Minister of Veteran Affairs and then Minister of War (1928–1932).
Part of the rationale for the Maginot Line stemmed from the severe French losses during the First World War, and their effects on French demographics. The drop in the national birth rate during and after the war, resulting in a national shortage of young men, created an "echo" effect in the generation that provided the French conscript army in the mid-1930s. Faced with inadequate personnel resources, French planners had to rely more on older and less fit reservists, who would take longer to mobilize, and would diminish French industry because they would leave their jobs. Static defensive positions were therefore intended not only to buy time, but also to defend an area with fewer and less mobile forces. In practice, France deployed about twice as many men, 36 divisions (roughly one third of its force), for defence of the Maginot Line in Alsace and Lorraine, whereas the opposing German Army Group C only contained 19 divisions, or less than one seventh of the total force committed in the Manstein Plan for the invasion of France.
The line was built in several phases from 1930 by the STG (Service Technique du Génie) overseen by CORF (Commission d'Organisation des Régions Fortifiées). The main construction was largely completed by 1939, at a cost of around 3 billion French francs.
The line stretched from Switzerland to Luxembourg, and a much lighter extension was extended to the Strait of Dover after 1934. The original line construction did not cover the area chosen by the Germans for their first challenge, which was through the Ardennes in 1940, a plan known as Fall Gelb, due to the neutrality of Belgium. The location of this attack, because of the Maginot Line, was through the Belgian Ardennes forest (sector 4), which is off the map to the left of Maginot Line sector 6 (as marked).
Purposes.
The Maginot Line was built to fulfill several purposes:
Organization.
Although the name "Maginot Line" suggests a rather thin linear fortification, it was quite deep, varying (i.e., from the border to the rear area) from between 20 to. It was composed of an intricate system of strong points, fortifications and military facilities such as border guard posts, communications centres, infantry shelters, barricades, artillery, machine gun and anti-tank gun emplacements, supply depots, infrastructure facilities and observation posts. These various structures reinforced a "principal line of resistance," made up of the most heavily armed "ouvrages", which can be roughly translated as fortresses or major defensive works.
From front to rear, (east to west) the line was composed of:
Inventory.
Ouvrages.
There are 142 ouvrages, 352 casemates, 78 shelters, 17 observatories and around blockhouses in the Maginot Line.
Armoured cloches.
There are several kinds of armoured cloches. The word "cloche" is a French term meaning "bell" due to its shape. All cloches were made in an alloy steel. Cloches are non-retractable turrets.
Retractable turrets.
The line included the following retractable turrets. 
Features.
The specification of the defences was very high, with extensive and interconnected bunker complexes for thousands of men; there were 45 main forts ("grands ouvrages") at intervals of 15 km, 97 smaller forts ("petits ouvrages") and 352 casemates between, with over 100 km of tunnels. Artillery was coordinated with protective measures to ensure that one fort could support the next in line by bombarding it directly without harm. The largest guns were therefore 135 mm fortress guns; larger weapons were to be part of the mobile forces and were to be deployed behind the lines.
The fortifications did not extend through the Ardennes Forest (which was believed to be impenetrable by Commander-in-Chief Maurice Gamelin) or along France's border with Belgium, because the two countries had signed an alliance in 1920, by which the French army would operate in Belgium if the German forces invaded. However, after France had failed to counter Germany's remilitarisation of the Rhineland, Belgium—thinking that France was not a reliable ally—abrogated the treaty in 1936 and declared neutrality. France quickly extended the Maginot Line along the Franco-Belgian border, but not to the standard of the rest of the line. As the water table in this region is high, there was the danger of underground passages getting flooded, which the designers of the line knew would be difficult and expensive to overcome.
When the British Expeditionary Force landed in France in September 1939, they and the French reinforced and extended the Maginot line to the sea in a flurry of construction in 1939–1940 accompanied by general improvements all along the line. The final line was strongest around the industrial regions of Metz, Lauter and Alsace, while other areas were in comparison only weakly guarded. In contrast, the propaganda about the line made it appear far greater a construction than it was; illustrations showed multiple stories of interwoven passages, and even underground railyards and cinemas. This reassured Allied civilians.
Czech connection.
Czechoslovakia was also in fear of Hitler and began building its own defences. As an ally of France, they were able to get advice on the Maginot design and apply it to Czechoslovak border fortifications. The design of the casemates is similar to the ones found in the southern part of the Maginot Line, and photos of such are often confused with those of the Maginot. Following the Munich Agreement and the German occupation of Czechoslovakia, the Germans were able to use the Czech fortifications to study and plan attacks that proved very successful against the western fortifications, (the Belgian Fort Eben-Emael is the best known example).
German invasion in World War II.
The World War II German invasion plan of 1940 ("Sichelschnitt") was designed to deal with the line. A decoy force sat opposite the line while a second Army Group cut through the Low Countries of Belgium and the Netherlands, as well as through the Ardennes Forest, which lay north of the main French defences. Thus the Germans were able to avoid a direct assault on the Maginot Line by violating the neutrality of Belgium, Luxembourg and the Netherlands. Attacking on 10 May, German forces were well into France within five days and they continued to advance until 24 May, when they stopped near Dunkirk.
During the advance to the English Channel, the Germans overran France's border defence with Belgium and several Maginot Forts in the Maubeuge area, whilst the Luftwaffe simply flew over it. On 19 May, the German 16th Army successfully captured the isolated petit ouvrage La Ferte (southeast of Sedan) after conducting a deliberate assault by combat engineers backed up by heavy artillery. The entire French crew of 107 soldiers was killed during the action. On 14 June 1940, the day Paris fell, the German 1st Army went over to the offensive in "Operation Tiger" and attacked the Maginot Line between St. Avold and Saarbrücken. The Germans then broke through the fortification line as defending French forces retreated southward. In the following days, infantry divisions of the 1st Army attacked fortifications on each side of the penetration; successfully capturing four petits ouvrages. The 1st Army also conducted two attacks against the Maginot Line further to the east in northern Alsace. One attack successfully broke through a weak section of the line in the Vosges Mountains, but a second attack was stopped by the French defenders near Wissembourg. On 15 June, infantry divisions of the German 7th Army attacked across the Rhine River in Operation "Small Bear", penetrating the defences deep and capturing the cities of Colmar and Strasbourg.
By early June the German forces had cut off the line from the rest of France and the French government was making overtures for an armistice, which was signed on 22 June in Compiègne. As the line was surrounded, the German Army attacked a few ouvrages from the rear, but were unsuccessful in capturing any significant fortifications. The main fortifications of the line were still mostly intact, a number of commanders were prepared to hold out, and the Italian advance had been successfully contained. Nevertheless, Maxime Weygand signed the surrender instrument and the army was ordered out of their fortifications, to be taken to POW camps.
When the Allied forces invaded in June 1944, the line, now held by German defenders, was again largely bypassed; fighting touched only portions of the fortifications near Metz and in northern Alsace towards the end of 1944. During the German offensive "Operation Nordwind" in January 1945, Maginot Line casemates and fortifications were utilized by Allied forces, especially in the region of Hatten-Rittershoffen, and some German units had been supplemented with flamethrower tanks in anticipation of this possibility. At one point during the fighting, General Martin, commander of the IX Corps, was ordered to advance from the Maginot Line against a German division, and consequently locked the concrete bunkers and left the keys with a colleague. When his fellow commander's unit was ordered south to reinforce French cities, Martin was forced to retreat from his attack and found himself pursued by a German tank division, and locked out of his own fortifications. He had to employ French engineers and sappers to break into the bunkers, which were subsequently overrun by the Germans.
After World War II.
After the war the line was re-manned by the French and underwent some modifications. With the rise of the French independent nuclear weapons by 1960 the line became an expensive anachronism. Some of the larger "ouvrages" were converted to command centres. When France withdrew from NATO's military component (in 1966) much of the line was abandoned, with the NATO facilities turned back over to French forces and the rest of it auctioned-off to the public or left to decay. A number of old fortifications have now been turned into wine cellars, a mushroom farm and even a disco. Besides that, a few private houses are built atop some of the blockhouses.
Ouvrage Rochonvillers was retained by the French Army as a command centre into the 1990s, but has recently been closed. Ouvrage Hochwald is the only facility in the main line that remains in active service, as a hardened command facility for the French Air Force known as Drachenbronn Air Base.
In 1968 when scouting locations for "On Her Majesty's Secret Service", producer Harry Saltzman used his French contacts to gain permission to use portions of the Maginot Line as SPECTRE headquarters in the film. Saltzman provided art director Syd Cain with a tour of the complex, but Cain said that not only would the location be difficult to light and film inside, but that artificial sets could be constructed at the studios for a fraction of the cost. The idea was shelved.
External links.
Deception

</doc>
<doc id="20353" url="http://en.wikipedia.org/wiki?curid=20353" title="Metrication">
Metrication

Metrication or metrification is conversion to the metric system of units of measurement. Worldwide, there has been a long process of independent conversions of countries from various local and traditional systems, beginning in France during the 1790s and spreading widely over the following two centuries, but the metric system has not been fully adopted in all countries and sectors.
Overview.
In 2006, three countries did not mainly use the metric system: the United States, Burma and Liberia. Some countries, however, still use non-metric units for some purposes. In the United Kingdom, for example, metric is the official system for most regulated trading by weight or measure purposes, but miles, yards, and feet remain the official units for road signage—and use of imperial units is widespread. The Imperial pint (expressed in metric units) also remains a permitted unit for milk in returnable bottles and for draught beer and cider in British pubs.
Some sources now identify Liberia as metric and in Burma it was announced in 2013 that the country is preparing to adopt the metric system. Both Burma and Liberia are substantially metric countries, trading internationally in metric units. Visiting advocates of metrication have stated that they use metric units for many things internally with exceptions such as old petrol pumps in Burma, calibrated in British Imperial gallons. These and other countries have adopted metric measures to some degree through international trade and standardization for example, Sierra Leone switched to selling fuel by the litre in May 2011. The United States mandated the acceptance of the metric system in 1866 for commercial and legal proceedings, without displacing their customary units.
In 1971 the National Bureau of Standards completed a three-year study of the impact of increasing worldwide metric use on the U.S. The study concluded with a report to Congress entitled "A Metric America – A Decision Whose Time Has Come". Since then metric use has increased in the U.S., principally in the manufacturing and educational sectors. Public Law 93-380, enacted 21 August 1974, states that it is the policy of the U.S. to encourage educational agencies and institutions to prepare students to use the metric system of measurement with ease and facility as a part of the regular education program. On 23 December 1975, President Gerald Ford signed Public Law 94-168, the Metric Conversion Act of 1975. This act declares a national policy of coordinating the increasing use of the metric system in the U.S. It established a U.S. Metric Board whose functions as of 1 October 1982 were transferred to the Dept of Commerce, Office of Metric Programs, to coordinate the voluntary conversion to the metric system.
A number of jurisdictions have laws mandating or permitting other systems of measurement in some or all contexts, such as the United Kingdom, which still uses many imperial measures, such as miles and yards for road-sign distances, road speed limits in miles per hour, pints of beer, and inches for clothes. Most countries have adopted the metric system officially over a transitional period where both units are used for a set period of time. Some countries such as Guyana, for example, have officially adopted the metric system, but have had some trouble over time implementing it. Antigua and Barbuda, also "officially" metric, is moving toward total implementation of the metric system, but slower than expected. The government had announced that they have plans to convert their country to the metric system by the first quarter of 2015. Other Caribbean countries such as Saint Lucia are officially metric but are still in the process toward full conversion.
The European Union used the Units of Measure Directive to attempt to achieve a common system of weights and measures and to facilitate the European Single Market. Throughout the 1990s, the European Commission helped accelerate the process for member countries to complete their metric conversion processes. The United Kingdom secured permanent exemptions for the mile and yard in road markings, and (with Ireland) for the pint (Imperial) of draught beer sold in pubs (see Metrication in the United Kingdom). In 2007, the European Commission also announced that (to appease British public opinion and to facilitate trade with the United States) it was to abandon the requirement for metric-only labelling on packaged goods, and to allow dual metric–imperial marking to continue indefinitely.
Other countries using the imperial system completed official metrication during the second half of the 20th century or the first decade of the 21st century. The most recent to complete this process was the Republic of Ireland, which began metric conversion in the 1970s and completed it in early 2005.
In January 2007 NASA decided to use metric units for all future moon missions, in line with the practice of other space agencies.
The United States, the United Kingdom, and Canada have some active opposition to metrication, particularly where updated weights and measures laws would make obsolete historic systems of measurement. Other countries, like France and Japan, that once had significant popular opposition to metrication now have complete acceptance of metrication.
Before the metric system.
The Roman empire used the "pes" (foot) measure. This was divided into 12 "unciae" ("inches"). The "libra" ("pound") was another measure that had wide effect on European weight and currency long after Roman times, e.g. lb, £. The measure came to vary greatly over time. Charlemagne was one of several rulers who launched reform programmes of various kinds to standardise units for measure and currency in his empire, but there was no real general breakthrough.
In medieval Europe, local laws on weights and measures were set by trade guilds on a city-by-city basis. For example, the "ell" or "elle" was a unit of length commonly used in Europe, but its length varied from 40.2 centimetres in one part of Germany to 70 centimetres in The Netherlands and 94.5 centimetres in Edinburgh. A survey of Switzerland in 1838 revealed that the "foot" had 37 different regional variations, the "ell" had 68, there were 83 different measures for dry grain, 70 measures for fluids and 63 different measures for "dead weights". When Isaac Newton wrote "Philosophiae Naturalis Principia Mathematica" in 1687, he quoted his measurements in "Parisian feet" so readers could understand the size. Examples of efforts to have local intercity or national standards for measurements include the Scottish law of 1641, and the British standard imperial system of 1824, which is still commonly used in the United Kingdom. At one time Imperial China had successfully standardised units for volume throughout its territory, but by 1936 official investigations uncovered 53 dimensions for the chi varying from 200 millimetres to 1250 millimetres; 32 dimensions of the cheng, between 500 millilitres and 8 litres; and 36 different tsin ranging from 300 grams to 2500 grams. However, revolutionary France was to produce the definitive "International System of Units" which has come to be used by most of the world today.
The desire for a single international system of measurement came from growing international trade and the need to apply common standards to goods. For a company to buy a product produced in another country, they need to ensure that the product would arrive as described. The medieval "ell" was abandoned in part because its value could not be standardised. One primary advantage of the International System of Units is simply that it is international, and the pressure on countries to conform to it grew as it became increasingly an international standard. However, it also simplified the teaching and learning of measurement as all SI units are based on a handful of base units (in particular, the metre, kilogram and second cover the majority of everyday measurements), using decimal prefixes to cover all magnitudes. This contrasts with pre-metric units, which largely have names that do not relate directly to one another (e.g. "inch", "foot", "yard", "mile") and are related to one another by inconsistent ratios which must simply be memorised, (e.g. 12, 3, 1760). As the values in an SI expression are always decimal (i.e. without vulgar fractions) and mixed units (such as "feet and inches") are not used with SI, measurements are easy to add or multiply. Moreover, scientific measurement and calculation are greatly simplified as the units for electricity, force etc. are part of the SI system and hence are all interrelated in a coherent manner (e.g. 1 J = 1 kg·m2·s−2 = 1 V·A·s). Standardization of measures has contributed significantly to the industrial revolution and technological development in general. SI is not the only example of international standardization; several powerful international standardization organizations exist for various industries, such as the International Organization for Standardization (ISO), the International Electrotechnical Commission (IEC), and the International Telecommunication Union (ITU).
Forerunners of the metric system.
Decimal numbers are an essential part of the metric system, with only one base unit and multiples created on the decimal base, the figures remain the same. This simplifies calculations. Although the Indians used decimal numbers for mathematical computations, it was Simon Stevin who in 1585 first advocated the use of decimal numbers for everyday purposes in his booklet "De Thiende" (old Dutch for 'the tenth'). He also declared that it would only be a matter of time before decimal numbers were used for currencies and measurements. His notation for decimal fractions was clumsy, but this was overcome with the introduction of the decimal point, generally attributed to Bartholomaeus Pitiscus who used this notation in his trigonometrical tables (1595).
In his "Essay towards a Real Character and a Philosophical Language", published in 1668, John Wilkins proposed a system of measurement that was very similar in concept to today's metric system. He proposed retaining the second as the basic unit of time and proposed that the length of a pendulum which crossed the zero position once a second (i.e. had a period of two seconds) should be the base unit of length. This length, for which he proposed the name "standard", would have been 994 mm. His base unit of mass, which he proposed calling a "hundred", would have been the mass of a cubic standard of distilled rainwater. The names that he proposed for decimal multiples and subunits of his base units of measure were the names of units of measure that were in use at the time.
In 1670, Gabriel Mouton published a proposal that was in essence similar to Wilkins' proposal, except that his base unit of length would have been 1/1000 of a minute of arc (about 1.852 m) of geographical latitude. He proposed calling this unit the virga. Rather than using different names for each unit of length, he proposed a series of names that had prefixes, rather like the prefixes found in SI.
In 1790, Thomas Jefferson submitted a report to the United States Congress in which he proposed the adoption of a decimal system of coinage and of weights and measures. He proposed calling his base unit of length a "foot" which he suggested should be either 3/10 or 1/3 of the length of a pendulum that had a period of one second – that is 3/10 or 1/3 of the "standard" proposed by Wilkins over a century previously. This would have equated to 11.755 English inches (29.8 cm) or 13.06 English inches (33.1 cm). Like Wilkins, the names that he proposed for multiples and subunits of his base units of measure were the names of units of measure that were in use at the time.
Conversion process.
The metric system was officially introduced in France in December 1799. In the 19th century, the metric system was adopted by almost all European countries: Portugal (1814); Netherlands, Belgium and Luxembourg (1820); Switzerland (1835); Spain (1850s); Italy (1861); Germany (1870, legally from 1 January 1872); and Austria-Hungary (1876, but the law was adopted in 1871). Thailand did not formally adopt the metric system until 1923, but the Royal Thai Survey Department used it for cadastral survey as early as 1896. Denmark and Iceland adopted the metric system in 1907.
Chronology and status of conversion by country.
"Links in the country point to articles about metrication in that country."
Notes
There are three common ways that nations convert from traditional measurement systems to the metric system. The first is the quick, or "Big-Bang" route which was used by India in the 1960s and several other nations including Australia and New Zealand since then. The second way is to phase in units over time and progressively outlaw traditional units. This method, favoured by some industrial nations, is slower and generally less complete. The third way is to redefine traditional units in metric terms. This has been used successfully where traditional units were ill-defined and had regional variations.
The "Big-Bang" way is to simultaneously outlaw the use of pre-metric measurement, metricate, reissue all government publications and laws, and change education systems to metric. India's changeover lasted from 1 April 1960, when metric measurements became legal, to 1 April 1962, when all other systems were banned. The Indian model was extremely successful and was copied over much of the developing world.
The phase-in way is to pass a law permitting the use of metric units in parallel with traditional ones, followed by education of metric units, then progressively ban the use of the older measures. This has generally been a slow route to metric. The British Empire permitted the use of metric measures in 1873, but the changeover was not completed in most Commonwealth countries until the 1970s and 1980s when governments took an active role in metric conversion. Japan also followed this route and did not complete the changeover for 70 years. In the United Kingdom, the process is still incomplete. By law, loose goods sold with reference to units of quantity have to be weighed and sold using the metric system. In 2001 the EU directive 80/181/EEC stated that supplementary units (imperial units alongside metric including labelling on packages) would become illegal from the beginning of 2010. In September 2007 a consultation process was started which resulted in the directive being modified to permit supplementary units to be used indefinitely.
The third method is to redefine traditional units in terms of metric values. These redefined "quasi-metric" units often stay in use long after metrication is said to have been completed. Resistance to metrication in post-revolutionary France convinced Napoleon to revert to "mesures usuelles" (usual measures) and to some extent the names remain throughout Europe. In 1814, Portugal adopted the metric system but with the names of the units substituted by Portuguese traditional ones. In this system the basic units were the "mão-travessa" (hand) = 1 decimetre (10 "mão-travessas" = 1 "vara" (yard) = 1 metre), the "canada" = 1 liter and the "libra" (pound) = 1 kilogram. In the Netherlands, 500 g is informally referred to as a "pond" (pound) and 100 g as an "ons" (ounce), and in Germany and France 500 g is informally referred to respectively as "ein Pfund" and "une livre" ("one pound"). In Denmark, the re-defined "pund" (500 g) is occasionally used, particularly among older people and (older) fruit growers, since these were originally paid according to the number of pounds of fruit produced. In Sweden and Norway a "mil" (Scandinavian mile) is informally equal to 10 km, and this has continued to be the predominantly used unit in conversation when referring to geographical distances. In the 19th century, Switzerland had a non-metric system completely based on metric terms (e.g. 1 "Fuss" (foot) = 30 cm, 1 "Zoll" (inch) = 3 cm, 1 "Linie" (line) = 3 mm). In China the "jin" now has a value of 500 g and the liang is 50 g.
It is difficult to judge the degree to which ordinary people change to using metric in their daily lives. In countries that have recently changed, older segments of the population tend to still use the older units. Also, local variations abound in which units are round metric quantities or not. In Canada, for example, ovens and cooking temperatures are usually measured in degrees Fahrenheit and Celsius. Except for in cases of import items all recipes and packaging includes both Celsius and Fahrenheit so Canadians are typically comfortable with both systems of measurement. This extends to manufacturing where companies are able to use both imperial and metric since major export is for the U.S. but metric is required for both domestic and for nearly all other export. This may be due to the overwhelming influence of the neighbouring United States; similarly, still many English Canadians (unlike most French Canadians) often use non-metric measurements in day-to-day discussions of height and weight, and for clothing sizes, which are invariably measured in inches, though most driver's licences and other official government documents record weight and height only in metric (Saskatchewan driver licences, prior to the introduction of the current one-piece licence, indicated height in feet and inches but have switched to centimetres following the new licence format). In Canadian schools though metric is the standard except when it comes up in recipes, where both are included, or in practical lessons involving measuring wood or other materials for manufacturing. In the United Kingdom, Fahrenheit is seldom encountered (except when some people talk about hot summer weather) while other metric units are often used in conjunction with older measurements, and road signs use miles rather than kilometres. Another example is "hard" and "soft" metric. Canada converted liquid dairy products to litres, 500 mL and 250 mL sizes, which caused some complaining at time of conversion, as a litre of milk is slightly over 35 imperial fluid ounces, while the former imperial quart used in Canada was 40 ounces. This is a "hard" metric conversion. Conversely, butter in Canada is sold primarily in a 454 g package, which converts to one Imperial pound. This is considered "soft" metric. Such countries could be said to be "semi-metric". However unlike in the rest of Canada, metrication in the Francophone province of Quebec has been more implemented and metric measures are more consistently used in Quebec than elsewhere in Canada.
Exceptions.
As of 2015, in most countries of the world the metric system officially dominates; but traditional units are still used in many places and industries. For example:
In some countries (such as Antigua, see above), the transition is still in progress. The Caribbean island nation of Saint Lucia announced metrication programmes in 2005 to be compatible with CARICOM.
United Kingdom.
In the United Kingdom some of the population continue to resist metrication to varying degrees. The traditional imperial measures are preferred by a majority and continue to have widespread use in some applications. The metric system is used by most businesses and is used for most trade transactions. Metric units must be used for certain trading activities (selling by weight or measure for example) although imperial units may continue to be displayed in parallel.
British law has enacted the provisions of European Union directive 80/181/EEC which catalogues the units of measure that may be used for "economic, public health, public safety and administrative purposes". These units consist of the recommendations of the General Conference on Weights and Measures supplemented by some additional units of measure that may be used for specified purposes.
Metric units could be legally used for trading purposes for nearly a century before metrication efforts began in earnest.
The government had been making preparations for the conversion of the Imperial unit since the 1862 "Select Committee on Weights and Measures" recommended the conversion and the "Weights and Measures Act of 1864" and the "Weights and Measures (Metric System) Act of 1896" legalised the metric system. In 1965, with lobbying from British industries and the prospects of joining the Common Market, the government set a 10-year target for full conversion and created the Metrication Board in 1969. Metrication did occur in some areas during this time period, including the re-surveying of Ordnance Survey maps in 1970, decimalisation of the currency in 1971, and teaching the metric system in schools. No plans were made to make the use of the metric system compulsory, and the Metrication Board was abolished in 1980 following a change in government.
The United Kingdom avoided having to comply with the 1989 European Units of Measurement Directive (89/617/EEC), which required all member states to make the metric system compulsory, by negotiating derogations (delayed switchovers), including for miles on road signs and for pints for draught beer, cider, and milk sales.
In popular conversation, the stone unit is still used for body weight and feet and inches are used to describe height.
United States and Canada.
Over time, the metric system has influenced the United States through international trade and standardization. The use of the metric system was made legal as a system of measurement in 1866 and the United States was a founding member of the International Bureau of Weights and Measures in 1875. The system was officially adopted by the federal government in 1975 for use in the military and government agencies, and as preferred system for trade and commerce. It has remained voluntary for federal and state road signage to use metric units, despite attempts in the 1990s to make it a requirement.
A 1992 amendment to the Fair Packaging and Labeling Act (FPLA), which took effect in 1994, required labels on federally regulated "consumer commodities" to include both metric and U.S. customary units. As of 2013, all but one US state (New York) have passed laws permitting metric-only labels for the products they regulate. Likewise, Canada also legally allows for dual labelling of goods provided that the metric unit is listed first and that there is a distinction of whether a liquid measure is a U.S. or a Canadian (Imperial) unit.
Today, the American public and much of the private business and industry still use U.S. customary units despite many years of informal or optional metrication. At least two states, Kentucky and California, have even moved towards demetrication of highway construction projects.
Air and sea transportation.
Air and sea transportation commonly use the nautical mile. This is about one minute of arc of latitude along any meridian arc and it is precisely defined as 1852 metres (about 1.151 statute miles). It is not an SI unit (although it is accepted for use in the SI by the BIPM). The prime unit of speed or velocity for maritime and air navigation remains the knot (nautical mile per hour).
The prime unit of measure for aviation (altitude, or flight level) is usually estimated based on air pressure values, and in many countries, it is still described in nominal feet, although many others employ nominal metres. The policies of the International Civil Aviation Organization (ICAO) relating to measurement are:
Consistent with ICAO policy, aviation has undergone a significant amount of metrication over the years. For example, runway lengths are usually given in metres. The United States metricated the data interchange format (METAR) for temperature reports in 1996. Metrication is also gradually taking place in cargo weights and dimensions and in fuel volumes and weights.
Accidents and incidents.
Confusion over units during the process of metrication can sometimes lead to accidents. One of the most notable examples was during metrication in Canada. In 1983, an Air Canada Boeing 767, nicknamed the "Gimli Glider" following the incident, ran out of fuel in midflight. The incident was caused, in a large part, by the confusion over the conversion between litres, kilograms, and pounds, resulting in the aircraft receiving 22,300 pounds of fuel instead of the required 22,300 kg.
During the reconstruction of Ypres after World War I, British Army engineers drew up the plans for rebuilding a bombed house in feet. However, the Belgian builders interpreted the numbers in metres. As a result the house had enormous doors and windows. It still exists today.
While not strictly an example of national metrication, the use of two different systems was a contributing factor in the loss of the Mars Climate Orbiter in 1998. The National Aeronautics and Space Administration (NASA) specified metric units in the contract. NASA and other organizations worked in metric units, but one subcontractor, Lockheed Martin, provided thruster performance data to the team in pound force-seconds instead of newton-seconds. The spacecraft was intended to orbit Mars at about 150 km in altitude, but the incorrect data meant that it descended to about 57 km. It probably burned up in the Martian atmosphere.
On September 25, 2009, the British Department for Transport published a draft version of legislation to amend its road signs legislation for comment. Among the proposed changes is an amendment to existing legislation to make dual-unit height and width warning and restriction signs mandatory. This is justified in Paragraph 53 of the Impact Analysis by the text "... Based on records from Network Rail's incident logs since April 2008, approximately 10 to 12 percent of bridge strikes involved foreign lorries. This is disproportionately high in terms of the number of foreign lorries on the road network." This proposal was shelved with the change of government in 2010, though many bridges are now signed both ways.

</doc>
<doc id="20354" url="http://en.wikipedia.org/wiki?curid=20354" title="Month">
Month

A month is a unit of time, used with calendars, which is approximately as long as a natural period related to the motion of the Moon; "month" and "Moon" are cognates. The traditional concept arose with the cycle of moon phases; such months (lunations) are synodic months and last approximately 29.53 days. From excavated tally sticks, researchers have deduced that people counted days in relation to the Moon's phases as early as the Paleolithic age. Synodic months, based on the Moon's orbital period with respect to the Earth-Sun line, are still the basis of many calendars today, and are used to divide the year.
Types of months in astronomy.
The following types of months are mainly of significance in astronomy, most of them (but not the distinction between sidereal and tropical months) first recognized in Babylonian lunar astronomy.
A synodic month is longer than a sidereal month because the Earth-Moon system is orbiting the Sun in the same direction as the Moon is orbiting the Earth. The Sun moves eastward with respect to the stars (as does the moon) and it takes about 2.2 days longer for the Moon to return to the same apparent position with respect to the Sun.
An anomalistic month is longer than a sidereal month because the perigee moves in the same direction as the Moon is orbiting the Earth, one revolution in nine years. Therefore, the Moon takes a little longer to return to perigee than to return to the same star.
A draconic month is shorter than a sidereal month because the nodes move in the opposite direction as the Moon is orbiting the Earth, one revolution in 18.6 years. Therefore, the Moon returns to the same node slightly earlier than it returns to the same star.
Calendrical consequences.
At the simplest level, most well-known lunar calendars are based on the initial approximation that 2 lunations last 59 days: a 30 day full month followed by a 29 day hollow month — but this is only roughly accurate, and eventually needs correction by using larger cycles, or the equivalent of leap days. Additionally, the synodic month does not fit easily into the year, which makes accurate, rule-based lunisolar calendars complicated. The most common solution to this problem is the Metonic cycle, which takes advantage of the fact that 235 lunations are approximately 19 tropical years (which add up to not quite 6940 days). However, a Metonic calendar will drift against the seasons by about 1 day every 200 years. Metonic calendars include the calendar used in the Antikythera Mechanism about 2000 years ago, and the Hebrew calendar.
The complexity required in an accurate lunisolar calendar may explain why solar calendars, with months which no longer relate to the phase of the Moon, but are based only on the motion of the Sun relative to the equinoxes and solstices, have generally replaced lunar calendars for civil use in most societies.
Months in various calendars.
Beginning of the lunar month.
The Hellenic calendars, the Hebrew Lunisolar calendar and the Islamic Lunar calendar started the month with the first appearance of the thin crescent of the new moon.
However, the motion of the Moon in its orbit is very complicated and its period is not constant. The date and time of this actual observation depends on the exact geographical longitude as well as latitude, atmospheric conditions, the visual acuity of the observers, etc. Therefore the beginning and lengths of months defined by observation cannot be accurately predicted.
While some like the Jewish Karaites still rely on actual moon observations, most people use the Gregorian solar calendar.
Julian and Gregorian calendars.
The Gregorian calendar, like the Julian calendar before it, has twelve months:
The mean month length of the Gregorian calendar is 30.436875 days.
Months existing in the Roman calendar in the past include:
The famous mnemonic "Thirty days hath September" is the most common way of teaching the lengths of the months in the English-speaking world.
Also, note that any five consecutive months (not including February) contain 153 days.
The knuckles of the four fingers of one's hand and the spaces between them can be used to remember the lengths of the months. By making a fist, each month will be listed as one proceeds across the hand. All months landing on a knuckle are 31 days long and those landing between them are not. When the knuckle of the index finger is reached (July), go back to the first knuckle (or over to the first knuckle on the other fist, held next to the first) and continue with August. This physical mnemonic has been taught to primary school students for many decades.
This cyclical pattern of month lengths matches the musical keyboard alternation of white and black keys (with the note 'F' correlating to the month of January).
Calends, nones, and ides.
The ides occur on the thirteenth day in eight of the months, but in March, May, July, and October, they occur on the fifteenth. The nones always occur 8 days (one Roman week) before the ides, i.e., on the fifth or the seventh. The calends are always the first day of the month, and before Julius Caesar's reform fell sixteen days (two Roman weeks) after the ides (except the ides of February and the intercalary month). 
Hebrew Calendar.
The Hebrew calendar has 12 or 13 months.
Adar 1 is only added 7 times in 19 years. In ordinary years, Adar 2 is simply called Adar.
In the bible the first month of the year was set to be Nisan, but nowadays Jews refer to Tishri as the first month of the year.
Islamic calendar.
There are also twelve months in the Islamic calendar. They are named as follows:
See Islamic calendar for more information on the Islamic calendar.
Hindu calendar.
The Hindu calendar has various systems of naming the months. The months in the lunar calendar are:
In Bengali reckoning, Baishakha is the first month.
These are also the names used in the Indian national calendar for the newly redefined months.
The names in the solar calendar are just the names of the zodiac sign in which the sun travels. They are
Bahá'í calendar.
The Bahá'í calendar is the calendar used by the Bahá'í Faith. It is a solar calendar with regular years of 365 days, and leap years of 366 days. Years are composed of 19 months of 19 days each (361 days), plus an extra period of "Intercalary Days" (4 in regular and 5 in leap years). The months are named after the attributes of God. Days of the year begin and end at sundown.
Iranian calendars (Persian calendars).
The Iranian / Persian calendar, currently used in Iran and Afghanistan, also has 12 months. The Persian names are included in the parentheses. It begins on the northern Spring equinox.
Reformed Bengali calendar.
The Bangla calendar, used in Bangladesh, follows solar months and it has six seasons. The months and seasons in the calendar are:
Grishho (Summer)
Borsha (Rainy)
Sharat (Autumn)
Hemanta (Late Autumn)
Sheeth (Winter)
Bashanta (Spring)
Nanakshahi calendar.
The months in the Nanakshahi calendar are:
Khmer calendar.
Like the Hindu calendar, the Khmer calendar consists of both a lunar calendar and a solar calendar.
The Khmer solar calendar is used more commonly than the lunar calendar. There are 12 months and the numbers of days follow the Julian and Gregorian calendar.
The Khmer lunar calendar contains 12 months; however, the eighth month is repeated (as a "leap-month") every two or three years, making 13 months instead of 12.
Tongan calendar.
The Tongan calendar is based on the cycles of the moon around the earth in one year. The months are:
Sinhala calendar.
The Sinhala calendar is the Buddhist calendar in Sri Lanka with Sinhala names. Each full moon Poya day marks the start of a Buddhist lunar month. The first month is Vesak. The Sinhala and Tamil New Year Day is the start of the Hindu solar calendar (usually 14 April), an event unrelated to the Buddhist calendar.
Germanic calendar.
The old Icelandic calendar is not in official use anymore, but some Icelandic holidays and annual feasts are still calculated from it. It has 12 months, broken down into two groups of six often termed "winter months" and "summer months". The calendar is peculiar in that the months always start on the same weekday rather than on the same date. Hence Þorri always starts on a Friday sometime between January 22 and January 28 "", Góa always starts on a Sunday between February 21 and February 27 "".
Old Georgian calendar.
 "New Year in ancient Georgia started from September."
Old English calendar.
Like the Old Norse calendar, the Anglo-Saxons had their own calendar before they were Christianized which reflected native traditions and deities. These months were attested by Bede in his work "On Chronology" written in the 8th century. His months are probably those as written in the Northumbrian dialect of Old English which he was familiar with. The months were so named after the moon; the new moon marking the end of an old month and start of a new month; the full moon occurring in the middle of the month, after which the month was named.
Old Hungarian calendar.
Nagyszombati kalendárium (in Latin: "Calendarium Tyrnaviense") from 1579.
Historically Hungary used a 12-month calendar that appears to have been zodiacal in nature but eventually came to correspond to the Gregorian months as shown below:
Old Egyptian calendar.
The ancient civil Egyptian calendar had a year that was 365 days long and was divided into 12 months of 30 days each, plus 5 extra days (epagomenes) at the end of the year. The months were divided into 3 "weeks" of ten days each. Because the ancient Egyptian year was almost a quarter of a day shorter than the solar year and stellar events "wandered" through the calendar, it is referred to as Annus Vagus or "Wandering Year".
Nisga'a calendar.
The Nisga'a calendar coincides with the Gregorian calendar with each month referring to the type of harvesting that is done during the month.
French Republican calendar.
This calendar was proposed during the French Revolution, and used by the French government for about twelve years from late 1793. There were twelve months of 30 days each, grouped into three ten-day weeks called "décades". The five or six extra days needed to approximate the tropical year were placed after the months at the end of each year. A period of four years ending on a leap day was to be called a "Franciade". It began at the autumn equinox:

</doc>
<doc id="20356" url="http://en.wikipedia.org/wiki?curid=20356" title="Mozambique Channel">
Mozambique Channel

The Mozambique Channel is an arm of the Indian Ocean located between Madagascar and Mozambique. The channel is about 1600 km long and 419 km across at its narrowest point, and reaches a depth of 3292 m about 230 km off the coast of Mozambique. A warm current flows in a southward direction in the channel, leading into the Agulhas Current off the east coast of South Africa.
Extent.
The International Hydrographic Organization (IHO) defines the limits of the Mozambique Channel as follows:
Despite being defined as the South African coast by the IHO, the western limit of the channel is more correctly defined as the coast of Southern Africa or, more specifically, of Mozambique.
Islands in the channel.
Mozambique.
Primeiras and Segundas Archipelago
History.
The Mozambique Channel was a World War II clashpoint during the Battle of Madagascar. 

</doc>
<doc id="20357" url="http://en.wikipedia.org/wiki?curid=20357" title="Medical psychology">
Medical psychology

Medical psychology is the application of psychological principles to the practice of medicine for both physical and mental disorders. The American Psychological Association (APA) defines medical psychology as "that branch of psychology that integrates somatic and psychotherapeutic modalities into the management of mental illness and emotional, cognitive, behavioral and substance use disorders". A medical psychologist does not automatically equate with a psychologist who has the authority to prescribe medication. Psychologists who hold prescriptive authority for specific psychiatric medications such as antidepressants and other pharmaceutical drugs must first obtain specific qualifications in Psychopharmacology. 
Medical psychologists apply psychological theories, scientific psychological findings, and techniques of psychotherapy, behavior modification, cognitive, interpersonal, family, and life-style therapy to improve the psychological and physical health of the patient. Psychologists with post doctoral specialty training as medical psychologists are the practitioners with refined skills in clinical observation in of the field of psychology, learning, central nervous system adaptation and change, and adaptation and lifestyle change applying a number of different methods in several different mediums of treatment. Highly qualified and post graduate specialized doctors are trained for service in primary care centers, hospitals, residential care centers, and long-term care facilities and in multidisciplinary collaboration and team treatment. They are trained and equipped to modify physical disease states and the actual cytoarchitecture and functioning of the central nervous and related systems using psychological and pharmacological techniques (when allowed by statute), and to provide prevention for the progression of disease having to do with poor personal and life-style choices and conceptualization, behavioral patterns, and chronic exposure to the effects of negative thinking, choosing, attitudes, and negative contexts.
Behavioral medicine.
Behavioral medicine (related to behavioral health, clinical health psychology and psychosomatic medicine) is a related branch of clinical practice in which psychologists emphasize the biopsychosocial approach to medicine, a model which recognizes the importance of addressing the interaction between physical, psychological and social factors in both the prevention and management of disease. Practitioners of behavioral medicine differ from medical psychologists in that they focus on the scientific application of behavioral interventions to a wide variety of medical conditions (e.g., asthma, gastrointestinal illnesses, cardiac conditions, spinal cord and brain injuries, chronic pain, headaches, and addictive illness).
Certifications.
The Academy of Medical Psychology defines medical psychology as a specialty trained at the post doctoral level and designed to deliver advanced diagnostic and clinical interventions in Medical and Healthcare Facilities utilizing the knowledge and skills of clinical psychology, health psychology, behavioral medicine, psychopharmacology and basic medical science. The Academy of Medical Psychology makes a distinction between the Psychopharmacologist who is a psychologist with advanced training in psychopharmacology and may prescribe medicine or consult with physician or nurse practitioner prescribers to diagnose mental illness and select and recommend appropriate psychoactive medicines, and the Medical Psychologists who are prepared to do the psychopharmacology consulting or prescribing, but also must have training which prepares them for functioning with Behavioral and Lifestyle components of physical disease and functioning in or in consultation with multidisciplinary healthcare teams in Primary Care Centers or Community Hospitals in addition to traditional roles in the treatment of mental illness and substance abuse disorders. The specialty of Medical Psychology and this distinction from Psychopharmacologist is recognized by the National Alliance of Professional Psychology Providers (the psychology national practitioner association; see www.nappp.org).
A specialty of medical psychology has established a specialty board certification, American Board of Medical Psychology and an Academy of Medical Psychology requiring a doctorate degree in psychology and extensive post doctoral training in the specialty and the passage of an oral or written examination. 
Although the Academy of Medical Psychology defines medical psychology as a "specialty" and has established a "specialty board certification," and is recognized by the national psychology practitioner association (www.nappp.org) there is a split in national psychology associations between NAPPP and APA and the American Psychological Association and the National Alliance of Professional Psychology Providers do not currently recognize the same specialties with the APA being a group that represents scientists, academics, and practitioners (as a minority) and NAPPP being an organization that represents only practitioners. However, Louisiana does recognize and restricts the term and practice of medical psychology by statute (the Medical Psychology Practice Act) as a "profession of the health sciences" with prescriptive authority. It is equally important to note than the American Psychological Association does not recognize that the term medical psychology has, as a prerequisite, nor should the term be equated with having, prescriptive authority. 
In 2006, the American Psychological Association (APA) recommended that the education and training of psychologists, who are specifically pursuing one of several prerequisites for prescribing medication, integrate instruction in the biological sciences, clinical medicine and pharmacology into a formalized program of postdoctoral education. In 2009, the National Alliance of Professional Providers in Psychology recognized the education and training specified by the American Board of Medical Psychology (www.amphome.org; ABMP) and the Academy of Medical Psychology as the approved standards for post graduate training and examination and qualifications in the nationally recognized specialty in Medical Psychology. Since then numerous hospitals, primary care centers, and other health facilities have recognized the ABMP standards and qualifications for privileges in healthcare facilities and verification of specialty status.
The following "Clinical Competencies" are identified as essential in the education and training of psychologists, wishing to pursue prescriptive authority. These recommended prerequisites are not required or specifically recommended by APA for the training and education of medical psychologists not pursuing prerequisites for prescribing medication.:
I. Basic Science: anatomy, & physiology, biochemistry; 
II. Neurosciences: neuroanatomy, neurophysiology, neurochemistry; 
III. Physical Assessment and Laboratory Exams: physical assessment, laboratory and radiological assessment, medical terminology; 
IV. Clinical Medicine and Pathophysiology: pathophysiology with emphasis on the principal physiological systems, clinical medicine, differential diagnosis, clinical correlation and case studies, chemical dependency, chronic pain management; 
V. Clinical and Research Pharmacology and Psychopharmacology: pharmacology, clinical pharmacology, pharmacogenetics, psychopharmacology, developmental psychopharmacology; 
VI. Clinical Pharmacotherapeutics: professional, ethical and legal issues, combined therapies and their interactions, computer-based aids to practice, pharmacoepidemiology; 
VII. Research: methodology and design of psychopharmacology research, interpretation and evaluation, FDA drug development and other regulatory processes.
The 2006 APA recommendations also include supervised clinical experience intended to integrate the above seven knowledge domains and assess competencies in skills and applied knowledge.
The national psychology practitioner association (NAPPP; www.nappp.org) and top national certifying body (Academy of Medical Psychology; www.amphome.org) have established the national training, examination, and specialty practice criterion and guidelines in the specialty of Medical Psychology and have established a national journal in the specialty. Such certifying bodies, view psychopharmacology training (either to prescribe or consult) as one component of the training of a specialist in Medical Psychology, but recognize that training and specialized skills in other aspects of the treatment of behavioral aspects of medical illness, and mental illness affecting physical illness is essential to practice at the specialty level in Medical Psychology. The Louisiana Academy of Medical Psychology (LAMP), currently the largest organization of psychologists with prescriptive authority in the world and the only organization representing practitioners of medical psychology in Louisiana as defined by Louisiana statute within any jurisdiction in the United States, no longer recognizes the Academy of Medical Psychology as an adequate certifying body for its practitioners, and its members have resigned from the Academy of Medical Psychology en masse. Similarly, virtually all members of LAMP have also resigned from the Louisiana Psychological Association (LPA) after many LPA members uncovered that the LAMP's prescriptive authority movement covertly came to an agreement with Louisiana's medical board to transfer the entire practice of psychology for psychologists with prescriptive authority to the medical board.

</doc>
<doc id="20358" url="http://en.wikipedia.org/wiki?curid=20358" title="Music lesson">
Music lesson

Music lessons are a type of formal instruction in playing a musical instrument or singing. Typically, a student taking music lessons meets a music teacher for one-on-one training sessions ranging from 30 minutes to one hour in length over a period of weeks or years. For vocal lessons, teachers show students how to sit or stand and breathe, and how to position the head, chest, and mouth for good vocal tone. For instrument lessons, teachers show students how to sit or stand with the instrument, how to hold the instrument, and how to manipulate the fingers and other body parts to produce tones and sounds from the instrument. Music teachers also assign technical exercises, musical pieces, and other activities to help the students improve their musical skills. While most music lessons are one-on-one (private), some teachers also teach groups of two to four students (semi-private lessons), and, for very basic instruction, some instruments are taught in large group lessons, such as piano and acoustic guitar. Private lessons can also take place through live video chat using webcam and videotelephony online.
Music lessons are part of both amateur music instruction and professional training. In amateur and recreational music contexts, children and adults take music lessons to improve their singing or playing skills and learn basic techniques. In professional training contexts, such as music conservatories, university music performance programs (e.g., Bachelor of music, Master of music, DMA, etc.), students take a music lesson once a week for an hour or more with a music professor over a period of years to learn advanced playing or singing techniques. Many instrumental performers and singers, including a number of music celebrities, have learned "by ear", especially in folk music styles such as blues and popular styles such as rock music. Nevertheless, even in folk and popular styles, a number of performers have had some type of music lessons, such as meeting with a vocal coach or getting childhood instruction in an instrument such as piano.
Posture.
For vocal lessons, teachers show students how to sit or stand and breathe, and how to position the head and mouth for good vocal tone. For instrument lessons, teachers show students how to sit or stand with the instrument, how to hold the instrument, and how to manipulate the fingers and other body parts to produce tones and sounds from the instrument. For wind and brass instruments, the teacher shows the student how to use their lips, tongue, and breath to produce tones and sounds. For some instruments, teachers also train students in the use of the feet, as in the case of piano or other keyboard instruments that have damper or sustain pedals on the piano, the pedal keyboard on a pipe organ, and some drums and cymbals in the drum kit such as the bass drum pedal and the hi-hat cymbal pedal. In addition to teaching fingering, teachers also provide other types of instruction. A guitar player learns how to strum and pluck strings; players of wind instruments learn about breath control and embouchure, and singers learn how to make the most of their vocal cords without hurting the throat or vocal cords.
Teachers also show students how to achieve the correct posture for most efficient playing results. For all instruments, the best way to move the fingers and arms to achieve a desired effect is to learn to play with the least tension in your hands and body. This also prevents forming habits which may lead to injuries resulting from incorrect use of the skeletal frame and muscles. For example, when playing the piano, "fingering" — that is, which fingers to put on which keys—is a skill slowly learned as the student advances, and there are many standard techniques which a teacher can pass on.
There are many myths and misconceptions among music teachers, especially in the Western classical tradition, about "good" posture and "bad" posture. Students who find that playing their instruments causes them physical pain should bring this to their teachers' attention. It could be a potentially serious health risk, but it is often overlooked when learning to play an instrument. Learning to use one's body in a manner consistent with the way their anatomy is designed to work can mean the difference between a crippling injury and a lifetime of enjoyment. Many music teachers would caution students about taking "no pain, no gain" as an acceptable response from their music teacher regarding a complaint of physical pain. Concerns about use-related injury and the ergonomics of musicianship have gained more mainstream acceptance in recent years. Musicians have increasingly been turning to medical professionals, physical therapists, and specialized techniques seeking relief from pain and prevention of serious injury. There exists a plurality of special techniques for an even greater plurality of potential difficulties. The Alexander Technique is just one example of these specialized approaches.
Theory and history.
In order to more fully understand the music being played, the student must learn about the underlying music theory. Along with reading musical notation, students learn rhythmic techniques like controlling tempo and recognizing time signatures, as well as the theory of harmony, including chords and key signatures. In addition to basic theory, a good teacher will stress "musicality", or how to make the music sound good. This includes tone, phrasing, and proper use of dynamics.
Most music lessons include some instruction in the history of the type of music that the student is learning. When a student is taking western classical music lessons, music teachers often spend some time explaining the different eras of western classical music, such as the Baroque Era, the Classical era, the Romantic Era, and the Modern Era, because each era is associated with different styles of music and different performance practice techniques. Instrumental music from the Baroque era is often played in the 2000s as teaching pieces for piano students, string instrument players, and wind instrument players. If students just try to play these Baroque pieces by reading the notes from the score, they might not get the right type of interpretation. However, once a student learns that most Baroque instrumental music was associated with dances, such as the gavotte and the sarabande, and keyboard music from the Baroque era was played on the harpsichord or the pipe organ, a modern-day student is better able to understand how the piece should be played. If, for example, a cello player is assigned a gavotte that was originally written for harpsichord, this gives the student insight in how to play the piece. Since it is a dance, it should have a regular, clear pulse, rather than a Romantic era-style shifting tempo rubato. As well, since it was originally written for the harpsichord, a light-sounding keyboard instrument in which the strings are plucked with quills, this suggests that the notes should be played relatively lightly, and with spaces between each note, rather than in a full-bodied, sustained legato.
Technical exercises.
Although not universally accepted, many teachers drill students with the repetitive playing of certain patterns, such as scales, arpeggios, and rhythms. Scales are often taught because they are the building blocks of melody in most Western art music. In addition, there are flexibility studies, which make it physically easier to play the instrument. 
Percussion instruments use rudiments that help in the development of sticking patterns, roll techniques and other little nuances such as flams and drags.
There are sets of exercises for piano designed to stretch the connection between fourth and fifth fingers, making them more independent. Brass players practice "lip slurs", which are unarticulated changes in embouchure between partials. Woodwind players (Saxophone, Clarinet, and Flute) have a multitude of exercises to help with tonguing techniques, finger dexterity, and tone development. Entire books of etudes have been written to this purpose.
Pieces.
The teacher will give the student a set of pieces of slowly increasing difficulty. Besides using pieces as an aid to teaching various elements of playing style, a good teacher will also inspire more intangible qualities such as expressiveness and musicianship. Pieces are more enjoyable for most students than theory or scales, and an emphasis on pieces is usually required to maintain motivation. However, the teacher must not give in to the student's desire for "fun" pieces. Often the student's idea of such is popular vocal selections, movie soundtracks, etc. The pieces that one plays should challenge and tone a persons skills. The student should learn something from every piece he/she plays. In addition, in order for a student to be well rounded he/she must play many types of pieces by many different composers from different eras. A varied library of repertoire will increase the student's musical understanding and skill.
Examinations.
A popular measure of progress, especially for children, is external assessment of the progress of the pupil by a regular examination. There are a number of exam boards which offer the chance for pupils to be assessed on either music theory or practice. These are available for almost every musical instrument.One common way to mark progress is to have graded examinations, for example from grade 1 (beginner) to grade 8 (ready to enter higher study at music school). Some teachers prefer other methods of target-setting for their pupils. The most common is the pupil's concert, which gives experience in playing in public and under a certain degree of pressure, without outright criticism or a more or less arbitrary marking system. Another is the graded system of books followed by teachers of the Suzuki method, in which the completion of each book is celebrated, without a system of marking or ranking of pupils.
Extra-musical benefits.
Some studies suggests that music lessons provide children with important developmental benefits beyond simply the knowledge or skill of playing a musical instrument. Research suggests that musical lessons may enhance intelligence and academic achievement, build self-esteem and improve discipline. A recent Rockefeller Foundation Study found that music majors have the highest rate of admittance to medical schools, followed by biochemistry and the humanities. On SAT tests, the national average scores were 427 on the verbal and 476 on math. At the same time, music students averaged 465 on the verbal and 497 on the math - 38 and 21 points higher, respectively. However, the observed correlation between musical and mathematical ability may be inherent rather than acquired. Furthermore, it is possible that the correlation between taking music lessons and academic ability exists because both are strongly correlated with parental income and education. Even if music lessons had no impact on academic ability, one would expect to see a correlation between music lessons and academic ability.
Skills learned through the discipline of music may transfer to study skills, communication skills, and cognitive skills useful in every part of a child's studies at school, though. An in-depth Harvard University study found evidence that spatial-temporal reasoning improves when children learn to make music, and this kind of reasoning improves temporarily when adults listen to certain kinds of music, including Mozart. This finding which has been named "The Mozart effect" suggests that music and spatial reasoning are related psychologically (i.e., they may rely on some of the same underlying skills) and perhaps neurologically as well. However, there has been considerable controversy over this as later researchers have failed to reproduce the original findings of Rauscher (e.g. Steele, Bass & Crook, 1999), questioned both theory and methodology of the original study (Fudis & Lembesis 2004) and suggested that the enhancing effects of music in experiments have been simply due to an increased level of arousal (Thompson, Schellenberg & Husain, 2001).
A relationship between music and the strengthening of math, dance, reading, creative thinking and visual arts skills has also been reported in literature. (Winner, Hetland, Sanni, as reported in "The Arts and Academic Achievement - What the Evidence Shows", 2000) However recent findings by Dr. Levitin of McGill University in Montreal, Canada, undermines the suggested connection between musical ability and higher math skills. In a study conducted on patients with Williams Syndrome (a genetic disorder causing low intelligence), he found that even though their intelligence was that of young children they still possessed unusually high level of musical ability.

</doc>
<doc id="20359" url="http://en.wikipedia.org/wiki?curid=20359" title="Mutagen">
Mutagen

In genetics, a mutagen is a physical or chemical agent that changes the genetic material, usually DNA, of an organism and thus increases the frequency of mutations above the natural background level. As many mutations can cause cancer, mutagens are therefore also likely to be carcinogens. Not all mutations are caused by mutagens: so-called "spontaneous mutations" occur due to spontaneous hydrolysis, errors in DNA replication, repair and recombination.
Discovery of mutagens.
The first mutagens to be identified were carcinogens, substances that were shown to be linked to cancer. Tumors were described more than 2,000 years before the discovery of chromosomes and DNA; in 500 B.C., the Greek physician Hippocrates named tumors resembling a crab "karkinos" (from which the word "cancer" is derived via Latin), meaning crab. In 1567, Swiss physician Paracelsus suggested that an unidentified substance in mined ore (identified as radon gas in modern times) caused a wasting disease in miners, and in England, in 1761, John Hill made the first direct link of cancer to chemical substances by noting that excessive use of snuff may cause nasal cancer. In 1775, Sir Percivall Pott wrote a paper on the high incidence of scrotal cancer in chimney sweeps, and suggested chimney soot as the cause of scrotal cancer. In 1915, Yamagawa and Ichikawa showed that repeated application of coal tar to rabbit's ears produced malignant cancer. Subsequently in the 1930s the carcinogen component in coal tar was identified as a polyaromatic hydrocarbon (PAH), benzo[a]pyrene. Polyaromatic hydrocarbons are also present in soot, which was suggested to be a causative agent of cancer over 150 years earlier.
The mutagenic property of mutagens was first demonstrated in 1927, when Hermann Muller discovered that x-rays can cause genetic mutations in fruit flies, producing phenotypic mutants as well as observable changes to the chromosomes, visible due to presence of enlarged 'polytene' chromosomes in fruit fly salivary glands. His collaborator Edgar Altenburg also demonstrated the mutational effect of UV radiation in 1928. Muller went on to use x-rays to create Drosophila mutants that he used in his studies of genetics. He also found that X-rays not only mutate genes in fruit flies but also have effects on the genetic makeup of humans. Similar work by Lewis Stadler also showed the mutational effect of X-ray on barley in 1928, and ultraviolet (UV) radiation on maize in 1936. The effect of sunlight had previously been noted in the nineteenth century where rural outdoor workers and sailors were found to be more prone to skin cancer.
Chemical mutagens were not demonstrated to cause mutation until the 1940s, when Charlotte Auerbach and J. M. Robson found that mustard gas can cause mutations in fruit flies. A large number of chemical mutagens have since been identified, especially after the development of the Ames test in the 1970s by Bruce Ames that screens for mutagens and allows for preliminary identification of carcinogens. Early studies by Ames showed around 90% of known carcinogens can be identified in Ames test as mutagenic (later studies however gave lower figures), and ~80% of the mutagens identified through Ames test may also be carcinogens. Mutagens are not necessarily carcinogens, and vice versa. Sodium Azide for example may be mutagenic (and highly toxic), but it has not been shown to be carcinogenic.
Effects of mutagens.
Mutagens cause changes to the DNA that can affect the transcription and replication of the DNA, which in severe cases can lead to cell death. The mutagen produces mutations in the DNA, and deleterious mutation can result in aberrant, impaired or loss of function for a particular gene, and accumulation of mutations may lead to cancer.
Different mutagens act on the DNA differently. Powerful mutagens may result in chromosomal instability, causing chromosomal breakages and rearrangement of the chromosomes such as translocation, deletion, and inversion. Such mutagens are called clastogens.
Mutagens may also modify the DNA sequence; the changes in nucleic acid sequences by mutations include substitution of nucleotide base-pairs and insertions and deletions of one or more nucleotides in DNA sequences. Although some of these mutations are lethal or cause serious disease, many have minor effects as they do not result in residue changes that have significant effect on the structure and function of the proteins. Many mutations are silent mutations, causing no visible effects at all, either because they occur in non-coding or non-functional sequences, or they do not change the amino-acid sequence due to the redundancy of codons. 
Some mutagens can cause aneuploidy and change the number of chromosomes in the cell.
In Ames test, where the varying concentrations of the chemical are used in the test, the dose response curve obtained is nearly always linear, suggesting that there is no threshold for mutagenesis. Similar results are also obtained in studies with radiations, indicating that there may be no safe threshold for mutagens. However, some proposed that low level of some mutagens may stimulate the DNA repair processes and therefore may not necessarily be harmful.
Types of mutagens.
Mutagens may be of physical, chemical or biological origin. They may act directly on the DNA, causing direct damage to the DNA, and most often result in replication error. Some however may act on the replication mechanism and chromosomal partition. Many mutagens are not mutagenic by themselves, but can form mutagenic metabolites through cellular processes. Such mutagens are called promutagens.
DNA reactive chemicals.
A large number of chemicals may interact directly with DNA. However, many such as PAHs, aromatic amines, benzene are not necessarily mutagenic by themselves, but through metabolic processes in cells they produce mutagenic compounds.
Metals.
Many metals, such as arsenic, cadmium, chromium, nickel and their compounds may be mutagenic, they may however act via a number of different mechanisms. Arsenic, chromium, iron, and nickel may be associated with the production of ROS, and some of these may also alter the fidelity of DNA replication. Nickel may also be linked to DNA hypermethylation and histone deacetylation, while some metals such as cobalt, arsenic, nickel and cadmium may also affect DNA repair processes such as DNA mismatch repair, and base and nucleotide excision repair.
Protection against mutagens.
Antioxidants are an important group of anticarcinogenic compounds that may help remove ROS or potentially harmful chemicals. These may be found naturally in fruits and vegetables. Examples of antioxidants are vitamin A and its carotenoid precursors, vitamin C, vitamin E, polyphenols, and various other compounds. β-Carotene is the red-orange colored compounds found in vegetables like carrots and tomatoes. Vitamin C may prevent some cancers by inhibiting the formation of mutagenic N-nitroso compounds (nitrosamine). Flavonoids, such as EGCG in green tea, have also been shown to be effective antioxidants and may have anti-cancer properties. Epidemiological studies indicate that a diet rich in fruits and vegetables is associated with lower incidence of some cancers and longer life expectancy, however, the effectiveness of antioxidant supplements in cancer prevention in general is still the subject of some debate.
Other chemicals may reduce mutagenesis or prevent cancer via other mechanisms, although for some the precise mechanism for their protective property may not be certain. Selenium, which is present as a micronutrient in vegetables, is a component of important antioxidant enzymes such as gluthathione peroxidase. Many phytonutrients may counter the effect of mutagens; for example, sulforaphane in vegetables such as broccoli has been shown to be protective against prostate cancer. Others that may be effective against cancer include indole-3-carbinol from cruciferous vegetables and resveratrol from red wine.
An effective precautionary measure an individual can undertake to protect themselves is by limiting exposure to mutagens such as UV radiations and tobacco smoke. In Australia, where people with pale skin are often exposed to strong sunlight, melanoma is the most common cancer diagnosed in people aged 15–44 years.
In 1981, human epidemiological analysis by Richard Doll and Richard Peto indicated that smoking caused 30% of cancers in the US. Diet is also thought to cause a significant number of cancer, and it has been estimated that around 32% of cancer deaths may be avoidable by modification to the diet. Mutagens identified in food include mycotoxins from food contaminated with fungal growths, such as aflatoxins which may be present in contaminated peanuts and corn; heterocyclic amines generated in meat when cooked at high temperature; PAHs in charred meat and smoked fish, as well as in oils, fats, bread, and cereal; and nitrosamines generated from nitrites used as food preservatives in cured meat such as bacon (ascobate, which is added to cured meat, however, reduces nitrosamine formation). Excessive alcohol consumption has also been linked to cancer; the possible mechanisms for its carcinogenicity include formation of the possible mutagen acetaldehyde, and the induction of the cytochrome P450 system which is known to produce mutagenic compounds from promutagens.
For certain mutagens, such as dangerous chemicals and radiations, as well as infectious agents known to cause cancer, government legislations and regulatory bodies are necessary for their control.
Mutagen test systems.
Many different systems for detecting mutagen have been developed. Animal systems may more accurately reflect the metabolism of human, however, they are expensive and time-consuming (may take around three years to complete), they are therefore not used as a first screen for mutagenicity or carcinogenicity.
Yeast.
Systems similar to Ames test have been developed in yeast. "Saccharomyces cerevisiae" is generally used. These systems can check for forward and reverse mutations, as well as recombinant events.
"Drosophila".
Sex-Linked Recessive Lethal Test – Males from a strain with yellow bodies are used in this test. The gene for the yellow body lies on the X-chromosome. The fruit flies are fed on a diet of test chemical, and progenies are separated by sex. The surviving males are crossed with the females of the same generation, and if no males with yellow bodies are detected in the second generation, it would indicate a lethal mutation on the X-chromosome has occurred.
Plant Assays.
Plants such as "Zea mays", "Arabidopsis thaliana" and "Tradescantia" have been used in various test assays for mutagenecity of chemicals.
Cell culture assay.
Mammalian cell lines such as Chinese hamster V79 cells, Chinese hamster ovary (CHO) cells or mouse lymphoma cells may be used to test for mutagenesis. Such systems include the HPRT assay for resistance to 8-azaguanine or 6-thioguanine, and ouabain-resistance (OUA) assay.
Rat primary hepatocytes may also be used to measure DNA repair following DNA damage. Mutagens may stimulate unscheduled DNA synthesis that results in more stained nuclear material in cells following exposure to mutagens.
Chromosome check systems.
These systems check for large scale changes to the chromosomes and may be used with cell culture or in animal test. The chromosomes are stained and observed for any changes. Sister chromatid exchange is a symmetrical exchange of chromosome material between sister chromatids and may be correlated to the mutagenic or carcinogenic potential of a chemical. In micronucleus Test, cells are examined for micronuclei, which are fragments or chromosomes left behind at anaphase, and is therefore a test for clastogenic agents that cause chromosome breakages. Other tests may check for various chromosomal aberrations such as chromatid and chromosomal gaps and deletions, translocations, and ploidy.
Animal test systems.
Rodents are usually used in animal test. The chemicals under
test are usually administered in the food and in the drinking water, but sometimes by dermal application, by gavage, or by inhalation, and carried out over the major part of the life span for rodents. In tests that check for carcinogens, maximum tolerated dosage is first determined, then a range of doses are given to around 50 animals throughout the notional lifespan of the animal of two years. After death the animals are examined for sign of tumours. Differences in metabolism between rat and human however means that human may not respond in exactly the same way to mutagen, and dosages that produce tumours on the animal test may also be unreasonably high for a human, i.e. the equivalent amount required to produce tumours in human may far exceed what a person might encounter in real life.
Mice with recessive mutations for a visible phenotype may also be used to check for mutagens. Females with recessive mutation crossed with wild-type males would yield the same phenotype as the wild-type, and any observable change to the phenotype would indicate that a mutation induced by the mutagen has occurred.
Mice may also be used for dominant lethal assays where early embryonic deaths are monitored. Male mice are treated with chemicals under test, mated with females, and the females are then sacrificed before parturition and early fetal deaths are counted in the uterine horns.
Transgenic Mouse Assay using a mouse strain infected with a viral shuttle vector is another method for testing mutagens. Animals are first treated with suspected mutagen, the mouse DNA is then isolated and the phage segment recovered and used to infect "E. coli". Using similar method as the blue-white screen, the plaque formed with DNA containing mutation are white, while those without are blue.
Use of mutagen in anti-cancer therapy.
Many mutagens are highly toxic to proliferating cells, and they are often used to destroy cancer cells. Alkylating agents such as cyclophosphamide and cisplatin, as well as intercalating agent such as daunorubicin and doxorubicin may be used in chemotherapy. Ionizing radiations are used in radiation therapy.
Mutagens in fiction.
In science fiction, mutagens are often represented as substances that are capable of completely changing the form of the recipient or gaining them superpower. Powerful radiations are the agents of mutation for the superheroes in Marvel Comics's Fantastic Four, Daredevil, and Hulk, while in the Teenage Mutant Ninja Turtles franchise the mutagen is chemical agent called the Ooze, and for Inhumans the mutagen is the Terrigen Mist. Mutagens are also featured in television series, computer and video games, such as the "Cyberia", "The Witcher", ', ', "Resident Evil", "Infamous", "Command & Conquer", "Gears of War 3", and "Fallout".

</doc>
<doc id="20360" url="http://en.wikipedia.org/wiki?curid=20360" title="Mychal Judge">
Mychal Judge

Mychal Judge, O.F.M. (aka Michael Fallon Judge) (May 11, 1933—September 11, 2001), was a Franciscan friar and Catholic priest who served as a chaplain to the New York City Fire Department. It was while serving in that capacity that he was killed, becoming the first certified fatality of the September 11, 2001 attacks.
Early life.
Mychal Judge was born Robert Emmett Judge on May 11, 1933 in Brooklyn, New York, the son of immigrants from County Leitrim, Ireland, and the firstborn of a pair of fraternal twins. His twin sister Dympna was born two days later. Judge was baptized in St. Paul's Church in Brooklyn on June 4. They and their older sister Erin, grew up during the Great Depression.
From the ages of three to six, he watched his father suffer and die of mastoiditis, a slow and painful illness of the skull and inner ear. To earn income following his father's death, Judge shined shoes at New York Penn Station from where he would visit St. Francis of Assisi Church, located across the street. Seeing the Franciscan friars there, he later said, "I realized that I didn't care for material things... I knew then that I wanted to be a friar."
Career.
After spending his freshman year at the St. Francis Preparatory School in Brooklyn, where he studied under the Franciscan Brothers of Brooklyn, in 1948, at the age of 15, Judge began the formation process to enter the Order of Friars Minor. He transferred to St. Joseph's Seraphic Seminary in Callicoon, New York, the minor seminary of the Holy Name Province of the Order. After graduation, he enrolled at St. Bonaventure University in Olean, New York. In 1954 he was admitted to the novitiate of the Province in Paterson, New Jersey. After completing that year of formation, he received the religious habit and professed his first vows as a member of the Order. At that time, he was given the religious name of Fallon Michael. (He later dropped 'Fallon' and changed 'Michael' to the Gaelic form, Mychal). He resumed his college studies at St. Bonaventure University, where he earned his Bachelor's degree in 1957. He was allowed to profess his solemn vows as a full member of the Order in 1958. Following this, he did his theological studies at Holy Name College Seminary in Washington, D.C.. Upon completing these studies in 1961, he was ordained a priest.
After his ordination, Judge was assigned to the Shrine of St. Anthony in Boston, Massachusetts. Following there, he served in various parishes served by the Franciscan friars: St. Joseph Parish in East Rutherford, New Jersey, Sacred Heart Parish in Rochelle Park, New Jersey, Holy Cross Parish in the Bronx and St. Joseph Parish in West Milford, New Jersey. For three years he served as assistant to the President of Siena College, operated by the friars in Loudonville, New York. In 1986 he was assigned to St. Francis of Assisi Church in Manhattan, where he had first come to know the friars. He lived and worked there until his death.
Around 1971, Judge became an alcoholic, although he never showed obvious signs. In 1978, with the support of Alcoholics Anonymous, he became sober and continued to share his personal story of alcoholism to help others facing addiction.
In 1992, Judge was appointed a chaplain to the New York City Fire Department. As chaplain, he offered encouragement and prayers at fires, rescues, and hospitals, and counseled firemen and their families, often working 16-hour days. "His whole ministry was about love. Mychal loved the fire department and they loved him." He was a member of AFSCME Local 299 (District Council 37).
In New York, Judge was also well known for ministering to the homeless, the hungry, recovering alcoholics, people with AIDS, the sick, injured, and grieving, immigrants, gays and lesbians and those alienated by the Church and society.
For example, Judge once gave the winter coat off his back to a homeless woman in the street, later saying, "She needed it more than me." When he anointed a man who was dying of AIDS, the man asked him, "Do you think God hates me?" Judge just picked him up, kissed him, and silently rocked him in his arms.
Even before his death, many considered Judge to be a living saint for his extraordinary works of charity and his deep spirituality. While praying, he would sometimes "become so lost in God, as if lost in a trance, that he'd be shocked to find several hours had passed." Judge's former spiritual director, the former Jesuit, John J. McNeill, observed that, "He achieved an extraordinary degree of union with the divine. We knew we were dealing with someone directly in line with God."
September 11th attacks.
On September 11, 2001, upon learning that the World Trade Center had been hit by the first of two jetliners, Judge rushed to the site. He was met by Rudolph Giuliani, the Mayor of New York City, who asked him to pray for the city and its victims. Judge prayed over some bodies lying on the streets, then entered the lobby of the World Trade Center North Tower, where an emergency command post had been organized. There he continued offering aid and prayers for the rescuers, the injured and the dead.
When the South Tower collapsed at 9:59 am, debris went flying through the North Tower lobby, killing many inside, including Judge. At the moment he was struck in the head and killed, Judge was repeatedly praying aloud, "Jesus, please end this right now! God, please end this!", according to Judge's biographer and "New York Daily News" columnist Michael Daly.
Shortly after his death, an NYPD lieutenant found Judge's body. He and two firemen, an FDNY Emergency Medical Technician detailed to the Office of Emergency Management (OEM), and one civilian bystander then carried Judge's body out of the North Tower. This event was captured in the documentary film "9/11", shot by Jules and Gedeon Naudet. Shannon Stapleton, a photographer from Reuters, photographed Judge's body being carried out of the rubble by the five men. It became one of the most famous images related to 9/11. The "Philadelphia Weekly" reported that the photograph is "considered an American "Pietà"." Judge's body was laid before the altar of St. Peter’s Catholic church before being taken to the medical examiner.
Mychal Judge was designated as "Victim 0001" and thereby recognized as the first official victim of the attacks. Although victims had been killed before him, including the crews, passengers, and hijackers of the first three planes, and occupants of the towers and the Pentagon, Judge was the first certified fatality because his was the first body to be recovered and taken to the medical examiner.
Judge's body was formally identified by NYPD Detective Steven McDonald, a long-time friend. The New York Medical Examiner found that Judge died of "blunt force trauma to the head".
Mourning and honors.
3,000 people attended Judge's funeral Mass on September 15, 2001, at St. Francis of Assisi Church, which was presided over by Cardinal Edward Egan, the Archbishop of New York at that time. Former President Bill Clinton, who attended the funeral, said that Judge's death was "a special loss. We should lift his life up as an example of what has to prevail ... We have to be more like Father Mike than the people who killed him."
Judge was buried in the friars' plot at Holy Sepulchre Cemetery in Totowa, New Jersey. On October 11, 2001 Brendan Fay organized A "Month's Mind Memorial" in Good Shepherd Chapel, General Theological Seminary, New York. It was an evening of prayer, stories, traditional Irish music, and personal testimonials about Mychal Judge.
There have been calls within the Roman Catholic Church to canonize Judge (declare his sainthood). While there is no indication that Rome is seriously considering this, several churches independent of Rome, most notably the Orthodox-Catholic Church of America, have declared him a saint. Some Catholic leaders recognize Judge as a "de facto" saint. There have been claims of miraculous healings through prayers to Judge. Evidence of miracles is required for canonization in the Catholic Church.
Judge's fire helmet was presented to Pope John Paul II. France awarded him the Légion d'honneur. Some members of the U.S. Congress have nominated him for the Congressional Gold Medal. as well as the Presidential Medal of Freedom. In 2002, the City of New York renamed the portion of West 31st Street on which the friary where he lived is located as "Father Mychal F. Judge Street", and christened a commuter ferry, the "Father Mychal Judge".
In 2002, the United States Congress passed "The Mychal Judge Police and Fire Chaplains Public Safety Officers Benefit Act" into law. The law extended federal death benefits to chaplains of police and fire departments, and also marked the first time the federal government extended equal benefits for same-sex couples by allowing the domestic partners of public safety officers killed in the line of duty to collect their federal death benefit.
The New York Press Club instituted The Rev. Mychal Judge Heart of New York Award, which is presented annually for the news story or series that is most complimentary of New York City.
A campaign has been started in Carlstadt, New Jersey to have a statue of Judge erected in its Memorial Park.
Alvernia University, a private independent college in the Franciscan tradition in Reading, Pennsylvania, named a new residence hall in honor of Judge.
The Father Mychal Judge Memorial in the village of Keshcarrigan, County Leitrim, Ireland was dedicated in 2005, on donated land which had belonged to Judge's ancestors. People from the village and surrounding area celebrate his life every year on the 9/11 anniversary.
In 2006 a documentary film, "Saint of 9/11", directed by Glenn Holsten, co-produced by Brendan Fay and narrated by Sir Ian McKellen, was released.
Larry Kirwan, leader of the Irish-American band Black 47, wrote a tribute song entitled "Mychal" in honor of Judge that appeared on the band's 2004 album "New York Town".
The Father Mychal Judge Walk of Remembrance takes place every year in New York on the Sunday before the 9/11 anniversary. It begins with a Mass at St. Francis Church on West 31st Street, then proceeds to the site of Ground Zero, retracing Judge's final journey and praying along the way. Every September 11, there is a Mass in memory of Judge in Boston, attended by many who lost family members on 9/11.
At the National 9/11 Memorial, Judge is memorialized at the South Pool, on Panel S-18, where other first responders are located.
In 2014 Judge was inducted into the Legacy Walk, an outdoor public display which celebrates LGBT history and people. 
Gay orientation and affiliations.
Following his death a few of his friends and associates revealed that Judge was gay – as a matter of orientation rather than practice, as he was a celibate priest. According to fire commissioner Thomas Von Essen: "I actually knew about his homosexuality when I was in the Uniformed Firefighters Association. I kept the secret, but then he told me when I became commissioner five years ago. He and I often laughed about it, because we knew how difficult it would have been for the other firemen to accept it as easily as I had. I just thought he was a phenomenal, warm, sincere man, and the fact that he was gay just had nothing to do with anything."
The revelations about Judge's sexual orientation were not without controversy, however. Dennis Lynch, a lawyer, wrote an article about Judge that appeared on the website catholic.org. Lynch claimed that the priest was not gay and that any attempt to define him as gay was due to "homosexual activists" who wanted to "attack the Catholic Church" and turn the priest into "a homosexual icon".
Others refuted Lynch’s claims with evidence that Judge did, in fact, identify himself as gay, both to others and in his personal journals.
Judge was a long-term member of Dignity, a Catholic LGBT activist organization that advocates for change in the Catholic Church's teaching on homosexuality. On October 1, 1986, the Vatican's Congregation for the Doctrine of the Faith issued an encyclical, "On the Pastoral Care of Homosexual Persons", which declared homosexuality to be a "strong tendency ordered toward an intrinsic moral evil". In response, many bishops, including John Cardinal O'Connor, banned Dignity from diocesan churches under their control. Judge then welcomed Dignity's AIDS ministry to the Church of Saint Francis of Assisi, which is under the control of the Franciscan friars, thereby partially circumventing the cardinal's ban of Dignity.
Judge disagreed with official Roman Catholic teaching regarding homosexuality, though by all accounts he remained celibate. Judge often asked, "Is there so much love in the world that we can afford to discriminate against any kind of love?"

</doc>
<doc id="20361" url="http://en.wikipedia.org/wiki?curid=20361" title="Moonfleet">
Moonfleet

Moonfleet is a tale of smuggling by the English novelist J. Meade Falkner, first published in 1898. The book was extremely popular among children worldwide up until the 1970s, mostly for its themes of adventure and gripping storyline. It remains a popular story widely read and is still sometimes studied in schools.
Plot summary.
In 1757, Moonfleet is a small village near the sea in the south of England. It gets its name from a formerly prominent local family, the Mohunes, whose coat of arms includes a symbol shaped like a capital 'Y'. John Trenchard is an orphan who lives with his aunt, Miss Arnold. Other notable residents are the sexton Mr Ratsey, who is friendly to John; Parson Glennie, the local clergyman who also teaches in the village school; Elzevir Block, the landlord of the local inn, called the "Mohune Arms" but nicknamed the "Why Not?" because of its sign with the Mohune 'Y'; and Mr Maskew, the unpopular local magistrate and his beautiful daughter, Grace.
Village legend tells of the notorious Colonel John "Blackbeard" Mohune who is buried in the family crypt under the church. He is reputed to have stolen a diamond from King Charles I and hidden it. His ghost is said to wander at night looking for it and the mysterious lights in the churchyard are attributed to his activities.
As the main part of the story opens, Block's youthful son, David, has just been killed by Maskew during an attack by the authorities on a smuggling boat. One night a bad storm hits the village and there is a flood. While attending the Sunday service at church, John hears strange sounds from the crypt below. He thinks it is the sound of the coffins of the Mohune family. The next day, he finds Elzevir and Ratsey against the south wall of the church. They claim to be checking for damage from the storm, but John suspects they are searching for Blackbeard's ghost.
Later John finds a large sinkhole has opened in the ground by a grave. He follows the passage and finds himself in the crypt with coffins on shelves and casks on the floor. He realises his friends are smugglers and this is their hiding place. He has to hide behind a coffin when he hears Ratsey and Elzevir coming. When they leave, they fill in the hole, inadvertently trapping him. John finds a locket in a coffin which holds a piece of paper with verses from the Bible. John eventually passes out after drinking too much of the wine while trying to quench his thirst, having not eaten or drunk for days. Later he wakes up in the "Why Not?" inn - he has been rescued by Elzevir and Ratsey. When he is better, he returns to his aunt's house, but she, suspecting him of drunken behaviour, throws him out. Fortunately, Elzevir takes him in.
But when Block's lease on the "Why Not?" comes up for renewal, Maskew bids against him in the auction and wins. Block must leave the inn and Moonfleet but plans one last smuggling venture. John feels honour-bound to go with him, and sadly, says goodbye to Grace Maskew, whom he loves and has been seeing in secret, and gets his mother's prayer book as a good luck charm. The excisemen and Maskew are aware of the planned smuggling run but do not know exactly where it will occur. During the landing Maskew appears and is caught by the smugglers. Elzevir is bent on vengeance for his son by killing Maskew, and while the rest land the cargo and leave, he and John keep watch over Maskew. Just as Block prepares to shoot Maskew the excisemen attack. They kill Maskew and wound John. Block carries John away to safety and they hide in some old quarries. While there, John inadvertently finds out that the verses from Blackbeard's locket contain a code which will reveal the location of his famous diamond.
Once John's wound heals, he and Block decide to recover the diamond from Carisbrooke Castle. After a suspenseful scene in the well where the jewel is hidden, they succeed in escaping to Holland where they try to sell it to a Jewish diamond merchant named Crispin Aldobrand. The merchant cheats them, claiming the diamond is fake. Elzevir falls for the deceit and angrily throws the diamond out of the window. John, however, knows they have been duped, and suggests they try to recover the diamond through burglary. The attempt fails and, they are arrested and sentenced to prison. John curses the merchant for his lies.
John and Elzevir go to prison for life. Eventually they are separated. Then, unexpectedly, ten years later, their paths cross again. They are being transported, and board a ship. A storm blows up, and by a strong coincidence, the ship is wrecked upon Moonfleet beach. While trying to reach the beach Elzevir helps John to safety, but is himself dragged under by the tide and drowned.
John arrives where he originally started, in the "Why Not?", and is reunited with Ratsey. He is also reunited with Grace. She is now a rich young lady, having inherited her father's money. However, she is still in love with John, and they decide to marry. John tells her about the diamond and his life in prison. He regrets having lost everything, but then Parson Glennie receives a letter from Aldobrand. The merchant suffered a guilty conscience, and in an attempt to make amends, has bequeathed the worth of the diamond to John.
John gives the money to the village, and new almshouses are built, and the school and the church renovated. John marries Grace and becomes Lord of the Manor and Justice of the Peace. Their three children grow up and their sons leave home, including their first-born son, Elzevir. But John and Grace themselves have no plans to leave their beloved Moonfleet ever again.
Backgammon.
A feature of the narrative is a continuing reference to the boardgame of backgammon which is played by the patrons of the "Why Not?" on an antique board which bears a Latin inscription "Ita in vita ut in lusu alae pessima jactura arte corrigenda est" (translated in the book as "As in life, so in a game of hazard, skill will make something of the worst of throws"). This inscription provides a moralistic metaphor to the story of the orphan boy who in the end overcomes his travails.
Geography of the book.
Falkner uses the local geography of Dorset and the Isle of Wight in the book, only changing some of the place names. The village of "Moonfleet" is based on East Fleet in Dorset by Chesil Beach. The headland in the book called "The Snout" is Portland Bill. The castle is Carisbrooke Castle on the Isle of Wight.
Adaptations in other media.
The book was filmed by Fritz Lang in 1955 and released under the same name, with a screenplay adapted by Jan Lustig from the novel. A handful of scenes from the book survived, including John's ordeal in the church crypt with the remains of Blackbeard (here renamed Redbeard), and his descent into the well to retrieve the diamond, but the movie altered the novel's plot substantially. Among other major changes, its young hero was given the newly invented rogue gentleman Jeremy Fox for a mentor (played by Stewart Granger), while the role of the working class Elzevir Block was reduced to leading a group of the smugglers seeking to kill John. Lang's film has enjoyed some cachet among French film critics.
In 1964, the BBC filmed a 6-episode TV adaptation under the title "Smuggler's Bay", starring future Doctor Who stars Frazer Hines and Patrick Troughton as John Trenchard and Ratsey, respectively.
In 1984, a TV mini-series was filmed, starring Adam Godley and David Daker. There is also a 90-minute BBC radio version, starring Richard Pearce (from BBC Radio's "The Adventures of Tintin", as well) as John Trenchard.
The Colonial Radio Theatre on the Air released a 300 min. production of the book in May 2009, Starring Jerry Robbins, David Ault, and Rob Cattell. It was dramatised by Deniz Cordell, and produced by M. J. Cogburn.
Angel Exit Theatre Company devised a production which toured the UK in 2009.
In 2010, Chris de Burgh released an album of songs called "Moonfleet & Other Stories" featuring a story based on the book.
Sky1 filmed a two part TV adaptation in Ireland in 2013 starring Ray Winstone, Aneurin Barnard and Karl McCrone. This was aired 28 December 2013 and 29 December 2013 Though more of the plot of the book remains than in the 1955 film, it still bears little relation; John is in his mid-twenties, Maskew has become an aristocrat and tyrannical descendant of Blackbeard, and Elzevir Block the leader of a brothel-frequenting, knife-fighting band of gangsters.

</doc>
<doc id="20362" url="http://en.wikipedia.org/wiki?curid=20362" title="Merge algorithm">
Merge algorithm

Merge algorithms are a family of algorithms that run sequentially over multiple sorted lists, typically producing more sorted lists as output. This is well-suited for machines with tape drives.
The general merge algorithm has a set of pointers p0..n that point to positions in a set of lists L0..n. Initially they point to the first item in each list. The algorithm is as follows:
While any of p0..n still point to data inside of L0..n instead of past the end:
Analysis.
Merge algorithms generally run in time proportional to the sum of the lengths of the lists; merge algorithms that operate on large numbers of lists at once will multiply the sum of the lengths of the lists by the time to figure out which of the pointers points to the lowest item, which can be accomplished with a heap-based priority queue in O(log "n") time, for O("m" log "n") time, where "n" is the number of lists being merged and "m" is the sum of the lengths of the lists. When merging two lists of length "m", there is a lower bound of 2"m" − 1 comparisons required in the worst case.
The classic merge (the one used in merge sort) outputs the data item with the lowest key at each step; given some sorted lists, it produces a sorted list containing all the elements in any of the input lists, and it does so in time proportional to the sum of the lengths of the input lists.
Language support.
Some computer languages provide built-in merge support for collections.
C++.
The C++'s Standard Template Library has the function codice_1, which merges two sorted ranges of iterators, and codice_2, which merges two consecutive sorted ranges "in-place". In addition, the codice_3 (linked list) class has its own codice_4 method which merges another list into itself. The type of the elements merged must support the less-than (<) operator, or it must be provided with a custom comparator.
Python.
Python (programming language)'s standard library (since 2.6) also has a codice_5 function in the codice_6 module, that takes multiple sorted iterables, and merges them into a single iterator.
C#.
In C#, merge can be achieved with LINQ.
Parallel merge.
In parallel computing, arrays of sorted values may be merged efficiently using an all nearest smaller values computation.
Parallel merge can also be implemented using a divide-and-conquer algorithm, developed and shown in pseudo-code in. This algorithm performs well when combined with a fast sequential merge as a base case for merging of small arrays. Implementation using Intel's Threading Building Blocks (TBB) and Microsoft's Parallel Pattern Library (PPL) to run on multi-core processors is shown to perform well in practice.

</doc>
<doc id="20363" url="http://en.wikipedia.org/wiki?curid=20363" title="ML">
ML

ml may refer to:
mL may refer to:
ML may refer to:

</doc>
<doc id="20366" url="http://en.wikipedia.org/wiki?curid=20366" title="Cuisine of the Midwestern United States">
Cuisine of the Midwestern United States

Midwestern cuisine is a regional cuisine of the American Midwest. It draws its culinary roots most significantly from the cuisines of Central, Northern and Eastern Europe, and is influenced by regionally and locally grown foodstuffs and cultural diversity.
Everyday Midwestern home cooking generally showcases simple and hearty dishes that make use of the abundance of locally grown foods. Its culinary profiles may seem synonymous with "American food." Quoted in a 2007 interview with the "Daily Herald", Chef Stephen Langlois, a pioneer in the Midwestern local food movement, described it: "Think of Thanksgiving dinner. Turkey and cranberry sauce and wild rice and apple pie."
In its urban centers, the Midwest's restaurants offer a diverse mix of ethnic cuisines as well as sophisticated, contemporary techniques.
Characteristics.
Sometimes called "the breadbasket of America," the Midwest serves as a center for grain production, particularly wheat, corn and soybeans. Midwestern states also produce most of the country's wild rice.
Beef and pork processing always have been important Midwestern industries, with a strong role in regional diets. Chicago and Kansas City were historically stockyard and processing centers of the beef trade, while Iowa remains the center of pork production in the U.S.
Far from the oceans, Midwesterners traditionally ate little seafood, relying on local freshwater fish, such as perch and trout, supplemented by canned tuna and canned or cured salmon and herring, although modern air shipping of ocean seafood has been increasing Midwesterners' taste for ocean fish.
Dairy products, especially cheese, form an important group of regional ingredients, with Wisconsin known as "America's Dairyland," although other Midwest states make cheese as well.
The upper Midwest, a prime fruit-growing region, sees the extensive use of apples, blueberries, cranberries, cherries, peaches and other cold-climate fruit in its cuisine.
As with many American regional cuisines, Midwestern cooking has been heavily influenced by immigrant groups. Throughout the northern Midwest, northern European immigrant groups predominated, so Swedish pancakes and Polish pierogi are common. Wisconsin, Missouri, Kansas, Ohio and Illinois were destinations for many ethnic German immigrants, so pork sausages and potatoes are prevalent. In the Rust Belt, many Greeks became restaurateurs, imparting a Mediterranean influence. Native American influences show up in the uses of corn and wild rice.
Traditionally, Midwestern cooks used a light hand with seasonings, preferring sage, dill, caraway, mustard and parsley to hot, bold and spicy flavors. However, with new waves of immigrants from Latin America and Asia moving into the region, these tastes are changing.
This section of the region is also headquarters for several seminal hamburger chains, including McDonald's in Oak Brook, Illinois (founded in California, but turned into the iconic franchise by Ray Kroc beginning with a still-standing store in Des Plaines, Illinois). The Midwest is also home to Hardee's in St. Louis, Missouri, Culver's in Sauk City, Wisconsin; Steak n Shake, founded in Normal, Illinois, and now based in Indianapolis; Wendy's in Dublin, Ohio; and both Pizza Hut (now based in Plano, Texas) and White Castle (now based in Columbus, Ohio) were founded in Wichita, Kansas. Diner chain Big Boy, known for burgers, is headquartered in Warren, Michigan.
Urban centers.
Major urban areas in the Midwest feature distinctive cuisines very different from those of the region's rural areas, and some larger cities have world-class restaurants.
Barberton, Ohio.
Part of the greater Akron area, this small industrial city with a strong Central and Eastern European heritage has a culinary contribution called Barberton Chicken, created by Serbian immigrants, deep fried in lard, and usually accompanied by a hot rice dish, vinegar coleslaw and french fries.
Chicago.
The ethnic mix of the people of Chicago has led to a distinctive cuisine of restaurant foods exclusive to the area, such as Italian beef, the Maxwell Street Polish, the Chicago-style hot dog, Chicago-style pizza, chicken Vesuvio and the jibarito, as well as a large number of steakhouses.
Chicago also boasts many gourmet restaurants, as well as a wide variety of ethnic food stores and eateries, most notably Mexican, Polish, Italian, Greek, Indian/Pakistani and Asian, often clustered in ethnic neighborhoods. Many of these cuisines have evolved significantly in Chicago. For example, the Greek cheese dish saganaki was first flambéed at the table in Greektown.
The Midwest is sometimes thought to be behind the coasts in culinary trends, yet, perhaps ironically, Chicago is the country's leading center of cutting-edge molecular gastronomy, likely due to the influence of Grant Achatz.
As a major rail hub, Chicago historically had access to a broad range of the country's foodstuffs, so even in the 19th century, Chicagoans could easily buy items like live oysters and reasonably fresh shrimp. Chicago's oldest signature dish, shrimp de Jonghe, was invented around the turn of the 20th century. Today, flights into O'Hare Airport bring Chicago fresh food from all over the world.
Cincinnati.
The Queen City is known for its namesake, Greek-influenced "Cincinnati chili". Unlike other forms of chili, Cincinnati-style chili is almost never consumed by itself and is a staple of "three-way" spaghetti, cheese coneys, and various dips. Goetta, a sausage made from pork and oats, often eaten at breakfast, and opera cream chocolates are less-famous local specialties. The city also has a strong German heritage and a variety of German-oriented restaurants can be found in the area.
Cleveland.
Cleveland's many immigrant groups and heavily blue-collar demographic have long played an important role in defining the area's cuisine. Ethnically, Italian foods as well as several Eastern European cuisines, particularly those of Poland and Hungary, have become gastronomical staples in the Greater Cleveland area. Prominent examples of these include cavatelli, rigatoni, pizza, Chicken paprikash, stuffed cabbage, pierogi, and kielbasa all of which are widely popular in and around the city. Local specialties, such as the pork-based dish City Chicken and the Polish Boy (a loaded sausage sandwich native to Cleveland), are dishes definitive of a cuisine that is based on hearty, inexpensive fare. Commercially, Hector Boiardi (aka Chef Boyardee) started his business in Cleveland's Little Italy, and Mr. Hero, a regional sandwich shop franchise, is based in the area.
Sweets specific to the Cleveland area include the coconut bar (similar in many respects to the Australian Lamington). Coconut bars, which are found in many Jewish bakeries in the area, are small squares of cake that have been dipped in chocolate and rolled in coconut. In Italian bakeries around the Cleveland area, a variation of the Cassata cake is widely popular. This local version is unlike those typically found elsewhere being that it is made with layers of sponge cake custard and strawberries, then frosted with whipped cream. In a celebrity-chef nod to this version, Mario Batali as 'the best Cassata cake in the USA.'
Columbus.
The Columbus, Ohio area is the home and birthplace of many well-known fast food chains, especially those known for hamburgers. Wendy's opened its first store in Columbus in 1969, and is now headquartered in nearby Dublin. America's oldest hamburger chain, White Castle, is based there. Besides burgers, Columbus is noted for the German Village, a neighborhood south of downtown where German cuisine such as sausages and kuchen are served. In recent years, local restaurants focused on organic, seasonal, and locally or regionally sourced food have become more prevalent, especially in the Short North area, between downtown and the OSU campus. Numerous Somali restaurants are also found in the city, particularly around Cleveland Avenue.
Columbus is also the birthplace of the famed Marzetti Italian Restaurant, opened in 1896. The restaurant's popular salad dressings became the foundation for the T. Marzetti Company, an international specialty foods manufacturer and distributor, headquartered in Columbus.
Detroit.
Detroit specialties include Coney Island hot dogs, found at hundreds of unaffiliated "Coney Island" restaurants. Not to be confused with a chili dog, a coney is served with a ground beef sauce, chopped onions and mustard. The Coney Special has an additional ground beef topping. It is often served with French fries.
Detroit also has its own style of pizza, a thick-crusted, Sicilian cuisine-influenced, rectangular type called square pizza. Other Detroit foods include zip sauce, served on steaks; the triple-decker Dinty Moore sandwich, corned beef layered with lettuce, tomato and Russian dressing; and a Chinese-American dish called "warr shu gai" or almond boneless chicken.
The Detroit area has many large groups of immigrants. A large Arabic-speaking population reside in and around the suburb of Dearborn, home to many Lebanese storefronts. Detroit also has a substantial number of Greek restaurateurs. Thus, numerous Mediterranean restaurants dot the region and typical foods such as gyros, hummus and falafel can be found in many run-of-the-mill grocery stores and restaurants.
Polish food is also prominent in the region, including popular dishes such as pierogi, borscht, and pączki. Bakeries concentrated in the Polish enclave of Hamtramck, Michigan, within the city, are celebrated for their "pączki," especially on Fat Tuesday.
Chinese restaurants in the Detoit area serve , a regional Chinese-American dish consisting of battered fried boneless chicken breasts served sliced on a bed of lettuce with a gravy-like chicken flavored sauce and slivered almonds.
In nearby Ann Arbor the Chipati, a tossed salad, served inside a freshly baked pita pocket with the "secret" Chipati sauce on the side. The Chipati's origination is claimed by both Pizza Bob's on S. State St. and by Pizza House on Church St.
Indianapolis.
Indianapolis was settled predominately by Americans of British descent and Irish and German immigrants, so much of the city's food draws upon these influences. Much of the food is considered to be "Classic American Cuisine". Later immigrants included many Jews, Poles, Eastern Europeans and Italians, all of whom influenced local food. Two of the city's most distinct dishes are the pork tenderloin sandwich and sugar cream pie.
A fast-growing immigrant population from places such as Mexico and India is also beginning to influence the local food. The area offers many diverse, locally owned ethnic restaurants, as well as nationally and internationally renowned restaurants. Indy is also home to many local pubs.
Kansas City.
Kansas City is an important barbecue and meat-processing center with a distinctive barbecue style. The Kansas City metropolitan area has more than 100 barbecue restaurants and proclaims itself to be the "world's barbecue capital." The Kansas City Barbeque Society spreads its influence across the nation through its barbecue-contest standards. The oldest continuously operating barbecue restaurant is Rosedale Barbecue near downtown Kansas City. Other popular barbecue restaurants are Gates Bar-B-Q, Oklahoma Joe's and Arthur Bryant's. Both Arthur Bryant's and Gates Bar-B-Q sell bottled versions of their barbecue sauces in restaurants and specialty stores in the surrounding areas.
Mansfield, Ohio.
Mansfield is the home of two well-known food companies. Isaly Dairy Company (AKA Isaly's) was a chain of family-owned dairies and restaurants started by William Isaly in the early 1900s until the 1970s, famous for creating the Klondike Bar ice cream treat, popularized by the slogan "What would you do for a Klondike Bar?". Stewart's Restaurants is a chain of root beer stands started in Mansfield by Frank Stewart in 1924, famous for their Stewart's Fountain Classics line of premium beverages now sold worldwide.
Milwaukee.
German immigrants settled Milwaukee. Sauerkraut, bratwurst, and beer as well as other traditional German favorites continue to be popular in homes as well as at Milwaukee's famous German restaurants. Milwaukee also offers a diverse selection of other ethnic restaurants.
Served under various names, a favorite sandwich for Milwaukeeans and Wisconsinites consists of a brat (often butterflied to lay flat) on top of a hamburger in a kaiser roll.
Frozen custard is a local favorite in the Cream City, with many competing stands throughout the area.
Cheese curds are another local favorite, and Wisconsinites also enjoy them fried.
Also known as Brew City, Milwaukee is home to many breweries and the traditional and nominal headquarters for national beer brands.
Minneapolis and Saint Paul.
Minneapolis and Saint Paul offer a diverse array of cuisines influenced by their many immigrant groups, as well as those restaurant chefs who follow the trends of larger cities. While at-home fare varies broadly within various ethnic groups and their culture, historically, the overall majority of Minnesotans were of European ancestry, many with farming backgrounds and many home cooked meals still reflect this, with comfort food items such as hotdish, hearty soups and stews and meat and potatoes commonly being served. Many Minnesotans claim some Scandinavian heritage, and while iconic dishes such as lefse and lutefisk are quite commonly served at home as well as church potlucks and community get-togethers, few restaurants serve these items. Another popular item in Minnesota is wild rice which has been gathered in area lakes by Native Americans for centuries. In the fall, the Twin Cities share along with Green Bay, Wisconsin, the tradition of the neighborhood booyah, a cuisine and cultural event featuring a hodge-podge of ingredients in stews. One item of note, Minneapolis and Saint Paul pioneered the Jucy Lucy (or "Juicy Lucy"), a hamburger with a core of melted cheese.
American restaurants in the Twin Cities supply a wide spectrum of choices and styles that range from small diners offering simple short order grill fare and the typical sports bars and decades old supper clubs to high-end steakhouses and eateries that serve new American cuisine using locally grown ingredients. Most types of American regional cuisine can be found at restaurants in the Twin Cities. Barbecue restaurants in the area tend to feature a combination of the various regional styles of this type of cooking.
Germans comprise the majority of the state's ethnic heritage and one can find authentic German cuisine at the Glockenspiel in Saint Paul, the Gasthaus Bavarian Hunter in nearby Stillwater, and at the Black Forest Inn and the Gasthof zur Gemutlichkeit both found in Minneapolis. The latter restaurant is in Minneapolis' Northeast community which is also home to thriving Czech, Polish, Ukrainian and other Eastern European restaurants such as Jax Café, Kramarczuk's, Mayslack's and Nye's Polonaise lending this area an old world character and charm. The Twin Cities can also boast of authentic French, Irish, Italian and Russian restaurants. Spanish tapas restaurants exist, but are more trendy than homage. In the Twin Cities, pizzerias tend to be American rather than rustic Italian (although they too exist and offer inventive recipes.)
Authentic Mexican and Tex-Mex restaurants are quite popular in the Twin Cities, as there are Hispanic neighborhoods in both Saint Paul and Minneapolis. Many entrepreneurs have taken authentic Mexican cuisine into the suburbs as well. Latin American purveyors are also pioneering their home cuisines from Argentina, Brazil, Cuba, Ecuador, Peru and the West Indies offering authentic churrasco and ceviche among their dining options.
Asian cuisine was initially dominated by Chinese Cantonese immigrants that served Americanized offerings. In 1883 Woo Yee Sing and his younger brother, Woo Du Sing, opened the Canton Cafe in Minneapolis, the first Chinese restaurant in Minnesota. Authentic offerings began at the influential Nankin Cafe which opened in 1919, and many new Chinese immigrants soon took this cuisine throughout the Twin Cities and to the suburbs. Authentic Chinese cuisine from the provinces of Hunan and Szechaun and from Beijing, Shanghai and Taiwan are relatively new. The cuisine of Japan has been present since the opening of the area's very first Japanese restaurant, Fuji Ya in 1959. Since then, sushi and teppanyaki restaurants have also become increasingly more common. In the 1970s the Twin Cities saw a large influx of Southeast Asian immigrants from Cambodia, Laos, Thailand and Vietnam. The urban areas are now proliferated by Vietnamese phở noodle shops and Thai curry restaurants. Since 1976 Supenn Supatanskinkasem (now Harrison) has been cooking and serving Thai food through her Minnesota State Fair Booth, Siam Café, and Sawatdee chain of Thai restaurants. Thanks to her persistence and success, others have opened Thai restaurants and there are now more than 100 establishments throughout Minnesota offering the food of Thailand.
Cambodian cuisine has also flourished given the large Hmong population familiar with it. Korean restaurants are few, as possibly their dining style and flavors have not been as adopted into the American mainstream. In the Twin Cities suburbs, Oriental buffets are popular for offering different Asian cuisines together. Restaurants offering other cuisines of Asia including those from Afghanistan, India, Nepal and the Philippines are also fairly recent additions to the Twin Cities dining scene and have been well received. Local ingredients are often integrated into Asian offerings, for example Chinese steamed walleye and Nepalese curried bison.
The Twin Cities are home to many restaurants that serve the cuisines of the Mediterranean and the Middle East. There are numerous Greek restaurants that range from fine dining to casual fast food shops that specialize in gyros. In both Minneapolis and Saint Paul, there exist long established Jewish cafes and delicatessens. Lebanese restaurants have also had a long time presence in both cities.
Authentic offerings of Arab cuisine, as well as other Middle Eastern cuisines, exist in the Minneapolis/St. Paul Metropolitan area. Egyptian, Iranian (Persian), Kurdish, and Turkish restaurants can be found throughout the Twin Cities.
Related cuisines from Northeast Africa can also be found throughout the Twin Cities metropolitan area. While restaurants that serve Ethiopian dishes have been in the Twin Cities for decades, more recent immigrants from Somalia have also opened a number of restaurants in Minnesota. Somali cuisine consists of an exotic mixture of native Somali, Ethiopian, Yemeni, Indian, Persian, Turkish and Italian culinary influences.
In addition, West African immigrants have introduced their own unique cuisine in recent years. There is also a presence of Afro-Caribbean restaurants.
The University of Minnesota has been a center for food research with inventions such as the Honeycrisp apple. The Minnesota State Fair offers a sampling of many cuisines each year and Twin Citians claim that the all-American Corn Dog and Pronto Pup made their very first appearances there. Additionally, many important agricultural conglomerates, including Cargill, General Mills/Pillsbury, and International Multifoods make their home in Minneapolis-Saint Paul. The Betty Crocker food brand (named after a non-existent housewife) was born there. Several national restaurant chains, such as Buca di Beppo, Famous Dave's and the now defunct Chi-Chi's started in the Twin Cities. Buffalo Wild Wings, Dairy Queen, KarmelKorn Shoppes, Old Country Buffet, Orange Julius and T.G.I. Friday's (a division of Carlson Companies) are also well known chains headquartered in the Twin Cities.
Omaha.
Omaha has some unusual steakhouses such as the famous Gorat's, several of which are Sicilian in origin or adjacent to the Omaha Stockyards. Central European and Southern influences can be seen in the local popularity of carp and South 24th Street contains a multitude of Mexican restaurants. North Omaha also has its own barbecue style.
Omaha is one of the places claiming to have invented the reuben sandwich, supposedly named for Reuben Kulakofsky, a grocer from the Dundee neighborhood.
Bronco's, Godfather's Pizza, and the Garden Cafe are among the chain restaurants that originated in Omaha.
Omaha also has a thriving local pizza scene, with popular restaurants including Zio's, La Casa and Valentino's. However, Big Fred's and Johnny Sortino's are the two that routinely vie for the title of the best pizza in town.
St. Louis.
The large number of Irish and German immigrants who came to St. Louis beginning in the early nineteenth century contributed significantly to the shaping of local cuisine as confirmed by a variety of uses of beef, pork and chicken, often roasted or grilled, as well as a variety of desserts including rich cakes, stollens, fruit pies, doughnuts and cookies. Even a local form of fresh stick pretzel, called Gus's Pretzels, has been sold singly or by the bagful by street corner vendors.
Mayfair salad dressing was invented at a St. Louis hotel of the same name, and is richer than Caesar salad dressing. St. Louis is also known for popularizing the ice cream cone and for inventing gooey butter cake (a rich, soft-centered coffee cake) and frozen custard. Iced tea is also rumored to have been invented at the World's Fair, as well as the hot dog.
Although St. Louis is typically not included on the list of major styles of barbecue in the United States, it was recognized by Kingsford as "America’s Top Grilling City" in its second annual list of "Top 10 Grilling Cities." A staple of grilling in St. Louis is the pork steak, which is sliced from the shoulder of the pig and often basted with or simmered in barbecue sauce during cooking. Other popular grilled items include crispy snoots, cut from the cheeks and nostrils of the pig; bratwurst; and Italian sausage, often referred to as "sah-zittsa," a localization of its Italian name, salsiccia. Maull's is a popular brand of barbecue sauce in the St. Louis area.
Restaurants on The Hill reflect the lasting influence of the early twentieth century Milanese and Sicilian immigrant community. Two unique Italian-American style dishes include "toasted" ravioli, which is breaded and fried, and St. Louis-style pizza, which has a crisp, thin crust and is usually made with Provel cheese instead of traditional mozzarella cheese.
A Poor boy sandwich is the traditional name in St. Louis for a submarine sandwich. A St. Paul sandwich is a St. Louis sandwich, available in Chinese-American restaurants. A Slinger is a diner and late night specialty consisting of eggs, hash browns and hamburger, topped with chili, cheese and onion.
Regional specialties.
Illinois.
Illinois is a top producer of corn and soybeans, but corn, particularly sweet corn, figures most substantially in its cuisine. Chicago-style cuisine is dominant in Northeastern Illinois, while other parts of the state mirror adjoining regions.
Springfield, Illinois, and the surrounding area are known for the horseshoe sandwich.
Indiana.
A popular dish seen almost exclusively in Indiana is sugar cream pie, which most likely originated in the state's Amish community. Persimmon pudding is also a favorite Indiana dessert very difficult to find outside of the Hoosier State.
The pork tenderloin sandwich is a popular state food. Beef and noodles is another homespun Hoosier dish.
Frog legs are traditional in old-fashioned Indiana restaurants, and brain sandwiches have a following. Fried biscuits with apple butter are served at many restaurants in southern Indiana, as are fried-brain sandwiches. 
Iowa.
The cuisine of Iowa includes the pork tenderloin sandwich, consisting of a lean section of boneless pork loin that is pounded flat, breaded, and deep fried before being served on a seeded hamburger bun with any or all of ketchup, mustard, mayonnaise, and dill pickle slices.
Iowa is the center for loose-meat sandwiches, such as those popularized by Maid-Rite, although they can also be found in western Illinois, Indiana and Nebraska. A unique food native to the Iowa State Fair that has spread all over the state and some neighboring states is the "Walking Taco". It consists of a bag of crushed chips (usually Nacho Cheese Doritos)and taco seasoned hamburger with additional items added to it such as lettuce, shredded cheese, tomatoes, onions, etc.
Michigan.
Michigan is a large producer of asparagus a vegetable crop widespread in Spring. Western and northern Michigan are notable in the production of apples, blueberries, and cherries. The Northwestern region of Michigan's Lower Peninsula accounts for approximately 75 percent of the U.S. crop of tart cherries, usually about 250 million pounds (11.3 Gg). A popular dish, Michigan Chicken Salad, includes cherries and often apples. Fruit salsas are also popular with cherry salsa being especially prominent. Michigan's wine and beer industries are substantial in the region. The Traverse City area is a popular destination to visit wineries and the state makes many varieties of wine, such as Rieslings, ice wines, and fruit wines. Micro-breweries continue to blossom creating a wide range of unique beers. Grand Rapids was voted Beer City USA 2013 in the Beer City USA poll, with Founders being the largest of Grand Rapids' breweries. Bell's, another large Michigan craft brewery is located further south in Kalamazoo.
Michigan is the home of both Post and Kellogg, with Battle Creek being called Cereal City. Vernor's ginger ale and Faygo pop also originate in Michigan. Vernor's ginger ale is often used as a home remedy for an upset stomach.
Coney Islands, a type of diner originating with Greek immigrants in Detroit, are fairly common throughout the state. Coney Dogs are always on the menu, which is a hot dog, usually on a bun or on fries, with raw onion, mustard, and Coney sauce, a type of Chili. Cheese may be added as well. These diners usually also have gyros served with cucumber or honey mustard sauce, as well as hamburgers, sandwiches, breakfast and dinner entrees. Most Coney Islands are open 24 hours and also a popular place to get a late or early coffee.
In Polish communities throughout the state Pączki can be found every year on Fat Tuesday (Mardi Gras) in a wide assortment of flavors including lemon, blueberry to custard. Fish fries are common during Lent. Fish fries are usually set up buffet style typically consisting of items including rolls, potatoes (typically in the form of french fries and mashed), salad, coleslaw, apple sauce, deep fried fish and sometimes fried shrimp and baked fish. Fish is generally popular thoroughout the state due to the states location on four of the Great Lakes. Sunfish and catfish are common. Whitefish is a regional specialty usually offered along the coast, with smoked whitefish and whitefish dip being noteworthy. 
Cornish immigrant miners introduced the pasty to Michigan's Upper Peninsula (U.P.) as a convenient meal to take to work in the numerous copper, silver, and nickel mines of that region. The pasty is today considered iconic of the U.P.
Fudge is commonly sold in tourist areas, with Mackinac Island being most famous for its fudge, traditionally chocolate, but there is a wide variety of flavors from mint to maple and may include nuts, fruit, or other candy pieces.
Minnesota.
Perhaps the most iconic Minnesota dishes are lefse and lutefisk, brought to the state with Scandinavian immigrants. Lefse and lutefisk dinners are held near Christmas and have become associated with that holiday. Lutefisk is a traditional dish of the Nordic countries made from stockfish (air-dried whitefish) and soda lye ("lut"). Walleye is the state fish of Minnesota and it is common to find it on restaurant menus. Its popularity with Minnesotans is such that the residents of the state consume more of the fish than does any other jurisdiction. Battered and deep-fried is a popular preparation for walleye, as is grilling. Many restaurants will feature walleye on their Friday night fish fry, which is popular at locales throughout the state.
Minnesota is known for its church potlucks, where hotdish is often served. Hotdish is any of a variety of casserole dishes, which are popular throughout the United States, although the term "hotdish" is used mainly in Minnesota, Wisconsin, North Dakota, and South Dakota. Hotdishes are filling comfort foods that are convenient and easy to make. "Tater Tot Hotdish" is a popular dish, and as Minnesota is one of the leading producers of wild rice, wild rice hotdishes are quite popular. Minnesota goulash, a famous combination of tomatoes, macaroni, ground beef and creamed corn is popular as well.
Bars are the second of the two essentials for potlucks in Minnesota. According to "You Know You're in Minnesota When...: 101 Quintessential Places, People, Events, Customs, Lingo, and Eats of the North Star State" by Berit Thorkelson, the bar is a Minnesota staple and a "typical Minnesota dessert". Thorkelson notes that bars are not included in Webster's Dictionary, and the word pronunciation of the "ar" is with "a pirate-like arrr" followed by a soft clipped s.
The immigrants that settled in the state in the 1800s were predominantly from Central and Eastern Europe (particularly Germany) and Scandinavia. They brought with them taste preferences that largely remain to this day. Those Minnesotans with this Northern European ancestry, in general, avoid hot spices in favor of earthy or aromatic spices.
In the northeastern section of the state, in the region collectively known as the Iron Range, the Mesabi Range area is known for Cornish pasties. The pasty, a meat and vegetable combination in a pastry crust, was brought to Minnesota by way of early Finnish iron miners as an easy lunch for miners working deep in the iron mines. It remains a favorite for both "locals" and summer tourists.
A traditional Slovenian nut bread called potica served at Easter and Christmas is still very popular in northern Minnesota. It is a yeast dough rolled and stretched paper thin and spread with a mixture of ground walnuts, butter, eggs, cream, and honey or sugar. It is then rolled jellyroll fashion and baked. Traditionally it was spiraled in a round pan, but now one is more likely to find it baked as a loaf.
The state is a productive area for chicken, dairy and turkey farms and crops such as corn, soybeans, and sugar beets and as such, eggs and meat along with potatoes and vegetables are mainstay foods. Warm baked goods along with stews and hearty soups are a favorite in the winter given the extreme Minnesota climate. Recipes using local wild game such as bison, deer or elk. are also common.
Other popular dishes statewide include glorified rice, Jell-O salad, and krumkake.
Missouri.
In Missouri, much of the cuisine is influenced by that of the Ozarks. Barbecue, both pork and beef, is popular in both St. Louis and Kansas City, as well as in much of the Southern half of the state. In the Bootheel, sweet tea is readily available everywhere. Missouri also leans heavily on beer and bratwurst, and St. Louis features the 'brain sandwich', the 'St. Paul Sandwich', toasted ravioli, St. Louis-style pizza, gooey butter-cake, and many other cuisines that are popular throughout the state. Chinese food is also very popular in the state, with Springfield being a big example. Fishing is a very popular sport throughout the state, with the presence of Missouri's many rivers and lakes, and like in Wisconsin, many fish fry events are popular throughout the state featuring catfish and large-mouthed bass. Like many of its fellow Midwestern states, Missouri is at the forefront of corn and soybean production, and items such as corn-on-the cob, mashed potatoes, basically a typical Midwestern meal, are very popular throughout the state. The middle of the state, known as the "Missouri Rhineland", lies along the valley of the Missouri River, and is known for its wineries.
North Dakota.
Cuisine in North Dakota has been heavily influences by both Norwegians and Germans from Russia, ethnic groups which have historically accounted for a large portion of North Dakota's population. Norwegian influences in the state include lefse, lutefisk, krumkake, and rosettes. Much of the Norwegian-influenced cuisine is also common in Minnesota and other states where Norwegians and their descendants live(d), although Norwegian influence may be the greater in North Dakota than any other state, as Norwegians played a large role in settling the area, and nearly one-third of North Dakotans claim Norwegian ancestry. Norwegian ancestry was historically more widespread throughout the northern half and eastern third of North Dakota, and therefore plays a stronger role in local cuisine in those parts of the state.
German-Russian cuisine is primarily influenced by that of the Schwarzmeerdeutsche, or Black Sea Germans, that heavily populated south-central and southwestern North Dakota (an area known as the German-Russian Triangle), as well as areas of South Dakota. While large numbers of Wolgadeutsche, Germans from Russia who lived near the Volga River in Russia (several hundred miles away from the Black Sea), also settled in the United States, they did not settle in large numbers in the Dakotas. Popular German-Russian cuisine includes kuchen, a thin, cheesecake-like custard pastry often filled with fruit such as cherries, apricot, prunes, and sometimes cottage cheese. Fleischkuekle (or fleischkuechle) is a popular meat-filled thin flatbread that is deep-fried and served hot. Another German-Russian specialty in the area is knoephla, a dumpling soup that almost always includes potatoes, and to a lesser extent, celery.
Ohio.
A confection indigenous to the state of Ohio is the local variation of a peanut butter cup known as a 'Buckeye'. Coated in chocolate, with a partially exposed peanut butter center, in appearance the candy resembles the chestnut that grows on the state tree.
Cincinnati-style chili is a dish consisting of spaghetti noodles, a thin meat chili, covered with shredded cheese, as served by Skyline Chili and others.
In the Cleveland and Cincinnati areas, a popular dish are Sauerkraut Balls. Sauerkraut Balls are meatball-like snack foods eaten as appetizers or as bar food. The recipe was invented in the late 1950s by two brothers, Max and Roman Gruber. They created the dish to serve in their five star restaurant, Gruber's, located in Shaker Heights, Ohio. These were a derivative of the various ethnic cultures of Northeast Ohio, which includes Akron and Greater Cleveland. A once-famous but now closed restaurant in Vermilion, Ohio, was McGarvey's, which was famous for its Sauerkraut Balls as well as for its charismatic owner, Captain Eddie, and its location near the scenic Vermilion River.
Clam bakes are more popular in Northeast Ohio than any other region of the United States outside of New England. The region which was originally part of the Connecticut Western Reserve, was initially settled by people from Connecticut and other New England states. A typical Northeast Ohio clam bake typically includes clams, chicken, sweet potatoes, corn, and other side dishes. Unlike in New England, seaweed is not used and the clams, chicken, and sweet potatoes are all steamed together in a large pot.
Some pizza restaurants sell pizza "fold over" style. A fold over pizza has a layer of crust on the bottom and on the top, with typical pizza toppings in between. Unlike a calzone or turnover, in which the ingredients are completely sealed in with dough, a fold over resembles a sandwich.
Wisconsin.
The Friday night fish fry, typically fried perch or walleye, is ubiquitous throughout Wisconsin, while in northeast Wisconsin along Lake Michigan, the Door County fish boil holds sway.
Besides beer, Wisconsinites drink large quantities of brandy, often mixed into the unique Badger libation, the "brandy Old Fashioned sweet."
Seymour, Wisconsin, claims to be the birthplace of the modern hamburger, although several other locations make similar claims. The southern Wisconsin city of Racine is known for its Danish kringle.
Wisconsin is "America's Dairyland," and is home to numerous frozen custard stands, particularly around Milwaukee and along the Lake Michigan corridor, as well as many cheesemakers, ranging from artisans who hand-craft their product from the milk of their own dairy herds to large factories. Cheese curds are common as a snack or fried as an appetizer.
Wisconsin is also well known for summer sausage and brats.
Dishes.
These dishes, while not all exclusive to the Midwest, are typical of Midwestern foods. Although many foods are shared with other U.S. regions, they often feature uniquely Midwestern preparation styles.

</doc>
<doc id="20367" url="http://en.wikipedia.org/wiki?curid=20367" title="Moor">
Moor

Moor may refer to:

</doc>
<doc id="20369" url="http://en.wikipedia.org/wiki?curid=20369" title="Mitosis">
Mitosis

Mitosis is a part of the cell cycle process by which chromosomes in a cell nucleus are separated into two identical sets of chromosomes, each in its own nucleus. In general, mitosis (division of the nucleus) is often followed by cytokinesis, which divides the cytoplasm, organelles and cell membrane into two new cells containing roughly equal shares of these cellular components. Mitosis and cytokinesis together define the mitotic (M) phase of an animal cell cycle—the division of the mother cell into two daughter cells, genetically identical to each other and to their parent cell.
The process of mitosis is highly complex. The sequence of events is divided into stages corresponding to the completion of one set of activities and the start of the next. These stages are prophase, prometaphase, metaphase, anaphase, and telophase. During mitosis, the chromosomes, which have already duplicated, condense and attach to fibers that pull one copy of each chromosome to opposite sides of the cell. The result is two genetically identical daughter nuclei. The cell may then divide by cytokinesis to produce two daughter cells. Errors during mitosis can induce apoptosis (programmed cell death) or cause mutations. Certain types of cancer can arise from such mutations.
Mitosis occurs only in eukaryotic cells and the process varies in different organisms. For example, animals undergo an "open" mitosis, where the nuclear envelope breaks down before the chromosomes separate, while fungi undergo a "closed" mitosis, where chromosomes divide within an intact cell nucleus. Furthermore, most animal cells undergo a shape change to adopt a near spherical morphology at the start of mitosis. Prokaryotic cells, which lack a nucleus, divide by a different process called binary fission.
Discovery.
German zoologist Otto Bütschli was one of the first researchers who might have claimed the discovery of the process presently known as "mitosis", a term coined by Walther Flemming in 1882.
Mitosis was discovered in frog, rabbit, and cat cornea cells in 1873 and described for the first time by the Polish histologist Wacław Mayzel in 1875.
The term is derived from the Greek word μίτος "mitos" "warp thread".
Overview of mitosis.
The primary result of mitosis and cytokinesis is the transfer of a parent cell's genome into two daughter cells. The genome is composed of a number of chromosomes—complexes of tightly coiled DNA that contain genetic information vital for proper cell function. Because each resultant daughter cell should be genetically identical to the parent cell, the parent cell must make a copy of each chromosome before mitosis. This occurs during the S phase of interphase. Chromosome duplication results in two identical "sister chromatids" bound together by cohesin proteins at the "centromere".
When mitosis begins, the chromosomes condense and become visible. In some eukaryotes, for example animals, the nuclear envelope, which segregates the DNA from the cytoplasm, disintegrates into small vesicles. The nucleolus, which makes ribosomes in the cell, also disappears. Microtubules project from opposite ends of the cell, attach to the centromeres, and align the chromosomes centrally within the cell. The microtubules then contract to pull the sister chromatids of each chromosome apart. Sister chromatids at this point are called "daughter chromosomes". As the cell elongates, corresponding daughter chromosomes are pulled toward opposite ends of the cell. A new nuclear envelope forms around the separated daughter chromosomes.
As mitosis concludes, the cell may begin cytokinesis. In animal cells, a cell membrane pinches inward between the two developing nuclei to produce two new cells. In plant cells, a cell plate forms between the two nuclei.Cytokinesis does not always occur; coenocytic (a type of multinucleate condition) cells undergo mitosis without cytokinesis.
Phases of cell cycle and mitosis.
Interphase.
The mitotic phase is a relatively short period of the cell cycle. It alternates with the much longer "interphase", where the cell prepares itself for the process of cell division. Interphase is divided into three phases: G1 (first gap), S (synthesis), and G2 (second gap). During all three phases, the cell grows by producing proteins and cytoplasmic organelles. However, chromosomes are replicated only during the S phase. Thus, a cell grows (G1), continues to grow as it duplicates its chromosomes (S), grows more and prepares for mitosis (G2), and finally divides (M) before restarting the cycle. All these phases in the cell cycle are highly regulated by cyclins, cyclin-dependent kinases, and other cell cycle proteins. The phases follow one another in strict order and there are "checkpoints" that give the cell cues to proceed from one phase to another. Cells may also temporarily or permanently leave the cell cycle and enter G0 phase to stop dividing. This can occur when cells become overcrowded (density-dependent inhibition) or when they differentiate to carry out specific functions for the organism, as is the case for human heart muscle cells and neurons. Some G0 cells have the ability to re-enter the cell cycle.
Preprophase (plant cells).
In plant cells only, prophase is preceded by a pre-prophase stage. In highly vacuolated plant cells, the nucleus has to migrate into the center of the cell before mitosis can begin. This is achieved through the formation of a phragmosome, a transverse sheet of cytoplasm that bisects the cell along the future plane of cell division. In addition to phragmosome formation, preprophase is characterized by the formation of a ring of microtubules and actin filaments (called preprophase band) underneath the plasma membrane around the equatorial plane of the future mitotic spindle. This band marks the position where the cell will eventually divide. The cells of higher plants (such as the flowering plants) lack centrioles; instead, microtubules form a spindle on the surface of the nucleus and are then organized into a spindle by the chromosomes themselves, after the nuclear envelope breaks down. The preprophase band disappears during nuclear envelope breakdown and spindle formation in prometaphase.
Prophase.
During prophase, which occurs after G2 interphase, the cell prepares to divide by tightly condensing its chromosomes and initiating mitotic spindle formation. During interphase, the genetic material in the nucleus consists of loosely packed chromatin. At the onset of prophase, chromatin fibers condense into discrete chromosomes that are typically visible at high magnification through a light microscope. 
Gene transcription ceases during prophase and does not resume until late anaphase to early G1 phase. The nucleolus also disappears during early prophase. 
 Close to the nucleus of animal cells are structures called centrosomes, consisting of a pair of centrioles surrounded by a loose collection of proteins. The centrosome is the coordinating center for the cell's microtubules. A cell inherits a single centrosome at cell division, which is duplicated by the cell before a new round of mitosis begins, giving a pair of centrosomes. The two centrosomes polymerize tubulin to help form a microtubule spindle apparatus. Motor proteins then push the centrosomes along these microtubules to opposite sides of the cell. Although centrosomes help organize microtubule assembly, they are not essential for the formation of the spindle apparatus, since they are absent from plants, and are not absolutely required for animal cell mitosis.
Prometaphase.
At the beginning of prometaphase in animal cells, phosphorylation of nuclear lamins causes the nuclear envelope to disintegrate into small membrane vesicles. As this happens, microtubules invade the nuclear space. This is called "open mitosis", and it occurs in some multicellular organisms. Fungi and some protists, such as algae or trichomonads, undergo a variation called "closed mitosis" where the spindle forms inside the nucleus, or the microtubules penetrate the intact nuclear envelope.
In late prometaphase, "kinetochore microtubules" begin to search for and attach to chromosomal kinetochores. A "kinetochore" is a proteinaceous microtubule-binding structure that forms on the chromosomal centromere during late prophase. A number of "polar microtubules" find and interact with corresponding polar microtubules from the opposite centrosome to form the mitotic spindle. Although the kinetochore structure and function are not fully understood, it is known that it contains some form of molecular motor. When a microtubule connects with the kinetochore, the motor activates, using energy from ATP to "crawl" up the tube toward the originating centrosome. This motor activity, coupled with polymerisation and depolymerisation of microtubules, provides the pulling force necessary to later separate the chromosome's two chromatids.
Metaphase.
After the microtubules have located and attached to the kinetochores in prometaphase, the two centrosomes begin pulling the chromosomes towards opposite ends of the cell. The resulting tension causes the chromosomes to align along the "metaphase plate" or "equatorial plane", an imaginary line that is centrally located between the two centrosomes (at approximately the midline of the cell). To ensure equitable distribution of chromosomes at the end of mitosis, the "metaphase checkpoint" guarantees that kinetochores are properly attached to the mitotic spindle and that the chromosomes are aligned along the metaphase plate. If the cell successfully passes through the metaphase checkpoint, it proceeds to anaphase.
Anaphase.
During "anaphase A", the cohesins that bind sister chromatids together are cleaved, forming two identical daughter chromosomes. Shortening of the kinetochore microtubules pulls the newly formed daughter chromosomes to opposite ends of the cell. During "anaphase B", polar microtubules push against each other, causing the cell to elongate. In most animal cells, anaphase A precedes anaphase B, but some vertebrate egg cells demonstrate the opposite order of events.
Telophase.
Telophase (from the Greek word "τελος" meaning "end") is a reversal of prophase and prometaphase events. At telophase, the polar microtubules continue to lengthen, elongating the cell even more. If the nuclear envelope has broken down, a new nuclear envelope forms using the membrane vesicles of the parent cell's old nuclear envelope. The new envelope forms around each set of separated daughter chromosomes (though the membrane does not enclose the centrosomes) and the nucleolus reappears. Both sets of chromosomes, now surrounded by new nuclear membrane, begin to "relax" or decondense. Mitosis is complete. Each daughter nuclear has an identical set of chromosomes. Cell division may or may not occur at this time depending on the organism.
Cytokinesis.
Cytokinesis is not a phase of mitosis but rather a separate process, necessary for completing cell division. In animal cells, a cleavage furrow (pinch) containing a contractile ring develops where the metaphase plate used to be, pinching off the separated nuclei. In both animal and plant cells, cell division is also driven by vesicles derived from the Golgi apparatus, which move along microtubules to the middle of the cell. In plants, this structure coalesces into a cell plate at the center of the phragmoplast and develops into a cell wall, separating the two nuclei. The phragmoplast is a microtubule structure typical for higher plants, whereas some green algae use a phycoplast microtubule array during cytokinesis. Each daughter cell has a complete copy of the genome of its parent cell. The end of cytokinesis marks the end of the M-phase.
There are many cells where mitosis and cytokinesis occur separately, forming single cells with multiple nuclei. The most notable occurrence of this is among the fungi, slime molds, and coenocytic algae, but the phenomenon is found in various other organisms. Even in animals, cytokinesis and mitosis may occur independently, for instance during certain stages of fruit fly embryonic development.
Significance.
Mitosis is important for the maintenance of the chromosomal set; each cell formed receives chromosomes that are alike in composition and equal in number to the chromosomes of the parent cell.
Mitosis occurs in the following circumstances:
Cell shape changes during mitosis.
 In animal tissue, most cells round up to a near-spherical shape during mitosis. In epithelia and epidermis, an efficient mitotic rounding process is correlated with proper spindle alignment and subsequent correct positioning of daughter cells. Moreover, researchers have recently found that if rounding is heavily suppressed it may result in spindle defects, primarily pole splitting and failure to efficiently capture chromosomes. Therefore rounding may play a protective role in ensuring accurate mitosis.
Rounding forces are driven by reorganization of F-actin and myosin (actomyosin) into a contractile homogeneous cortex that 1) rigidifies the cell periphery and 2) facilitates generation of intracellular hydrostatic pressure (up to 10 fold higher than interphase). The generation of pressure is particularly critical under confinement, such as would be important in a tissue scenario, where outward forces must be produced to round up against surrounding cells and/or the extracellular matrix. Generation of pressure is dependent on formin-mediated F-actin nucleation and Rho kinase (ROCK)-mediated myosin II contraction, both of which are governed upstream by signaling pathways RhoA and ECT2 through the activity of Cdk1. Due to its importance in mitosis, the molecular components and dynamics of the mitotic actomyosin cortex is an area of active research.
Errors and variations of mitosis.
Errors can occur during mitosis, especially during early embryonic development in humans. Mitotic errors can create aneuploid cells that have too few or too many of one or more chromosomes, a condition associated with cancer.
In "nondisjunction", sister chromatids fail to separate during anaphase. One daughter cell receives both sister chromatids from the nondisjoining chromosome and the other cell receives none. As a result, the former cell gets three copies of the chromosome, a condition known as "trisomy", and the latter will have only one copy, a condition known as "monosomy". On occasion, when cells experience nondisjunction, they fail to complete cytokinesis and retain both nuclei in one cell, resulting in binucleated cells.
"Anaphase lag" occurs when the movement of one chromatid is impeded during anaphase. This may be caused by a failure of the mitotic spindle to properly attach to the chromosome. The lagging chromatid is excluded from both nuclei and is lost. Therefore, one of the daughter cells will be monosomic for that chromosome.
"Endoreduplication" (or endoreplication) occurs when chromosomes duplicate but the cell does not subsequently divide. This results in polyploid cells or, if the chromosomes duplicates repeatedly, polytene chromosomes. Endoreduplication is found in many species and appears to be a normal part of development. Endomitosis is a variant of endoreduplication in which cells replicate their chromosomes during S phase and enter, but prematurely terminate, mitosis. Instead of being divided into two new daughter nuclei, the replicated chromosomes are retained within the original nucleus. The cells then re-enter G1 and S phase and replicate their chromosomes again. This may occur multiple times, increasing the chromosome number with each round of replication and endomitosis. Platelet-producing megakaryocytes go through endomitosis during cell differentiation.
Timeline in pictures.
Mitotic cells can be visualized microscopically by staining them with fluorescent antibodies and dyes.

</doc>
<doc id="20374" url="http://en.wikipedia.org/wiki?curid=20374" title="Metabolism">
Metabolism

Metabolism (from Greek: μεταβολή "metabolē", "change") is the set of life-sustaining chemical transformations within the cells of living organisms. These enzyme-catalyzed reactions allow organisms to grow and reproduce, maintain their structures, and respond to their environments. The word metabolism can also refer to all chemical reactions that occur in living organisms, including digestion and the transport of substances into and between different cells, in which case the set of reactions within the cells is called intermediary metabolism or intermediate metabolism.
Metabolism is usually divided into two categories. Catabolism, that breaks down organic matter and harvests energy by way of cellular respiration, and anabolism that uses energy to construct components of cells such as proteins and nucleic acids.
The chemical reactions of metabolism are organized into metabolic pathways, in which one chemical is transformed through a series of steps into another chemical, by a sequence of enzymes. Enzymes are crucial to metabolism because they allow organisms to drive desirable reactions that require energy that will not occur by themselves, by coupling them to spontaneous reactions that release energy. Enzymes act as catalysts that allow the reactions to proceed more rapidly. Enzymes also allow the regulation of metabolic pathways in response to changes in the cell's environment or to signals from other cells.
The metabolic system of a particular organism determines which substances it will find nutritious and which poisonous. For example, some prokaryotes use hydrogen sulfide as a nutrient, yet this gas is poisonous to animals. The speed of metabolism, the metabolic rate, influences how much food an organism will require, and also affects how it is able to obtain that food.
A striking feature of metabolism is the similarity of the basic metabolic pathways and components between even vastly different species. For example, the set of carboxylic acids that are best known as the intermediates in the citric acid cycle are present in all known organisms, being found in species as diverse as the unicellular bacterium "Escherichia coli" and huge multicellular organisms like elephants. These striking similarities in metabolic pathways are likely due to their early appearance in evolutionary history, and their retention because of their efficacy.
Key biochemicals.
Most of the structures that make up animals, plants and microbes are made from three basic classes of molecule: amino acids, carbohydrates and lipids (often called fats). As these molecules are vital for life, metabolic reactions either focus on making these molecules during the construction of cells and tissues, or by breaking them down and using them as a source of energy, by their digestion. These biochemicals can be joined together to make polymers such as DNA and proteins, essential macromolecules of life.
Amino acids and proteins.
Proteins are made of amino acids arranged in a linear chain joined together by peptide bonds. Many proteins are enzymes that catalyze the chemical reactions in metabolism. Other proteins have structural or mechanical functions, such as those that form the cytoskeleton, a system of scaffolding that maintains the cell shape. Proteins are also important in cell signaling, immune responses, cell adhesion, active transport across membranes, and the cell cycle. Amino acids also contribute to cellular energy metabolism by providing a carbon source for entry into the citric acid cycle (tricarboxylic acid cycle), especially when a primary source of energy, such as glucose, is scarce, or when cells undergo metabolic stress.
Lipids.
Lipids are the most diverse group of biochemicals. Their main structural uses are as part of biological membranes both internal and external, such as the cell membrane, or as a source of energy. Lipids are usually defined as hydrophobic or amphipathic biological molecules but will dissolve in organic solvents such as benzene or chloroform. The fats are a large group of compounds that contain fatty acids and glycerol; a glycerol molecule attached to three fatty acid esters is called a triacylglyceride. Several variations on this basic structure exist, including alternate backbones such as sphingosine in the sphingolipids, and hydrophilic groups such as phosphate as in phospholipids. Steroids such as cholesterol are another major class of lipids.
Carbohydrates.
Carbohydrates are aldehydes or ketones, with many hydroxyl groups attached, that can exist as straight chains or rings. Carbohydrates are the most abundant biological molecules, and fill numerous roles, such as the storage and transport of energy (starch, glycogen) and structural components (cellulose in plants, chitin in animals). The basic carbohydrate units are called monosaccharides and include galactose, fructose, and most importantly glucose. Monosaccharides can be linked together to form polysaccharides in almost limitless ways.
Nucleotides.
The two nucleic acids, DNA and RNA, are polymers of nucleotides. Each nucleotide is composed of a phosphate attached to a ribose or deoxyribose sugar group which is attached to a nitrogenous base. Nucleic acids are critical for the storage and use of genetic information, and its interpretation through the processes of transcription and protein biosynthesis. This information is protected by DNA repair mechanisms and propagated through DNA replication. Many viruses have an RNA genome, such as HIV, which uses reverse transcription to create a DNA template from its viral RNA genome. RNA in ribozymes such as spliceosomes and ribosomes is similar to enzymes as it can catalyze chemical reactions. Individual nucleosides are made by attaching a nucleobase to a ribose sugar. These bases are heterocyclic rings containing nitrogen, classified as purines or pyrimidines. Nucleotides also act as coenzymes in metabolic-group-transfer reactions.
Coenzymes.
Metabolism involves a vast array of chemical reactions, but most fall under a few basic types of reactions that involve the transfer of functional groups of atoms and their bonds within molecules. This common chemistry allows cells to use a small set of metabolic intermediates to carry chemical groups between different reactions. These group-transfer intermediates are called coenzymes. Each class of group-transfer reactions is carried out by a particular coenzyme, which is the substrate for a set of enzymes that produce it, and a set of enzymes that consume it. These coenzymes are therefore continuously made, consumed and then recycled.
One central coenzyme is adenosine triphosphate (ATP), the universal energy currency of cells. This nucleotide is used to transfer chemical energy between different chemical reactions. There is only a small amount of ATP in cells, but as it is continuously regenerated, the human body can use about its own weight in ATP per day. ATP acts as a bridge between catabolism and anabolism. Catabolism breaks down molecules and anabolism puts them together. Catabolic reactions generate ATP and anabolic reactions consume it. It also serves as a carrier of phosphate groups in phosphorylation reactions.
A vitamin is an organic compound needed in small quantities that cannot be made in cells. In human nutrition, most vitamins function as coenzymes after modification; for example, all water-soluble vitamins are phosphorylated or are coupled to nucleotides when they are used in cells. Nicotinamide adenine dinucleotide (NAD+), a derivative of vitamin B3 (niacin), is an important coenzyme that acts as a hydrogen acceptor. Hundreds of separate types of dehydrogenases remove electrons from their substrates and reduce NAD+ into NADH. This reduced form of the coenzyme is then a substrate for any of the reductases in the cell that need to reduce their substrates. Nicotinamide adenine dinucleotide exists in two related forms in the cell, NADH and NADPH. The NAD+/NADH form is more important in catabolic reactions, while NADP+/NADPH is used in anabolic reactions.
Minerals and cofactors.
Inorganic elements play critical roles in metabolism; some are abundant (e.g. sodium and potassium) while others function at minute concentrations. About 99% of a mammal's mass is made up of the elements carbon, nitrogen, calcium, sodium, chlorine, potassium, hydrogen, phosphorus, oxygen and sulfur. Organic compounds (proteins, lipids and carbohydrates) contain the majority of the carbon and nitrogen; most of the oxygen and hydrogen is present as water.
The abundant inorganic elements act as ionic electrolytes. The most important ions are sodium, potassium, calcium, magnesium, chloride, phosphate and the organic ion bicarbonate. The maintenance of precise ion gradients across cell membranes maintains osmotic pressure and pH. Ions are also critical for nerve and muscle function, as action potentials in these tissues are produced by the exchange of electrolytes between the extracellular fluid and the cell's fluid, the cytosol. Electrolytes enter and leave cells through proteins in the cell membrane called ion channels. For example, muscle contraction depends upon the movement of calcium, sodium and potassium through ion channels in the cell membrane and T-tubules.
Transition metals are usually present as trace elements in organisms, with zinc and iron being most abundant of those. These metals are used in some proteins as cofactors and are essential for the activity of enzymes such as catalase and oxygen-carrier proteins such as hemoglobin. Metal cofactors are bound tightly to specific sites in proteins; although enzyme cofactors can be modified during catalysis, they always return to their original state by the end of the reaction catalyzed. Metal micronutrients are taken up into organisms by specific transporters and bind to storage proteins such as ferritin or metallothionein when not in use.
Catabolism.
Catabolism is the set of metabolic processes that break down large molecules. These include breaking down and oxidizing food molecules. The purpose of the catabolic reactions is to provide the energy and components needed by anabolic reactions. The exact nature of these catabolic reactions differ from organism to organism and organisms can be classified based on their sources of energy and carbon (their primary nutritional groups), as shown in the table below. Organic molecules are used as a source of energy by organotrophs, while lithotrophs use inorganic substrates and phototrophs capture sunlight as chemical energy. However, all these different forms of metabolism depend on redox reactions that involve the transfer of electrons from reduced donor molecules such as organic molecules, water, ammonia, hydrogen sulfide or ferrous ions to acceptor molecules such as oxygen, nitrate or sulfate. In animals these reactions involve complex organic molecules that are broken down to simpler molecules, such as carbon dioxide and water. In photosynthetic organisms such as plants and cyanobacteria, these electron-transfer reactions do not release energy, but are used as a way of storing energy absorbed from sunlight.
The most common set of catabolic reactions in animals can be separated into three main stages. In the first, large organic molecules such as proteins, polysaccharides or lipids are digested into their smaller components outside cells. Next, these smaller molecules are taken up by cells and converted to yet smaller molecules, usually acetyl coenzyme A (acetyl-CoA), which releases some energy. Finally, the acetyl group on the CoA is oxidised to water and carbon dioxide in the citric acid cycle and electron transport chain, releasing the energy that is stored by reducing the coenzyme nicotinamide adenine dinucleotide (NAD+) into NADH.
Digestion.
Macromolecules such as starch, cellulose or proteins cannot be rapidly taken up by cells and must be broken into their smaller units before they can be used in cell metabolism. Several common classes of enzymes digest these polymers. These digestive enzymes include proteases that digest proteins into amino acids, as well as glycoside hydrolases that digest polysaccharides into simple sugars known as monosaccharides.
Microbes simply secrete digestive enzymes into their surroundings, while animals only secrete these enzymes from specialized cells in their guts. The amino acids or sugars released by these extracellular enzymes are then pumped into cells by active transport proteins.
Energy from organic compounds.
Carbohydrate catabolism is the breakdown of carbohydrates into smaller units. Carbohydrates are usually taken into cells once they have been digested into monosaccharides. Once inside, the major route of breakdown is glycolysis, where sugars such as glucose and fructose are converted into pyruvate and some ATP is generated. Pyruvate is an intermediate in several metabolic pathways, but the majority is converted to acetyl-CoA and fed into the citric acid cycle. Although some more ATP is generated in the citric acid cycle, the most important product is NADH, which is made from NAD+ as the acetyl-CoA is oxidized. This oxidation releases carbon dioxide as a waste product. In anaerobic conditions, glycolysis produces lactate, through the enzyme lactate dehydrogenase re-oxidizing NADH to NAD+ for re-use in glycolysis. An alternative route for glucose breakdown is the pentose phosphate pathway, which reduces the coenzyme NADPH and produces pentose sugars such as ribose, the sugar component of nucleic acids.
Fats are catabolised by hydrolysis to free fatty acids and glycerol. The glycerol enters glycolysis and the fatty acids are broken down by beta oxidation to release acetyl-CoA, which then is fed into the citric acid cycle. Fatty acids release more energy upon oxidation than carbohydrates because carbohydrates contain more oxygen in their structures. Steroids are also broken down by some bacteria in a process similar to beta oxidation, and this breakdown process involves the release of significant amounts of acetyl-CoA, propionyl-CoA, and pyruvate, which can all be used by the cell for energy. "M. tuberculosis" can also grow on the lipid cholesterol as a sole source of carbon, and genes involved in the cholesterol use pathway(s) have been validated as important during various stages of the infection lifecycle of "M. tuberculosis".
Amino acids are either used to synthesize proteins and other biomolecules, or oxidized to urea and carbon dioxide as a source of energy. The oxidation pathway starts with the removal of the amino group by a transaminase. The amino group is fed into the urea cycle, leaving a deaminated carbon skeleton in the form of a keto acid. Several of these keto acids are intermediates in the citric acid cycle, for example the deamination of glutamate forms α-ketoglutarate. The glucogenic amino acids can also be converted into glucose, through gluconeogenesis (discussed below).
Energy transformations.
Oxidative phosphorylation.
In oxidative phosphorylation, the electrons removed from organic molecules in areas such as the protagon acid cycle are transferred to oxygen and the energy released is used to make ATP. This is done in eukaryotes by a series of proteins in the membranes of mitochondria called the electron transport chain. In prokaryotes, these proteins are found in the cell's inner membrane. These proteins use the energy released from passing electrons from reduced molecules like NADH onto oxygen to pump protons across a membrane.
Pumping protons out of the mitochondria creates a proton concentration difference across the membrane and generates an electrochemical gradient. This force drives protons back into the mitochondrion through the base of an enzyme called ATP synthase. The flow of protons makes the stalk subunit rotate, causing the active site of the synthase domain to change shape and phosphorylate adenosine diphosphate – turning it into ATP.
Energy from inorganic compounds.
Chemolithotrophy is a type of metabolism found in prokaryotes where energy is obtained from the oxidation of inorganic compounds. These organisms can use hydrogen, reduced sulfur compounds (such as sulfide, hydrogen sulfide and thiosulfate), ferrous iron (FeII) or ammonia as sources of reducing power and they gain energy from the oxidation of these compounds with electron acceptors such as oxygen or nitrite. These microbial processes are important in global biogeochemical cycles such as acetogenesis, nitrification and denitrification and are critical for soil fertility.
Energy from light.
The energy in sunlight is captured by plants, cyanobacteria, purple bacteria, green sulfur bacteria and some protists. This process is often coupled to the conversion of carbon dioxide into organic compounds, as part of photosynthesis, which is discussed below. The energy capture and carbon fixation systems can however operate separately in prokaryotes, as purple bacteria and green sulfur bacteria can use sunlight as a source of energy, while switching between carbon fixation and the fermentation of organic compounds.
In many organisms the capture of solar energy is similar in principle to oxidative phosphorylation, as it involves the storage of energy as a proton concentration gradient. This proton motive force then drives ATP synthesis. The electrons needed to drive this electron transport chain come from light-gathering proteins called photosynthetic reaction centres or rhodopsins. Reaction centers are classed into two types depending on the type of photosynthetic pigment present, with most photosynthetic bacteria only having one type, while plants and cyanobacteria have two.
In plants, algae, and cyanobacteria, photosystem II uses light energy to remove electrons from water, releasing oxygen as a waste product. The electrons then flow to the cytochrome b6f complex, which uses their energy to pump protons across the thylakoid membrane in the chloroplast. These protons move back through the membrane as they drive the ATP synthase, as before. The electrons then flow through photosystem I and can then either be used to reduce the coenzyme NADP+, for use in the Calvin cycle, which is discussed below, or recycled for further ATP generation.
Anabolism.
Anabolism is the set of constructive metabolic processes where the energy released by catabolism is used to synthesize complex molecules. In general, the complex molecules that make up cellular structures are constructed step-by-step from small and simple precursors. Anabolism involves three basic stages. First, the production of precursors such as amino acids, monosaccharides, isoprenoids and nucleotides, secondly, their activation into reactive forms using energy from ATP, and thirdly, the assembly of these precursors into complex molecules such as proteins, polysaccharides, lipids and nucleic acids.
Organisms differ in how many of the molecules in their cells they can construct for themselves. Autotrophs such as plants can construct the complex organic molecules in cells such as polysaccharides and proteins from simple molecules like carbon dioxide and water. Heterotrophs, on the other hand, require a source of more complex substances, such as monosaccharides and amino acids, to produce these complex molecules. Organisms can be further classified by ultimate source of their energy: photoautotrophs and photoheterotrophs obtain energy from light, whereas chemoautotrophs and chemoheterotrophs obtain energy from inorganic oxidation reactions.
Carbon fixation.
Photosynthesis is the synthesis of carbohydrates from sunlight and carbon dioxide (CO2). In plants, cyanobacteria and algae, oxygenic photosynthesis splits water, with oxygen produced as a waste product. This process uses the ATP and NADPH produced by the photosynthetic reaction centres, as described above, to convert CO2 into glycerate 3-phosphate, which can then be converted into glucose. This carbon-fixation reaction is carried out by the enzyme RuBisCO as part of the Calvin – Benson cycle. Three types of photosynthesis occur in plants, C3 carbon fixation, C4 carbon fixation and CAM photosynthesis. These differ by the route that carbon dioxide takes to the Calvin cycle, with C3 plants fixing CO2 directly, while C4 and CAM photosynthesis incorporate the CO2 into other compounds first, as adaptations to deal with intense sunlight and dry conditions.
In photosynthetic prokaryotes the mechanisms of carbon fixation are more diverse. Here, carbon dioxide can be fixed by the Calvin – Benson cycle, a reversed citric acid cycle, or the carboxylation of acetyl-CoA. Prokaryotic chemoautotrophs also fix CO2 through the Calvin – Benson cycle, but use energy from inorganic compounds to drive the reaction.
Carbohydrates and glycans.
In carbohydrate anabolism, simple organic acids can be converted into monosaccharides such as glucose and then used to assemble polysaccharides such as starch. The generation of glucose from compounds like pyruvate, lactate, glycerol, glycerate 3-phosphate and amino acids is called gluconeogenesis. Gluconeogenesis converts pyruvate to glucose-6-phosphate through a series of intermediates, many of which are shared with glycolysis. However, this pathway is not simply glycolysis run in reverse, as several steps are catalyzed by non-glycolytic enzymes. This is important as it allows the formation and breakdown of glucose to be regulated separately, and prevents both pathways from running simultaneously in a futile cycle.
Although fat is a common way of storing energy, in vertebrates such as humans the fatty acids in these stores cannot be converted to glucose through gluconeogenesis as these organisms cannot convert acetyl-CoA into pyruvate; plants do, but animals do not, have the necessary enzymatic machinery. As a result, after long-term starvation, vertebrates need to produce ketone bodies from fatty acids to replace glucose in tissues such as the brain that cannot metabolize fatty acids. In other organisms such as plants and bacteria, this metabolic problem is solved using the glyoxylate cycle, which bypasses the decarboxylation step in the citric acid cycle and allows the transformation of acetyl-CoA to oxaloacetate, where it can be used for the production of glucose.
Polysaccharides and glycans are made by the sequential addition of monosaccharides by glycosyltransferase from a reactive sugar-phosphate donor such as uridine diphosphate glucose (UDP-glucose) to an acceptor hydroxyl group on the growing polysaccharide. As any of the hydroxyl groups on the ring of the substrate can be acceptors, the polysaccharides produced can have straight or branched structures. The polysaccharides produced can have structural or metabolic functions themselves, or be transferred to lipids and proteins by enzymes called oligosaccharyltransferases.
Fatty acids, isoprenoids and steroids.
Fatty acids are made by fatty acid synthases that polymerize and then reduce acetyl-CoA units. The acyl chains in the fatty acids are extended by a cycle of reactions that add the acyl group, reduce it to an alcohol, dehydrate it to an alkene group and then reduce it again to an alkane group. The enzymes of fatty acid biosynthesis are divided into two groups: in animals and fungi all these fatty acid synthase reactions are carried out by a single multifunctional type I protein, while in plant plastids and bacteria separate type II enzymes perform each step in the pathway.
Terpenes and isoprenoids are a large class of lipids that include the carotenoids and form the largest class of plant natural products. These compounds are made by the assembly and modification of isoprene units donated from the reactive precursors isopentenyl pyrophosphate and dimethylallyl pyrophosphate. These precursors can be made in different ways. In animals and archaea, the mevalonate pathway produces these compounds from acetyl-CoA, while in plants and bacteria the non-mevalonate pathway uses pyruvate and glyceraldehyde 3-phosphate as substrates. One important reaction that uses these activated isoprene donors is steroid biosynthesis. Here, the isoprene units are joined together to make squalene and then folded up and formed into a set of rings to make lanosterol. Lanosterol can then be converted into other steroids such as cholesterol and ergosterol.
Proteins.
Organisms vary in their ability to synthesize the 20 common amino acids. Most bacteria and plants can synthesize all twenty, but mammals can only synthesize eleven nonessential amino acids, so nine essential amino acids must be obtained from food. Some simple parasites, such as the bacteria "Mycoplasma pneumoniae", lack all amino acid synthesis and take their amino acids directly from their hosts. All amino acids are synthesized from intermediates in glycolysis, the citric acid cycle, or the pentose phosphate pathway. Nitrogen is provided by glutamate and glutamine. Amino acid synthesis depends on the formation of the appropriate alpha-keto acid, which is then transaminated to form an amino acid.
Amino acids are made into proteins by being joined together in a chain of peptide bonds. Each different protein has a unique sequence of amino acid residues: this is its primary structure. Just as the letters of the alphabet can be combined to form an almost endless variety of words, amino acids can be linked in varying sequences to form a huge variety of proteins. Proteins are made from amino acids that have been activated by attachment to a transfer RNA molecule through an ester bond. This aminoacyl-tRNA precursor is produced in an ATP-dependent reaction carried out by an aminoacyl tRNA synthetase. This aminoacyl-tRNA is then a substrate for the ribosome, which joins the amino acid onto the elongating protein chain, using the sequence information in a messenger RNA.
Nucleotide synthesis and salvage.
Nucleotides are made from amino acids, carbon dioxide and formic acid in pathways that require large amounts of metabolic energy. Consequently, most organisms have efficient systems to salvage preformed nucleotides. Purines are synthesized as nucleosides (bases attached to ribose). Both adenine and guanine are made from the precursor nucleoside inosine monophosphate, which is synthesized using atoms from the amino acids glycine, glutamine, and aspartic acid, as well as formate transferred from the coenzyme tetrahydrofolate. Pyrimidines, on the other hand, are synthesized from the base orotate, which is formed from glutamine and aspartate.
Xenobiotics and redox metabolism.
All organisms are constantly exposed to compounds that they cannot use as foods and would be harmful if they accumulated in cells, as they have no metabolic function. These potentially damaging compounds are called xenobiotics. Xenobiotics such as synthetic drugs, natural poisons and antibiotics are detoxified by a set of xenobiotic-metabolizing enzymes. In humans, these include cytochrome P450 oxidases, UDP-glucuronosyltransferases, and glutathione "S"-transferases. This system of enzymes acts in three stages to firstly oxidize the xenobiotic (phase I) and then conjugate water-soluble groups onto the molecule (phase II). The modified water-soluble xenobiotic can then be pumped out of cells and in multicellular organisms may be further metabolized before being excreted (phase III). In ecology, these reactions are particularly important in microbial biodegradation of pollutants and the bioremediation of contaminated land and oil spills. Many of these microbial reactions are shared with multicellular organisms, but due to the incredible diversity of types of microbes these organisms are able to deal with a far wider range of xenobiotics than multicellular organisms, and can degrade even persistent organic pollutants such as organochloride compounds.
A related problem for aerobic organisms is oxidative stress. Here, processes including oxidative phosphorylation and the formation of disulfide bonds during protein folding produce reactive oxygen species such as hydrogen peroxide. These damaging oxidants are removed by antioxidant metabolites such as glutathione and enzymes such as catalases and peroxidases.
Thermodynamics of living organisms.
Living organisms must obey the laws of thermodynamics, which describe the transfer of heat and work. The second law of thermodynamics states that in any closed system, the amount of entropy (disorder) cannot decrease. Although living organisms' amazing complexity appears to contradict this law, life is possible as all organisms are open systems that exchange matter and energy with their surroundings. Thus living systems are not in equilibrium, but instead are dissipative systems that maintain their state of high complexity by causing a larger increase in the entropy of their environments. The metabolism of a cell achieves this by coupling the spontaneous processes of catabolism to the non-spontaneous processes of anabolism. In thermodynamic terms, metabolism maintains order by creating disorder.
Regulation and control.
As the environments of most organisms are constantly changing, the reactions of metabolism must be finely regulated to maintain a constant set of conditions within cells, a condition called homeostasis. Metabolic regulation also allows organisms to respond to signals and interact actively with their environments. Two closely linked concepts are important for understanding how metabolic pathways are controlled. Firstly, the "regulation" of an enzyme in a pathway is how its activity is increased and decreased in response to signals. Secondly, the "control" exerted by this enzyme is the effect that these changes in its activity have on the overall rate of the pathway (the flux through the pathway). For example, an enzyme may show large changes in activity ("i.e." it is highly regulated) but if these changes have little effect on the flux of a metabolic pathway, then this enzyme is not involved in the control of the pathway.
There are multiple levels of metabolic regulation. In intrinsic regulation, the metabolic pathway self-regulates to respond to changes in the levels of substrates or products; for example, a decrease in the amount of product can increase the flux through the pathway to compensate. This type of regulation often involves allosteric regulation of the activities of multiple enzymes in the pathway. Extrinsic control involves a cell in a multicellular organism changing its metabolism in response to signals from other cells. These signals are usually in the form of soluble messengers such as hormones and growth factors and are detected by specific receptors on the cell surface. These signals are then transmitted inside the cell by second messenger systems that often involved the phosphorylation of proteins.
A very well understood example of extrinsic control is the regulation of glucose metabolism by the hormone insulin. Insulin is produced in response to rises in blood glucose levels. Binding of the hormone to insulin receptors on cells then activates a cascade of protein kinases that cause the cells to take up glucose and convert it into storage molecules such as fatty acids and glycogen. The metabolism of glycogen is controlled by activity of phosphorylase, the enzyme that breaks down glycogen, and glycogen synthase, the enzyme that makes it. These enzymes are regulated in a reciprocal fashion, with phosphorylation inhibiting glycogen synthase, but activating phosphorylase. Insulin causes glycogen synthesis by activating protein phosphatases and producing a decrease in the phosphorylation of these enzymes.
Evolution.
The central pathways of metabolism described above, such as glycolysis and the citric acid cycle, are present in all three domains of living things and were present in the last universal ancestor. This universal ancestral cell was prokaryotic and probably a methanogen that had extensive amino acid, nucleotide, carbohydrate and lipid metabolism. The retention of these ancient pathways during later evolution may be the result of these reactions having been an optimal solution to their particular metabolic problems, with pathways such as glycolysis and the citric acid cycle producing their end products highly efficiently and in a minimal number of steps. Mutation changes that affect non-coding DNA segments may merely affect the metabolic efficiency of the individual for whom the mutation occurs.
The first pathways of enzyme-based metabolism may have been parts of purine nucleotide metabolism, while previous metabolic pathways were a part of the ancient RNA world.
Many models have been proposed to describe the mechanisms by which novel metabolic pathways evolve. These include the sequential addition of novel enzymes to a short ancestral pathway, the duplication and then divergence of entire pathways as well as the recruitment of pre-existing enzymes and their assembly into a novel reaction pathway. The relative importance of these mechanisms is unclear, but genomic studies have shown that enzymes in a pathway are likely to have a shared ancestry, suggesting that many pathways have evolved in a step-by-step fashion with novel functions created from pre-existing steps in the pathway. An alternative model comes from studies that trace the evolution of proteins' structures in metabolic networks, this has suggested that enzymes are pervasively recruited, borrowing enzymes to perform similar functions in different metabolic pathways (evident in the MANET database) These recruitment processes result in an evolutionary enzymatic mosaic. A third possibility is that some parts of metabolism might exist as "modules" that can be reused in different pathways and perform similar functions on different molecules.
As well as the evolution of new metabolic pathways, evolution can also cause the loss of metabolic functions. For example, in some parasites metabolic processes that are not essential for survival are lost and preformed amino acids, nucleotides and carbohydrates may instead be scavenged from the host. Similar reduced metabolic capabilities are seen in endosymbiotic organisms.
Investigation and manipulation.
Classically, metabolism is studied by a reductionist approach that focuses on a single metabolic pathway. Particularly valuable is the use of radioactive tracers at the whole-organism, tissue and cellular levels, which define the paths from precursors to final products by identifying radioactively labelled intermediates and products. The enzymes that catalyze these chemical reactions can then be purified and their kinetics and responses to inhibitors investigated. A parallel approach is to identify the small molecules in a cell or tissue; the complete set of these molecules is called the metabolome. Overall, these studies give a good view of the structure and function of simple metabolic pathways, but are inadequate when applied to more complex systems such as the metabolism of a complete cell.
An idea of the complexity of the metabolic networks in cells that contain thousands of different enzymes is given by the figure showing the interactions between just 43 proteins and 40 metabolites to the right: the sequences of genomes provide lists containing anything up to 45,000 genes. However, it is now possible to use this genomic data to reconstruct complete networks of biochemical reactions and produce more holistic mathematical models that may explain and predict their behavior. These models are especially powerful when used to integrate the pathway and metabolite data obtained through classical methods with data on gene expression from proteomic and DNA microarray studies. Using these techniques, a model of human metabolism has now been produced, which will guide future drug discovery and biochemical research. These models are now used in network analysis, to classify human diseases into groups that share common proteins or metabolites.
Bacterial metabolic networks are a striking example of bow-tie organization, an architecture able to input a wide range of nutrients and produce a large variety of products and complex macromolecules using a relatively few intermediate common currencies.
A major technological application of this information is metabolic engineering. Here, organisms such as yeast, plants or bacteria are genetically modified to make them more useful in biotechnology and aid the production of drugs such as antibiotics or industrial chemicals such as 1,3-propanediol and shikimic acid. These genetic modifications usually aim to reduce the amount of energy used to produce the product, increase yields and reduce the production of wastes.
History.
The term "metabolism" is derived from the Greek Μεταβολισμός – "Metabolismos" for "change", or "overthrow". The first documented references of metabolism were made by Ibn al-Nafis in his 1260 AD work titled Al-Risalah al-Kamiliyyah fil Siera al-Nabawiyyah (The Treatise of Kamil on the Prophet's Biography) which included the following phrase "Both the body and its parts are in a continuous state of dissolution and nourishment, so they are inevitably undergoing permanent change.". 
The history of the scientific study of metabolism spans several centuries and has moved from examining whole animals in early studies, to examining individual metabolic reactions in modern biochemistry. The first controlled experiments in human metabolism were published by Santorio Santorio in 1614 in his book "Ars de statica medicina". He described how he weighed himself before and after eating, sleep, working, sex, fasting, drinking, and excreting. He found that most of the food he took in was lost through what he called "insensible perspiration".
In these early studies, the mechanisms of these metabolic processes had not been identified and a vital force was thought to animate living tissue. In the 19th century, when studying the fermentation of sugar to alcohol by yeast, Louis Pasteur concluded that fermentation was catalyzed by substances within the yeast cells he called "ferments". He wrote that "alcoholic fermentation is an act correlated with the life and organization of the yeast cells, not with the death or putrefaction of the cells." This discovery, along with the publication by Friedrich Wöhler in 1828 of a paper on the chemical synthesis of urea, and is notable for being the first organic compound prepared from wholly inorganic precursors. This proved that the organic compounds and chemical reactions found in cells were no different in principle than any other part of chemistry.
It was the discovery of enzymes at the beginning of the 20th century by Eduard Buchner that separated the study of the chemical reactions of metabolism from the biological study of cells, and marked the beginnings of biochemistry. The mass of biochemical knowledge grew rapidly throughout the early 20th century. One of the most prolific of these modern biochemists was Hans Krebs who made huge contributions to the study of metabolism. He discovered the urea cycle and later, working with Hans Kornberg, the citric acid cycle and the glyoxylate cycle. Modern biochemical research has been greatly aided by the development of new techniques such as chromatography, X-ray diffraction, NMR spectroscopy, radioisotopic labelling, electron microscopy and molecular dynamics simulations. These techniques have allowed the discovery and detailed analysis of the many molecules and metabolic pathways in cells.
Further reading.
Introductory
 and #redirect 
, "The Chemistry of Life." (Penguin Press Science, 1999), ISBN 0-14-027273-9
 and #redirect 
, "Into the Cool: Energy Flow, Thermodynamics, and Life." (University Of Chicago Press, 2005), ISBN 0-226-73936-8
, "Oxygen: The Molecule that Made the World." (Oxford University Press, USA, 2004), ISBN 0-19-860783-0
Advanced
 and #redirect 
, "Fundamentals of Enzymology: Cell and Molecular Biology of Catalytic Proteins." (Oxford University Press, 1999), ISBN 0-19-850229-X
 #redirect 
 and #redirect 
, "Biochemistry." (W. H. Freeman and Company, 2002), ISBN 0-7167-4955-6
 and #redirect 
, "Lehninger Principles of Biochemistry." (Palgrave Macmillan, 2004), ISBN 0-7167-4339-6
 #redirect 
 #redirect 
 and #redirect 
, "Brock's Biology of Microorganisms." (Benjamin Cummings, 2002), ISBN 0-13-066271-2
 and #redirect 
, "The Biological Chemistry of the Elements: The Inorganic Chemistry of Life." (Clarendon Press, 1991), ISBN 0-19-855598-9
 and #redirect 
, "Bioenergetics." (Academic Press Inc., 2002), ISBN 0-12-518121-3
External links.
External links

</doc>
<doc id="20375" url="http://en.wikipedia.org/wiki?curid=20375" title="Medieval Inquisition">
Medieval Inquisition

The Medieval Inquisition was a series of Inquisitions (Catholic Church bodies charged with suppressing heresy) from around 1184, including the Episcopal Inquisition (1184-1230s) and later the Papal Inquisition (1230s). The Medieval Inquisition was established in response to movements considered apostate or heretical to Christianity, in particular Catharism and Waldensians in Southern France and Northern Italy. These were the first inquisition movements of many that would follow.
The Cathars were first noted in the 1140s in Southern France, and the Waldensians around 1170 in Northern Italy. Before this point, individual heretics such as Peter of Bruis had often challenged the Church. However, the Cathars were the first mass organization in the second millennium that posed a serious threat to the authority of the Church. This article covers only these early inquisitions, not the Roman Inquisition of the 16th century onwards, or the somewhat different phenomenon of the Spanish Inquisition of the late 15th century, which was under the control of the Spanish monarchy using local clergy. The Portuguese Inquisition of the 16th century and various colonial branches followed the same pattern.
History.
French historian Jean-Baptiste Guiraud (1866–1953) defined Medieval Inquisition as "... a system of repressive means, some of temporal and some others of spiritual kind, concurrently issued by ecclesiastical and civil authorities in order to protect religious orthodoxy and social order, both threatened by theological and social doctrines of heresy".
Bishop of Lincoln, Robert Grosseteste, defined heresy as "an opinion chosen by human perception, created by human reason, founded on the Scriptures, contrary to the teachings of the Church, publicly avowed, and obstinately defended." The fault was in the obstinate adherence rather than theological error, which could be corrected; and by referencing scripture Grosseteste excludes Jews, Muslims, and other non-Christians from the definition of heretic.
There were many different types of inquisitions depending on the location and methods; historians have generally classified them into the "episcopal inquisition" and the "papal inquisition". All major medieval inquisitions were decentralized, and each tribunal worked independently. Authority rested with local officials based on guidelines from the Holy See, but there was no central top-down authority running the inquisitions, as would be the case in post-medieval inquisitions. 
Early Medieval courts generally followed a process called "accusatio", largely based on Germanic practices. In this procedure, an individual would make an accusation against someone to the court. However, if the suspect was judged innocent, the accusers faced legal penalties for bringing false charges. This provided a disincentive to make any accusation unless the accusers were sure it would stand. By the twelfth and early thirteenth centuries, there was a shift away from the accusatorial model toward the legal procedure used in the Roman Empire. Instead of an individual making accusations based on first-hand knowledge, judges now took on the prosecutorial role based on information collected. Under inquisitorial procedures, guilt or innocence was proved by the inquiry ("inquisitio") of the judge into the details of a case.
The first medieval inquisition, the episcopal inquisition, was established in the year 1184 by a papal bull of Pope Lucius III entitled "Ad abolendam", "For the purpose of doing away with." It was a response to the growing Catharist movement in southern France. It was called "episcopal" because it was administered by local bishops, which in Latin is "episcopus", and obliged bishops to visit their diocese twice a year in search of heretics.
In 1231 Pope Gregory IX appointed a number of Papal Inquisitors (Inquisitores haereticae pravitatis), mostly Dominicans and Franciscans, for the various regions of Europe. As mendicants, they were accustomed to travel. Unlike the haphazard episcopal methods, the papal inquisition was thorough and systematic, keeping detailed records. Some of the few documents from the Middle Ages involving first-person speech by medieval peasants come from papal inquisition records. This tribunal or court functioned in France, Italy and parts of Germany and had virtually ceased operation by the early fourteenth century.
Another reason for Pope Gregory IX's creation of the Inquisition was to bring order and legality to the process of dealing with heresy, since there had been tendencies by mobs of townspeople to burn alleged heretics without much of a trial. Pope Gregory's original intent for the Inquisition was a court of exception to inquire into and glean the beliefs of those differing from Catholic teaching, and to instruct them in the orthodox doctrine. It was hoped that heretics would see the falsity of their opinion and would return to the Roman Catholic Church. If they persisted in their heresy, however, Pope Gregory, finding it necessary to protect the Catholic community from infection, would have suspects handed over to civil authorities, since public heresy was a crime under civil law as well as Church law. The secular authorities would apply their own brands of punishment for civil disobedience which, at the time, included burning at the stake. Over centuries the tribunals took different forms, investigating and stamping out various forms of heresy, including witchcraft.
Throughout the Inquisition's history, it was rivaled by local ecclesiastical and secular jurisdictions. No matter how determined, no pope succeeded in establishing complete control over the prosecution of heresy. Medieval kings, princes, bishops, and civil authorities all had a role in prosecuting heresy, except where they individually opposed the practice. The practice reached its apex in the second half of the 13th century. During this period, the tribunals were almost entirely free from any authority, including that of the pope. Therefore, it was almost impossible to eradicate abuse.
In southern Europe, Church-run courts existed in the kingdom of Aragon during the medieval period, but not elsewhere in the Iberian peninsula or some other kingdoms, including England In Scandinavian kingdoms it had hardly any impact.
According to historian Thomas Madden, "[t]he Inquisition was not born out of desire to crush diversity or oppress people; it was rather an attempt to stop unjust executions. Yes, you read that correctly. Heresy was a crime against the "state". Roman law in the Code of Justinian made heresy a capital offense" [emphasis in original]. In the early Middle Ages, people accused of heresy were judged by the local lord, many of whom lacked theological training. Madden claims that "The simple fact is that the medieval Inquisition "saved" uncounted thousands of innocent (and even not-so-innocent) people who would otherwise have been roasted by secular lords or mob rule" [emphasis in original].
Madden argues that while medieval secular leaders were trying to safeguard their kingdoms, the Church was trying to save souls. The Inquisition provided a means for heretics to escape death and return to the community.
Inquisitions against non-Catholic movements.
The spread of other movements from the 12th century, can be seen at least in part as a reaction to the increasing moral corruption of the clergy, which included illegal marriages and the possession of extreme wealth.
In the Middle Ages, the Inquisition's main focus was to eradicate these new sects. Thus its range of action was predominantly in Italy and France, where the Cathars and the Waldensians, the two main heretic movements of the period were.
The Cathars were mostly in the South of France, in cities like Toulouse. They appear to have been originally founded by some soldiers from the Second Crusade, who, on their way back, were converted by a Bulgarian sect, the Bogomils.
The Cathars' main heresy was their belief in dualism: the evil God created the materialistic world and the good God created the spiritual world. Therefore, Cathars preached poverty, chastity, modesty and all those values which in their view helped people to detach themselves from materialism. The Cathars presented a problem to feudal government by their attitude towards oaths, which they declared under no circumstances allowable. Therefore, considering the religious homogeneity of that age, heresy was an attack against social and political order, besides orthodoxy.
The Waldensians were mostly in Germany and North Italy. The Waldensians were a group of orthodox laymen concerned about the increasing wealth of the Church. As time passed, however, they found themselves stepping beyond the bounds of orthodoxy as defined by the hierarchy of the Western Church. In contrast with the Cathars and in line with the Church, they believed in only one God, but they did not recognize a special class of priesthood, believing in the priesthood of all believers. They also objected to the veneration of saints and martyrs, which were part of the Church's orthodoxy.
The complaints of the two main preaching orders of the period, the Dominicans and the Franciscans, against the moral corruption of the Church, to some extent echoed those of the heretical movements, but they were doctrinally conventional, and were enlisted by Pope Innocent III in the fight against heresy. As a result, many Franciscans and Dominicans became inquisitors. For example, Robert le Bougre, the "Hammer of Heretics" (Malleus Haereticorum), was a Dominican friar who became an inquisitor known for his cruelty and violence. Another example was the case of the province of Venice, which was handed to the Franciscan inquisitors, who quickly became notorious for their frauds against the Church, by enriching themselves with confiscated property from the heretics and the selling of absolutions. Because of their corruption, they were eventually forced by the Pope to suspend their activities in 1302.
At the beginning of the fourteenth century, two other movements attracted the attention of the Inquisition, the Knights Templar and the Beguines.
It is not clear if the process against the Templars was initiated by the Inquisition on the basis of suspected heresy or if the Inquisition itself was exploited by the king of France, Philip the Fair, who wanted the knights' wealth. In the search for Templars, two inquisitors were also sent to the British Isles. This is the only instance of inquisitorial action in the British Isles and not a successful one, mainly because the inquisitors could not instigate false confessions through torture, as its use was forbidden by common law.
The Beguines were mainly a women's movement, recognized by the Church since their foundation in the thirteenth century as mystics. However, with the Council of Vienne in the fourteenth century, they were proclaimed heretics and persecuted, with large numbers being burned at the stake in Narbonne, Toulouse and other French cities. They were also attacked in Germany, the first attempt of the Inquisition to operate in the area.
Another aspect of the medieval Inquisition is that little attention was paid to sorcery. In fact several Popes were suspected of having a strong interest or practicing alchemy and it was only with Pope John XXII, who was himself suspected of being a magician, that sorcery became another form of heresy and thus liable to prosecution by the Inquisition.
Joan of Arc.
In the spring of 1429 during the Hundred Years' War, in obedience to what she said was the command of God, Joan inspired the Dauphin's armies in a series of stunning military victories which lifted the siege of Orleans and destroyed a large percentage of the remaining English forces at the battle of Patay. A series of military setbacks eventually led to her capture in the Spring of 1430 by the Burgundians, who were allied with the English. They delivered her to them for 10,000 livres. In December of that same year she was transferred to Rouen, the military headquarters and administrative capital in France of King Henry VI of England, and placed on trial for heresy before a Church court headed by Bishop Pierre Cauchon, a supporter of the English.
The trial was politically motivated. Cauchon, although a native of France, had served as an English official since 1418, and he was therefore hostile to a woman who had worked for the opposing side. The same was true of the other tribunal members. Ascribing a diabolic origin to her victories would be an effective way to ruin her reputation and bolster the morale of English troops. Thus the decision to involve the Inquisition, which did not initiate the trial and in fact showed a reluctance throughout its duration. Seventy charges were brought against her, including accusations of heresy and dressing as a male (i.e., wearing soldiers' clothing and armor). Eyewitnesses later said that Joan had told them she was wearing this clothing and keeping it "firmly laced and tied together" because the tunic could be tied to the long boots to keep her guards from pulling her clothing off during their occasional attempts to rape her. Joan was first condemned to life imprisonment and the deputy-inquisitor, Jean Le Maitre (whom the eyewitness said only attended because of threats from the English) obtained from her assurances of relinquishing her male clothes. However, after four days, during which she was said to have been subjected to attempted rape by English soldiers, she put her soldier's clothing back on because (according to the eyewitnesses) she needed protection against rape. Cauchon declared her a relapsed heretic, and she was burned at the stake two days later on 30 May 1431.
In 1455, a petition by Joan of Arc's mother Isabelle led to a re-trial designed to investigate the dubious circumstances which led to Joan's execution. The Inquisitor-General of France was put in charge of the new trial, which opened in Notre Dame de Paris on 7 November 1455. After analyzing all the proceedings, including Joan's answers to the allegations and the testimony of 115 witnesses who were called to testify during the appellate process, the inquisitor overturned her condemnation on 7 July 1456. Joan of Arc was eventually canonized in 1920.
Historian Edward Peters identifies a number a illegalities in Joan's first trial in which she had been convicted. 
Inquisition procedure.
The papal inquisition developed a number of procedures to discover and prosecute heretics.
Investigation.
When a papal inquisition arrived at a town it had a set of procedures and rules to identify likely heretics. Legally, there had to be at least two witnesses, although conscientious judges rarely contented themselves with that number.
First, the townspeople would be gathered in a public place. Although attendance was voluntary, those who failed to show would automatically be suspect, so most would come. The inquisitors would provide an opportunity for anyone to step forward and denounce themselves in exchange for easy punishment. As part of this bargain they would need to inform on other heretics.
Trial.
The inquisitorial trial generally favored the prosecution (the Church). Confessing 'in full' was the best hope of receiving a lighter punishment - but with little hope of escaping at least some punishment. And a 'full' confession was one which implicated others, including other family members. It was acceptable to take testimony from criminals, persons of bad reputation, excommunicated people, and convicted heretics. The inquisitor could keep a defendant in prison for years before the trial to obtain new information, and could return them to prison if he felt that the witness had not fully confessed.
Despite the unfairness of the procedures, the inquisitors did provide some rights to the defendant. At the beginning of the trial, defendants were invited to name those who had "mortal hatred" against them. If the accusers were among those named, the defendant was set free and the charges dismissed; the accusers would face life imprisonment. This option was meant to keep the inquisition from becoming involved in local grudges. Early legal consultations on conducting inquisition stress that it is better that the guilty go free than that the innocent be punished. Gregory IX urged Conrad of Marburg: "ut puniatur sic temeritas perversorum quod innocentiae puritas non laedatur" — i.e., "not to punish the wicked so as to hurt the innocent".
Torture.
Like the inquisitorial process itself, torture was an ancient Roman legal practice revived in Europe in the thirteenth century.
On May 15, 1252, Pope Innocent IV issued a papal bull entitled "Ad extirpanda", which authorized the use of torture by inquisitors. Torture was undoubtedly used in the trial of the Templars, but is in fact not much found in heresy trials until the later fourteenth century. Torture methods that resulted in bloodshed, births, mutilation or death were forbidden. Also, torture could be performed only once. However, it was common practice to consider a second torture session to be a "continuation" of the first.
In preparation for the Jubilee in 2000, the Vatican opened the archives of the Holy Office (the modern successor to the Inquisition) to a team of 30 scholars from around the world. According to the governor general of the Order of the Holy Sepulchre, recent studies "seem to indicate" that "torture and the death penalty were not applied with the pitiless rigor" often ascribed to the Inquisition. Other methods such as threats and imprisonment seem to have proven more effective.
Punishment.
The most extreme penalty available in antiheretical proceedings was reserved for relapsed or stubborn heretics. Of 5,400 people interrogated in Toulouse between 1245-1246, 184 received penitential yellow crosses, 23 were imprisoned for life, and none were sent to the stake.
The unrepentant and apostates could be "relaxed" to secular authority, however, opening the convicted to the possibility of various corporal punishments, up to and including being burned at the stake. Execution was neither performed by the Church, nor was it a sentence available to the officials involved in the inquisition, who, as clerics, were forbidden to kill. The accused also faced the possibility that his or her property might be confiscated. In some cases, accusers may have been motivated by a desire to take the property of the accused, though this is a difficult assertion to prove in the majority of areas where the inquisition was active, as the inquisition had several layers of oversight built into its framework in a specific attempt to limit prosecutorial misconduct.
The inquisitors generally preferred not to hand over heretics to the secular arm for execution if they could persuade the heretic to repent: "Ecclesia non novit sanguinem". For example, Bernard Gui, a famous inquisitor working in the area of Carcassonne (in modern France), executed 42 people out of over 900 guilty verdicts in fifteen years of office. Execution was to admit defeat, that the Church was unable to save a soul from heresy, which was the goal of the inquisition. 
Legacy.
The inquisitions in combination with the brutal Albigensian Crusade were very successful in eliminating the Cathar movement. When they started, the other sects were quite strong and growing, but by the 14th century the Waldensians had been driven underground and the Cathars had been slaughtered en masse or forced to recant. Some residents of the Pays Cathare identify themselves as Cathars even today. They claim to be descended from the Cathars of the Middle Ages. However, the delivering of the consolamentum, on which historical Catharism was based, required a linear succession by a bon homme in good standing. It is believed that one of the last known bons hommes, Guillaume Belibaste, was burned in 1321.
Further reading.
</dl>

</doc>
<doc id="20377" url="http://en.wikipedia.org/wiki?curid=20377" title="Microorganism">
Microorganism

A microorganism (from the Greek: μικρός, "mikros", "small" and ὀργανισμός, "organismós", "organism") is a microscopic living organism, which may be single celled or multicellular. The study of microorganisms is called microbiology, a subject that began with the discovery of microorganisms in 1674 by Antonie van Leeuwenhoek, using a microscope of his own design.
Microorganisms are very diverse and include all the bacteria and archaea and almost all the protozoa. They also include some fungi, algae, and certain animals, such as rotifers. Many macroscopic animals and plants have microscopic juvenile stages. Some microbiologists also classify viruses (and viroids) as microorganisms, but others consider these as nonliving.
Microorganisms live in every part of the biosphere, including soil, hot springs, "seven miles deep" in the ocean, "40 miles high" in the atmosphere and inside rocks far down within the Earth's crust (see also endolith). Microorganisms, under certain test conditions, have been observed to thrive in the vacuum of outer space. The total amount of soil and subsurface bacterial carbon is estimated as 5 x 1017 g, or the "weight of the United Kingdom". The mass of prokaryote microorganisms — which includes bacteria and archaea, but not the nucleated eukaryote microorganisms — may be as much as 0.8 trillion tons of carbon (of the total biosphere mass of 4 trillion tons). On 17 March 2013, researchers reported data that suggested microbial life forms thrive in the Mariana Trench. the deepest spot in the Earth's oceans. Other researchers reported related studies that microorganisms thrive inside rocks up to 580 m below the sea floor under 2590 m of ocean off the coast of the northwestern United States as well as 2400 m beneath the seabed off Japan. On 20 August 2014, scientists confirmed the existence of microorganisms living 800 m below the ice of Antarctica. According to one researcher,"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are."
Microorganisms are crucial to nutrient recycling in ecosystems as they act as decomposers. As some microorganisms can fix nitrogen, they are a vital part of the nitrogen cycle, and recent studies indicate that airborne microorganisms may play a role in precipitation and weather. Microorganisms are also exploited in biotechnology, both in traditional food and beverage preparation, and in modern technologies based on genetic engineering. A small proportion of microorganisms are pathogenic and cause disease and even death in plants and animals. Microorganisms are often referred to as microbes, but this is usually used in reference to pathogens.
Evolution.
Single-celled microorganisms were the first forms of life to develop on Earth, approximately 3–4 billion years ago. Further evolution was slow, and for about 3 billion years in the Precambrian eon, all organisms were microscopic. So, for most of the history of life on Earth, the only forms of life were microorganisms. Bacteria, algae and fungi have been identified in amber that is 220 million years old, which shows that the morphology of microorganisms has changed little since the Triassic period. The newly discovered biological role played by nickel, however — especially that engendered by volcanic eruptions from the Siberian Traps (site of the modern city of Norilsk) — is thought to have accelerated the evolution of methanogens towards the end of the Permian–Triassic extinction event.
Microorganisms tend to have a relatively fast rate of evolution. Most microorganisms can reproduce rapidly, and bacteria are also able to freely exchange genes through conjugation, transformation and transduction, even between widely divergent species. This horizontal gene transfer, coupled with a high mutation rate and many other means of genetic variation, allows microorganisms to swiftly evolve (via natural selection) to survive in new environments and respond to environmental stresses. This rapid evolution is important in medicine, as it has led to the recent development of "super-bugs", pathogenic bacteria that are resistant to modern antibiotics.
Pre-microbiology.
The possibility that microorganisms exist was discussed for many centuries before their discovery in the 17th century. The existence of unseen microbiological life was postulated by Jainism, which is based on Mahavira's teachings as early as 6th century BCE. Paul Dundas notes that Mahavira asserted the existence of unseen microbiological creatures living in earth, water, air and fire. The Jain scriptures also describe nigodas, which are sub-microscopic creatures living in large clusters and having a very short life, which are said to pervade every part of the universe, even the tissues of plants and animals. The earliest known idea to indicate the possibility of diseases spreading by yet unseen organisms was that of the Roman scholar Marcus Terentius Varro in a 1st-century BC book titled "On Agriculture" in which he warns against locating a homestead near swamps:
… and because there are bred certain minute creatures that cannot be seen by the eyes, which float in the air and enter the body through the mouth and nose and they cause serious diseases.
In "The Canon of Medicine" (1020), Abū Alī ibn Sīnā (Avicenna) hypothesized that tuberculosis and other diseases might be contagious
In 1546, Girolamo Fracastoro proposed that epidemic diseases were caused by transferable seedlike entities that could transmit infection by direct or indirect contact, or even without contact over long distances.
All these early claims about the existence of microorganisms were speculative and while grounded on indirect observations, they were not based on direct observation of microorganisms or systematized empirical investigation, e.g. experimentation. Microorganisms were neither proven, observed, nor accurately described until the 17th century. The reason for this was that all these early studies lacked the microscope.
History of microorganisms' discovery.
Antonie Van Leeuwenhoek (1632–1723) was one of the first people to observe microorganisms, using microscopes of his own design. Robert Hooke, a contemporary of Leeuwenhoek, also used microscopes to observe microbial life; his 1665 book "Micrographia" describes these observations and coined the term "cell".
Before Leeuwenhoek's discovery of microorganisms in 1675, it had been a mystery why grapes could be turned into wine, milk into cheese, or why food would spoil. Leeuwenhoek did not make the connection between these processes and microorganisms, but using a microscope, he did establish that there were forms of life that were not visible to the naked eye. Leeuwenhoek's discovery, along with subsequent observations by Spallanzani and Pasteur, ended the long-held belief that life spontaneously appeared from non-living substances during the process of spoilage.
Lazzaro Spallanzani (1729–1799) found that boiling broth would sterilise it, killing any microorganisms in it. He also found that new microorganisms could only settle in a broth if the broth was exposed to air.
Louis Pasteur (1822–1895) expanded upon Spallanzani's findings by exposing boiled broths to the air, in vessels that contained a filter to prevent all particles from passing through to the growth medium, and also in vessels with no filter at all, with air being admitted via a curved tube that would not allow dust particles to come in contact with the broth. By boiling the broth beforehand, Pasteur ensured that no microorganisms survived within the broths at the beginning of his experiment. Nothing grew in the broths in the course of Pasteur's experiment. This meant that the living organisms that grew in such broths came from outside, as spores on dust, rather than spontaneously generated within the broth. Thus, Pasteur dealt the death blow to the theory of spontaneous generation and supported germ theory.
In 1876, Robert Koch (1843–1910) established that microorganisms can cause disease. He found that the blood of cattle which were infected with anthrax always had large numbers of "Bacillus anthracis". Koch found that he could transmit anthrax from one animal to another by taking a small sample of blood from the infected animal and injecting it into a healthy one, and this caused the healthy animal to become sick. He also found that he could grow the bacteria in a nutrient broth, then inject it into a healthy animal, and cause illness. Based on these experiments, he devised criteria for establishing a causal link between a microorganism and a disease and these are now known as Koch's postulates. Although these postulates cannot be applied in all cases, they do retain historical importance to the development of scientific thought and are still being used today.
On 8 November 2013, scientists reported the discovery of what may be the earliest signs of life on Earth—the oldest complete fossils of a microbial mat (associated with sandstone in Western Australia) estimated to be 3.48 billion years old.
Classification and structure.
Microorganisms can be found almost anywhere in the taxonomic organization of life on the planet. Bacteria and archaea are almost always microscopic, while a number of eukaryotes are also microscopic, including most protists, some fungi, as well as some animals and plants. Viruses are generally regarded as not living and therefore not considered as microorganisms, although the field of microbiology also encompasses the study of viruses.
Prokaryotes.
Prokaryotes are organisms that lack a cell nucleus and the other membrane bound organelles. They are almost always unicellular, although some species such as myxobacteria can aggregate into complex structures as part of their life cycle.
Consisting of two domains, bacteria and archaea, the prokaryotes are the most diverse and abundant group of organisms on Earth and inhabit practically all environments where the temperature is below +140 °C. They are found in water, soil, air, animals' gastrointestinal tracts, hot springs and even deep beneath the Earth's crust in rocks. Practically all surfaces that have not been specially sterilized are covered by prokaryotes. The number of prokaryotes on Earth is estimated to be around five million trillion trillion, or 5 × 1030, accounting for at least half the biomass on Earth.
Bacteria.
Almost all bacteria are invisible to the naked eye, with a few extremely rare exceptions, such as "Thiomargarita namibiensis". They lack a nucleus and other membrane-bound organelles, and can function and reproduce as individual cells, but often aggregate in multicellular colonies. Their genome is usually a single loop of DNA, although they can also harbor small pieces of DNA called plasmids. These plasmids can be transferred between cells through bacterial conjugation. Bacteria are surrounded by a cell wall, which provides strength and rigidity to their cells. They reproduce by binary fission or sometimes by budding, but do not undergo meiotic sexual reproduction. However, many bacterial species can transfer DNA between individual cells by a process referred to as natural transformation. In nature, the development of competence for transformation is usually associated with stressful environmental conditions, and seems to be an adaptation for facilitating repair of DNA damage in recipient cells. Some species form extraordinarily resilient spores, but for bacteria this is a mechanism for survival, not reproduction. Under optimal conditions bacteria can grow extremely rapidly and can double as quickly as every 20 minutes.
Archaea.
Archaea are also single-celled organisms that lack nuclei. In the past, the differences between bacteria and archaea were not recognised and archaea were classified with bacteria as part of the kingdom Monera. However, in 1990 the microbiologist Carl Woese proposed the three-domain system that divided living things into bacteria, archaea and eukaryotes. Archaea differ from bacteria in both their genetics and biochemistry. For example, while bacterial cell membranes are made from phosphoglycerides with ester bonds, archaean membranes are made of ether lipids.
Archaea were originally described in extreme environments, such as hot springs, but have since been found in all types of habitats. Only now are scientists beginning to realize how common archaea are in the environment, with crenarchaeota being the most common form of life in the ocean, dominating ecosystems below 150 m in depth. These organisms are also common in soil and play a vital role in ammonia oxidation.
Eukaryotes.
Most living things that are visible to the naked eye in their adult form are eukaryotes, including humans. However, a large number of eukaryotes are also microorganisms. Unlike bacteria and archaea, eukaryotes contain organelles such as the cell nucleus, the Golgi apparatus and mitochondria in their cells. The nucleus is an organelle that houses the DNA that makes up a cell's genome. DNA itself is arranged in complex chromosomes.
Mitochondria are organelles vital in metabolism as they are the site of the citric acid cycle and oxidative phosphorylation. They evolved from symbiotic bacteria and retain a remnant genome. Like bacteria, plant cells have cell walls, and contain organelles such as chloroplasts in addition to the organelles in other eukaryotes. Chloroplasts produce energy from light by photosynthesis, and were also originally symbiotic bacteria.
Unicellular eukaryotes consist of a single cell throughout their life cycle. This qualification is significant since most multicellular eukaryotes consist of a single cell called a zygote only at the beginning of their life cycles. Microbial eukaryotes can be either haploid or diploid, and some organisms have multiple cell nuclei.
Unicellular eukaryotes usually reproduce asexually by mitosis under favorable conditions. However, under stressful conditions such as nutrient limitations and other conditions associated with DNA damage, they tend to reproduce sexually by meiosis and syngamy.
Protists.
Of eukaryotic groups, the protists are most commonly unicellular and microscopic. This is a highly diverse group of organisms that are not easy to classify. Several algae species are multicellular protists, and slime molds have unique life cycles that involve switching between unicellular, colonial, and multicellular forms. The number of species of protists is unknown since we may have identified only a small portion. Studies from 2001-2004 have shown that a high degree of protist diversity exists in oceans, deep sea-vents, river sediment and an acidic river which suggests that a large number of eukaryotic microbial communities have yet to be discovered.
Animals.
Some micro animals are multicellular but at least one animal group, Myxozoa, is unicellular in its adult form. Microscopic arthropods include dust mites and spider mites. Microscopic crustaceans include copepods, some cladocera and water bears. Many nematodes are also too small to be seen with the naked eye. A common group of microscopic animals are the rotifers, which are filter feeders that are usually found in fresh water. Some micro-animals reproduce both sexually and asexually and may reach new habitats by producing eggs which can survive harsh environments that would kill the adult animal. However, some simple animals, such as rotifers, tardigrades and nematodes, can dry out completely and remain dormant for long periods of time.
Fungi.
The fungi have several unicellular species, such as baker's yeast ("Saccharomyces cerevisiae") and fission yeast ("Schizosaccharomyces pombe"). Some fungi, such as the pathogenic yeast "Candida albicans", can undergo phenotypic switching and grow as single cells in some environments, and filamentous hyphae in others. Fungi reproduce both asexually, by budding or binary fission, as well by producing spores, which are called conidia when produced asexually, or basidiospores when produced sexually.
Plants.
The green algae are a large group of photosynthetic eukaryotes that include many microscopic organisms. Although some green algae are classified as protists, others such as charophyta are classified with embryophyte plants, which are the most familiar group of land plants. Algae can grow as single cells, or in long chains of cells. The green algae include unicellular and colonial flagellates, usually but not always with two flagella per cell, as well as various colonial, coccoid, and filamentous forms. In the Charales, which are the algae most closely related to higher plants, cells differentiate into several distinct tissues within the organism. There are about 6000 species of green algae.
Habitats and ecology.
Microorganisms are found in almost every habitat present in nature. Even in hostile environments such as the poles, deserts, geysers, rocks, and the deep sea. Some types of microorganisms have adapted to the extreme conditions and sustained colonies; these organisms are known as extremophiles. Extremophiles have been isolated from rocks as much as 7 kilometres below the Earth's surface, and it has been suggested that the amount of living organisms below the Earth's surface may be comparable with the amount of life on or above the surface. Extremophiles have been known to survive for a prolonged time in a vacuum, and can be highly resistant to radiation, which may even allow them to survive in space. Many types of microorganisms have intimate symbiotic relationships with other larger organisms; some of which are mutually beneficial (mutualism), while others can be damaging to the host organism (parasitism). If microorganisms can cause disease in a host they are known as pathogens and then they are sometimes referred to as microbes.
Extremophiles.
Extremophiles are microorganisms that have adapted so that they can survive and even thrive in conditions that are normally fatal to most life-forms. For example, some species have been found in the following extreme environments:
Extremophiles are significant in different ways. They extend terrestrial life into much of the Earth's hydrosphere, crust and atmosphere, their specific evolutionary adaptation mechanisms to their extreme environment can be exploited in bio-technology, and their very existence under such extreme conditions increases the potential for extraterrestrial life.
Soil microorganisms.
The nitrogen cycle in soils depends on the fixation of atmospheric nitrogen. One way this can occur is in the nodules in the roots of legumes that contain symbiotic bacteria of the genera "Rhizobium", "Mesorhizobium", "Sinorhizobium", "Bradyrhizobium", and "Azorhizobium".
Symbiotic microorganisms.
Symbiotic microorganisms such as fungi and algae form an association in lichen. Certain fungi form mycorrhizal symbioses with trees that increase the supply of nutrients to the tree.
Importance.
Microorganisms are vital to humans and the environment, as they participate in the carbon and nitrogen cycles, as well as fulfilling other vital roles in virtually all ecosystems, such as recycling other organisms' dead remains and waste products through decomposition. Microorganisms also have an important place in most higher-order multicellular organisms as symbionts. Many blame the failure of Biosphere 2 on an improper balance of microorganisms.
Use in digestion.
Some forms of bacteria that live in animals' stomachs help in their digestion. For example, cows have a variety of different microorganisms in their stomachs that are essential in their digestion of grass and hay.
The gastrointestinal tract contains an immensely complex ecology of microorganisms. A typical person harbors more than 500 distinct species of bacteria, representing dozens of different lifestyles and capabilities. The composition and distribution of this menagerie varies with age, state of health and diet.
The number and type of bacteria in the gastrointestinal tract vary dramatically by region. In healthy individuals the stomach and proximal small intestine contain few microorganisms, largely a result of the bacteriocidal activity of gastric acid; those that are present are aerobes and facultative anaerobes. One interesting testimony to the ability of gastric acid to suppress bacterial populations is seen in patients with achlorhydria, a genetic condition which prevents secretion of gastric acid. Such patients, which are otherwise healthy, may have as many as 10,000 to 100,000,000 microorganisms per ml of stomach contents.
In sharp contrast to the stomach and small intestine, the contents of the colon literally teem with bacteria, predominantly strict anaerobes (bacteria that survive only in environments virtually devoid of oxygen). Between these two extremes is a transitional zone, usually in the ileum, where moderate numbers of both aerobic and anaerobic bacteria are found.
The gastrointestinal tract is sterile at birth, but colonization typically begins within a few hours of birth, starting in the small intestine and progressing caudally over a period of several days. In most circumstances, a "mature" microbial flora is established by 3 to 4 weeks of age.
It is also clear that microbial populations exert a profound effect on structure and function of the digestive tract. For example:
The morphology of the intestine of germ-free animals differs considerably from normal animals - villi of the small intestine are remarkably regular, the rate of epithelial cell renew is reduced and, as one would expect, the number and size of Peyer's patches is reduced.
The cecum of germ-free rats is roughly 10 times the size of that in a conventional rat.
Bacteria in the intestinal lumen metabolize a variety of sterols and steroids. For example, bacteria convert the bile salt cholic acid to deoxycholic acid. Small intestinal bacteria also have an important role in sex steroid metabolism.
Finally, bacterial populations in the large intestine digest carbohydrates, proteins and lipids that escape digestion and absorption in small intestine. This fermentation, particularly of cellulose, is of critical importance to herbivores like cattle and horses which make a living by consuming plants. However, it seems that even species like humans and rodents derive significant benefit from the nutrients liberated by intestinal microorganisms.
Use in food production.
Microorganisms are used in brewing, wine making, baking, pickling and other food-making processes.
They are also used to control the fermentation process in the production of cultured dairy products such as yogurt and cheese. The cultures also provide flavour and aroma, and inhibit undesirable organisms.
Use in water treatment.
The majority of all oxidative sewage treatment processes rely on a large range of microorganisms to oxidise organic constituents which are not amenable to sedimentation or flotation. Anaerobic microorganisms are also used to reduce sludge solids producing methane gas (amongst other gases) and a sterile mineralised residue. In potable water treatment, one method, the slow sand filter, employs a complex gelatinous layer composed of a wide range of microorganisms to remove both dissolved and particulate material from raw water.
Use in energy.
Microorganisms are used in fermentation to produce ethanol, and in biogas reactors to produce methane. Scientists are researching the use of algae to produce liquid fuels, and bacteria to convert various forms of agricultural and urban waste into usable fuels.
Use in production of chemicals, enzymes etc..
Microorganisms are used for many commercial and industrial production of chemicals, enzymes and other bioactive molecules.<br>
Examples of organic acid produced include
Microorganisms are used for preparation of bioactive molecules and enzymes.
Use in science.
Microorganisms are essential tools in biotechnology, biochemistry, genetics, and molecular biology. The yeasts ("Saccharomyces cerevisiae") and fission yeast ("Schizosaccharomyces pombe") are important model organisms in science, since they are simple eukaryotes that can be grown rapidly in large numbers and are easily manipulated. They are particularly valuable in genetics, genomics and proteomics. Microorganisms can be harnessed for uses such as creating steroids and treating skin diseases. Scientists are also considering using microorganisms for living fuel cells, and as a solution for pollution.
Use in warfare.
In the Middle Ages, diseased corpses were thrown into castles during sieges using catapults or other siege engines. Individuals near the corpses were exposed to the pathogen and were likely to spread that pathogen to others.
Importance in human health.
Human digestion.
Microorganisms can form an endosymbiotic relationship with other, larger organisms. For example, the bacteria that live within the human digestive system contribute to gut immunity, synthesise vitamins such as folic acid and biotin, and ferment complex indigestible carbohydrates.
Diseases caused by microbes.
Microorganisms are the cause of many infectious diseases. The organisms involved include pathogenic bacteria, causing diseases such as plague, tuberculosis and anthrax; protozoa, causing diseases such as malaria, sleeping sickness, dysentery and toxoplasmosis; and also fungi causing diseases such as ringworm, candidiasis or histoplasmosis. However, other diseases such as influenza, yellow fever or AIDS are caused by pathogenic viruses, which are not usually classified as living organisms and are not, therefore, microorganisms by the strict definition. s of 2007[ [update]], no clear examples of archaean pathogens are known, although a relationship has been proposed between the presence of some archaean methanogens and human periodontal disease.
Importance in ecology.
Microorganisms play critical roles in Earth's biogeochemical cycles as they are responsible for decomposition and play vital parts in carbon and nitrogen fixation, as well as oxygen production.
Hygiene.
Hygiene is the avoidance of infection or food spoiling by eliminating microorganisms from the surroundings. As microorganisms, in particular bacteria, are found virtually everywhere, the levels of harmful microorganisms can be reduced to acceptable levels. However, in some cases, it is required that an object or substance be completely sterile, i.e. devoid of all living entities and viruses. A good example of this is a hypodermic needle.
In food preparation microorganisms are reduced by preservation methods (such as the addition of vinegar), clean utensils used in preparation, short storage periods, or by cool temperatures. If complete sterility is needed, the two most common methods are irradiation and the use of an autoclave, which resembles a pressure cooker.
There are several methods for investigating the level of hygiene in a sample of food, drinking water, equipment, etc. Water samples can be filtrated through an extremely fine filter. This filter is then placed in a nutrient medium. Microorganisms on the filter then grow to form a visible colony. Harmful microorganisms can be detected in food by placing a sample in a nutrient broth designed to enrich the organisms in question. Various methods, such as selective media or polymerase chain reaction, can then be used for detection. The hygiene of hard surfaces, such as cooking pots, can be tested by touching them with a solid piece of nutrient medium and then allowing the microorganisms to grow on it.
There are no conditions where all microorganisms would grow, and therefore often several methods are needed. For example, a food sample might be analyzed on three different nutrient mediums designed to indicate the presence of "total" bacteria (conditions where many, but not all, bacteria grow), molds (conditions where the growth of bacteria is prevented by, e.g., antibiotics) and coliform bacteria (these indicate a sewage contamination).

</doc>
<doc id="20378" url="http://en.wikipedia.org/wiki?curid=20378" title="Modulus">
Modulus

Modulus may refer to:
Moduli, the plural form may refer to:

</doc>
<doc id="20379" url="http://en.wikipedia.org/wiki?curid=20379" title="Micronation">
Micronation

A micronation, sometimes referred to as a model country or new country project, is an entity that claims to be an independent nation or state but is not officially recognized by world governments or major international organizations.
Micronations are distinguished from imaginary countries and from other kinds of social groups (such as eco-villages, campuses, tribes, clans, sects, and residential community associations) by expressing a formal and persistent, even if unrecognized, claim of sovereignty over some physical territory.
Several micronations have issued coins, flags, postage stamps, passports, medals, and other items, which are rarely accepted outside of their own community.
The earliest known micronations date from the beginning of the 19th century. The advent of the Internet provided the means for people to create many new micronations, whose members are scattered all over the world and interact mostly by electronic means, often calling their nations Nomadic Countries. The differences between such Internet micronations, other kinds of social networking groups, and role playing games are often difficult to define.
The term "micronation" to describe those entities dates at least to the 1970s. The term micropatrology is sometimes used to describe the study of both micronations and microstates by micronationalists, some of whom refer to sovereign nation-states as "macronations".
Etymology.
The term 'micronation' literally means "small nation". It is a neologism originating in the mid-1970s to describe the many thousands of small unrecognised state-like entities that have mostly arisen since that time. It is generally accepted that the term was invented by Robert Ben Madison.
The term has since also come to be used retrospectively to refer to earlier unrecognised entities, some of which date to as far back as the 19th century. Supporters of micronations ("micronationalists") use the term "macronation" for any UN-recognized sovereign nation-state.
Not all micronations are small; some can be rather large, like the Dominion of British West Florida or those with claims on Antarctica or other planets.
Definition.
Micronations generally have a number of common features, although these may vary widely. They may have a structure similar to established sovereign states, including territorial claims, government institutions, official symbols and citizens, albeit on a much smaller scale. Micronations are often quite small, in both their claimed territory and claimed populations — although there are some exceptions to this rule, with different micronations having different methods of citizenship. Micronations may also issue formal instruments such as postage stamps, coins, banknotes and passports, and bestow honours and titles of nobility.
The Montevideo Convention was one attempt to create a legal definition distinguishing between states and non-states. Some micronations meet this definition, while some do not, and others reject the Convention altogether.
The academic study of micronations, microstates, and alternative governments is known as micropatrology, and the hobby of establishing and operating micronations is known as micronationalism.
Categories.
In the present day, nine main types of micronations are prevalent:
Social, economic, or political simulations.
These micronations tend to have a reasonably serious intent, and often involve significant numbers of people interested in recreating the past or simulating political or social processes. Examples include:
Exercises in personal entertainment or self-aggrandisement.
With literally thousands in existence, micronations of the second type are by far the most common. This type can also be known as "political simulationism" or simply "simulationism". They generally exist "for fun," have relatively few participants, are ephemeral, today usually Internet-based, and many do not survive more than a few months—although there are notable exceptions. They are usually concerned solely with arrogating to their founders the outward symbols of statehood. The use of grand-sounding titles, awards, honours, and heraldic symbols derived from European feudal traditions, the conduct of "wars" (often known as recwars) and "diplomacy" with other micronations, and simulated continents or planets are common manifestations of their activities. Examples include:
Exercises in fantasy or creative fiction.
Micronations of the third type include stand-alone artistic projects, deliberate exercises in creative online fiction, social justification or gratification, and artistamp creations. Examples include:
Vehicles for agenda promotion.
These types of micronations are typically associated with a political or social reform agenda. Some are maintained as media and public relations exercises. Examples of this type include:
Entities created for allegedly fraudulent purposes.
A number of micronations have been established for fraudulent purposes, by seeking to link questionable or illegal financial actions with "seemingly" legitimate nations.
Historical anomalies, legal anomalies and aspirant states.
A small number of micronations are founded based on historical anomalies or on legal anomalies (deriving from disputed interpretations of law). These types of micronations are usually located on small (usually disputed) territorial enclaves, generate limited economic activity founded on tourism and philatelic and numismatic sales, and are tolerated or ignored by the nations from which they claim to have seceded. This category includes:
New-country projects.
New-country projects are attempts to found completely new nation-states. They typically involve plans to construct artificial islands (few of which are ever realised), and a large percentage have embraced or purported to embrace libertarian or democratic principles. Examples include:
Exercises in historical revisionism.
In Germany, numerous individuals and groups—collectively labeled "Kommissarische Reichsregierungen" (KRR)—assert that the German Empire continues to exist in its pre-World War II borders and that they are its government, or government-in-exile.
Alternative governments.
"Alternative Governments" are mostly defined as "a government that declares itself to be an authority over a set territory, but do not recognize themselves as the only or sole authority on said territory"..
Some alternative governments include:
History.
Early history and evolution.
The oldest extant micronation to arise in modern times is the Kingdom of Redonda, founded in 1865 in the Caribbean. It failed to establish itself as a real country, but has nonetheless managed to survive into the present day as a unique literary foundation with its own king and aristocracy—although it is not without its controversies: there are presently at least four competing claimants to the Redondan throne.
Martin Coles Harman, owner of the British island of Lundy in the early decades of the 20th century, declared himself King and issued private coinage and postage stamps for local use. Although the island was ruled as a virtual fiefdom, its owner never claimed to be independent of the United Kingdom, so Lundy can at best be described as a precursor to later territorial micronations. Another example is the Principality of Outer Baldonia, a 16 acre rocky island off the coast of Nova Scotia, founded by Russell Arundel, chairman of the Pepsi Cola Company (later: PepsiCo), in 1945 and comprising a population of 69 fishermen.
History during 1960 to 1980.
The 1960s and 1970s witnessed the foundation of a number of territorial micronations. The first of these, Sealand, was established in 1967 on an abandoned World War II gun platform in the North Sea just off the East Anglian coast of England, and has survived into the present day. Others were founded on libertarian principles and involved schemes to construct artificial islands, but only three are known to have had even limited success in realizing that goal.
The Republic of Rose Island was a 400 m2 platform built in 1968 in Italian national waters in the Adriatic Sea, 7 mi off the Italian town of Rimini. It is known to have issued stamps, and to have declared Esperanto to be its official language. Shortly after completion, however, it was seized and destroyed by the Italian Navy for failing to pay state taxes.
In the late 1960s, Leicester Hemingway, brother of author Ernest, was involved in another such project—a small timber platform in international waters off the west coast of Jamaica. This territory, consisting of an 8 ft by 30 ft barge, he called "New Atlantis". Hemingway was an honorary citizen and President; however, the structure was damaged by storms and finally pillaged by Mexican fishermen. In 1973, Hemingway was reported to have moved on from New Atlantis to promoting a 1000 sqyd platform near the Bahamas. The new country was called "Tierra del Mar" ("Land of the Sea"). (Ernest Hemingway's adopted hometown of Key West was later itself part of another micronation; see Conch Republic.)
The Republic of Minerva was set up in 1972 as a libertarian new-country project by Nevada businessman Michael Oliver. Oliver's group conducted dredging operations at the Minerva Reefs, a shoal located in the Pacific Ocean south of Fiji. They succeeded in creating a small artificial island, but their efforts at securing international recognition met with little success, and near-neighbour Tonga sent a military force to the area and annexed it.
On April 1, 1977, bibliophile Richard Booth declared the Welsh town of Hay-on-Wye an independent kingdom with himself as its monarch. The town subsequently developed a healthy tourism industry based on literary interests, and "King Richard" (whose sceptre is a recycled toilet plunger) continues to award Hay-on-Wye peerages and honours to anyone prepared to pay for them.
Japanese micronations in the 1980s.
In 1981, drawing on a news report about Leicester Hemingway's "New Atlantis", novelist Inoue Hisashi wrote a 700-page work of magic realism, "Kirikirijin", about a village that secedes from Japan and proclaims its bumpkinish, marginalized dialect its national language, and its subsequent war of independence. This single-handedly inspired a large number of Japanese villages, mostly in the northern regions, to "declare independence", generally as a move to raise awareness of their unique culture and crafts for urban Japanese who saw village life as backwards and uncultured. These micronations even held "international summits" from 1983 to 1985, and some of them formed confederations. Throughout the 1980s there was a "micronation boom" in Japan that brought many urban tourists to these wayward villages. But the harsh economic impact of the Japanese asset price bubble in 1991 ended the boom. Many of the villages were forced to merge with larger cities, and the micronations and confederations were generally dissolved.
Australian and New Zealand developments.
Micronational developments that occurred in New Zealand and Australia in the final three decades of the 20th century included:
Effects of the Internet.
Micronationalism shed much of its traditionally eccentric anti-establishment mantle and took on a distinctly hobbyist perspective in the mid-1990s, when the emerging popularity of the Internet made it possible to create and promote statelike entities in an entirely electronic medium with relative ease. An early example is the Kingdom of Talossa, a micronation created in 1979 by then 14-year-old Robert Ben Madison, which went online in November 1995, and was reported in the "New York Times" and other print media in 2000. As a result, the number of exclusively online, fantasy or simulation-based micronations expanded dramatically.
The activities of these types of micronations are almost exclusively limited to simulations of diplomatic activity (including the signing of "treaties" and participation in "supra-micronational" forums such as the League of Micronations) and contribution to wikis. With the introduction of the Internet, many articles on how to create micronations were made available on such wikis, which serve as a hub of online activity for micronations.
A number of traditional territorial micronations, including the Hutt River Province, Seborga, and Sealand, maintain websites that serve largely to promote their claims and sell merchandise.
Legitimacy.
In international law, the Montevideo Convention on the Right and Duties of States sets down the criteria for statehood in article 1: "The state as a person of international law should possess the following qualifications: (a) a permanent population; (b) a defined territory; (c) government; and (d) capacity to enter into relations with the other states."
The first sentence of article 3 of the Montevideo Convention explicitly states that "The political existence of the state is independent of recognition by the other states."
Under these guidelines, any entity which meets all of the criteria set forth in article 1 can be regarded as sovereign under international law, whether or not other states have recognized it.
The Sovereign Military Order of Malta, as an independent subject of international law does not meet all the criteria for recognition as a State (however it does not claim itself a State either), but is and has been recognized as a sovereign nation for centuries.
The doctrine of territorial integrity does not effectively prohibit unilateral secession from established states in international law, per the relevant section from the text of the Final Act of the Conference on Security and Cooperation in Europe, also known as the Helsinki Final Act, Helsinki Accords or Helsinki Declaration:
IV. Territorial integrity of States
The participating States will respect the territorial integrity of each of the participating States.
Accordingly, they will refrain from any action inconsistent with the purposes and principles of the Charter of the United Nations against the territorial integrity, political independence or the unity of any participating State, and in particular from any such action constituting a threat or use of force.
The participating States will likewise refrain from making each other's territory the object of military occupation or other direct or indirect measures of force in contravention of international law, or the object of acquisition by means of such measures or the threat of them. No such occupation or acquisition will be recognized as legal.
In effect, this states that "other" states (i.e., third parties), may not encourage secession in a state. This does not make any statement as regards persons within a state electing to secede of their own accord.
Academic, literary, and media attention.
There has been a small but growing amount of attention paid to the micronation phenomenon in recent years. Most interest in academic circles has been concerned with studying the apparently anomalous legal situations affecting such entities as Sealand and the Hutt River Province, in exploring how some micronations represent grassroots political ideas, and in the creation of role-playing entities for instructional purposes.
In 2000, Professor Fabrice O'Driscoll, of the Aix-Marseille University, published a book about micronations: "Ils ne siègent pas à l'ONU" ("They are not in the United Nations"), with more than 300 pages dedicated to the subject.
In May 2000, an article in the New York Times entitled "Utopian Rulers, and Spoofs, Stake Out Territory Online" brought the phenomenon to a wider audience. Similar articles were published by newspapers such as the Italian "La Repubblica", "O Estado de São Paulo" in Brazil, and Portugal's "Visão" at around the same time.
Several recent publications have dealt with the subject of particular historic micronations, including "Republic of Indian Stream" (University Press), by Dartmouth College geographer Daniel Doan, 'The Land that Never Was", about Gregor MacGregor and the Principality of Poyais, by David Sinclair (Review, 2003, ISBN 0-7553-1080-2) and "An Australian Monarch" about the Principality of Hutt River by William Pitt (CopyRight Publishing, ISBN 9-781876-344672).
In August 2003, a summit of micronations took place in Helsinki at Finlandia Hall, the site of the Conference for Security and Co-operation in Europe (CSCE). The summit was attended by delegations of the Principality of Sealand, the Kingdoms of Elgaland-Vargaland, NSK-State in Time, Ladonia, the Transnational republic|Transnational Republic, the State of Sabotage and by scholars from various academic institutions.
From 7 November through 17 December 2004, the Reg Vardy Gallery at the University of Sunderland (UK) hosted an exhibition on the subject of micronational group identity and symbolism. The exhibition focused on numismatic, philatelic and vexillological artifacts, as well as other symbols and instruments created and used by a number of micronations from the 1950s through to the present day. A summit of micronations conducted as part of this exhibition was attended by representatives of Sealand, Elgaland-Vargaland, New Utopia, Atlantium, Frestonia and Fusa. The exhibition was reprised at the Andrew Kreps Gallery in New York City from 24 June – 29 July of the following year and organized by R. Blackson and Peter Coffin. Peter Coffin organized a more extensive exhibition about micronations at Paris' Palais de Tokyo in early 2007 called ÉTATS (faites-le vous-même)/States (Do it yourself).
The Sunderland summit was later featured in the 5-part BBC light entertainment television series "How to Start Your Own Country" presented by Danny Wallace. The series told the story of Wallace's experience of founding a micronation, Lovely, located in his London flat. It screened in the UK in 2005.
Similar programs have also aired on television networks in other parts of Europe. In France, several Canal+ programs have centered on the satirical , while in Belgium a series by Rob Vanoudenhoven and broadcast on the Flemish commercial network VTM in April 2006 was reminiscent of Wallace's series, and centred around the producer's creation of . Among other things Vanoudenhoven minted his own coins denominated in "Robbies".
On September 9, 2006, The Guardian newspaper reported that the travel guide company Lonely Planet had published the world's first travel guide devoted to micronations, "".
The Democratic Empire of Sunda, which claims to be the Government of the Kingdom of Sunda (an ancient kingdom, in present-day Indonesia) in exile in Switzerland, made media headlines when two so-called princesses, Lamia Roro Wiranatadikusumah Siliwangi Al Misri, 21, and Fathia Reza Wiranatadikusumah Siliwangi Al Misiri, 23, were detained by Malaysian authorities at the border with Brunei, on 13 July 2007, and are charged for entering the country without a valid pass. The hearing continues.
In 2010, a conference of micronations was held on Dangar Island in Sydney, Australia. Micronations with representatives in attendance included the Empire of Atlantium, the Principality of Hutt River, the Principality of Wy and the Gay and Lesbian Kingdom of the Coral Sea Islands
In 2010, a documentary film by Jody Shapiro entitled "How to Start your Own Country" was screened as part of the Toronto International Film Festival. The documentary explored various micronations around the world, and included an analysis of the concept of statehood and citizenship. Erwin Strauss, author of the eponymous book, was interviewed as part of the film.
In 2012, a conference of micronations (Polination 2012) was held in London . Micronations with representatives in attendance included the Empire of Atlantium, the Republic of Molossia, the Grand Duchy of Flandrensis, Ladonia, Neue Slowenische Kunst and Austenasia.
The manga and anime series "", in which the main characters are the stereotyped personifications of the nations of the world, features several micronations as characters. As of 2011 micronations represented include Sealand, Seborga, Wy, Kugelmugel, Molossia, Hutt River, Ladonia, and the former micronation of Nikko Nikko.
The Australian television comedy series "Micro Nation" is set on the fictional island micronation of Pullamawang, which remained independent from Australia because they "forgot to mail in their paperwork" at the Federation of Australia in 1901.

</doc>
<doc id="20381" url="http://en.wikipedia.org/wiki?curid=20381" title="Mining">
Mining

Mining is the extraction of valuable minerals or other geological materials from the earth from an orebody, lode, vein, seam, or reef, which forms the mineralized package of economic interest to the miner.
Ores recovered by mining include metals, coal, oil shale, gemstones, limestone, dimension stone, rock salt, potash, gravel, and clay. Mining is required to obtain any material that cannot be grown through agricultural processes, or created artificially in a laboratory or factory. Mining in a wider sense includes extraction of any non-renewable resource such as petroleum, natural gas, or even water.
Mining of stone and metal has been done since pre-historic times. Modern mining processes involve prospecting for ore bodies, analysis of the profit potential of a proposed mine, extraction of the desired materials, and final reclamation of the land after the mine is closed.
The nature of mining processes creates a potential negative impact on the environment both during the mining operations and for years after the mine is closed. This impact has led most of the world's nations to adopt regulations designed to moderate the negative effects of mining operations. Safety has long been a concern as well, and modern practices have improved safety in mines significantly.
History.
Prehistoric mining.
Since the beginning of civilization, people have used stone, ceramics and, later, metals found close to the Earth's surface. These were used to make early tools and weapons; for example, high quality flint found in northern France and southern England was used to create flint tools. Flint mines have been found in chalk areas where seams of the stone were followed underground by shafts and galleries. The mines at Grimes Graves are especially famous, and like most other flint mines, are Neolithic in origin (ca 4000 BC-ca 3000 BC). Other hard rocks mined or collected for axes included the greenstone of the Langdale axe industry based in the English Lake District.
The oldest known mine on archaeological record is the "Lion Cave" in Swaziland, which radiocarbon dating shows to be about 43,000 years old. At this site Paleolithic humans mined hematite to make the red pigment ochre. Mines of a similar age in Hungary are believed to be sites where Neanderthals may have mined flint for weapons and tools. 
Ancient Egypt.
Ancient Egyptians mined malachite at Maadi. At first, Egyptians used the bright green malachite stones for ornamentations and pottery. Later, between 2613 and 2494 BC, large building projects required expeditions abroad to the area of Wadi Maghara in order "to secure minerals and other resources not available in Egypt itself." Quarries for turquoise and copper were also found at "Wadi Hamamat, Tura, Aswan and various other Nubian sites" on the Sinai Peninsula and at Timna.
Mining in Egypt occurred in the earliest dynasties. The gold mines of Nubia were among the largest and most extensive of any in Ancient Egypt, and are described by the Greek author Diodorus Siculus. He mentions that fire-setting was one method used to break down the hard rock holding the gold. One of the complexes is shown in one of the earliest known maps. The miners crushed the ore and ground it to a fine powder before washing the powder for the gold dust.
Ancient Greek and Roman Mining.
Mining in Europe has a very long history. Examples include the silver mines of Laurium, which helped support the Greek city state of Athens. However, it is the Romans who developed large scale mining methods, especially the use of large volumes of water brought to the minehead by numerous aqueducts. The water was used for a variety of purposes, including removing overburden and rock debris, called hydraulic mining, as well as washing comminuted, or crushed, ores and driving simple machinery.
The Romans used hydraulic mining methods on a large scale to prospect for the veins of ore, especially a now obsolete form of mining known as hushing. It involved building numerous aqueducts to supply water to the minehead where it was stored in large reservoirs and tanks. When a full tank was opened, the wave of water sluiced away the overburden to expose the bedrock underneath and any gold veins. The rock was then attacked by fire-setting to heat the rock, which would be quenched with a stream of water. The thermal shock cracked the rock, enabling it to be removed, aided by further streams of water from the overhead tanks. The Roman miners used similar methods to work cassiterite deposits in Cornwall and lead ore in the Pennines.
The methods had been developed by the Romans in Spain in 25 AD to exploit large alluvial gold deposits, the largest site being at Las Medulas, where seven long aqueducts were built to tap local rivers and to sluice the deposits. Spain was one of the most important mining regions, but all regions of the Roman Empire were exploited. In Great Britain the natives had mined minerals for millennia, but when the Romans came, the scale of the operations changed dramatically.
The Romans needed what Britain possessed, especially gold, silver, tin, and lead. Roman techniques were not limited to surface mining. They followed the ore veins underground once opencast mining was no longer feasible. At Dolaucothi they stoped out the veins, and drove adits through barren rock to drain the stopes. The same adits were also used to ventilate the workings, especially important when fire-setting was used. At other parts of the site, they penetrated the water table and dewatered the mines using several kinds of machine, especially reverse overshot water-wheels. These were used extensively in the copper mines at Rio Tinto in Spain, where one sequence comprised 16 such wheels arranged in pairs, and lifting water about 80 ft. They were worked as treadmills with miners standing on the top slats. Many examples of such devices have been found in old Roman mines and some examples are now preserved in the British Museum and the National Museum of Wales.
Medieval Europe.
Mining as an industry underwent dramatic changes in medieval Europe. The mining industry in the early Middle Ages was mainly focused on the extraction of copper and iron. Other precious metals were also used mainly for gilding or coinage. Initially, many metals were obtained through open-pit mining, and ore was primarily extracted from shallow depths, rather than though the digging of deep mine shafts. Around the 14th century, the demand for weapons, armour, stirrups, and horseshoes greatly increased the demand for iron. Medieval knights, for example, were often laden with up to 100 pounds of plate or chain link armour in addition to swords, lances and other weapons. The overwhelming dependency on iron for military purposes helped to spur increased iron production and extraction processes.
The silver crisis of 1465 occurred when the mines had all reached depths at which the shafts could no longer be pumped dry with the available technology. Although an increased use of bank notes and credit during this period did decrease the value of, and dependence on, precious metals, these forms of currency still remained vital to the story of medieval mining.
In the mid-sixteenth century the great attack on mineral deposits spread from central Europe to England. England had iron, zinc, copper, lead, and tin ores. On the continent all mineral deposits belonged to the crown, and this regalian right was stoutly maintained; but in England it was pared down to gold and silver (of which there was virtually none) by a judicial decision of 1568 and a law of 1688. Landlords therefore owned the base metals and coal under their estates and had a strong inducement to extract them or to lease the deposits and collect royalties from mine operators. English, German, and Dutch capital combined to finance extraction and refining. Hundreds of German technicians and skilled workers were brought over; in 1642 a colony of 4,000 foreigners was mining and smelting copper at Keswick in the northwestern mountains.
Use of water power in the form of water mills was extensive. The water mills were employed in crushing ore, raising ore from shafts, and ventilating galleries by powering giant bellows. Black powder was first used in mining in Selmecbánya, Kingdom of Hungary in 1627. Black powder allowed blasting of rock and earth to loosen and reveal ore veins. Blasting was much faster than fire-setting and allowed the mining of previously impenetrable metals and ores. In 1762, the world's first mining academy was established in the same town.
The widespread adoption of agricultural innovations such as the iron plowshare, as well as the growing use of metal as a building material, was also a driving force in the tremendous growth of the iron industry during this period. Inventions like the arrastra were often used by the Spanish to pulverize ore after being mined. This device was powered by animals and used the same principles used for grain threshing.
Much of the knowledge of medieval mining techniques comes from books such as Biringuccio’s "De la pirotechnia" and probably most importantly from Georg Agricola's "De re metallica" (1556). These books detail many different mining methods used in German and Saxon mines. One of the prime issues confronting medieval miners (and one which Agricola explains in detail) was the removal of water from mining shafts. As miners dug deeper to access new veins, flooding became a very real obstacle. The mining industry became dramatically more efficient and prosperous with the invention of mechanical and animal driven pumps.
Classical Philippine Civilization.
Mining in the Philippines began around 1000 BC. The early Filipinos worked various mines of gold, silver, copper and iron. Jewels, gold ingots, chains, calombigas and earrings were handed down from antiquity and inherited from their ancestors. Gold dagger handles, gold dishes, tooth plating, and huge gold ornamets were also used. In Laszlo Legeza's "Tantric elements in pre-Hispanic Philippines Gold Art", he mentioned that gold jewelry of Philippine origin was found in Ancient Egypt. According to Antonio Pigafetta, the people of Mindoro possessed great skill in mixing gold with other metals and gave it a natural and perfect appearance that could deceive even the best of silversmiths. The natives were also known for the jewelries made of other precious stones such as carnelian, agate and pearl. Some outstanding examples of Philippine jewelry included necklaces, belts, armlets and rings placed around the waist.
The Americas.
There are ancient, prehistoric copper mines along Lake Superior, and metallic copper was still found there, near the surface, in colonial times.
Indians availed themselves of this copper starting at least 5,000 years ago," and copper tools, arrowheads, and other artifacts that were part of an extensive native trade network have been discovered. In addition, obsidian, flint, and other minerals were mined, worked, and traded. Early French explorers who encountered the sites made no use of the metals due to the difficulties of transporting them, but the copper was eventually traded throughout the continent along major river routes. In Saskatchewan, Canada, there also are ancient quartz mines near Waddy Lake and surrounding regions.
In the early colonial history of the Americas, "native gold and silver was quickly expropriated and sent back to Spain in fleets of gold- and silver-laden galleons," the gold and silver mostly from mines in Central and South America. Turquoise dated at 700 A.D. was mined in pre-Columbian America; in the Cerillos Mining District in New Mexico, estimates are that "about 15,000 tons of rock had been removed from Mt. Chalchihuitl using stone tools before 1700."
Mining in the United States became prevalent in the 19th century, and the General Mining Act of 1872 was passed to encourage mining of federal lands. As with the California Gold Rush in the mid-19th century, mining for minerals and precious metals, along with ranching, was a driving factor in the Westward Expansion to the Pacific coast. With the exploration of the West, mining camps were established and "expressed a distinctive spirit, an enduring legacy to the new nation;" Gold Rushers would experience the same problems as the Land Rushers of the transient West that preceded them. Aided by railroads, many traveled West for work opportunities in mining. Western cities such as Denver and Sacramento originated as mining towns.
As new areas were explored, it was usually the gold (placer and then load) and then silver that were taken first, with other metals often waiting for railroads or canals. Coarse gold dust and nuggets do not require smelting and are easy to identify and transport.
Modern period.
In the early 20th century, the gold and silver rush to the western United States also stimulated mining for base metals such as copper, lead, and iron as well as coal. Areas in modern Montana, Utah, Arizona, and later Alaska became predominate suppliers of copper to the world, which was increasingly demanding copper for electrical and households goods. Canada's mining industry grew more slowly than did the United States' due to limitations in transportation, capital, and U.S. competition; Ontario was the major producer of the early 20th century with nickel, copper, and gold.
Meanwhile, Australia experienced the Australian gold rushes and by the 1850s was producing 40% of the world's gold, followed by the establishment of large mines such as the Mount Morgan Mine, which ran for nearly a hundred years, Broken Hill ore deposit (one of the largest zinc-lead ore deposits), and the iron ore mines at Iron Knob. After declines in production, another boom in mining occurred in the 1960s. Now, in the early 21st century, Australia remains a major world mineral producer.
As the 21st century begins, a globalized mining industry of large multinational corporations has arisen. Peak minerals and environmental impacts have also become a concern. Different elements, particularly rare earth minerals, have begun to increase in demand as a result of new technologies.
Mine development and lifecycle.
The process of mining from discovery of an ore body through extraction of minerals and finally to returning the land to its natural state consists of several distinct steps. The first is discovery of the ore body, which is carried out through prospecting or exploration to find and then define the extent, location and value of the ore body. This leads to a mathematical resource estimation to estimate the size and grade of the deposit.
This estimation is used to conduct a pre-feasibility study to determine the theoretical economics of the ore deposit. This identifies, early on, whether further investment in estimation and engineering studies is warranted and identifies key risks and areas for further work. The next step is to conduct a feasibility study to evaluate the financial viability, the technical and financial risks, and the robustness of the project.
This is when the mining company makes the decision whether to develop the mine or to walk away from the project. This includes mine planning to evaluate the economically recoverable portion of the deposit, the metallurgy and ore recoverability, marketability and payability of the ore concentrates, engineering concerns, milling and infrastructure costs, finance and equity requirements, and an analysis of the proposed mine from the initial excavation all the way through to reclamation. The proportion of a deposit that is economically recoverable is dependent on the enrichment factor of the ore in the area.
To gain access to the mineral deposit within an area it is often necessary to mine through or remove waste material which is not of immediate interest to the miner. The total movement of ore and waste constitutes the mining process. Often more waste than ore is mined during the life of a mine, depending on the nature and location of the ore body. Waste removal and placement is a major cost to the mining operator, so a detailed characterization of the waste material forms an essential part of the geological exploration program for a mining operation.
Once the analysis determines a given ore body is worth recovering, development begins to create access to the ore body. The mine buildings and processing plants are built, and any necessary equipment is obtained. The operation of the mine to recover the ore begins and continues as long as the company operating the mine finds it economical to do so. Once all the ore that the mine can produce profitably is recovered, reclamation begins to make the land used by the mine suitable for future use.
Mining techniques.
Mining techniques can be divided into two common excavation types: surface mining and sub-surface (underground) mining. Today, surface mining is much more common, and produces, for example, 85% of minerals (excluding petroleum and natural gas) in the United States, including 98% of metallic ores.
Targets are divided into two general categories of materials: "placer deposits", consisting of valuable minerals contained within river gravels, beach sands, and other unconsolidated materials; and "lode deposits", where valuable minerals are found in veins, in layers, or in mineral grains generally distributed throughout a mass of actual rock. Both types of ore deposit, placer or lode, are mined by both surface and underground methods.
Some mining, including much of the rare earth elements and uranium mining, is done by less-common methods, such as in-situ leaching: this technique involves digging neither at the surface nor underground. The extraction of target minerals by this technique requires that they be soluble, e.g., potash, potassium chloride, sodium chloride, sodium sulfate, which dissolve in water. Some minerals, such as copper minerals and uranium oxide, require acid or carbonate solutions to dissolve.
Surface mining.
Surface mining is done by removing (stripping) surface vegetation, dirt, and, if necessary, layers of bedrock in order to reach buried ore deposits. Techniques of surface mining include: open-pit mining, which is the recovery of materials from an open pit in the ground, quarrying or gathering building materials from an open-pit mine; strip mining, which consists of stripping surface layers off to reveal ore/seams underneath; and mountaintop removal, commonly associated with coal mining, which involves taking the top of a mountain off to reach ore deposits at depth. Most (but not all) placer deposits, because of their shallowly buried nature, are mined by surface methods. Finally, landfill mining involves sites where landfills are excavated and processed.
Garzweiler surface mine, Germany
Underground mining.
Sub-surface mining consists of digging tunnels or shafts into the earth to reach buried ore deposits. Ore, for processing, and waste rock, for disposal, are brought to the surface through the tunnels and shafts. Sub-surface mining can be classified by the type of access shafts used, the extraction method or the technique used to reach the mineral deposit. Drift mining utilizes horizontal access tunnels, slope mining uses diagonally sloping access shafts, and shaft mining utilizes vertical access shafts. Mining in hard and soft rock formations require different techniques.
Other methods include shrinkage stope mining, which is mining upward, creating a sloping underground room, long wall mining, which is grinding a long ore surface underground, and room and pillar mining, which is removing ore from rooms while leaving pillars in place to support the roof of the room. Room and pillar mining often leads to retreat mining, in which supporting pillars are removed as miners retreat, allowing the room to cave in, thereby loosening more ore. Additional sub-surface mining methods include hard rock mining, which is mining of hard rock (igneous, metamorphic or sedimentary) materials, bore hole mining, drift and fill mining, long hole slope mining, sub level caving, and block caving.
Machines.
Heavy machinery is used in mining to explore and develop sites, to remove and stockpile overburden, to break and remove rocks of various hardness and toughness, to process the ore, and to carry out reclamation projects after the mine is closed. Bulldozers, drills, explosives and trucks are all necessary for excavating the land. In the case of placer mining, unconsolidated gravel, or alluvium, is fed into machinery consisting of a hopper and a shaking screen or trommel which frees the desired minerals from the waste gravel. The minerals are then concentrated using sluices or jigs.
Large drills are used to sink shafts, excavate stopes, and obtain samples for analysis. Trams are used to transport miners, minerals and waste. Lifts carry miners into and out of mines, and move rock and ore out, and machinery in and out, of underground mines. Huge trucks, shovels and cranes are employed in surface mining to move large quantities of overburden and ore. Processing plants utilize large crushers, mills, reactors, roasters and other equipment to consolidate the mineral-rich material and extract the desired compounds and metals from the ore.
Processing.
Once the mineral is extracted, it is often then processed. The science of extractive metallurgy is a specialized area in the science of metallurgy that studies the extraction of valuable metals from their ores, especially through chemical or mechanical means.
Mineral processing (or mineral dressing) is a specialized area in the science of metallurgy that studies the mechanical means of crushing, grinding, and washing that enable the separation (extractive metallurgy) of valuable metals or minerals from their gangue (waste material). Processing of placer ore material consists of gravity-dependent methods of separation, such as sluice boxes. Only minor shaking or washing may be necessary to disaggregate (unclump) the sands or gravels before processing. Processing of ore from a lode mine, whether it is a surface or subsurface mine, requires that the rock ore be crushed and pulverized before extraction of the valuable minerals begins. After lode ore is crushed, recovery of the valuable minerals is done by one, or a combination of several, mechanical and chemical techniques.
Since most metals are present in ores as oxides or sulfides, the metal needs to be reduced to its metallic form. This can be accomplished through chemical means such as smelting or through electrolytic reduction, as in the case of aluminium. Geometallurgy combines the geologic sciences with extractive metallurgy and mining.
Environmental effects.
Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil. Contamination resulting from leakage of chemicals can also affect the health of the local population if not properly controlled. Extreme examples of pollution from mining activities include coal fires, which can last for years or even decades, producing massive amounts of environmental damage.
Mining companies in most countries are required to follow stringent environmental and rehabilitation codes in order to minimize environmental impact and avoid impacting human health. These codes and regulations all require the common steps of environmental impact assessment, development of environmental management plans, mine closure planning (which must be done before the start of mining operations), and environmental monitoring during operation and after closure. However, in some areas, particularly in the developing world, government regulations may not be well enforced.
For major mining companies and any company seeking international financing, there are a number of other mechanisms to enforce good environmental standards. These generally relate to financing standards such as the Equator Principles, IFC environmental standards, and criteria for Socially responsible investing. Mining companies have used this oversight from the financial sector to argue for some level of self-policing. In 1992, a Draft Code of Conduct for Transnational Corporations was proposed at the Rio Earth Summit by the UN Centre for Transnational Corporations (UNCTC), but the Business Council for Sustainable Development (BCSD) together with the International Chamber of Commerce (ICC) argued successfully for self-regulation instead.
This was followed by the Global Mining Initiative which was begun by nine of the largest metals and mining companies and which led to the formation of the International Council on Mining and Metals, whose purpose was to "act as a catalyst" in an effort to improve social and environmental performance in the mining and metals industry internationally. The mining industry has provided funding to various conservation groups, some of which have been working with conservation agendas that are at odds with an emerging acceptance of the rights of indigenous people – particularly the right to make land-use decisions.
Certification of mines with good practices occurs through the International Organization for Standardization (ISO). For example, ISO 9000 and ISO 14001, which certify an "auditable environmental management system", involve short inspections, although they have been accused of lacking rigor.:183–4 Certification is also available through Ceres' Global Reporting Initiative, but these reports are voluntary and unverified. Miscellaneous other certification programs exist for various projects, typically through nonprofit groups.:185–6
The purpose of a 2012 EPS PEAKS paper was to provide evidence on policies managing ecological costs and maximise socio-economic benefits of mining using host country regulatory initiatives. It found existing literature suggesting donors encourage developing countries to:
Waste.
Ore mills generate large amounts of waste, called tailings. For example, 99 tons of waste are generated per ton of copper, with even higher ratios in gold mining. These tailings can be toxic. Tailings, which are usually produced as a slurry, are most commonly dumped into ponds made from naturally existing valleys. These ponds are secured by impoundments (dams or embankment dams). In 2000 it was estimated that 3,500 tailings impoundments existed, and that every year, 2 to 5 major failures and 35 minor failures occurred; for example, in the Marcopper mining disaster at least 2 million tons of tailings were released into a local river. Subaqueous tailings disposal is another option. The mining industry has argued that submarine tailings disposal (STD), which disposes of tailings in the sea, is ideal because it avoids the risks of tailings ponds; although the practice is illegal in the United States and Canada, it is used in the developing world.
The waste is classified as either sterile or mineralised, with acid generating potential, and the movement and storage of this material forms a major part of the mine planning process. When the mineralised package is determined by an economic cut-off, the near-grade mineralised waste is usually dumped separately with view to later treatment should market conditions change and it becomes economically viable. Civil engineering design parameters are used in the design of the waste dumps, and special conditions apply to high-rainfall areas and to seismically active areas. Waste dump designs must meet all regulatory requirements of the country in whose jurisdiction the mine is located. It is also common practice to rehabilitate dumps to an internationally acceptable standard, which in some cases means that higher standards than the local regulatory standard are applied.
Mining industry.
Mining exists in many countries. London is known as the capital of global "mining houses" such as Rio Tinto Group, BHP Billiton, and Anglo American PLC. The US mining industry is also large, but it is dominated by the coal and other nonmetal minerals (e.g., rock and sand), and various regulations have worked to reduce the significance of mining in the United States. In 2007 the total market capitalization of mining companies was reported at US$962 billion, which compares to a total global market cap of publicly traded companies of about US$50 trillion in 2007. In 2002, Chile and Peru were reportedly the major mining countries of South America. The mineral industry of Africa includes the mining of various minerals; it produces relatively little of the industrial metals copper, lead, and zinc, but according to one estimate has as a percent of world reserves 40% of gold, 60% of cobalt, and 90% of the world's platinum group metals. Mining in India is a significant part of that country's economy. In the developed world, mining in Australia, with BHP Billiton founded and headquartered in the country, and mining in Canada are particularly significant. For rare earth minerals mining, China reportedly controlled 95% of production in 2013.
While exploration and mining can be conducted by individual entrepreneurs or small businesses, most modern-day mines are large enterprises requiring large amounts of capital to establish. Consequently, the mining sector of the industry is dominated by large, often multinational, companies, most of them publicly listed. It can be argued that what is referred to as the 'mining industry' is actually two sectors, one specializing in exploration for new resources and the other in mining those resources. The exploration sector is typically made up of individuals and small mineral resource companies, called "juniors", which are dependent on venture capital. The mining sector is made up of large multinational companies that are sustained by production from their mining operations. Various other industries such as equipment manufacture, environmental testing, and metallurgy analysis rely on, and support, the mining industry throughout the world. Canadian stock exchanges have a particular focus on mining companies, particularly junior exploration companies through Toronto'sTSX Venture Exchange; Canadian companies raise capital on these exchanges and then invest the money in exploration globally. Some have argued that below juniors there exists a substantial sector of illegitimate companies primarily focused on manipulating stock prices.
Mining operations can be grouped into five major categories in terms of their respective resources. These are oil and gas extraction, coal mining, metal ore mining, nonmetallic mineral mining and quarrying, and mining support activities. Of all of these categories, oil and gas extraction remains one of the largest in terms of its global economic importance. Prospecting potential mining sites, a vital area of concern for the mining industry, is now done using sophisticated new technologies such as seismic prospecting and remote-sensing satellites. Mining is heavily affected by the prices of the commodity minerals, which are often volatile. The 2000s commodities boom ("commodities supercycle") increased the prices of commodities, driving aggressive mining. In addition, the price of gold increased dramatically in the 2000s, which increased gold mining; for example, one study found that conversion of forest in the Amazon increased six-fold from the period 2003–2006 (292 ha/yr) to the period 2006–2009 (1,915 ha/yr), largely due to artisanal mining.
Corporate classifications.
Mining companies can be classified based on their size and financial capabilities:
Regulation and governance.
New regulation and process of legislative reforms aims to enrich the harmonization and stability of the mining sector in mineral-rich countries. The new legislation for mining industry in the African countries still appears as an emerging issue with a potential to be solved, until a consensus is reached on the best approach. By the beginning of 20th century the booming and more complex mining sector in mineral-rich countries provided only slight benefits to local communities in terms of sustainability. Increasing debates and influence by NGOs and communities appealed for a new program which would have had also included a disadvantaged communities, and would have had worked towards sustainable development even after mine closure (included transparency and revenue management). By the early 2000s, community development issues and resettlements became mainstreamed in Bank mining projects. Mining-industry expansion after an increase of mineral prices in 2003 and also potential fiscal revenues in those countries created an omission in the other economic sectors in terms of finances and development. Furthermore, it had highlighted regional and local demand of mining-revenues and lack of ability of sub-national governments to use the revenues. The Fraser Institute (a Canadian think tank) has highlighted the environmental protection laws in developing countries, as well as the voluntary efforts by mining companies to improve their environmental impact.
In 2007 the Extractive Industries Transparency Initiative (EITI) was mainstreamed in all countries cooperating with the World Bank in mining industry reform. The EITI is operating and implementing with a support of EITI Multi-Donor Trust Fund, managed by The World Bank. The Extractive Industries Transparency Initiative (EITI) aims to increase transparency in transactions between governments and companies within extractive industries by monitoring the revenues and benefits between industries and recipient governments. The entrance process is voluntary for each country and is being monitored by multi-stakeholders involving government, private companies and civil society representatives, responsible for disclosure and dissemination of the reconciliation report; however, the competitive disadvantage of company-by company public report is for some of the businesses in Ghana, the main constraint. Therefore, the outcome assessment in terms of failure or success of the new EITI regulation does not only "rest on the government's shoulders" but also on civil society and companies.
On the other hand, criticism points out two main implementation issues; inclusion or exclusion of artisanal mining and small-scale mining (ASM) from the EITI and how to deal with “non-cash” payments made by companies to subnational governments. Furthermore, disproportion of the revenues mining industry creates to the comparatively small number of people that it employs, causes another controversy. The issue of artisanal mining is clearly an issue in EITI Countries such as the Central African Republic, D.R. Congo, Guinea, Liberia and Sierra Leone – i.e. almost half of the mining countries implementing the EITI. Among other things, limited scope of the EITI involving disparity in terms of knowledge of the industry and negotiation skills, thus far flexibility of the policy (e.g. liberty of the countries to expand beyond the minimum requirements and adapt it to their needs), creates another risk of unsuccessful implementation. Public awareness increase, where government should act as a bridge between public and initiative for a successful outcome of the policy is an important element to be considered.
World Bank.
The World Bank has been involved in mining since 1955, mainly through grants from its International Bank for Reconstruction and Development, with the Bank's Multilateral Investment Guarantee Agency offering political risk insurance. Between 1955 and 1990 it provided about $2 billion to fifty mining projects, broadly categorized as reform and rehabilitation, greenfield mine construction, mineral processing, technical assistance, and engineering. These projects have been criticized, particularly the Ferro Carajas project of Brazil, begun in 1981. The World Bank established mining codes intended to increase foreign investment; in 1988 it solicited feedback from 45 mining companies on how to increase their involvement.:20
In 1992 the World Bank began to push for privatization of government-owned mining companies with a new set of codes, beginning with its report "The Strategy for African Mining". In 1997, Latin America's largest miner Companhia Vale do Rio Doce (CVRD) was privatized. These and other developments such as the Philippines 1995 Mining Act led the bank to publish a third report ("Assistance for Minerals Sector Development and Reform in Member Countries") which endorsed mandatory environment impact assessments and attention to the concerns of the local population. The codes based on this report are influential in the legislation of developing nations. The new codes are intended to encourage development through tax holidays, zero custom duties, reduced income taxes, and related measures.:22 The results of these codes were analyzed by a group from the University of Quebec, which concluded that the codes promote foreign investment but "fall very short of permitting sustainable development". The observed negative correlation between natural resources and economic development is known as the resource curse.
Safety.
Safety has long been a concern in the mining business especially in sub-surface mining. The Courrières mine disaster, Europe's worst mining accident, involved the death of 1,099 miners in Northern France on March 10, 1906. This disaster was surpassed only by the Benxihu Colliery accident in China on April 26, 1942, which killed 1,549 miners. While mining today is substantially safer than it was in previous decades, mining accidents still occur. Government figures indicate that 5,000 Chinese miners die in accidents each year, while other reports have suggested a figure as high as 20,000. Mining accidents continue worldwide, including accidents causing dozens of fatalities at a time such as the 2007 Ulyanovskaya Mine disaster in Russia, the 2009 Heilongjiang mine explosion in China, and the 2010 Upper Big Branch Mine disaster in the United States.
Mining ventilation is a significant safety concern for many miners. Poor ventilation inside sub-surface mines causes exposure to harmful gases, heat, and dust, which can cause illness, injury, and death. The concentration of methane and other airborne contaminants underground can generally be controlled by dilution (ventilation), capture before entering the host air stream (methane drainage), or isolation (seals and stoppings). Rock dusts, including coal dust and silicon dust, can cause long-term lung problems including silicosis, asbestosis, and pneumoconiosis (also known as miners lung or black lung disease). A ventilation system is set up to force a stream of air through the working areas of the mine. The air circulation necessary for effective ventilation of a mine is generated by one or more large mine fans, usually located above ground. Air flows in one direction only, making circuits through the mine such that each main work area constantly receives a supply of fresh air. Watering down in coal mines also helps to keep dust levels down: by spraying the machine with water and filtering the dust-laden water with a scrubber fan, miners can successfully trap the dust.
Gases in mines can poison the workers or displace the oxygen in the mine, causing asphyxiation. For this reason, the U.S. Mine Safety and Health Administration requires that groups of miners in the United States carry gas detection equipment that can detect common gases, such as CO, O2, H2S, CH4, as well as calculate % Lower Explosive Limit. Regulation requires that all production stop if there is a concentration of 1.4% of flammable gas present. Additionally, further regulation is being requested for more gas detection as newer technology such as nanotechnology is introduced.
Ignited methane gas is a common source of explosions in coal mines, which in turn can initiate more extensive coal dust explosions. For this reason, rock dusts such as limestone dust are spread throughout coal mines to diminish the chances of coal dust explosions as well as to limit the extent of potential explosions, in a process known as rock dusting. Coal dust explosions can also begin independently of methane gas explosions. Frictional heat and sparks generated by mining equipment can ignite both methane gas and coal dust. For this reason, water is often used to cool rock-cutting sites.
Miners utilize equipment strong enough to break through extremely hard layers of the Earth's crust. This equipment, combined with the closed work space in which underground miners work, can cause hearing loss. For example, a roof bolter (commonly used by mine roof bolter operators) can reach sound power levels of up to 115 dB. Combined with the reverberant effects of underground mines, a miner without proper hearing protection is at a high risk for hearing loss. By age 50, nearly 90% of U.S. coal miners have some hearing loss, compared to only 10% among workers not exposed to loud noises. Roof bolters are among the loudest machines, but auger miners, bulldozers, continuous mining machines, front end loaders, and shuttle cars and trucks are also among those machines most responsible for excessive noise in mine work.
Since mining entails removing dirt and rock from its natural location, thereby creating large empty pits, rooms, and tunnels, cave-ins as well as ground and rock falls are a major concern within mines. Modern techniques for timbering and bracing walls and ceilings within sub-surface mines have reduced the number of fatalities due to cave-ins, but ground falls continue to represent up to 50% of mining fatalities. Even in cases where mine collapses are not instantly fatal, they can trap mine workers deep underground. Cases such as these often lead to high-profile rescue efforts, such as when 33 Chilean miners were trapped deep underground for 69 days in 2010.
High temperatures and humidity may result in heat-related illnesses, including heat stroke, which can be fatal. The presence of heavy equipment in confined spaces also poses a risk to miners. To improve the safety of mine workers, modern mines use automation and remote operation including, for example, such equipment as automated loaders and remotely operated rockbreakers. However, despite modern improvements to safety practices, mining remains a dangerous occupation throughout the world.
Abandoned mines.
There are upwards of 560,000 abandoned mines on public and privately owned lands in the United States alone. Abandoned mines may be dangerous to anyone who attempts to explore them without proper knowledge and safety training.
Records.
As of 2008, the deepest mine in the world is TauTona in Carletonville, South Africa at 3.9 km, replacing the neighboring Savuka Mine in the North West Province of South Africa at 3774 m. East Rand Mine in Boksburg, South Africa briefly held the record at 3585 m, and the first mine declared the deepest in the world was also TauTona when it was at 3581 m.
The Moab Khutsong gold mine in North West Province (South Africa) has the world's longest winding steel wire rope, able to lower workers to 3054 m in one uninterrupted four-minute journey.
The deepest mine in Europe is the 16th shaft of the uranium mines in Příbram, Czech Republic at 1838 m, second is Bergwerk Saar in Saarland, Germany at 1750 m. 
The deepest open-pit mine in the world is Bingham Canyon Mine in Bingham Canyon, Utah, United States at over 1200 m. The largest and second deepest open-pit copper mine in the world is Chuquicamata in Chuquicamata, Chile at 900 m, 940,600 tons of copper and 17,700 tons of molybdenum produced annually. 
The deepest open-pit mine with respect to sea level is Tagebau Hambach in Germany, where the base of the pit is 293 m below sea level.
The largest underground mine is Kiirunavaara Mine in Kiruna, Sweden. With 450 km of roads, 40 million tonnes of ore produced yearly, and a depth of 1270 m, it is also one of the most modern underground mines. The deepest borehole in the world is Kola Superdeep Borehole at 12262 m. This, however, is not a matter of mining but rather related to scientific drilling.
Metal reserves and recycling.
During the twentieth century, the variety of metals used in society grew rapidly. Today, the development of major nations such as China and India and advances in technologies are fueling an ever greater demand. The result is that metal mining activities are expanding and more and more of the world’s metal stocks are above ground in use rather than below ground as unused reserves. An example is the in-use stock of copper. Between 1932 and 1999, copper in use in the USA rose from 73 kg to 238 kg per person.
95% of the energy used to make aluminium from bauxite ore is saved by using recycled material. However, levels of metals recycling are generally low. In 2010, the International Resource Panel, hosted by the United Nations Environment Programme (UNEP), published reports on metal stocks that exist within society and their recycling rates.
The report's authors observed that the metal stocks in society can serve as huge mines above ground. However, they warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars, and fuel cells are so low that unless future end-of-life recycling rates are dramatically stepped up these critical metals will become unavailable for use in modern technology.

</doc>
<doc id="20388" url="http://en.wikipedia.org/wiki?curid=20388" title="Geography of Burma">
Geography of Burma

The Republic of the Union of Myanmar (also known as Burma) is the northwestern-most country on the mainland of southeast Asia. It is strategically located near major Indian Ocean shipping lanes.
Climate.
Tropical monsoon in the lowlands below 2000 m; cloudy, rainy, hot, humid summers (southwest monsoon, June to September); less cloudy, scant rainfall, mild temperatures, lower humidity during winter (northeast monsoon, December to April). Climate varies in the highlands depending on elevation; subtropical temperate climate at around 2500 m, temperate at 3000 m, cool, alpine at 3500 m and above the alpine zone, cold, harsh tundra and Arctic climate. The higher elevations are subject to heavy snowfall and bad weather.
Mountains.
Burma is characterized by its central lowlands with the Sittaung Valley and Chindwin Valley and the small mountain ranges of Zeebyu Taungdan, Min-wun Taungdan, Hman-kin Taungdan and Gangaw Taungdan as well as the Bago Yoma. The Central Valley Region is ringed by steep, rugged highlands, with the country's highest point at the 5881 m Hkakabo Razi located in the northern end of the country. This mountain is part of a series of parallel ranges that run from the foothills of the Himalaya through the border areas with Assam, Nagaland and Mizoram.
The Arakan Mountains in the west run from Manipur into western Burma southwards through Rakhine State almost to Cape Negrais in the shores of the Bay of Bengal. The Arakan Range includes the Naga Hills, the Chin Hills, and the Patkai range which includes the Lushai Hills.
Mountain ranges in the southern end of the Hengduan System form the border between Burma and China.
The Pegu Range is a relatively low mountain chain between the Irrawaddy and the Sittaung River in central Burma. In Eastern Burma the highest point of the Shan Hills is 2,563 m high Loi Pangnao, one of the ultra prominent peaks of Southeast Asia. The Shan Hills form, together with the Karen Hills, Dawna Range and Tenasserim Hills, a natural border with Thailand as well as the Kayah-Karen/Tenasserim moist forests ecoregion which is included in the Global 200 list of ecoregions identified by the World Wildlife Fund (WWF) as priorities for conservation.
Southern Burma consists largely of the western slopes of the Bilauktaung, the highest part of the Tenasserim Range, which extends southwards forming the central range of the Malay Peninsula.
Rivers.
The Irrawaddy, the main river of Burma, flows from north to south through the Central Burma Basin and ends in a wide delta. The Mekong runs from the Tibetan Plateau through China's Yunnan province entering Northeastern Burma into Laos.
In the east the Salween and the Sittaung River run along the western side of the Shan Hills and the northern end of the Dawna Range.
In the narrow southeastern part of Burma, the Ye, Heinze, Dawei (Tavoy), Great Tenasserim (Tanintharyi) and the Lenya rivers are relatively short and flow into the Andaman Sea. Further south the Kraburi River forms the southern border between Thailand and Burma.
Maritime claims.
"contiguous zone:"
24 nmi
"continental shelf:"
200 nmi or to the edge of the continental margin
"exclusive economic zone:"
200 nmi
Land use and natural resources.
Natural resources in Myanmar are petroleum, timber, tin, antimony, zinc, copper, tungsten, lead, coal, marble, limestone, precious stones, natural gas, and hydropower.
Natural hazards.
Destructive earthquakes and cyclones; flooding and landslides common during rainy season (June to September); periodic droughts
Environment.
Deforestation; industrial pollution of air, soil, and water; inadequate sanitation and water treatment contribute to disease
Environment - international agreements.
"party to:"
Biodiversity, Climate Change, Desertification, Endangered Species, Law of the Sea, Nuclear Test Ban, Ozone Layer Protection, Ship Pollution, Tropical Timber 83, Tropical Timber 94
"signed, but not ratified:" none of the selected agreements
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="20389" url="http://en.wikipedia.org/wiki?curid=20389" title="Demographics of Burma">
Demographics of Burma

The following is an overview of the demographics of Burma (or Myanmar), including statistics such as population, ethnicity, language, education level and religious affiliation.
Population.
1983 census.
At the time of the 1983 census in Burma, as of 31 March 1983, the population was 35,442,972. As of July 2012[ [update]], this was estimated by the "CIA World Factbook" to have increased to 60,584,650. Other estimates put place the total population at around 60 million. China's "People's Daily" reported that Burma had a census in 2007, and at the end of 2009 has 59.2 million people, and growing at 2% annually. with exception for Cyclone Nargis in 2008. Most of these estimates have indeed overlooked the demographic changes that were at work since the 1970s in the country.
Britain-based human rights agencies place the population as high as 70 million. Estimates for the country explicitly take into account the effects of excess mortality due to AIDS. This can result in lower life expectancy, higher infant mortality and death rates, lower population and growth rates, and changes in the distribution of population by age and sex than would otherwise be expected.
No trustworthy census has occurred since the 1930s. In the 1940s, the detailed census results were destroyed during the Japanese invasion of 1942. Census results after that time have been flawed by civil wars and a series of military governments. The census in 1983 occurred at a time when parts of the country were controlled by insurgent groups and inaccessible to the government.
2014 census.
The Provisional results of the 2014 census show that the total population of Myanmar is 51,419,420—a population well below the official estimates of more than 60 million. This total population includes 50,213,067 persons counted during the census and an estimated 1,206,353 persons in parts of northern Rakhine State, Kachin State and Kayin State who were not counted. More females (51.8%) were counted than males (48.2%). People who were out of the country at the time of the census are not included in these figures.
The provisional census results indicated that there were 10,889,348 households in Myanmar. On average, 4.4 people lived in each household in the country. The average household size was highest in Kachin State and Chin State at 5.1. The lowest household sizes were observed in Ayeyawady Region, Bago Region, Magway Region and Naypyidaw Union Territory, each at 4.1.
Fertility statistics.
Burma has a low fertility rate (2.23 in 2011), slightly above replacement level, especially as compared to other Southeast Asian countries of similar economic standing, like Cambodia (3.18) and Laos (4.41), representing a significant decline from 4.7 in 1983 to 2.4 in 2001, despite the absence of any national population policy.
The fertility rate is much pronouncedly lower in urban areas. This is attributed to extreme delays in marriage (almost unparalleled in the region, with the exception of developed countries), the prevalence of illegal abortions, and the high proportion of single, unmarried women of reproductive age (with 25.9% of women aged 30–34 and 33.1% of men and women aged 25–34 single).
These patterns stem from several cultural and economic dynamics. The first is economic hardship, which results in the delay of marriage and family-building (the average age of marriage in Burma is 27.5 for men, 26.4 for women). The second is the social acceptability of celibacy among the Burmese, who are predominantly Buddhist and value celibacy as a means of spiritual development.
Ethnic groups.
Government classifications.
The Burmese government identifies eight major national ethnic races (which comprise 135 "distinct" ethnic groups), which include the Bamar (68%), Shan (9%), Kayin (7%), Rakhine (4%), Mon (2%), Kayah, and Kachin. However, the government classification system is flawed, because it groups ethnic groups under ethnic races by geography, rather than by linguistic or genetic similarity (e.g. the Kokang are under the Shan ethnic race, although they are ethnic Chinese).
Unrecognised ethnic groups include Burmese Indians and Burmese Chinese, who form 2% and 3% of the population respectively. The remaining 5% of the population belong to small ethnic groups such as the remnants of the Anglo-Burmese and Anglo-Indian communities, as well as the Lisu, Rawang, Naga, Padaung, Moken, and many minorities across Shan State.
Language.
The official language and primary medium of instruction of Burma is Burmese (65%). Multiple languages are spoken in Burma, and include Shan (6.4%), Karen (5.2%), Kachin (1.8%), Chin (1.6%), Mon (1.5%), and Rakhine (1.5%). English is also spoken, particularly by the educated urban elite, and is the secondary language learnt in government schools.Burma influence on other languages making bamar language as the one and only official and dominant language
"CIA World Factbook" demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Population growth rate.
1.07% (2011 est.)
Education expenditures.
1.2% of total GDP (2001)

</doc>
<doc id="20390" url="http://en.wikipedia.org/wiki?curid=20390" title="Politics of Burma">
Politics of Burma

Burma (also known as Myanmar) is a unitary presidential constitutional republic under its 2008 constitution.
Political conditions.
Historically, Burma was a monarchy ruled by various dynasties prior to the 19th century. The British colonized Burma in the late 19th century, and it was under the jurisdiction of the British Raj until 1937.
Burma was ruled as a British colony from 1885 until 1948. While the Bamar heartland was directly administered (first as a part of India and then, from 1937, as British Burma), ethnic regions outside the heartland were allowed some measure of self-rule along the lines of the Princely States of India. This led to split loyalties among the various ethnic groups to outside powers (either to the British or Japanese) as well between the indigenous people in Burma. The dominant ethnic group in Burma are the Bamar, who make up approximately sixty-eight percent of the population. During World War II, many members of the Bamar ethnic group volunteered to fight alongside the Japanese in hopes of overthrowing the occupying British forces.
Meanwhile, many other ethnic groups supported the Allied forces in combating the Japanese and Burman forces. This conflict would come to be very significant in the aftermath of World War Two when Burma was granted its independence from Great Britain in 1948. By granting independence to Burma, the British government gave the new ruler, Aung San, control over areas that were not traditionally controlled by the Bamar. This conglomeration of formerly British-owned land created a state that is home to over twenty distinct minority ethnic groups.
From the time of the signing of the Burmese Constitution in 1948, ethnic minorities have been denied Constitutional rights, access to lands that were traditionally controlled by their peoples and participation in the government. The various minority ethnic groups have been consistently oppressed by the dominant Burman majority, but have also suffered at the hands of warlords and regional ethnic alliances. Religion also plays a role in the ethnic conflicts that have taken place. Muslims, Hindus, Christians and Buddhists all live in Burma. These religious differences have led to several incidents that have affected hundreds of thousands of citizens in Burma.
The SPDC had been responsible for the displacement of several hundred thousand citizens, both inside and outside of Burma. The Karen, Karenni, and Mon ethnic groups have been forced to seek asylum in neighboring Thailand, where they are also abused by an unfriendly and unsympathetic government. These groups are perhaps more fortunate than the Wa and Shan ethnic groups who have become Internally Displaced Peoples in their own state since being removed from lands by the military junta in 2000. There are reportedly 600,000 of these Internally Displaced Peoples living in Burma today. Many are trying to escape forced labour in the military or for one of the many state-sponsored drug cartels. This displacement of peoples has led to both human rights violations as well as the exploitation of minority ethnic groups at the hands of the dominant Burman group. The primary actors in these ethnic struggles include but are not limited to the Government of Burma (junta), the Karen National Union and the Mong Tai Army.
History.
Independence era.
On 4 January 1948, Burma achieved independence from Britain, and became a democracy based on the parliamentary system.
On the 19th of July 1947, Aung San became Deputy Chairman of the Executive Council of Burma, a transitional government. But in July 1947, political rivals assassinated Aung San and several cabinet members. On 4 January 1948, the nation became an independent republic, named the "Union of Burma", with Sao Shwe Thaik as its first President and U Nu as its first Prime Minister. Unlike most other former British colonies, it did not become a member of the Commonwealth. A bicameral parliament was formed, consisting of a Chamber of Deputies and a Chamber of Nationalities. The geographical area Burma encompasses today can be traced to the Panglong Agreement, which combined Burma proper, which consisted of Lower Burma and Upper Burma, and the Frontier Areas, which had been administered separately by the British.
AFPFL/Union Government.
In 1961, U Thant, then Burma's Permanent Representative to the United Nations and former Secretary to the Prime Minister, was elected Secretary-General of the United Nations; he was the first non-Westerner to head any international organization and would serve as UN Secretary-General for ten years. Among the Burmese to work at the UN when he was Secretary-General was a young Aung San Suu Kyi.
Military socialist era.
In 1962, General Ne Win led a coup d'état and established a nominally socialist military government that sought to follow the "Burmese Way to Socialism." The military expropriated private businesses and followed an economic policy of autarky, or economic isolation.
There were sporadic protests against military rule during the Ne Win years and these were almost always violently suppressed. On 7 July 1962, the government broke up demonstrations at Rangoon University, killing 15 students. In 1974, the military violently suppressed anti-government protests at the funeral of U Thant. Student protests in 1975, 1976 and 1977 were quickly suppressed by overwhelming force.
SPDC era.
The former Head of state was Senior General Than Shwe who held the title of "Chairman of the State Peace and Development Council." His appointed prime minister was Khin Nyunt until 19 October 2004, when he was forcibly deposed in favor of Gen. Soe Win. Almost all cabinet offices are held by military officers.
US and European government sanctions against the military government, combined with consumer boycotts and shareholder pressure organized by Free Burma activists, have succeeded in forcing most western corporations to withdraw from Burma. However, some western oil companies remain due to loopholes in the sanctions. For example, the French oil company Total S.A. and the American oil company Chevron continue to operate the Yadana natural gas pipeline from Burma to Thailand. Total (formerly TotalFinaElf) is the subject of a lawsuit in French and Belgian courts for alleged complicity in human rights abuses along the gas pipeline. Before it was acquired by Chevron, Unocal settled a similar lawsuit for a reported multi-million dollar amount. Asian businesses, such as Daewoo, continue to invest in Burma, particularly in natural resource extraction.
The United States and European clothing and shoe industry became the target of Free Burma activists for buying from factories in Burma that were wholly or partly owned by the government or the military. Many stopped sourcing from Burma after protests, starting with Levi Strauss in 1992. From 1992 to 2003, Free Burma activists successfully forced dozens of clothing and shoe companies to stop sourcing from Burma. These companies included Eddie Bauer, Liz Claiborne, Macy's, J. Crew, JoS. A. Banks, Children's Place, Burlington Coat Factory, Wal-Mart, and Target. The U.S. government banned all imports from Burma as part of the "Burmese Freedom and Democracy Act" of 2003. Sanctions have been criticized for their adverse effects on the civilian population. However, Burmese democracy movement leader Aung San Suu Kyi has repeatedly credited sanctions for putting pressure on the ruling military regime.
Human Rights Watch and Amnesty International have documented egregious human rights abuses by the military government. Civil liberties are severely restricted. Human Rights Defenders and Promoters, formed in 2002 to raise awareness among the people of Burma about their human rights, claims that on 18 April 2007, several of its members were met by approximately a hundred people led by a local USDA Secretary U Nyunt Oo and beaten up. The HRDP believes that this attack was condoned by the authorities.
There is no independent judiciary in Burma and the military government suppresses political activity. The government restricts Internet access, including blocking of Google, Gmail, Yahoo, and Hotmail. The government uses software-based filtering from US company Fortinet to limit the materials citizens can access on-line, including free email services, free web hosting and most political opposition and pro-democracy pages.
In 2001, the government permitted NLD office branches to re-open throughout Burma. However, they were shut down or heavily restricted beginning 2004, as part of a government campaign to prohibit such activities. In 2006, many members resigned from NLD, citing harassment and pressure from the Tatmadaw (Armed Forces) and the Union Solidarity and Development Association.
The military government placed Aung San Suu Kyi under house arrest again on 31 May 2003, following an attack on her convoy in northern Burma by a mob reported to be in league with the military. The regime extended her house arrest for yet another year in late November 2005. Despite a direct appeal by Kofi Annan to Than Shwe and pressure from ASEAN, the Burmese government extended Aung San Suu Kyi's house arrest another year on 27 May 2006. She was released in 2010.
The United Nations urged the country to move towards inclusive national reconciliation, the restoration of democracy, and full respect for human rights. In December 2008, the United Nations General Assembly passed a resolution condemning the human rights situation in Burma and calling for Aug San Suu Kyi's release—80 countries voting for the resolution, 25 against and 45 abstentions. Other nations, such as China and Russia, have been less critical of the regime and prefer to cooperate on economic matters.
Facing increasing international isolation, Burma's military government agreed to embark upon a programme of reform, including permitting multiple political parties to contest elections in 2010 and 2012 and the release of political prisoners. However, organisations such as Human Rights Watch allege continued human rights abuses in ongoing conflicts in border regions such as Kachin State.
New constitution.
Myanmar's army-drafted constitution was overwhelmingly approved (by 92.4% of the 22 million voters with alleged voter turnout of 99%) on 10 May in the first phase of a two-stage referendum amid Cyclone Nargis. It was the first national vote since the 1990 election. Multi-party elections in 2010 would end 5 decades of military rule, as the new charter gives the military an automatic 25% of seats in parliament. NLD spokesman Nyan Win, inter alia, criticized the referendum: "This referendum was full of cheating and fraud across the country. In some villages, authorities and polling station officials ticked the ballots themselves and did not let the voters do anything".
2010 Election.
An election was held in 2010, with 40 parties approved to contest the elections by the Electoral Commission. some of which are linked to ethnic minorities. The National League for Democracy, which overwhelmingly won the previous 1990 elections but were never allowed to take power, decided not to participate.
The military-backed Union Solidarity and Development Party declared victory, winning 259 of the 330 contested seats. The United Nations and many Western countries have condemned the elections as fraudulent, although the decision to hold elections was praised by China and Russia.
2012 By Elections.
In by-elections held in 2012, the main opposition party National League for Democracy, which was only re-registered for the by-elections on 13 December 2011 won in 43 of the 44 seats they contested (out of 46). Significantly, international observers were invited to monitor the elections, although the government was criticised for placing too many restrictions on election monitors, some of whom were denied visas.
The Union Solidarity and Development Party said it would lodge official complaints to the Union Election Commission on poll irregularities, voter intimidation, and purported campaign incidents that involved National League for Democracy members and supporters, while the National League for Democracy also sent an official complaint to the commission, regarding ballots that had been tampered with.
However, President Thein Sein remarked that the by-elections were conducted "in a very successful manner", and many foreign countries have indicated willingness to lift or loosen sanctions on Burma and its military leaders.
Executive branch.
The President is the head of state and head of government. He oversees the Cabinet of Burma.
Legislative branch.
Under the 2008 Constitution the legislative power of the Union is shared among the "Pyidaungsu Hluttaw", State and Region Hluttaws. The "Pyidaungsu Hluttaw" consists of the People's Assembly ("Pyithu Hluttaw") elected on the basis of township as well as population, and the House of Nationalities ("Amyotha Hluttaw") with on an equal number of representatives elected from Regions and States. The People's Assembly consists of 440 representatives, with 110 being military personnel nominated by the Commander-in-Chief of the Defence Services. The House of Nationalities consists of 224 representatives with 56 being military personnel nominated by the Commander-in-Chief of the Defence Services.
Judicial system.
Burma's judicial system is limited. British-era laws and legal systems remain much intact, but there is no guarantee of a fair public trial. The judiciary is independent of the executive branch. Burma does not accept compulsory International Court of Justice jurisdiction. The highest court in the land is the Supreme Court. The Chief Justice of the Supreme Court is Tun Tun Oo, and Attorney General is Dr Tun Shin.
"Wareru dhammathat".
Wareru dhammathat or the Manu dhammathat (မနုဓမ္မသတ်) was the earliest law-book in Burma. It consists of laws ascribed to the ancient Indian sage, Manu, and brought to Burma by Hindu colonists. The collection was made at Wareru’s command, by monks from the writings of earlier Mon scholars preserved in the monasteries of his kingdom. (Wareru seized Martaban in 1281 and obtained the recognition of China as the ruler of Lower Burma and founded a kingdom which lasted until 1539. Martaban was its first capital, and remained so until 1369. It stretched southwards as far as Tenasserim.)
"Dhammazedi pyatton".
Mon King Dhammazedi (1472–92) was the greatest of the Mon rulers of Wareru’s line. He was famous for his wisdom and the collection of his rulings were recorded in the Kalyani stone inscriptions and known as the Dammazedi pyatton.
Administrative divisions.
Burma is divided into seven regions (previously called divisions) divisions ("taing") and seven states ("pyi-nè"), classified by ethnic composition. The seven regions are Ayeyarwady Region, Bago Division, Magway Division, Mandalay Division, Sagaing Division, Tanintharyi Division and Yangon Division; the seven states are Chin State, Kachin State, Kayin State, Kayah State, Mon State, Rakhine State and Shan State.
There are also five Self-administrated zones and a Self-administrated Division "for National races with suitable population"
Within the Sagain Region
Within the Shan State 
International organization participation.
AsDB, ASEAN, CCC, CP, ESCAP, FAO, G-77, IAEA, IBRD, ICAO, ICRM, IDA, IFAD, IFC, IFRCS, IMF, IMO, Intelsat (nonsignatory user), Interpol, IOC, ITU, NAM, OPCW, UN, UNCTAD, UNESCO, UNIDO, UPU, WHO, WMO, WToO, WTrO, GJC.
External links.
Burmese democracy and human rights online media.
There are a number of web sites for more information, including the following:

</doc>
<doc id="20391" url="http://en.wikipedia.org/wiki?curid=20391" title="Economy of Burma">
Economy of Burma

The Economy of Burma (Myanmar) is an emerging economy with an estimated nominal GDP of $59.43 billion and a purchasing power adjusted GDP of $111.1 billion. Real growth rate is estimated at 5.5% for the 2011 fiscal year.
Historically, Burma was the main trade route between India and China since 100 BC. The Mon Kingdom of lower Burma served as important trading center in the Bay of Bengal. After Burma was conquered by British, it became the wealthiest country in Southeast Asia. It was also once the world's largest exporter of rice. It produced 75% of the world's teak and had a highly literate population.
After a parliamentary government was formed in 1948, Prime Minister U Nu embarked upon a policy of nationalization. The government also tried to implement a poorly thought out Eight-Year plan. By the 1950s, rice exports had fallen by two thirds and mineral exports by over 96%. The 1962 coup d'état was followed by an economic scheme called the Burmese Way to Socialism, a plan to nationalize all industries. The catastrophic program turned Burma into one of the world's most impoverished countries.
In 2011, when new President Thein Sein's government came to power, Burma embarked on a major policy of reforms including anti-corruption, currency exchange rate, foreign investment laws and taxation. Foreign investments increased from US$300 million in 2009-10 to a US$20 billion in 2010-11 by about 667%. Large inflow of capital results in stronger Burmese currency, kyat by about 25%. In response, the government relaxed import restrictions and abolished export taxes. Despite current currency problems, Burmese economy is expected to grow by about 8.8% in 2011. After the completion of 58-billion dollar Dawei deep seaport, Burma is expected be at the hub of trade connecting Southeast Asia and the South China Sea, via the Andaman Sea, to the Indian Ocean receiving goods from countries in the Middle East, Europe and Africa, and spurring growth in the ASEAN region.
According to a report released on 30 May 2013 by the McKinsey Global Institute, Burma's economy is expected to quadruple by 2030 if it invests in more high-tech industries.
History.
Classical era.
According to Michael Adas, Ian Brown, and other economic historians of Burma, Burma's pre-colonial economy in Burma was essentially a subsistence economy, with the majority of the population involved in rice production and other forms of agriculture. Burma also lacked a formal monetary system until the reign of King Mindon Min in the middle 19th century.
All land was technically owned by the Burmese monarch. Exports, along with oil wells, gem mining and teak production were controlled by the monarch. Burma was vitally involved in the Indian Ocean trade. Logged teak was a prized export that was used in European shipbuilding, because of its durability, and became the focal point of the Burmese export trade from the 1700s to the 1800s.
British Burma (1885 - 1948).
During British occupation, Burma was the second wealthiest country in Southeast Asia after the Philippines. It was also once the world's largest exporter of rice. During British administration, Burma supplied oil through the Burmah Oil Company. Burma also had a wealth of natural and labor resources. It produced 75% of the world's teak and had a highly literate population. The country was believed to be on the fast track to development.
Post-independence (1948-).
After a parliamentary government was formed in 1948, Prime Minister U Nu attempted to make Burma a welfare state and adopted central planning. Rice exports fell by two thirds and mineral exports by over 96%. Plans were partly financed by printing money, which led to inflation.
The 1962 coup d'état was followed by an economic scheme called the Burmese Way to Socialism, a plan to nationalize all industries, with the exception of agriculture. The catastrophic program turned Burma into one of the world's most impoverished countries. Burma's admittance to least developed country status by the United Nations in 1987 highlighted its economic bankruptcy.
Military rule (1988 - 2011).
After 1988, the regime retreated from totalitarian socialism. It permitted modest expansion of the private sector, allowed some foreign investment, and received much needed foreign exchange. The economy is rated in 2009 as the least free in Asia (tied with North Korea). All fundamental market institutions are suppressed. Private enterprises are often co-owned or indirectly owned by state. The corruption watchdog organization Transparency International in its 2007 Corruption Perceptions Index released on 26 September 2007 ranked Burma the most corrupt country in the world, tied with Somalia.
The national currency is the kyat. Burma currently has a dual exchange rate system similar to Cuba. The market rate was around two hundred times below the government-set rate in 2006. In 2011, the Burmese government enlisted the aid of International Monetary Fund to evaluate options to reform the current exchange rate system, to stabilize the domestic foreign exchange trading market and creates economic distortions. The dual exchange rate system allows for the government and state-owned enterprises to divert funds and revenues, but also gives the government more control over the local economy and temporarily subdue inflation.
Inflation averaged 30.1% between 2005 and 2007. Inflation is a serious problem for the economy. In April 2007, the National League for Democracy organized a two-day workshop on the economy. The workshop concluded that skyrocketing inflation was impeding economic growth. "Basic commodity prices have increased from 30% to 60% since the military regime promoted a salary increase for government workers in April 2006," said Soe Win, the moderator of the workshop. "Inflation is also correlated with corruption." Myint Thein, an NLD spokesperson, added: "Inflation is the critical source of the current economic crisis."
In recent years, both China and India have attempted to strengthen ties with the government for economic benefit. Many nations, including the United States and Canada, and the European Union, have imposed investment and trade sanctions on Burma. The United States banned all imports from Burma, though this restriction was since lifted. Foreign investment comes primarily from People's Republic of China, Singapore, South Korea, India, and Thailand.
Economic liberalization (2011-present).
In 2012, the Asian Development Bank formally began re-engaging with the country, to finance infrastructure and development projects in the country.
. The $512 million loan is the first issued by the ADB to Myanmar in 30 years and will target banking services, ultimately leading to other major investments in road, energy, irrigation and education projects.
In March 2012, a draft foreign investment law emerged, the first in more than 2 decades. This law would oversee unprecedented liberalization of the economy. Foreigners will no longer require a local partner to start a business in the country, and will be able to legally lease land. The draft law also stipulates that Burmese citizens must constitute at least 25% of the firm's skilled workforce, and with subsequent training, up to 50-75%. The draft includes a proposal to transform the Myanmar Investment Commission from a government-appointed body into an independent board. This could bring greater transparency to the process of issuing investment licenses, according to the proposed reforms drafted by experts and senior officials.
In a first ever countrywide study the Myanmar government found that 37 per cent of the nation’s population are unemployed and an average of 26 per cent live in poverty.
Myanmar on January 28 has announced deals with international lenders to cancel or refinance nearly $6 billion of its debt, almost 60 per cent of what it owes to foreign lenders. Japan wrote off US$3 Billion, nations in the group of Paris Club wrote off US$2.2 Billion and Norway wrote off US$534 Million.
Myanmar's inward foreign direct investment has steadily increased since its reform. The country approved US$4.4 billion worth of investment projects between January and November 2014. 
Industries.
The major agricultural produce is rice which covers about 60% of the country's total cultivated land area. Rice accounts for 97% of total food grain production by weight. Through collaboration with the International Rice Research Institute (IRRI), 52 modern rice varieties were released in the country between 1966 and 1997, helping increase national rice production to 14 million tons in 1987 and to 19 million tons in 1996. By 1988, modern varieties were planted on half of the country's rice fields, including 98% of the irrigated areas. In 2011, Myanmar's total milled rice production accounted for 10.26 million tons, an increase from the 1.8 per cent back in 2010.
In northern Burma opium, bans have ended a century old tradition of growing poppy. Between 20,000 and 30,000 ex-poppyfarmers left the Kokang region as a result of the ban in 2002. People from the Wa region, where the ban was implemented in 2005, fled to areas where growing opium is still possible. Other ex-poppyfarmers are being relocated to areas near rubber plantations. These are often mono-plantations from Chinese investors.
Rubber plantations are being promoted in areas of high elevation like Mong Mao. Sugar plantations are grown in the lowlands such as Mong Pawk District.
The lack of an educated workforce skilled in modern technology contributes to the growing problems of the economy.
Today, the country lacks adequate infrastructure. Goods travel primarily across the Thai border (where most illegal drugs are exported) and along the Ayeyarwady River. Railroads are old and rudimentary, with few repairs since their construction in the late nineteenth century. Highways are normally unpaved, except in the major cities. Energy shortages are common throughout the country including in Yangon. More than 45 million of the country's population is without electricity, with 70 per cent of people living in rural areas.
Burma is also the world's second largest producer of opium, accounting for 8% of entire world production and is a major source of illegal drugs, including amphetamines. Other industries include agricultural goods, textiles, wood products, construction materials, gems, metals, oil and natural gas.
The private sector dominates in agriculture, light industry, and transport activities, while the military government controls energy, heavy industry, and rice trade.
Garment production.
In March 2012, 6 of Thailand's largest garment manufacturers announced that they would move production to Burma, principally to the Yangon area, citing lower labor costs.
Illegal drug trade.
Burma (Myanmar) is the largest producer of methamphetamines in the world, with the majority of "ya ba" found in Thailand produced in Burma, particularly in the Golden Triangle and Northeastern Shan State, which borders Thailand, Laos and China. Burmese-produced "Ya ba" is typically trafficked to Thailand via Laos, before being transported through the northeastern Thai region of Isan.
In 2010, Burma trafficked 1 billion tablets to neighboring Thailand. In 2009, Chinese authorities seized over 40 million tablets that had been illegally trafficked from Burma. Ethnic militias and rebel groups (in particular the United Wa State Army) are responsible for much of this production; however, the Burmese military units are believed to be heavily involved in the trafficking of the drugs.
Burma is also the 2nd largest supplier of opium (following Afghanistan) in the world, with 95% of opium grown in Shan State. Illegal narcotics have generated $1 to $2 billion USD in exports annually, with estimates of 40% of the country's foreign exchange coming from drugs. Efforts to eradicate opium cultivation have pushed many ethnic rebel groups, including the United Wa State Army and the Kokang to diversify into methamphetamine production.
Prior to the 1980s, heroin was typically transported from Burma to Thailand, before being trafficked by sea to Hong Kong, which was and still remains the major transit point at which heroin enters the international market. Now, drug trafficking has circumvented to southern China (from Yunnan, Guizhou, Guangxi, Guangdong) because of a growing market for drugs in China, before reaching Hong Kong.
The prominence of major drug traffickers have allowed them to penetrate other sectors of the Burmese economy, including the banking, airline, hotel and infrastructure industries. Their investment in infrastructure have allowed them to make more profits, facilitate drug trafficking and money laundering.
Gemstones.
The Union of Myanmar's rulers depend on sales of precious stones such as sapphires, pearls and jade to fund their regime. Rubies are the biggest earner; 90% of the world's rubies come from the country, whose red stones are prized for their purity and hue. Thailand buys the majority of the country's gems. Burma's "Valley of Rubies", the mountainous Mogok area, 200 km north of Mandalay, is noted for its rare pigeon's blood rubies and blue sapphires.
In 2007, following the crackdown on pro-democracy protests in Myanmar, human rights organizations, gem dealers, and US First Lady Laura Bush called for a boycott of a Myanmar gem auction held twice yearly, arguing that the sale of the stones profits the dictatorial regime in that country. Debbie Stothard of the Alternative ASEAN Network on Burma stated that mining operators used drugs on employees to improve productivity, with needles shared, raising the risk of HIV infection: "These rubies are red with the blood of young people." Brian Leber (41-year-old jeweler who founded The Jewellers' Burma Relief Project) stated that: "For the time being, Burmese gems should not be something to be proud of. They should be an object of revulsion. It's the only country where one obtains really top quality rubies, but I stopped dealing in them. I don't want to be part of a nation's misery. If someone asks for a ruby now I show them a nice pink sapphire."
Richard W. Hughes, author of Ruby and Sapphire, a Bangkok-based gemologist who has made many trips to Burma makes the point that for every ruby sold through the junta, another gem that supports subsistence mining is smuggled over the Thai border. Burma's gemstone industry is a cornerstone of the Burmese economy with exports topping $1 billion.
The permits for new gem mines in Mogoke, Mineshu and Nanyar state will be issued by the ministry according to a statement issued by the ministry on February 11. While many sanctions placed on the former regime were eased or lifted in 2012, the US has left restrictions on importing rubies and jade from Myanmar intact. According to recent amendments to the new Myanmar foreign investment law, there is no longer a minimum capital requirement for investments, except in mining ventures, which require substantial proof of capital and must be documented through a domestic bank. Another important clarification in the investment law is the dropping of foreign ownership restrictions in joint ventures, except in restricted sectors, such as mining, where FDI will be capped at 80 per cent.
Tourism.
Since 1992, the government has encouraged tourism. Until 2008, fewer than 750,000 tourists entered the country annually, but there has been substantial growth over the past years. In 2012, 1.06 million tourists visited the country, and 1.8 million are expected to visit by the end of 2013.
Tourism is thus a growing sector of the economy of Burma. Burma has diverse and varied tourist attractions and is served internationally by numerous airlines via direct flights. Domestic and foreign airlines also operate flights within the country. Cruise ships also dock at Yangon. Overland entry with a border pass is permitted at several border checkpoints. The government requires a valid passport with an entry visa for all tourists and business people. As of May 2010, foreign business visitors from any country can apply for a visa on arrival when passing through Yangon and Mandalay international airports without having to make any prior arrangements with travel agencies. Both the tourist visa and business visa are valid for 28 days, renewable for an additional 14 days for tourism and 3 months for business. Seeing Burma through a personal tour guide is popular. Travelers can hire guides through travel agencies.
 Aung San Suu Kyi has requested that international tourists not visit Burma. The junta's forced labour programmes were focused around tourist destinations which have been heavily criticised for their human rights records. Even disregarding the obviously governmental fees, Burma’s Minister of Hotels and Tourism Major-General Saw Lwin recently admitted that the government receives a significant percentage of the income of private sector tourism services. Not to mention the fact that only a very small minority of impoverished ordinary people in Burma ever see any money with any relation to tourism.
Much of the country is completely off-limits to tourists, and the military very tightly controls interactions between foreigners and the people of Burma. They are not to discuss politics with foreigners, under penalty of imprisonment, and in 2001, the Myanmar Tourism Promotion Board issued an order for local officials to protect tourists and limit "unnecessary contact" between foreigners and ordinary Burmese people.
Macro-economic trends.
This is a chart of trend of gross domestic product of Burma at market prices by the International Monetary Fund and EconStats with figures in millions of Myanma kyats.
Foreign investment.
Though foreign investment has been encouraged, it has so far met with only moderate success. This is because foreign investors have been adversely affected by the junta government policies and because of international pressure to boycott the junta government. The United States has placed trade sanctions on Burma. The European Union has placed embargoes on arms, non-humanitarian aid, visa bans on military regime leaders, and limited investment bans. Both the European Union and the U.S. have placed sanctions on grounds of human rights violations in the country. Many nations in Asia, particularly India, Thailand and China have actively traded with Burma. However, on April 22 the EU suspended economic and political sanctions against Burma.
The public sector enterprises remain highly inefficient and also privatization efforts have stalled. The estimates of Burmese foreign trade are highly ambiguous because of the great volume of black market trading. A major ongoing problem is the failure to achieve monetary and fiscal stability. Due to this, Burma remains a poor country with no improvement of living standards for the majority of the population over the past decade. The main causes for continued sluggish growth are poor government planning, internal unrest, minimal foreign investment and the large trade deficit. One of the recent government initiatives is to utilize Burma's large natural gas deposits. Currently, Burma has attracted investment from Thai, Malaysian, Filipino, Russian, Australian, Indian, and Singaporean companies. Trade with the US amounted to $243.56 million as of February 2013, accounting for 15 projects and just 0.58 per cent of the total, according to government statistics.
The Economist special report on Burma points to increased economic activity resulting from Burma's political transformation and influx of foreign direct investment from Asian neighbors. Near the Mingaladon Industrial Park, for example, Japanese-owned factories have risen from the "debris" caused by “decades of sanctions and economic mismanagement.” Japanese Prime Minister Shinzo Abe has identified Burma as economically attractive market that will help stimulate the Japanese economy. Among its various enterprises, Japan is helping build the Thilawa Port, which is part of the Thilawa Special Economic Zone, and helping fix the electricity supply in Yangon.
Japan isn’t the largest investor in Myanmar. “Thailand, for instance, the second biggest investor in Myanmar after China, is forging ahead with a bigger version of Thilawa at Dawei, on Myanmar’s Tenasserim Coast. . . Thai rulers have for centuries been toying with the idea of building a canal across the Kra Isthmus, linking the Gulf of Thailand directly to the Andaman Sea and the Indian Ocean to avoid the journey round peninsular Malaysia through the Strait of Malacca.”
Dawei would give Thailand that connection. China, by far the biggest investor in Burma, has focused on constructing oil and gas pipelines that “crisscross the country, starting from a new terminus at Kyaukphyu, just below Sittwe, up to Mandalay and on to the Chinese border town of Ruili and then Kunming, the capital of Yunnan province.” This would prevent China from “having to funnel oil from Africa and the Middle East through the bottleneck around Singapore.”
According to the CIA World Factbook,
Burma, a resource-rich country, suffers from pervasive government controls, inefficient economic policies, and rural poverty. The junta took steps in the early 1990s to liberalize the economy after decades of failure under the "Burmese Way to Socialism," but those efforts stalled, and some of the liberalization measures were rescinded. Burma does not have monetary or fiscal stability, so the economy suffers from serious macroeconomic imbalances - including inflation, multiple official exchange rates that overvalue the Burmese kyat, and a distorted interest rate regime. Most overseas development assistance ceased after the junta began to suppress the democracy movement in 1988 and subsequently refused to honor the results of the 1990 legislative elections. In response to the government of Burma's attack in May 2003 on Aung San Suu Kyi and her convoy, the US imposed new economic sanctions against Burma - including a ban on imports of Burmese products and a ban on provision of financial services by US persons. A poor investment climate further slowed the inflow of foreign exchange. The most productive sectors will continue to be in extractive industries, especially oil and gas, mining, and timber. Other areas, such as manufacturing and services, are struggling with inadequate infrastructure, unpredictable import/export policies, deteriorating health and education systems, and corruption. A major banking crisis in 2003 shuttered the country's 20 private banks and disrupted the economy. As of December 2005, the largest private banks operate under tight restrictions limiting the private sector's access to formal credit. Official statistics are inaccurate. Published statistics on foreign trade are greatly understated because of the size of the black market and unofficial border trade - often estimated to be as large as the official economy. Burma's trade with Thailand, China, and India is rising. Though the Burmese government has good economic relations with its neighbors, better investment and business climates and an improved political situation are needed to promote foreign investment, exports, and tourism.
Foreign aid.
The level of international aid to Burma ranks amongst the lowest in the world (and the lowest in the Southeast Asian region)—Burma receives the $4 per capita in development assistance, as compared to the average of $42.30 per capita.
In April 2007, the U.S. Government Accountability Office (GAO) identified the financial and other restrictions that the military government places on international humanitarian assistance in the Southeast Asian country. The GAO report, entitled "Assistance Programs Constrained in Burma," outlines the specific efforts of the Burmese government to hinder the humanitarian work of international organizations, including by restricting the free movement of international staff within the country. The report notes that the regime has tightened its control over assistance work since former Prime Minister Khin Nyunt was purged in October 2004.
Furthermore, the reports states that the military government passed guidelines in February 2006, which formalized Burma's restrictive policies. According to the report, the guidelines require that programs run by humanitarian groups "enhance and safeguard the national interest" and that international organizations coordinate with state agents and select their Burmese staff from government-prepared lists of individuals. United Nations officials have declared these restrictions unacceptable.
The shameful behavior of Burma's military regime in tying the hand of humanitarian organizations is laid out in these pages for all to see, and it must come to an end," said U.S. Representative Tom Lantos (D-CA). "In eastern Burma, where the military regime has burned or otherwise destroyed over 3,000 villages, humanitarian relief has been decimated. At least one million people have fled their homes and many are simply being left to die in the jungle."
U.S. Representative Ileana Ros-Lehtinen (R-FL) said that the report "underscores the need for democratic change in Burma, whose military regime arbitrarily arrests, tortures, rapes and executes its own people, ruthlessly persecutes ethnic minorities, and bizarrely builds itself a new capital city while failing to address the increasingly urgent challenges of refugee flows, illicit narcotics and human trafficking, and the spread of HIV/AIDS and other communicable diseases." 
Other statistics.
Electricity - production:
5.961 billion kWh (2006 est.)
Electricity - consumption:
4.298 billion kWh (2006 est.)
Electricity - exports:
0 kWh (2007)
Electricity - imports:
0 kWh (2007)
Agriculture - products:
rice, pulses, beans, sesame, groundnuts, sugarcane; hardwood; fish and fish products
Currency:
1 kyat (K) = 100 pyas
Exchange rates:
kyats per US dollar - 1,205 (2008 est.), 1,296 (2007), 1,280 (2006), 5.82 (2005), 5.7459 (2004), 6.0764 (2003)
note: unofficial exchange rates ranged in 2004 from 815 kyat/US dollar to nearly 970 kyat/US dollar, and by year end 2005, the unofficial exchange rate was 1,075 kyat/US dollar; data shown for 2003-05 are official exchange rates
Foreign Direct Investment
In the first nine months of 2012-2013, Myanmar has received investment of USD 794 million. China has biggest of investment commitments for this fiscal.
Foreign Trade
Total foreign trade for 2012 was recorded to USD 13.3 billion. It was 27% of Myanmar's GDP.
Impact on population.
The current state of the Burmese economy has also had a significant impact on the demographics of Burma, as economic hardship results in extreme delays of marriage and family building. The average age of marriage in Burma is 27.5 for men, 26.4 for women, almost unparalleled in the region, with the exception of developed countries like Singapore.
Burma also has a low fertility rate, of 2.07 children per woman (2010), especially as compared to other Southeast Asian countries of similar economic standing, like Cambodia (3.18) and Laos (4.41), representing a significant decline from 4.7 in 1983, despite the absence of a national population policy. This is at least partly attributed to the economic strain that additional children place on the family income, and has resulted in the prevalence of illegal abortions in the country, as well as use of other forms of birth control.

</doc>
<doc id="20392" url="http://en.wikipedia.org/wiki?curid=20392" title="Telecommunications in Burma">
Telecommunications in Burma

All communications in Burma are controlled by the government.
Telecommunication networks.
Installation of telephones and the cost of calling are prohibitively expensive for most people. To call overseas for two minutes would cost more than most earn in a month.
Telephones - main lines in use:
503,900 (2005)
Telephones - mobile cellular:
5,400,000 (2012) 
Telephone system:
<br>"general assessment:"
meets minimum requirements for local and intercity service for business and government
<br>"domestic:"
system barely capable of providing basic service; cellular phone system is grossly underdeveloped with a subscribership base of less than 1 per 100 persons
<br>"international:"
country code - 95; landing point for the SEA-ME-WE 3 optical telecommunications submarine cable that provides links to Asia, the Middle East, and Europe; satellite earth stations - 2, Intelsat (Indian Ocean) and ShinSat (2007)
Bids have been offered for two fresh telecom licenses by the Myanmar Government. The deadline is set to be Feb 8, 2013. The licenses are expected to be issued in June and carry a contract duration of up to 20 years. Two more licenses are expected to offered following this round of bidding.
According to government statistics, 5.4 million of Myanmar’s 60 million population had a mobile phone subscription at end-2012, giving the country a mobile penetration of 9 per cent.
According to official figures released in mid 2012, Myanmar had 857 Base Transceiver Stations (BTS) for 1,654,667 local GSM mobile users, 188 BTSs for 225,617 local WCDMA mobile users, 366 BTSs for 633,569 local CDMA-450 mobile users, and 193 BTSs for 341,687 CDMA-800 mobile users. Huawei who has built 40 percent of the towers and ZTE has built 60 percent in Myanmar, which amounts to 1500 across the country, said it has built the towers mostly in Yangon, Mandalay and Naypyidaw.
The Myanmar Telecommunications Operator Tender Evaluation and Selection Committee selected Norwegian Telenor Group and Ooredoo of Qatar as winners of the bidding, for the two telecom licences issued by the government of Myanmar. The licenses allow the operators to build and operate a nationwide wireless network for 15 years. Ooredoo began selling low price SIM cards at a price of USD 1.5 in Yangon, Mandalay and Naypyidaw in August 2014.
Media.
Radio broadcast stations
AM 1, FM 2, shortwave 3 (2007)
Television broadcast stations:
4 (2008)
Press
Television
Radio
News agency
Internet.
The government now allows access to the entire Internet, a lot of people are using the internet freely. Usually with their mobiles.
Myanmar Teleport (formerly Bagan Cybertech), Information Technology Central Services (ITCS), and the state-owned Myanmar Post and Telecommunication (MPT) are the Internet service providers in Myanmar. Internet cafés are common in the larger cities of the country. Faster satellite internet connection is also available from Skynet, a satellite television provider. 
According to MPT's official statistics as of July 2010, the country had over 400,000 Internet users (0.8% of the population) with the vast majority of the users located in the two largest cities, Yangon and Mandalay.
Most of the people use the internet with the Mobile Data(Cellular Data) 
Edited 2014

</doc>
<doc id="20393" url="http://en.wikipedia.org/wiki?curid=20393" title="Transport in Burma">
Transport in Burma

The government of Myanmar (known as Burma before gaining independence from the British Empire) has two ministries controlling transportation:
Railways.
s of February 2008[ [update]], Burma had 5099 km of railways, all gauge. There are currently no rail links to adjacent countries.
Roadways.
<br>"total:" 27000 km
<br>"paved:" 3200 km
<br>"unpaved:" 23800 km (2006)
The main highways are as follows:
There is one expressway in the country, which features double carriageway and four lanes on its entire length:
Waterways.
12800 km; 3200 km navigable by large commercial vessels. (2008)
Orient-Express Hotels Ltd. operates its business in Ayeyarwady River by the name "Road to Mandalay River Cruise". Irrawaddy Flotilla Company was also in service along the Ayeyarwady River in the 20th century, until 2003.
Merchant marine.
<br>"total:"
24 ships (with a volume of  gross register tons (GRT) or over) totaling  GRT/ tonnes deadweight (DWT)
<br>"Ships by type:"
bulk carrier 1, cargo ship 17, passenger ship 2, passenger/cargo 3, specialized tanker 1 (2008)
<br>"note:"
a flag of convenience registry; includes ships of 3 countries: Cyprus 1, Germany 1, Japan 1
Airports.
In July 2010, the country had 69 airports. Only 11 of them had runways over 10,000 feet (3248 meters). Of the 11, only Yangon International and Mandalay International had adequate facilities to handle larger jets.
<br>"total:" 69
<br>"over 3,047 m:" 11
<br>"1524 to 3,047 m:" 27
<br>"Under 1524 m:" 31
Heliports.
4 (2007)
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="20394" url="http://en.wikipedia.org/wiki?curid=20394" title="Tatmadaw">
Tatmadaw

The Myanmar Armed Forces, officially known as Tatmadaw (Burmese: တပ်မတော်; MLCTS: "tap ma. taw", ]), is the military organization of Burma (Myanmar). The armed forces are administered by the Ministry of Defense and are composed of the Army, the Navy and the Air Force. Auxiliary services include Myanmar Police Force, People Militia Units and Frontier Forces, locally known as Na Sa Kha.
Currently, there is no military draft. Thus, all service personnel are volunteers in theory, although there is a law on the books (the People's Militia Law) that allows for conscription if the President considers it necessary for Myanmar's defense that the provisions of the law be activated. In practice, it has been claimed that the Tatmadaw conscripts adults and children and uses civilians as forced labour and even human mine-sweepers. The Tatmadaw has been engaged in a bitter battle with ethnic insurgents and the narco-armies since the country gained its independence from Great Britain in 1948. However, in a 2014 survey conducted by the International Republican Institute across all Myanmar demographics shows military is the most favorable institution with 84% of respondents saying either "very favorable" or "favorable" ahead of other institutions such as media, government and Burmese opposition.
The military proposed a defence budget of K 2.36 trillion (USD 2.39 billion) for 2014-15 and was approved by the Parliament. The incumbent Minister for Defence Wai Lwin revealed at a Parliament section on 28 October 2014 that 46.2 percent of the budget is spent on personal cost, 32.89 percent on operation & procurement, 14.49 percent on construction related projects and 2.76 percent on health and education.
History.
Burmese Monarchy.
The Royal Armed Forces was the armed forces of the Burmese monarchy from the 9th to 19th centuries. It refers to the military forces of the Pagan Dynasty, the Ava Kingdom, the Toungoo Dynasty and the Konbaung Dynasty in chronological order. The army was one of the major armed forces of Southeast Asia until it was defeated by the British over a six-decade span in the 19th century.
The army was organized into a small standing army of a few thousand, which defended the capital and the palace, and a much larger conscription-based wartime army. Conscription was based on the ahmudan system, which required local chiefs to supply their predetermined quota of men from their jurisdiction on the basis of population in times of war. The wartime army also consisted of elephantry, cavalry, artillery and naval units.
Firearms, first introduced from China in the late 14th century, became integrated into strategy only gradually over many centuries. The first special musket and artillery units, equipped with Portuguese matchlocks and cannon, were formed in the 16th century. Outside the special firearm units, there was no formal training program for the regular conscripts, who were expected to have a basic knowledge of self-defense, and how to operate the musket on their own. As the technological gap between European powers widened in the 18th century, the army was dependent on Europeans' willingness to sell more sophisticated weaponry.
While the army had held its own against the armies of the kingdom's neighbors, its performance against more technologically advanced European armies deteriorated over time. While it defeated the Portuguese and French intrusions in the 17th and 18th centuries respectively, the army could not stop the advance of the British Empire in the 19th century, losing all three Anglo-Burmese wars. On 1 January 1886, the millennium-old Burmese monarchy and its military arm, the Royal Burmese Army, were formally abolished by the British.
British Burma (1824–1948).
The British used mainly Indian and Gurkha troops to conquer and pacify the country. In a divide-and-rule maneuver, the British enforced their rule in the province of Burma mainly with Indian troops later joined by indigenous military units of three indigenous ethnic minorities: Karens, Kachins and Chins. The British did not trust the Burmans. Before 1937, with few exceptions, no Burmans were allowed to serve in the military.
At the beginning of World War I, the only indigenous military regiment in the British India army, the 70th Burma Rifles, consisted of three battalions, made up of Karens, Kachins and Chins. During the war, the British relaxed the ban, raising a Burman battalion in the 70th Burma Rifles, a Burman company in the 85th Burma Rifles, and seven Burman Mechanical Transport companies. In addition, three companies of Burma Sappers and Miners, made up of mostly Burmans, and a company of Labour Corps, made up of Chins and Burmans, were also raised. All these units began their overseas assignment in 1917. The 70th Burma Rifles served in Egypt for garrison duties while the Burmese Labour Corps served in France. One company of Burma Sappers and Miners distinguished themselves in Mesopotamia at the crossing the Tigris.
After the war, the British stopped recruiting Burmans, and discharged all but one Burman companies had been abolished by 1925. The last Burman company of Burma Sappers and Miners too was disbanded in 1929. The British used Indian and ethnic minority dominated troops to ruthlessly put down ethnic majority dominated rebellions such as Saya San's peasant rebellion in 1930–1931. These policies would lead to long-term negative tensions among the country's ethnic groups. On 1 April 1937, Burma was made a separate colony, and Burmans were now eligible to join the army. But few Burmans bothered to join. Before World War II began, the British Burma Army consisted of Karen (27.8%), Chin (22.6%), Kachin (22.9%), and Burman 12.3%, without counting their British officer corps.
In December 1941, a group of Burmese independence activists founded the Burma Independence Army (BIA) with Japanese help. The army led by Aung San fought in the Burma Campaign on the side of the Imperial Japanese Army. Thousands of young men joined its ranks—reliable estimates range from 15,000 to 23,000. The great majority of the recruits were Burman, with little ethnic minority representation. Many of the fresh recruits lacked discipline. At Myaungmya in the Irrawaddy delta, an ethnic war broke out between Burman BIA men and Karens, with both sides responsible for massacres. The BIA was soon replaced with the Burma Defense Army, founded on 26 August 1942 with three thousand BIA veterans. The army became Burma National Army with Ne Win as its commander on 1 August 1943 when Burma received nominal independence. In late 1944, it had a strength of approximately 15,000.
Disillusioned by the Japanese occupation, the BNA switched sides, and joined the allied forces on 27 March 1945.
Post-independence (1948–present).
At the time of Myanmar's independence in 1948, the Tatmadaw was weak, small and disunited. Cracks appeared along the lines of ethnic background, political affiliation, organisational origin and different services. Its unity and operational efficiency was further weakened by the interference of civilians and politicians in military affairs, and the perception gap between the staff officers and field commanders. The most serious problem was the tension between Karen Officers, coming from the British Burma Army and Burman officers, coming from the Patriotic Burmese Force (PBF).
In accordance with agreement reached at the Kandy Conference in September 1945, the "Tatmadaw" was reorganised by incorporating the British Burma Army and the Patriotic Burmese Force. The officer corps shared by ex-PBF officers and officers from the British Burma Army and Army of Burma Reserve Organisation (ARBO). The British also decided to form what were known as "Class Battalions" based on ethnicity. There were a total of 15 rifle battalions at the time of independence and four of them were made up of former members of PBF. None of the influential positions within the War Office and commands were manned with former PBF Officers. All services including military engineers, supply and transport, ordnance and medical services, Navy and Air Force were commanded by former Officers from ABRO and British Burma Army.
The War Office was officially opened on 8 May 1948 under the Ministry of Defence and managed by a War Office Council chaired by the Minister of Defence. At the head of War Office was Chief of Staff, Vice Chief of Staff, Chief of Naval Staff, Chief of Air Staff, Adjutant General and Quartermaster General. Vice Chief of Staff, who was also Chief of Army Staff and the head of General Staff Office. VCS oversee General Staff matters and there were three branch offices: GS-1 Operation and Training, GS-2 Staff Duty and Planning; GS-3 Intelligence. Signal Corps and Field Engineering Corps are also under the command of General Staff Office.
According to the war establishment adopted on 14 April 1948, Chief of Staff was under the War Office with the rank of Major General. It was subsequently upgraded to a Lieutenant General. Vice Chief of Staff was a Brigadier General. The Chief of Staff was staffed with GSO-I with the rank of Lieutenant Colonel, three GSO-II with the rank of Major, four GSO-III with the rank of captain for operation, training, planning and intelligence, and one Intelligence Officer (IO). The Chief of Staff office also had one GSO-II and one GSO-III for field engineering, and the Chief Signal Officer and a GSO-II for signal. Directorate of Signal and Directorate Field Engineering are also under General Staff Office.
Under Adjutant General Office were Judge Advocate General, Military Secretary, Vice Adjutant General. The Adjutant General (AG) was a Brigadier General whereas the Judge Advocate General (JAG), Military Secretary (MS) and Vice Adjutant General (VAG) were Colonels. VAG handles adjutant staff matters and there were also three branch offices; AG-1 planning, recruitment and transfer; AG-2 discipline, moral, welfare, and education; AG-3 salary, pension, and other financial matters. The Medical Corps and the Provost Marshall Office were under the Adjutant General Office.
The Quarter Master General office also had three branch offices: QG-1 planning, procurement, and budget; QG-2 maintenance, construction, and cantonment; and QG-3 transportation. Under the QMG office were Garrison Engineering Corps, Electrical and Mechanical Engineering Corps, Military Ordnance Corps, and the Supply and Transport Corps.
Both AG and QMG office similar structure to the General Staff Office, but they only had three ASO-III and three QSO-III respectively.
The Navy and Air Force were separate services under the War office but under the Chief of Staff.
Reorganization in 1956.
As per War Office order No. (9) 1955 on 28 September 1955, the Chief of Staff become Commander in Chief, the Chief of Army Staff become Vice Chief of Staff (Army), the Chief of Naval Staff become Vice Chief of Staff (Navy) and the Chief of Air Staff become Vice Chief of Staff (Air).
On 1 January 1956, War Office was officially renamed as Ministry of Defence. General Ne Win became the first Chief of Staff of Tatmadaw (Myanmar Armed Forces) to command all three services - Army, Navy and Airforce - under a single unified command for the first time.
Brigadier General Aung Gyi was given the post of Vice Chief of Staff (Army). Brigadier General D. A Blake became commander of South Burma Subdistrict Command (SBSD) and Brigadier General Kyaw Zaw, a member of the Thirty Comrades, became Commander of North Burma Subdistrict Command (NBSD).
Caretaker Government.
Due to deteroriating political situations in 1957, the then Prime minister of Burma, U Nu invited General Ne Win to form a "Caretaker Government" and handed over power on 28 October 1958. Under the stewardship of the Military Caretaker Government, parliamentary elections were held in February 1960. Several high-ranking and senior officers were dismissed due to their involvement and supporting various political parties.
1962 Coup d'etat.
The elections of 1960 had put U Nu back as the Prime Minister and Pyidaungsu Party (Union Party) led civilian government resume control of the country.
On 2 March 1962, the then Chief of Staff of Armed Forces, General Ne Win staged a coup d'état and formed the "Union Revolutionary Council". Around midnight the troops began to move into Yangon to take up strategic position. Prime Minister U Nu and his cabinet ministers were taken into protective custody. At 8:50 am, General Ne Win announce the coup over the radio. He said "I have to inform you, citizens of the Union that Armed Forces have taken over the responsibility and the task of keeping the country's safety, owing to the greatly deteriorating conditions of the Union." 
The country would be ruled by the military for the next 12 years. The Burma Socialist Programme Party became the sole political party and it the majority of its full members were military. Government servants underwent military training and the Military Intelligence Service functioned as the secret police of the state.
1988 Coup d'etat.
At the height of the Four Eights Uprising against the socialist government, Former General Ne Win, who at the time was Chairman of the ruling Burma Socialist Programme Party (BSPP), issued a warning against potential protestors during a televised speech. He stated that if the "disturbances" continued the "Army would have to be called and I would like to make it clear that if the Army shoots, it has no tradition of shooting into the Air, it would shoot straight to hit".
Subsequently, the 22 Light Infantry Division, 33 Light Infantry Division and the 44 Light Infantry Division were redeployed to Yangon from front line fighting against ethnic insurgents in the Karen states. Battalions from three Light Infantry Divisions, augmented by infantry battalions under Yangon Regional Military Command and supporting units from Directorate of Artillery and Armour Corps were deployed during the suppression of protests in and around the then capital city of Yangon.
Initially, these troops were deployed in support of the then People's Police Force (now known as Myanmar Police Force) security battalions and to patrol the streets of the capital and to guard government offices and building. However, at midnight of 8 August 1988 troops from 22 Light Infantry Division guarding Yangon City Hall opened fire on unarmed protesters as the crack down against the protests began.
The armed forces under General Saw Maung formed a State Law and Order Restoration Council, repealed the constitution and declared martial law on 18 September 1988. By late September the military had complete control of the country.
Doctrine.
Post-independence/civil war era (1948-1958).
The initial development of Burmese military doctrine post-independence was developed in the early 1950s to cope with external threats from more powerful enemies with a strategy of Strategic Denial under conventional warfare. The perception of threats to state security was more external than internal threats. The internal threat to state security was managed through the use of a mixture of force and political persuasion. Lieutenant Colonel Maung Maung drew up defence doctrine based on conventional warfare concepts, with large infantry divisions, armoured brigades, tanks and motorised war with mass mobilisation for the war effort being the important element of the doctrine.
The objective was to contain the offensive of the invading forces at the border for at least three months, while waiting for the arrival of international forces, similar to the police action by international intervention forces under the directive of United Nations during the war on Korean peninsula. However, the conventional strategy under the concept of total war was undermined by the lack of appropriate command and control system, proper logistical support structure, sound economic bases and efficient civil defence organisations.
Kuomintang invasion.
At the beginning of the 1950s, while Tatmadaw was able to reassert its control over most part of the country, Kuomintang (KMT) troops under General Li Mai, with support from United States, invaded Burma and used the country's frontier as a springboard for attack against People's Republic of China, which in turn became the external threat to state security and sovereignty of Burma. The first phase of the doctrine was tested for the first time in Operation "Naga Naing" in February 1953 against invading KMT forces. The doctrine did not take into account logistic and political support for KMT from United States and as a result it failed to deliver the objectives and ended in humiliating defeat for the Tatmadaw.
The then Tatmadaw leadership argued that the excessive media coverage was partly to blame for the failure of Operation "Naga Naing". For example, Brigadier General Maung Maung pointed out that newspapers, such as the "Nation", carried reports detailing the training and troops positioning, even went as far to the name and social background of the commanders who are leading the operation thus losing the element of surprise. Colonel Saw Myint, who was second in command for the operation, also complained about the long lines of communications and the excessive pressure imposed upon the units for public relations activities in order to prove that the support of the people was behind the operation.
KMT invasion/Burma Socialist Programme Party era (1958-1988).
Despite failure, Tatmadaw continued to rely on this doctrine until the mid-1960s. The doctrine was under constant review and modifications throughout KMT invasion and gained success in anti-KMT operations in the mid and late 1950s. However, this strategy became increasingly irrelevant and unsuitable in the late 1950s as the insurgents and KMT changed their positional warfare strategy to hit and run guerrilla warfare.
At the 1958 Tatmadaw's annual Commanding Officers (COs) conference, Colonel Kyi Win submitted a report outlining the requirement for new military doctrine and strategy. He stated that 'Tatmadaw did not have a clear strategy to cope with insurgents', even though most of Tatmadaw's commanders were guerrilla fighters during the anti-British and Japanese campaigns during the Second World War, they had very little knowledge of anti-guerrilla or counterinsurgency warfare. Based upon Colonel Kyi Win's report, Tatmadaw begin developing an appropriate military doctrine and strategy to meet the requirements of counterinsurgency warfare.
This second phase of the doctrine was to suppress insurgency with people's war and the perception of threats to state security was more of internal threats. During this phase, external linkage of internal problems and direct external threats were minimised by the foreign policy based on isolation. It was common view of the commanders that unless insurgency was suppressed, foreign interference would be highly probable, therefore counterinsurgency became the core of the new military doctrine and strategy. Beginning in 1961, the Directorate of Military Training took charge the research for national defence planning, military doctrine and strategy for both internal and external threats. This included reviews of international and domestic political situations, studies of the potential sources of conflicts, collection of information for strategic planning and defining the possible routes of foreign invasion.
In 1962, as part of new military doctrine planning, principles of anti-guerrilla warfare were outlined and counterinsurgency-training courses were delivered at the training schools. The new doctrine laid out three potential enemies and they are internal insurgents, historical enemies with roughly an equal strength (i.e. Thailand), and enemies with greater strength. It states that in suppressing insurgencies, Tatmadaw must be trained to conduct long-range penetration with a tactic of continuous search and destroy. Reconnaissance, Ambush and all weather day and night offensive and attack capabilities along with winning the hearts and minds of people are important parts of anti-guerrilla warfare. For countering an historical enemy with equal strength, Tatmadaw should fight a conventional warfare under total war strategy, without giving up an inch of its territory to the enemy. For powerful enemy and foreign invaders, Tatmadaw should engage in total people's war, with a special focus on guerrilla strategy.
To prepare for the transition to the new doctrine, Brigadier General San Yu, the then Vice Chief of Staff (Army), sent a delegation led by Lieutenant Colonel Thura Tun Tin was sent to Switzerland, Yugoslavia, Czechoslovakia and East Germany in July 1964 to study organisation structure, armaments, training, territorial organisation and strategy of people's militias. A research team was also formed at General Staff Office within the War Office to study defence capabilities and militia formations of neighbouring countries.
The new doctrine of total people's war, and the strategy of anti-guerrilla warfare for counterinsurgency and guerrilla warfare for foreign invasion, were designed to be appropriate for Burma. The doctrine flowed from the country's independent and active foreign policy, total people's defence policy, the nature of perceived threats, its geography and the regional environment, the size of its population in comparison with those of its neighbours, the relatively underdeveloped nature of its economy and its historical and political experiences.
The doctrine was based upon 'three totalities': population, time and space (du-thone-du) and 'four strengths': manpower, material, time and morale (Panama-lay-yat). The doctrine did not develop concepts of strategic denial or counter-offensive capabilities. It relied almost totally on irregular low-intensity warfare, such as its guerrilla strategy to counter any form of foreign invasion. The overall counterinsurgency strategy included not only elimination of insurgents and their support bases with the 'four cut' strategy, but also the building and designation of 'white area' and 'black area' as well.
In April 1968, Tatmadaw introduced special warfare training programmes at "Command Training Centres" at various regional commands. Anti-Guerrilla warfare tactics were taught at combat forces schools and other training establishments with special emphasis on ambush and counter-ambush, counterinsurgency weapons and tactics, individual battle initiative for tactical independence, commando tactics, and reconnaissance. Battalion size operations were also practised in the South West Regional Military Command area. The new military doctrine was formally endorsed and adopted at the first party congress of the BSPP in 1971. BSPP laid down directives for "complete annihilation of the insurgents as one of the tasks for national defence and state security" and called for "liquidation of insurgents through the strength of the working people as the immediate objective". This doctrine ensures the role of Tatmadaw at the heart of national policy making.
Throughout BSPP era, the total people's war doctrine was solely applied in counterinsurgency operations, since Burma did not face any direct foreign invasion throughout the period. In 1985, the then Lieutenant General Saw Maung, Vice-Chief of Staff of Tatmadaw reminded his commanders during his speech at the Command and General Staff College:
In Myanmar, out of nearly 35 million people, the combined armed forces (army, navy and air force) are about two hundred thousand. In terms of percentage, that is about 0.01%. It is simply impossible to defend a country the size of ours with only this handful of troops... therefore, what we have to do in the case of foreign invasion is to mobilise people in accordance with the "total people's war" doctrine. In order to defend our country from aggressors, the entire population must be involved in the war effort as the support of people dictate the outcome of the war.
SLORC/SPDC era (1988-present).
The third phase of doctrinal development of Myanmar Armed Forces came after the military take over and formation of State Law and Order Restoration Council (SLORC) in September 1988 as part of armed forces modernization programme. The development was the reflection of sensitivity towards direct foreign invasion or invasion by proxy state during the turbulent years of the late 1980s and early 1990s, for example: the unauthorized presence of a US aircraft carrier Battle Group in Myanmar's territorial waters during the 1988 political uprising as evidence of an infringement of Myanmar's sovereignty. Also, the "Tatmadaw" leadership was concerned that foreign powers might arm the insurgents on the border to exploit the political situation and tensions in the country. This new threat perception, previously insignificant under the nation's isolationist foreign policy, led "Tatmadaw" leaders to review the defense capability and doctrine of the "Tatmadaw".
The third phase was to face the lower level external threats with a strategy of strategic denial under total people's defence concept. Current military leadership has successfully dealt with 17 major insurgent groups, whose 'return to legal fold' in the past decade has remarkably decreased the internal threats to state security, at least for the short and medium terms, even though threat perception of the possibility of external linkage to internal problems, perceived as being motivated by the continuing human rights violations, religious suppression and ethnic cleansing, remains high.
Within the policy, the role of the Tatmadaw was defined as a `modern, strong and highly capable fighting force'. Since the day of independence, the Tatmadaw has been involved in restoring and maintaining internal security and suppressing insurgency. It was with this background that Tatmadaw's "multifaceted" defence policy was formulated and its military doctrine and strategy could be interpreted as defence-in-depth. It was influenced by a number of factors such as history, geography, culture, economy and sense of threats.
The Tatmadaw has developed an 'active defence' strategy based on guerrilla warfare with limited conventional military capabilities, designed to cope with low intensity conflicts from external and internal foes, which threatens the security of the state. This strategy, revealed in joint services exercises, is built on a system of total people's defence, where the armed forces provide the first line of defence and the training and leadership of the nation in the matter of national defence.
It is designed to deter potential aggressors by the knowledge that defeat of the Tatmadaw's regular forces in conventional warfare would be followed by persistent guerrilla warfare in the occupied areas by people militias and dispersed regular troops which would eventually wear down the invading forces, both physically and psychologically, and leave it vulnerable to a counter-offensive. If the conventional strategy of strategic denial fails, then the Tatmadaw and its auxiliary forces will follow Mao's strategic concepts of 'strategic defensive', 'strategic stalemate' and 'strategic offensive'.
Over the past decade, through a series of modernisation programs, the Tatmadaw has developed and invested in better Command, Control, Communication and Intelligence system; real-time intelligence; formidable air defence system; and early warning systems for its 'strategic denial' and 'total people's defence' doctrine.
Organizational, Command and Control structure.
Before 1988.
Overall command of Tatmadaw (armed forces) rested with the country's highest-ranking military officer, a General, who acted concurrently as Defence Minister and Chief of Staff of Defence Services. He thus exercised supreme operational control over all three services, under the direction of the President, State Council and Council of Ministers. There was also a National Security Council which acted in advisory capacity. The Defence Minister cum Chief-of-Staff of Defence Services exercised day-to-day control of the armed forces and assisted by three Vice-Chiefs of Staff, one each for the army, navy and air force. These officers also acted as Deputy Ministers of Defence and commanders of their respective Services. They were all based at Ministry of Defence ("Kakweyay Wungyi Htana") in Rangoon/Yangon. It served as a government ministry as well as joint military operations headquarters.
The Joint Staff within the Ministry of Defence consisted of three major branches, one each for Army, Navy and Air Force, along with a number of independent departments. The Army Office had three major departments; the General (G) Staff to oversee operations, the Adjutant General's (A) Staff administration and the Quartermaster General's (Q) Staff to handle logistics. The General Staff consisted two Bureaus of Special Operations (BSO), which were created in April 1978 and June 1979 respectively.
These BSO are similar to "Army Groups" in Western armies, high level staff units formed to manage different theatres of military operations. They were responsible for the overall direction and coordination of the Regional Military Commands (RMC) with BSO-1 covering Northern Command (NC), North Eastern Command (NEC), North Western Command (NWC), Western Command (WC) and Eastern Command (EC). BSO-2 responsible for South Eastern Command (SEC), South Western Command (SWC), Western Command (WC) and Central Command (CC).
The Army's elite mobile Light Infantry Divisions (LID) were managed separately under a Staff Colonel. Under G Staff, there were also a number of directorates which corresponded to the Army's functional corps, such as Intelligence, Signals, Training, Armour and Artillery. The A Staff was responsible for the Adjutant General, Directorate of Medical Services and the Provost Marshal's Office. The Q Staff included the Directorates of Supply and Transport, Ordnance Services, Electrical and Mechanical Engineering, and Military Engineers.
The Navy and Air Force Offices within the Ministry were headed by the Vice Chiefs of Staff for those Services. Each was supported by a staff officer at full Colonel level. All these officers were responsible for the overall management of the various naval and air bases around the country, and the broader administrative functions such as recruitment and training.
Operational Command in the field was exercised through a framework of Regional Military Commands (RMC), the boundaries of which corresponded with the country's Seven States and Seven Divisions. The Regional Military Commanders, all senior army officers, usually of Brigadier General rank, were responsible for the conduct of military operations in their respective RMC areas. Depending on the size of RMC and its operational requirements, Regional Military Commanders have at their disposal 10 or more infantry battalions ("Kha La Ya").
1988 to 2005.
The Tatmadaw's organizational and command structure dramatically changed after the military coup in 1988. In 1990, the country's most senior army officer become a Senior General (equivalent to Field Marshal rank in Western armies) and held the positions of Chairman of State Law and Order Restoration Council (SLORC), Prime Minister and Defence Minister, as well as being appointed Commander in Chief of the Defence Services. He thus exercised both political and operational control over the entire country and armed forces.
From 1989, each Service has had its own Commander in Chief and Chief of Staff. The Army Commander in Chief is now elevated to full General ("Bo gyoke Kyii") rank and also acted as Deputy Commander in Chief of the Defence Services. The C-in-C of the Air Force and Navy hold the equivalent of Lieutenant General rank, while all three Service Chiefs of Staff were raised to Major General level. Chiefs of Bureau of Special Operations (BSO), the heads of Q and A Staffs and the Director of Defence Services Intelligence (DDSI) were also elevated to Lieutenant General rank. The reorganization of the armed forces after 1988 resulted in the upgrading by two ranks of most of the senior positions.
A new command structure was introduced at the Ministry of Defence level in 2002.The most important position created is the Joint Chief of Staff (Army,Navy, Air Force)that commands commanders-in-chief of the Navy and the Air Force.
The Office of Strategic Studies (OSS, or "Sit Maha Byuha Leilaryay Htana") was formed around 1994 and charged with formulating defence policies, and planning and doctrine of the Tatmadaw. The OSS was commanded by Lieutenant General Khin Nyunt, who is also the Director of Defence Service Intelligence (DDSI). Regional Military Commands (RMC) and Light Infantry Divisions (LID) were also reorganized, and LIDs are now directly answerable to Commander in Chief of the Army.
A number of new subordinate command headquarters were formed in response to the growth and reorganization of the Army. These include Regional Operation Commands (ROC, or Da Ka Sa), which are subordinate to RMCs, and Military Operations Commands (MOC, or Sa Ka Kha), which are equivalent to Western infantry divisions.
The Chief of Staff (Army) retained control of the Directorates of Signals, Directorate of Armour Corps, Directorate of Artillery Corps, Defence Industries, Security Printing, Public Relations and Psychological Warfare, and Military Engineering (field section),People's Militias and Border Troops, Directorate of Defence Services Computers (DDSC), the Defence Services Museum and Historical Research Institute.
Under the Adjutant General Office, there are three directorates: Medical Services, Resettlement, and Provost Martial. Under the Quartermaster General Office are the directorates of Military Engineering (garrison section), Supply and Transport, Ordnance Services, and Electricaland Mechanical Engineering.
Other independent department within the Ministry of Defence are Judge Advocate General, Inspector General, Military Appointment General, Directorate of Procurement, Record Office, Central Military Accounting, and Camp Commandant.
All RMC Commander positions were raised to the level of Major General and also serve as appointed Chairmen of the State- and Division-level Law and Order Restoration Committees. They were formally responsible for both military and civil administrative functions for their command areas. Also, three additional regional military commands were created. In early 1990, a new RMC was formed in Burma's north west, facing India. In 1996, the Eastern Command in Shan State was split into two RMCs, and South Eastern Command was divided to create a new RMC in country's far south coastal regions.
In 1997, the SLORC was abolished and the military government created the State Peace and Development Council (SPDC). The council includes all senior military officers and commanders of the RMCs. A new Ministry of Military Affairs was established and headed by a Lieutenant General. This new ministry was abolished after its minister Lt. Gen. Tin Hla was sacked in 2001.
2005 to 2010.
In 18 October 2004, the OSS and DDSI were abolished during the purge of General Khin Nyint and military intelligence units. OSS ordered 4 regiment to raid in DDSI Headquarter in Yangon. At the same time, all of the MIU in the whole country were raided and arrested by OSS corps. Nearly two thirds of MIU officers were arrested for long years. A new military intelligence unit called Military Affairs Security (MAS) was formed to take over the functions of the DDSI, but MAS units were much fewer than DDSI's and MAS was under control by local Division commander.
In early 2006, a new Regional Military Command (RMC) was created at the newly formed administrative capital, Naypyidaw.
Service branches.
Myanmar army ("Tatmadaw Kyee").
The Myanmar Army has always been by far the largest service and has always received the lion's share of Burma's defence budget. It has played the most prominent part in Burma's struggle against the 40 or more insurgent groups since 1948 and acquired a reputation as a tough and resourceful military force. In 1981, it was described as "probably the best [army] in Southeast Asia, apart from Vietnam's".
This judgment was echoed in 1983, when another observer noted that "Myanmar's infantry is generally rated as one of the toughest, most combat seasoned in Southeast Asia".
Myanmar air force ("Tatmadaw Lei").
Personnel: 23,000 
The Myanmar Air Force was formed on 16 January 1947, while Myanmar (also known as Burma) was still under British rule. By 1948, the new air force fleet included 40 Airspeed Oxfords, 16 de Havilland Tiger Moths, 4 Austers and 3 Supermarine Spitfires transferred from Royal Air Force with a few hundred personnel. The primary mission of Myanmar Air Force since its inception has been to provide transport, logistical, and close air support to Myanmar Army in counter-insurgency operations.
Myanmar navy ("Tatmadaw Yay").
The Myanmar Navy is the naval branch of the armed forces of Burma with estimated 19,000 men and women. The Myanmar Navy was formed in 1940 and, although very small, played an active part in Allied operations against the Japanese during the Second World War. The Myanmar Navy currently operates more than 122 vessels. Before 1988, the Myanmar Navy was small and its role in the many counterinsurgency operations was much less conspicuous than those of the army and air force. Yet the navy has always been, and remains, an important factor in Burma's security and it was dramatically expanded in recent years to a provide blue water capability and external threat defense role in Burma's territorial waters. Its personnel number 19,000 (including two naval infantry battalions).
Myanmar police force ("Myanmar Ye Tat Hpwe").
The Myanmar Police Force, formally known as The People's Police Force (Burmese: ပြည်သူ့ရဲတပ်ဖွဲ့; MLCTS: "Pyi Thu Yae Tup Pwe"), was established in 1964 as independent department under the Ministry of Home Affairs. It was reorganised on 1 October 1995 and informally become part of Tatmadaw. Current Director General of Myanmar Police Force is Brigadier General Kyaw Kyaw Tun with its headquarters at Naypyidaw. Its command structure is based on established civil jurisdictions. Each of Burma's seven states and seven divisions has their own Police Forces with headquarters in the respective capital cities. Israel and Australia often provide specialists to enhance the training of Burma's police.
Personnel: 72,000 (including 4,500 Combat/SWAT Police)
Myanmar Frontier Forces ("Na Sa Ka").
The Frontier Forces (abbreviation: Na Sa Ka, နယ္စပ္ေဒသ လူ၀င္မႈ စစ္ေဆးေရး ကြပ္ကဲမႈဌာနခ်ဳပ္ - နစက) was formed in 1992 and was present on all five of Burma's international borders. The forces consisted primarily of military personnel (including intelligence officers), assisted by members of the Myanmar Police Force, Immigration and Custom officials. Its exact total strength was unknown but estimated to be in thousands.
The President's Office has announced on 12 July 2013 that the frontier forces has been abolished.
Human rights abuses.
Forced labour.
According to the International Confederation of Free Trade Unions several hundred thousand men, women, children and elderly people are forced to work against their will by the Burmese army. Individuals refusing to work may be victims of torture, rape or murder.
The International Labour Organization has continuously called on Burma to end the practice of forced labour since the 1960s. In June 2000, the ILO Conference adopted a resolution calling on governments to cease any relations with the country that might aid the junta to continue the use of forced labour.
Torture and rape.
A 2002 report by The Shan Human Rights Foundation and The Shan Women's Action Network, "Licence to rape", details 173 incidents of rape and other forms of sexual violence, involving 625 girls and women, committed by Burmese army troops in Shan State, mostly between 1996 and 2001. The authors note that the figures are likely to be far lower than the reality. According to the report, "the Burmese military regime is allowing its troops systematically and on a widespread scale to commit rape with impunity in order to terrorize and subjugate the ethnic peoples of Shan State.
The report illustrates there is a strong case that war crimes and crimes against humanity, in the form of sexual violence, have occurred and continue to occur in Shan State. The report gives clear evidence that rape is officially condoned as a 'weapon of war' against the civilian populations in Shan State." Furthermore, the report states that "25% of the rapes resulted in death, in some incidences with bodies being deliberately displayed to local communities. 61% were gang-rapes; women were raped within military bases, and in some cases women were detained and raped repeatedly for periods of up to 4 months."
In a 2003 report, "No Safe Place: Burma's Army and the Rape of Ethnic Women", Refugees International document the widespread use of rape by Burma’s soldiers to brutalize women from five different ethnic nationalities.
Child soldiers.
According to Human Rights Watch , recruiting and kidnapping of children to the military is commonplace. An estimated 70,000 of the country’s 350,000-400,000 soldiers are children. There are also multiple reports of widespread child labour.
Defence industries.
The Myanmar Defence Industries (DI) consists of 13 major factories throughout the country that produce approximately 70 major products for Army, Navy and Air Force. The main products include automatic rifles, machine guns, sub-machine guns, anti-aircraft guns, complete range of mortar and artillery ammunition, aircraft and anti aircraft ammunition, tank and anti-tank ammunition, bombs, grenades, anti-tank mines, anti-personnel mines such as the M14 pyrotechnics, commercial explosives and commercial products, and rockets and so forth. DI have produced new assault rifles and light machine-guns for the infantry. The MA series of weapons were designed to replace the old German-designed but locally manufactured Heckler & Koch G3s and G4s that equipped Burma's army since the 1960s.
Factories.
The major factories of the DI are the following:
Heavy Industries.
Heavy Industries were established with Ukrainian assistance mainly to assemble the BTR-3U fleet of the Myanmar Army. Total of 1,000 BTR-3U wheeled APCs are to be assembled in Burma over the next 10 years from parts sent by Ukraine. The BTR-3U is fitted with a number of modern weapon systems including 30 mm gun, 7.62 mm coaxial machine gun, 30 mm automatic grenade launcher and anti-tank guided weapons.
HI has also built APC/IFV such as MAV 1, MAV 2 and BAAC APCs. Little is known about MAV infantry fighting vehicles but it appeared that only 60% of the components are produced locally and some vital components such as fire control systems, turrets, engines and transmissions are imported from China NORINCO industries. Apart from BTR 3Us, MAVs and BAACs, HI is also producing a number of military trucks and jeeps for the Army, Navy and Air Force.
Products.
Products of DI are as follow:-

</doc>
<doc id="20395" url="http://en.wikipedia.org/wiki?curid=20395" title="Foreign relations of Burma">
Foreign relations of Burma

Historically strained, Burma's foreign relations, particularly with Western nations, have improved in recent months. Burma has generally maintained warmer relations with neighbouring states and is a member of the Association of Southeast Asian Nations.
Europe and America.
The United States has placed broad sanctions on Burma because of the military crackdown in 1988 and the military regime's refusal to honour the election results of the 1990 People's Assembly election. Similarly, the European Union has placed embargoes on Burma, including an arms embargo, cessation of trade preferences, and suspension of all aid with the exception of humanitarian aid.
US and European government sanctions against the military government, alongside boycotts and other types direct pressure on corporations by western supporters of the Burmese democracy movement, have resulted in the withdrawal from Burma of most U.S. and many European companies. However, several Western companies remain due to loopholes in the sanctions. Asian corporations have generally remained willing to continue investing in Burma and to initiate new investments, particularly in natural resource extraction.
The French oil company Total S.A. is able to operate the Yadana natural gas pipeline from Burma to Thailand despite the European Union's sanctions on Burma. Total is currently the subject of a lawsuit in French and Belgian courts for the condoning and use of Burman civilian slavery to construct the named pipeline. Experts say that the human rights abuses along the gas pipeline are the direct responsibility of Total S.A. and its American partner Chevron Corporation with aid and implementation by the Tatmadaw. Prior to its acquisition by Chevron, Unocal settled a similar human rights lawsuit for a reported multi-million dollar amount. There remains active debate as to the extent to which the American-led sanctions have had adverse effects on the civilian population or on the military rulers.
Kingdom of Denmark.
Burma is represented in Denmark through its embassy in the United Kingdom, and Denmark is represented in Burma, through its embassy in Thailand. Diplomatic relations were established in 1955. Relations between the two countries are friendly, but economically, Denmark has the "worst" trade with Burma in the European Union. Denmark also supports the Norwegian based radio station, Democratic Voice of Burma.
Assistance to Burma.
Development assistance to Burma is a top priority of the Danish International Development Agency's engagement in Southeast Asia. 93 million DKK was given to education and healthcare projects.
Danish development assistance has focused on promoting democracy and human rights. Denmark was one of the first countries to respond to cyclone Nargis by providing humanitarian assistance to Burma. Three Diseases Fund was founded in 2006, and Denmark joined in 2009. Three Diseases Fund helps Burma fight HIV and AIDS, and has assisted with 73 million dollars.
Burmese Consul incident.
In 1996, the consul in Burma for Denmark, James Leander Nichols, was sentenced to three years in jail. The sentence was for illegal possession of two facsimile machines and a telephone switchboard. Two months later, he died in prison. Despite Danish insistence, Burmese authorities refused to allow an independent autopsy. Soon after, the European Union, with Canada, called for a United Nations gathering on the democratisation process.
Day's Work Day.
On 3 November 2010, students from 140 different gymnasiums in Denmark and DanChurchAid, participated in the annual Day's Work Day. The money earned by the students goes to improve education for young people in Burma.
Republic of Ireland.
The Government of Ireland established diplomatic relations with Burma on a non-resident basis on 10 February 2004. The Irish Government was still concerned about the arbitrary detention of the opposition leader Aung San Suu Kyi. Burma Action Ireland is a pro-democracy group that freely operates in the Republic of Ireland.
Ireland supported a UN commission of inquiry and international level monitoring of Burma after 2008, as part of their efforts to support democracy and human rights movements in Burma. This became public knowledge after official papers were leaked in September 2010.
France.
Franco-Burmese relations go back to the early 18th century, as the French East India Company attempted to extend its influence into Southeast Asia. French involvement started in 1729 when it built a shipyard in the city of Syriam. The 1740 revolt of the Mon against Burmese rule, however, forced the French to depart in 1742. They were able to return to Siam in 1751 when the Mon requested French assistance against the Burmese. A French envoy, Sieur de Bruno was sent to evaluate the situation and help in the defence against the Burmese. French warships were sent to support the Mon rebellion, but in vain. In 1756, the Burmese under Alaungpaya vanquished the Mon. Many French were captured and incorporated into the Burmese Army as an elite gunner corps, under Chevalier Milard. In 1769, official contacts resumed when a trade treaty was signed between King Hsinbyushin and the French East India Company.
Soon after, however, France was convulsed by the French Revolution and Napoleonic Wars, thus allowing overwhelming British influence in Burma. French contacts with Burma, effectively a British colony, became almost non-existent. Instead, from the second half of the 19th century, France concentrated on the establishment of French Indochina and the conflicts with China leading to the Sino-French War. Following the end of World War II, ambassador-level diplomatic relationships between France and Burma were established in 1948, soon after the Burmese nation became an independent republic on 4 January 1948, as "Union of Burma", with Sao Shwe Thaik as its first President and U Nu as its first Prime Minister.
Burma maintains an embassy in Paris, whilst France does not yet have an embassy in Rangoon.
United States.
The political relations between the United States of America and Burma began to face major problems following the 1988 military coup and the junta's outbursts of repression against pro-democracy activists. Subsequent repression, including that of protestors in September 2007, further strained the relationship. However, following signs of democratisation and economic liberalisation, Hillary Clinton, as Secretary of State, and others called for the mending of America's relations with Burma in 2011. As a result of the refurbishment of ties, the American authorities in 2012 planned for the re-establishment of ambassador-level relations with Burma for the first time since 1990.
Historic relations and diplomacy.
Massachusetts, as a U.S. state, attempted to place sanctions against Burma on its own in 1996 but the concept proved to be contradictory to the U.S. Constitution. Later, the United States federal government imposed broad sanctions against Burma under several different legislative and policy vehicles. The Burma Freedom and Democracy Act (BFDA), passed by both the U.S. Senate and their House of Representatives and signed by then President George W. Bush in 2003, imposed a ban on all imports from Burma, a ban on the export of financial services to Burma, a freeze on the assets of certain Burmese financial institutions, alongside further visa restrictions against Burmese officials. American legislators then renewed the BFDA on an almost annual basis, most recently in July 2010.
Since 27 September 2007, the U.S. Department of Treasury froze assets of 25 high-ranking officials Burmese government officials as it was authorised to do so by Executive Order 13310. On 19 October 2007, President George W. Bush imposed a new Executive Order (E.O. 13448) authorising the freezing of assets against individuals who stand accused by the Government of the United States of being party to human rights violations and acts of public corruption, as well as against those who provide material and financial support to the military junta.
In addition, since May 1997, the U.S. Government prohibited new investment by American people and other entities. A number of American companies exited the Burma market prior to the imposition of sanctions due to a worsening business climate and mounting criticism from human rights groups, consumers, and shareholders. The United States has also imposed countermeasures on Burma due to its inadequate measures to eliminate money laundering.
Due to its particularly severe violations of religious freedom, the United States has designated Burma a Country of Particular Concern (CPC) under the International Religious Freedom Act. Burma is also designated a Tier 3 Country in the Trafficking in Persons Report for utilising forced labour, and is subject to additional sanctions as a result. The political relationship between the United States and Burma worsened after the 1988 military coup and violent suppression of pro-democracy demonstrations. Subsequent repression, including the brutal crackdown on peaceful protestors in September 2007, further strained relations.
The United States lowered its level of representation in Burma from Ambassador to Chargé d'Affaires after the government's major outbreaks against opposition groups and protesters in 1988 and its alleged failure to honour the results of the 1990 parliamentary election, although it upgraded back on 13 January 2012, appointing Derek Mitchel as Ambassador and head of mission
Recent moves.
U.S. Secretary of State, Hillary Clinton, visited Burma in November–December 2011. In this visit, the first by a Secretary of State since 1955, Hillary met with the President of Burma, Thein Sein, in the official capital Naypyidaw, and later met with democracy activist Aung San Suu Kyi in Yangon. The US announced a reduction of laws against providing aid to Burma and raised the possibility of an exchange of ambassadors.
On 13 January 2012, the US Secretary of State Hillary Clinton announced the US will exchange ambassadors with Burma, after a landmark Burmese political prisoner amnesty.
On Thursday, 17 May 2012, the White House Press Office announced that President Barack Obama of the U.S. Democratic Party had nominated Derek Mitchell to the U.S. Senate for confirmation to serve as U.S. Ambassador to Burma. After being approved by the U.S. Senate in late June, Derek Mitchell, the first U.S ambassador to Myanmar in 22 years formally assumed his job on 11 July 2012 by presenting his credentials to President Thein Sein at the presidential mansion in the capital Naypyitaw.
In July 2012 the United States formally reduced sanctions against Burma. Secretary of State Hillary Rodham Clinton announced plans in the spring of 2012 for a “targeted easing” of sanctions to allow minor US investment in the country, but companies could not move ahead until the sanctions were formally suspended. In July 2012, President Obama ordered the U.S. State Department to issue two special licences, one providing special authorisation to invest in Burma and the other authorising to provide financial services in Burma. Although plans to lift investment restrictions were announced in May 2012, the change awaited what administration officials labelled 'detailed reporting requirements' on U.S. companies doing business in Burma, alongside the creation of mechanisms to prevent U.S. economic ties to the powerful Burmese military and individuals and companies involved in human rights abuses. President Obama also issued an executive order expanding existing sanctions against individuals who violate human rights to include those who threaten Burma’s political restructuring process.
President Obama created a new power for the U.S. government to impose “blocking sanctions” on any individual threatening peace in Myanmar. Also, businesses with more than US$500,000 worth of investment in the country will need to file an annual report with the State Department, in which they will be required to provide details on workers’ rights, land acquisitions and any payments of more than US$10,000 to government entities, including Myanmar’s state-owned enterprises. Although the policy was criticized by human rights groups, American companies and people will be allowed to invest in the state-owned Myanmar Oil and Gas Enterprise—all investors need to notify the State Department within a 60-day period. Human Rights Watch (HRW) expressed its objection in an official statement: “The new United States government policy allowing business activity in Burma’s controversial oil sector with reporting requirements will not adequately prevent new investments from fueling abuses and undermining reform”. HRW’s Business and Human Rights Director Arvind Ganesan stated: “By allowing deals with Burma’s state-owned oil company, the U.S. looks like it caved to industry pressure and undercut Aung San Suu Kyi and others in Burma who are promoting government accountability”.
In May 2013, Sein became the first Myanmar president to visit the U.S. White House in 47 years and President Barack Obama praised the former general for political and economic reforms, and the cessation of tensions between Myanmar and the U.S. Political activists objected to the visit due to concerns over human rights abuses in Myanmar but Obama assured Sein that Myanmar will receive the support from the U.S. Prior to President Sein, the last Myanmar leader to visit the White House was Ne Win in September 1966. The two leaders discussed Sein's intention to release more political prisoners, the institutionalization of political reform and rule of law, and ending ethnic conflict in Myanmar—the two governments agreed to sign a bilateral trade and investment framework agreement on May 21, 2013.
US activities in Burma.
On 10 September 2007, the Burmese Government accused the CIA of assassinating a rebel Karen commander from the Karen National Union who wanted to negotiate with the military government. For background on the conflict, see
It is more fully explored on: Namebase (cross-references books on CIA activities in Burma).
In 2011 the "Guardian" newspaper published WikiLeaks cable information regarding Burma. The cables revealed that the US funded some of the civil society groups in Burma that forced the government to suspend the controversial Chinese Myitsone Dam on the Irrawaddy river.
According to media reports citing documents published by Germany's "Der Spiegel" in 2010, the Embassy of the United States in Yangon is the site of an electronic surveillance facility used to monitor telephones and communications networks. The facility is run jointly by the Central Intelligence Agency (CIA) and the National Security Agency (NSA) through a group known as Special Collection Service.
Diplomatic missions.
The U.S. Embassy in Burma is located in Rangoon, whilst the Burmese diplomatic representation to America is based in Washington, D.C.
Russia.
Bilateral relations with the Russian Federation are among the strongest enjoyed by largely isolated Burma. Russia had established diplomatic relations with Burma at independence and these continued after the fall of the Soviet Union. China and Russia once vetoed a U.N. Security Council resolution designed to punish Burma. Today Russia, along with China, still opposes the imposition of sanctions on Burma and supports a policy of dialogue. Russia, along with China, remains part of the UN Security Council which occasionally shields or weakens Burma from global pressure and criticism.
Russia maintains an embassy in Rangoon whilst Burma maintains one in Moscow.
Nuclear centre deal.
In 2007 Russia and Burma engaged in a deal regarding Burma's nuclear programme. According to the press release, Russia and Burma shall construct a nuclear research centre that 'will comprise a 10MW light-water reactor working on 20%-enriched uranium-235, an activation analysis laboratory, a medical isotope production laboratory, silicon doping system, nuclear waste treatment and burial facilities'.
Association of Southeast Asian Nations.
Burma is a member of the Association of Southeast Asian Nations (ASEAN) and part of ASEAN+3 and the East Asia Summit. Even as Burma's presence in ASEAN has been seen as a test of the organisation's philosophy of constructive engagement, it has started to be seen as being perhaps somewhat of an improper member-state for the organisation, because of Burma's human rights record and its alleged lack of democracy. Burma agreed to relinquish its turn to hold the rotating ASEAN presidency in 2006 due to others member states' concern.
Asean has announced that it shall not provide defence for Burma at any international forum regarding the authoritarian junta's refusal to restore democracy. In April 2007, the Malaysian Foreign Ministry parliamentary secretary Ahmad Shabery Cheek said that Malaysia and other Asean members had decided not to defend Burma if the country was raised for discussion at any international conference. "Now Burma has to defend itself if it was bombarded at any international forum," he said when winding up a debate at committee stage for the Foreign Ministry. He was replying to queries from Opposition Leader Lim Kit Siang on the next course of action to be taken by Malaysia and Asean with the Burmese military junta. Lim had said Malaysia must play a proactive role in pursuing regional initiatives to bring about a change in Burma and support efforts to bring the situation in Burma to the UN Security Council's attention. Recently, ASEAN did take a stronger tone with Burma, particularly regards to the detention of now-released Aung San Suu Kyi.
Despite border (both territorial and nautical) tensions and the forced migration of 270,000 Rohingya Muslims from Buddhist Burma in 1978, relations with Bangladesh have generally been cordial, albeit somewhat tense at times.
Many Rohingya refugees, not recognised as a sanctioned ethnic group and allegedly suffering abuse from the Burmese authorities, remain in Bangladesh, and have been threatened with forced repatriation to Burma. There are about 28,000 documented refugees remaining in camps in southern Bangladesh.
At the 2008 ASEAN Regional forum summit in Singapore, Bangladesh and Burma have pledged to solve their maritime boundary disputes as quickly as possible especially that a UN deadline in claiming maritime territories will expire in three years time. However in late 2008, Burma sent in ships into disputed waters in the Bay of Bengal for the exploration of oil and natural gas. Bangladesh responded by sending in three warships to the area and diplomatically pursued efforts to pressure the Burmese junta to withdraw their own ships. During the crisis Burma deployed thousands of troops on its border with Bangladesh. However, within a week the ships withdrew and the crisis ended.
Brunei.
Brunei has an embassy in Yangon, and Burma has an embassy in Gadong. The relations has been establish since 21 September 1993.
Malaysia.
The relations between the two countries were established on 1 March 1957 and the first Burma mission at the legation level was set up in Kuala Lumpur in June 1959 and later raised to the embassy level.
Thailand.
Relations between Burma and Thailand focus mainly on economic issues and trade. There is sporadic conflict with Thailand over the alignment of the border. Recently, Prime Minister Abhisit Vejjajiva made it clear that dialogue encouraging political change is a priority for Thailand, but not through economic sanctions. He also publicised intentions to help reconstruct temples damaged in the aftermath of Cyclone Nargis. However, there were tensions over detained opposition leader Aung San Suu Kyi, with Thailand calling for her release. She was released in 2010. In the Thaksin Shinawatra administration, relations have been characterised by conflicts and confrontations. Border disputes are now coming more prominent and Thailand as disturbed by the imprisonment of Burma’s dissident Aung San Suu Kyi.
Burma has diplomatic offices in Bangkok whilst Thailand maintains an embassy in Rangoon.
Philippines.
Philippines established relations with Burma on 1956 and recognized its political name Myanmar. In 2012, Myanmar ranked 3rd to the lowest among the Philippines' trading partners in ASEAN. It only fared better than Cambodia and Laos. The Philippines and Myanmar traded only $47.07 million in 2012. The Philippines grant Burmese citizens visa-free access for 30 days. Myanmar on the other hand signed the visa exemption for Filipinos on December 5, 2013 effective January 4, 2014. The agreement allows Filipinos to stay in Myanmar up to 14 days visa-free.
People's Republic of China.
The People's Republic of China had poor relations with Burma until the late 1980s. Between 1967 and 1970, Burma broke relations with Beijing because of the latter's support for the Communist Party of Burma (CPB). Deng Xiaoping visited Yangon in 1978 and withdrew support for the long running insurgency of the Communist Party of Burma. However, in the early 1950s Burma enjoyed a hot-and-cold relationship with China. Burma's U Thant and U Nu lobbied for China's entry as a permanent member into the UN Security Council, but denounced the invasion of Tibet.
China and Burma have had many border disputes, dating long before the British annexation of Burma. The last border dispute culminated in 1956, when the People's Liberation Army invaded northern Burma, but were repulsed. A border agreement was reached in 1960.
In the late 1960s, due to Ne Win's propaganda that the PRC was to blame for crop failures, and the increasing number of ethnic Chinese students supporting Chairman Mao Zedong, by carrying the Quotatians from his books, anti-Chinese riots broke out in June 1967. At the same time, many Sino-Burmese were influenced by the Cultural Revolution in China and began to wear Mao badges. Shops and homes were ransacked and burned. The Chinese government heavily berated the Burmese government and started a war of words, but no other actions were taken. The anti-Chinese riots continued till the early 1970s.
However, after 1986, China withdrew support for the CPB and began supplying the military junta with the majority of its arms in exchange for increased access to Burmese markets and a rumoured naval base on Coco Islands in the Andaman Sea. China is supposed to have an intelligence gathering station on the Great Coco Island to monitor Indian naval activity as and ISRO & DRDO missile and space launch activities. The influx of Chinese arms turned the tide in Burma against the ethnic insurgencies, many of which had relied indirectly on Chinese complicity. As a result the military junta of Burma is highly reliant on the Chinese for their currently high level of power.
Burma maintains diplomatic offices in Beijing and consular offices in Kunming and Hong Kong, whilst the PRC has a diplomatic mission in Rangoon and a consulate in Mandalay.
India.
Bilateral relations between Burma and the Republic of India have improved considerably since 1993, overcoming disagreements related to drug trafficking, the suppression of democracy and the rule of the military junta in Burma. Burma is situated to the south of the states of Mizoram, Manipur, Nagaland and Arunachal Pradesh in Northeast India. The proximity of the People's Republic of China give strategic importance to Indo-Burmese relations. The Indo-Burmese border stretches over 1,600 kilometers. India is generally friendly with Burma, but is concerned by the flow of tribal refugees and the arrest of Aung San Suu Kyi.
As a result of increased Chinese influence in Burma as well as the safe haven and arms trafficking occurring along the Indo-Burmese border, India has sought in recent years to refurbish ties with the Union of Burma. Numerous economic arrangements have been established including a roadway connecting the isolated provinces of Northeastern India with Mandalay which opens up trade with China, Burma, and gives access to the Burmese ports. Relations between India and Burma have been strained in the past however due to India's continuing support for the pro-democracy movement in Burma.
In an interview on the BBC, George Fernandes, former Indian Defence Minister and prominent Burma critic, said that Coco Island was part of India until it was donated to Burma by former Prime Minister of India Jawaharlal Nehru. Coco Island is located at 18 km from the Indian Nicobar Islands.
Burma has a fully operating embassy based in New Delhi and India has one in Rangoon, the former capital of Burma. Like the PRC, the Republic of India maintains a Consulate-General in Mandalay.
Economic relations.
India is the largest market for Burmese exports, buying about US$220 million worth of goods in 2000; India's exports to Burma stood at US$75.36 million. India is Burma’s 4th largest trading partner after Thailand, the PRC and Singapore, and second largest export market after Thailand, absorbing 25 percent of its total exports. India is also the seventh most important source of Burma’s imports. The governments of India and Burma had set a target of achieving $1 billion and bilateral trade reached US$650 million by 2006. The Indian government has worked to extend air, land and sea routes to strengthen trade links with Myanmar and establish a gas pipeline. While the involvement of India's private sector has been low and growing at a slow pace, both governments are proceeding to enhance cooperation in agriculture, telecommunications, information technology, steel, oil, natural gas, hydrocarbons and food processing. The bilateral border trade agreement of 1994 provides for border trade to be carried out from three designated border points, one each in Manipur, Mizoram and Nagaland.
On 13 February 2001 India and Burma inaugurated a major 160 kilometre highway, called the Indo-Myanmar Friendship Road, built mainly by the Indian Army's Border Roads Organisation and aimed to provide a major strategic and commercial transport route connecting North-East India, and South Asia as a whole, to Southeast Asia.
India and Myanmar have agreed to a four-lane, 3200 km triangular highway connecting India, Myanmar and Thailand. The route, which is expected to be completed by sometime during 2016, will run from India's northeastern states into Myanmar, where over 1,600 km of roads will be built or improved. The first phase connecting Guwahati to Mandalay is set to complete by 2016. This will eventually be extended to Cambodia and Vietnam. This is aimed at creating a new economic zone ranging from Kolkata on the Bay of Bengal to Ho Chi Minh City on the South China Sea.
Operation Leech.
Operation "Leech" is the name given to an armed operation on the Indo-Burmese border in 1998. As the major player in South Asia, India always sought to promote democracy and install friendly governments in the region. To these ends, India's external intelligence agency, R&AW, cultivated Burmese rebel groups and pro-democracy coalitions, especially the Kachin Independence Army (KIA). India allowed the KIA to carry a limited trade in jade and precious stones using Indian territory and even supplied them with weapons.
However, with increasing bonhomie between the Indian government and the Burmese junta, KIA becomes the main source of training and weapons for all northeastern rebel groups in India. R&AW initiated Operation "Leech", with the help of Indian Army and paramilitary forces, to assassinate the leaders of the Burmese rebels as an example to other groups.
Bangladesh.
Historical relations between Burma and Bangladesh include centuries of trade, cultural interactions and migration between Bengal and the kingdoms of Burma, particularly Arakan. The two nations also share a heritage of colonial commerce during the British Empire. The Bengali community in Burma is present in Rangoon and the Rakhine. In Bangladesh, a large population of Burmese ancestry resides in Chittagong and southeastern hill districts, including Rakhines and Bohmong, as well as Burmese-Bengalis. After the Bangladesh Liberation War in 1971, Burma became one of the first countries to recognize the independence of Bangladesh. 
The presence of 270,000 Burmese Muslim refugees (Rohingya people) in southern Bangladesh have often caused irritants in bilateral relations, which are generally cordial. A 40-year maritime boundary dispute in the Bay of Bengal was resolved by the two countries at a UN tribunal in March 2012.
Bangladesh has sought transit rights through Burma, to establish connectivity with China and ASEAN through projects such as the proposed Chittagong-Mandalay-Kunming highway. The governments of both countries are also in discussions on the possible export of Burmese gas to Bangladesh, as well as setting up a joint hydroelectric power plant in Rakhine State.
The political class and civil society of Bangladesh often voiced support for Burma's pro-democracy struggle. In 2006 a petition by 500 Bangladeshi politicians and intellectuals, including Sheikh Hasina and Kamal Hossain, expressed support for Aung San Suu Kyi and called for the release of all political prisoners in Burma. After winning elections in 2008, Sheikh Hasina reiterated her position on Burma's pro-democracy struggle, calling for an end to the detention of Suu Kyi and Burmese political prisoners. The Democratic Voice of Burma radio station operates bureaus in Dhaka and Chittagong.
Burma has an embassy in Dhaka, whilst Bangladesh has an embassy in Rangoon and a consular office in Sittwe. Bangladesh is also one of the first countries to begin constructing a diplomatic mission in Nay Pyi Taw.
Sri Lanka.
History.
Theravada Buddhism was the link between Sri Lanka and Burma from the earliest times. There were frequent exchanges of pilgrims and scriptural knowledge with Ramanna (ancient name of the Burmese Kingdom). The resuscitation of the Sinhalese Sangha after the destructive effects of the Chola conquest owned a great deal to Bhikkus from upper Burma sent over for this purpose by the Burmese King at the request of Vijayabahu I.
By the 11th century these early religious times matured into diplomatic ties. Vijayabahu I (1055–1110 A.D.) who was engaged in a grim struggle against the Cholas received economic aid from King Anawarta of Burma. The alliance with the Burmese appears according to the chronicles to have continued after the expulsion of the Cholas and it was to Burma that Vijayabahu I turned for assistance in re-organizing the Sangha in Sri Lanka, thus underlining the connection between political ties and a common commitment to Buddhism.
The influence of Burmes architecture on Sri Lanka's religious building in Polonnaruwa is also evident. The Satmahalprasada, a setup with an unusual pyramid like form in several levels or storeys in Polonnnaruwa is the best example.
In 1865 the establishment of the Ramanna Nikaya is another major link. The Ramanna Nikaya lays greater stress on poverty and humility. This Nikaya aimed at returning to a purer form of Buddhism.
Biltateral visits.
Sri Lankan officials visiting Burma.
•Official visit of Hon. Madam Sirimavo Bandaranaike, Prime Minister in January (1976) 
•Visit of Hon. A.C.S. Hameed, Foreign Minister (1987) 
•Visit of Hon. Lakshman Kadirgamar, Foreign Minister (1999) 
•Visit of Hon. W.J.M. Loku Bandara, Minister of Buddha Sasana (2003) 
•Visit of Hon. Loku Bandara, Speaker of the Parliament (2005) 
•Visit of Hon Mahinda Rajapakse, Prime Minister (2004) 
•Visit of Hon. Loku Bandara, Speaker (2005) 
•Visit of Hon. Prime Minister (2006) 
•Visit of the Hon. Minister of Foreign Affairs for First Joint Commission (2007)
Burmese officials visiting Sri Lanka.
•State Visit of H.E. Gen U Ne Win, President of Myanmar (1966) 
•Visit of H.E. U Win Aung, Foreign Minister of Myanmar in (1999) 
•Visit of H.E. Professor Kyaw Myint, Minister of Health (2005) 
•Visit of Acting Prime Minister, Lt. Gen. Thein Sein (2007) 
•Visit of the Foreign Minister of Myanmar (to participate at ECOSOC) (2009)
Other Asian countries.
Republic of China (Taiwan).
Although Burma officially recognises the PRC and not the Republic of China, there is much other interaction between the two countries. Many Taiwanese nationals own businesses in Burma. There are direct air flights to Taipei, as there are to some major cities in the People's Republic of China, including Kunming, Guangzhou and Hong Kong.
North Korea.
Burma and North Korea generally enjoy good relations. Burma has an embassy in Pyongyang and North Korea has an embassy in Rangoon.
History.
Since they both achieved independence in 1948, Burma and North Korea have enjoyed a chequered relationship. Burma supported the UN forces during the Korean War, but after the signing of the 1953 armistice it established good working relations with the two Koreas. Consular links with both states were established in 1961 and full diplomatic relations followed in 1975. During the 1960s and 1970s, General Ne Win’s government made efforts to balance the competing demands of North Korea and South Korea for recognition, diplomatic support and trade. However, during the late 1970s the relationship with Pyongyang became slightly stronger than that with Seoul, as Ne Win and the Burma Socialist Programme Party forged fraternal ties with Kim Il-sung and the Workers' Party of Korea.
The assassination attempt in 1983.
The bilateral relationship with North Korea dramatically collapsed in 1983, after Pyongyang allegedly sent three agents to Rangoon to assassinate South Korean President Chun Doo Hwan, who was making a state visit to Burma. Due to a last minute, unannounced change to his schedule Chun survived the massive bomb attack at the Martyr’s Mausoleum, but 17 South Korean and four Burmese officials, including four Korean Cabinet ministers, were killed. Forty-six others were injured.
There was probably at least one bilateral agreement as early as 2000, but the relationship seemed to reach a major turning point around 2003. In July that year, it was reported that between 15 and 20 North Korean technicians were working at the Monkey Point naval base in Rangoon.
Pakistan.
Pakistan and Burma have cordial relations with each other, with embassies in each other's capitals. Pakistan International Airlines has flown to Yangon in the past and still operates Hajj charter flights on behalf of the Burmese government.
Pakistan has a diplomatic mission in Rangoon, whilst Burma maintains a diplomatic office in Islamabad.
Timeline of diplomatic representation.
Below are the years that countries have established ambassador-level diplomatic relationships with Burma.
United Nations.
In 1961, U Thant, then Burma's Permanent Representative to the United Nations and former Secretary to the Prime Minister, was elected Secretary-General of the United Nations; he was the first non-Westerner to head any international organization and would serve as UN Secretary-General for ten years. Among the Burmese to work at the UN when he was Secretary-General was the young Aung San Suu Kyi.
Until 2005, the United Nations General Assembly annually adopted a detailed resolution about the situation in Burma by consensus. But in 2006 a divided United Nations General Assembly voted through a resolution that strongly called upon the government of Burma to end its systematic violations of human rights.
In January 2007, Russia and China vetoed a draft resolution before the United Nations Security Council calling on the government of Burma to respect human rights and begin a democratic transition. South Africa also voted against the resolution, arguing that since there were no peace and security concerns raised by its neighbours, the question did not belong in the Security Council when there were other more appropriate bodies to represent it, adding, "Ironically, should the Security Council adopt [this resolution] ... the Human Rights Council would not be able to address the situation in Myanmar while the Council remains seized with the matter." The issue had been forced onto the agenda against the votes of Russia and the China by the United States (veto power applies only to resolutions) claiming that the outflow from Burma of refugees, drugs, HIV-AIDS, and other diseases threatened international peace and security.
The following September after the uprisings began and the human rights situation deteriorated, the Secretary-General dispatched his special envoy for the region, Ibrahim Gambari, to meet with the government. After seeing most parties involved, he returned to New York and briefed the Security Council about his visit. During this meeting, the ambassador said that the country "indeed [has experienced] a daunting challenge. However, we have been able to restore stability. The situation has now returned to normalcy. Currently, people all over the country are holding peaceful rallies within the bounds of the law to welcome the successful conclusion of the national convention, which has laid down the fundamental principles for a new constitution, and to demonstrate their aversion to recent provocative demonstrations.
On 11 October the Security Council met and issued a statement and reaffirmed its "strong and unwavering support for the Secretary-General's good offices mission", especially the work by Ibrahim Gambari (During a briefing to the Security Council in November, Gambari admitted that no timeframe had been set by the Government for any of the moves that he had been negotiating for.)
Throughout this period the World Food Program has continued to organise shipments from the Mandalay Division to the famine-struck areas to the north.
In December 2008, the United Nations General Assembly voted for a resolution condemning Burma's human rights record; it was supported by 80 countries, with 25 voting against and 45 abstaining.
Bibliography.
</dl>

</doc>
<doc id="20396" url="http://en.wikipedia.org/wiki?curid=20396" title="Michael Schumacher">
Michael Schumacher

Michael Schumacher (]; born 3 January 1969) is a German retired racing driver. Schumacher is a seven-time Formula One World Champion and is widely regarded as one of the greatest Formula One drivers of all time. He was named Laureus World Sportsman of the Year twice. He holds many of Formula One's driver records, including most championships, race victories, fastest laps, pole positions and most races won in a single season – 13 in 2004 (the last of these records was equalled by fellow German Sebastian Vettel nine years later). In 2002, he became the only driver in Formula One history to finish in the top three in every race of a season and then also broke the record for most consecutive podium finishes.
According to the official Formula One website, he is "statistically the greatest driver the sport has ever seen".
After beginning with karting, Schumacher won the German drivers' championships in Formula König and Formula Three before joining Mercedes in the World Sportscar Championship. After one Mercedes-funded race for the Jordan Formula One team, Schumacher signed as a driver for the Benetton Formula One team in 1991. After winning consecutive championships with Benetton in 1994/5, Schumacher moved to Ferrari in 1996 and won another five consecutive drivers' titles with them from 2000 to 2004. Schumacher retired from Formula One driving in 2006 staying with Ferrari as an advisor. Schumacher agreed to return for Ferrari part-way through 2009, as cover for the badly injured Felipe Massa, but was prevented by a neck injury. Schumacher returned to Formula One on a permanent basis from 2010 with the Mercedes team before retiring for a second time at the conclusion of the 2012 season.
His career was not without controversy, including being twice involved in collisions in the final race of a season that determined the outcome of the World Championship, with Damon Hill in 1994 in Adelaide, and with Jacques Villeneuve in 1997 in Jerez. Off the track Schumacher is an ambassador for UNESCO and a spokesman for driver safety. He has been involved in numerous humanitarian efforts throughout his life and donated tens of millions of dollars to charity. Schumacher and his younger brother, Ralf, are the only brothers to win races in Formula One, and they were the first brothers to finish 1st and 2nd in the same race, a feat they repeated in four subsequent races.
In December 2013, Schumacher suffered a serious head injury while skiing. He was airlifted to a hospital and placed in a medically induced coma, having suffered a traumatic brain injury. He was in the coma from 29 December 2013 until 16 June 2014. He left the hospital in Grenoble for further rehabilitation at the University Hospital (CHUV) in Lausanne. On 9 September 2014, Schumacher was brought back to his home for further rehabilitation. In November 2014, Schumacher was reported to be paralyzed and wheelchair-bound as a result of the accident.
Early years.
Schumacher was born in Hürth, North Rhine-Westphalia, to Rolf Schumacher, a bricklayer, and his wife Elisabeth. When Schumacher was four, his father modified his pedal kart by adding a small motorcycle engine. When Schumacher crashed it into a lamp post in Kerpen, his parents took him to the karting track at Kerpen-Horrem, where he became the youngest member of the karting club. His father soon built him a kart from discarded parts and at the age of six Schumacher won his first club championship. To support his son's racing, Rolf Schumacher took on a second job renting and repairing karts, while his wife worked at the track's canteen. Nevertheless, when Michael needed a new engine costing 800 DM, his parents were unable to afford it; he was able to continue racing with support from local businessmen.
Regulations in Germany require a driver to be at least 14 years old to obtain a kart license. To get around this, Schumacher obtained a license in Luxembourg at the age of 12.
In 1983, he obtained his German license, a year after he won the German Junior Kart Championship. From 1984 on, Schumacher won many German and European kart championships. He joined Eurokart dealer Adolf Neubert in 1985 and by 1987 he was the German and European kart champion, then he quit school and began working as a mechanic. In 1988 he made his first step into single-seat car racing by participating in the German Formula Ford and Formula König series, winning the latter.
In 1989, Schumacher signed with Willi Weber's WTS Formula Three team. Funded by Weber, he competed in the German Formula 3 series, winning the title in 1990. He also won the Macau Grand Prix. At the end of 1990, along with his Formula 3 rivals Heinz-Harald Frentzen and Karl Wendlinger, he joined the Mercedes junior racing programme in the World Sports-Prototype Championship. This was unusual for a young driver: most of Schumacher's contemporaries would compete in Formula 3000 on the way to Formula One. However, Weber advised Schumacher that being exposed to professional press conferences and driving powerful cars in long distance races would help his career. In the 1990 World Sportscar Championship season, Schumacher won the season finale at the Autódromo Hermanos Rodríguez in a Sauber–Mercedes C11, and finished fifth in the drivers' championship despite only driving in three of the nine races. He continued with the team in the 1991 World Sportscar Championship season, winning again at the final race of the season at Autopolis in Japan with a Sauber–Mercedes-Benz C291, leading to a ninth-place finish in the drivers' championship. He also competed at Le Mans during that season, finishing 5th in a car shared with Karl Wendlinger and Fritz Kreutzpointner. In 1991, he competed in one race in the Japanese Formula 3000 Championship, finishing second.
Formula One career.
Schumacher was noted throughout his career for his ability to produce fast laps at crucial moments in a race, to push his car to the very limit for sustained periods. Motor sport author Christopher Hilton observed in 2003 that "A measure of a driver's capabilities is his performance in wet races, because the most delicate car control and sensitivity are needed", and noted that like other great drivers, Schumacher's record in wet conditions shows very few mistakes: up to the end of the 2003 season, Schumacher won 17 of the 30 races in wet conditions he contested. Some of Schumacher's best performances occurred in such conditions, earning him the nicknames ""Regenkönig" (rain king) or "Regenmeister"" (rain master), even in the non-German-language media. He is known as "the Red Baron", because of his red Ferrari and in reference to the German Manfred von Richthofen, the famous flying ace of World War I. Schumacher's nicknames include "Schumi",
"Schuey"
and "Schu".
Schumacher is often credited with popularising Formula One in Germany, where it was formerly considered a fringe sport. When Schumacher retired in 2006, three of the top ten drivers were German, more than any other nationality and more than have ever been present in Formula One history. Younger German drivers, such as Sebastian Vettel, felt Schumacher was key in their becoming Formula One drivers. In the latter part of his Formula One career, and as one of the senior drivers, Schumacher was the president of the Grand Prix Drivers' Association. In a 2006 FIA survey, Michael Schumacher was voted the most popular driver of the season among Formula One fans.
Debut with Jordan.
Schumacher made his Formula One debut with the Jordan-Ford team at the 1991 Belgian Grand Prix, driving car number 32 as a replacement for the imprisoned Bertrand Gachot. Schumacher, still a contracted Mercedes driver, was signed by Eddie Jordan after Mercedes paid Jordan $150,000 for his debut. The week before the race, Schumacher impressed Jordan designer Gary Anderson and team manager Trevor Foster during a test drive at Silverstone. His manager Willi Weber assured Jordan that Schumacher knew the challenging Spa track well, although in fact he had only seen it as a spectator. During the race weekend, team-mate Andrea de Cesaris was meant to show Schumacher the circuit, but was held up with contract negotiations. Schumacher then learned the track on his own, by cycling around the track on a fold-up bike he had brought with him. He impressed the paddock by qualifying seventh in this race. This matched the team's season-best grid position, and out-qualified 11-year veteran de Cesaris. Motorsport journalist Joe Saward reported that after qualifying "clumps of German journalists were talking about 'the best talent since Stefan Bellof'". Schumacher retired on the first lap of the race with clutch problems.
Benetton (1991–1995).
After his debut, and despite Jordan's signed agreement in principle with Schumacher's Mercedes management for the remainder of the season, Schumacher was signed by Benetton-Ford for the following race. Jordan applied for an injunction in the UK courts to prevent Schumacher driving for Benetton, but lost the case as they had not yet signed a contract. Schumacher finished the 1991 season with four points out of six races. His best finish was fifth in his second race, the Italian Grand Prix, in which he finished ahead of his team-mate and three-time World Champion Nelson Piquet.
At the start of the 1992 season the Sauber team, planning their Formula One debut with Mercedes backing for the following year, invoked a clause in Schumacher's contract that stated that if Mercedes entered Formula One, Schumacher would drive for them. It was eventually agreed that Schumacher would stay with Benetton, Peter Sauber said that "[Schumacher] didn't want to drive for us. Why would I have forced him?". The year was dominated by the Williams of Nigel Mansell and Riccardo Patrese, featuring powerful Renault engines, semi-automatic gearboxes and active suspension to control the car's ride height. In the "conventional" Benetton B192 Schumacher took his place on the podium for the first time, finishing third in the Mexican Grand Prix. He went on to take his first victory at the Belgian Grand Prix, in a wet race at the Spa-Francorchamps circuit, which by 2003 he would call "far and away my favourite track". He finished third in the Drivers' Championship in 1992 with 53 points, three points behind runner-up Patrese.
The Williams of Damon Hill and Alain Prost also dominated the 1993 season. Benetton introduced their own active suspension and traction control early in the season, last of the frontrunning teams to do so. Schumacher won one race, the Portuguese Grand Prix where he beat Prost, and had nine podium finishes, but retired in seven of the other 15 races. He finished the season in fourth, with 52 points.
1994–1995: World Championship years.
The 1994 season was Schumacher's first Drivers' Championship. The season, however, was marred by the deaths of Ayrton Senna (witnessed by Schumacher, who was directly behind in 2nd position) and Roland Ratzenberger during the San Marino Grand Prix, and by allegations that several teams, but most particularly Schumacher's Benetton team, broke the sport's technical regulations.
Schumacher won six of the first seven races and was leading the Spanish Grand Prix, before a gearbox failure left him stuck in fifth gear. Schumacher finished the race in second place. Following the San Marino Grand Prix, the Benetton, Ferrari and McLaren teams were investigated on suspicion of breaking the FIA-imposed ban on electronic aids. Benetton and McLaren initially refused to hand over their source code for investigation. When they did so, the FIA discovered hidden functionality in both teams' software, but no evidence that it had been used in a race. Both teams were fined $100,000 for their initial refusal to cooperate. However, the McLaren software, which was a gearbox program that allowed automatic shifts, was deemed legal. By contrast, the Benetton software was deemed to be a form of "launch control" that would have allowed Schumacher to make perfect starts, which was explicitly outlawed by the regulations. However, there was no evidence to suggest that this software was actually used. At the British Grand Prix, Schumacher was penalised for overtaking on the formation lap. He then ignored the penalty and the subsequent black flag, which indicates that the driver must immediately return to the pits, for which he was disqualified and later given a two-race ban. Benetton blamed the incident on a communication error between the stewards and the team. Schumacher was also disqualified after winning the Belgian Grand Prix after his car was found to have illegal wear on its skidblock, a measure used after the accidents at Imola to limit downforce and hence cornering speed. Benetton protested that the skidblock had been damaged when Schumacher spun over a kerb, but the FIA rejected their appeal because of the pattern of wear and damage visible on the block. These incidents helped Damon Hill close the points gap, and Schumacher led by a single point going into the final race in Australia. On lap 36 Schumacher hit the guardrail on the outside of the track while leading. Hill attempted to pass, but as Schumacher's car returned to the track there was a collision on the corner causing them both to retire. As a result Schumacher won a very controversial championship, the first German to do so (Jochen Rindt raced under the Austrian flag).
In 1995 Schumacher successfully defended his title with Benetton. He now had the same Renault engine as Williams. He accumulated 33 more points than second-placed Damon Hill. With team-mate Johnny Herbert, he took Benetton to its first Constructors' Championship and became the youngest two-time World Champion in Formula One history.
The season was marred by several collisions with Hill, in particular an overtaking manoeuvre by Hill took them both out of the British Grand Prix on lap 45 and again on lap 23 of the Italian Grand Prix. Schumacher won nine of the 17 races, and finished on the podium 11 times. Only once did he qualify worse than fourth; at the Belgian Grand Prix, he qualified 16th, but went on to win the race.
Ferrari (1996–2006).
In 1996, Schumacher joined Ferrari, a team that had last won the Drivers' Championship in 1979 and the Constructors' Championship in 1983, for a salary of $60 million over 2 years. He left Benetton a year before his contract with them expired; he later cited the team's damaging actions in 1994 as his reason for opting out of his deal. A year later Benetton employees Rory Byrne (designer) and Ross Brawn (Technical Director) joined Ferrari.
Ferrari had previously come close to the championship in 1982 and 1990. The team had suffered a disastrous downturn in the early 1990s, partially as its famous V12 engine was no longer competitive against the smaller, lighter and more fuel efficient V10s of its competitors. Various drivers, notably Alain Prost, had given the vehicles labels such as "truck", "pig", and "accident waiting to happen". Furthermore, the poor performance of the Ferrari pit crews was considered a running joke. At the end of 1995, though the team had improved into a solid competitor, it was still considered inferior to front-running teams such as Benetton and Williams. Schumacher declared the Ferrari 412T good enough to win the Championship.
Schumacher, Ross Brawn, Rory Byrne, and Jean Todt (hired in 1993), have been credited as turning this once struggling team into the most successful team in Formula One history. Three-time World Champion Jackie Stewart believes the transformation of the Ferrari team was Schumacher's greatest feat. Eddie Irvine also joined the team, moving from Jordan.
1996–1999.
"It was not a race. It was a demonstration of brilliance."
Stirling Moss about Schumacher at the 1996 Spanish GP
Schumacher finished third in the Drivers' Championship in 1996 and helped Ferrari to second place in the Constructors' Championship ahead of his old team Benetton. He won three races, more than the team's total tally for the period from 1991 to 1995. During the initial part of the 1996 season, the car had had reliability trouble and Schumacher did not finish six of the 16 races. He took his first win for Ferrari at the Spanish Grand Prix, where he lapped the entire field up to third place in the wet. In the French Grand Prix Schumacher qualified in pole position, but suffered engine failure on the race's formation lap. However at Spa-Francorchamps, Schumacher used well-timed pit-stops to fend off Williams's Jacques Villeneuve. Following that, at Monza, Schumacher won in front of the tifosi. Schumacher's ability, combined with the improving reliability of Ferrari, enabled him to end the season putting up a challenge to eventual race and championship winner Damon Hill at the Suzuka.
Michael Schumacher and Jacques Villeneuve vied for the title in 1997. Villeneuve, driving the superior Williams FW19, led the championship in the early part of the season. However by mid-season, Schumacher had taken the championship lead, winning five races, and entered the season's final Grand Prix with a one-point advantage. Towards the end of the race, held at Jerez, Schumacher's Ferrari developed a coolant leak and loss of performance indicating he may not finish the race. As Villeneuve approached to pass his rival, Schumacher attempted to provoke an accident, but got the short end of the stick, retiring from the race. Villeneuve went on and scored four points to take the championship. Schumacher was punished for unsportsmanlike conduct for the collision and was disqualified from the Drivers' Championship.
In 1998, Finnish driver Mika Häkkinen became Schumacher's main title competition. Häkkinen won the first two races of the season, gaining a 16-point advantage over Schumacher. Schumacher then won in Argentina and, with the Ferrari improving significantly in the second half of the season, Schumacher took six victories and had five other podium finishes. Ferrari took a 1–2 finish at the French Grand Prix, the first Ferrari 1–2 finish since 1990, and the Italian Grand Prix, which tied Schumacher with Häkkinen for the lead of the Drivers' Championship with 80 points, but Häkkinen won the Championship by winning the final two races. There were two controversies; at the British Grand Prix Schumacher was leading on the last lap when he turned into the pit lane, crossed the start finish line and stopped for a ten-second stop go penalty. There was some doubt whether this counted as serving the penalty, but, because he had crossed the finish line when he came into the pit lane, the win was valid. At Spa, Schumacher was leading the race by 40 seconds in heavy spray, but collided with David Coulthard's McLaren when the Scot, a lap down, slowed in very poor visibility to let Schumacher past. After both cars returned to the pits, Schumacher leaped out of his car and headed to McLaren's garage in an infuriated manner and accused Coulthard of trying to kill him.
Rumours circulated that Coulthard may be replaced by Schumacher for the 1999 season and beyond and, in a previous edition of the F1 Racing magazine, Ron Dennis revealed that he had approached Schumacher to sign a deal with McLaren. However, peripheral financial issues that tied Schumacher with Ferrari, such as sponsorship agreements and payment, could not be rectified in a move to the rival team and so no deal came to fruition.
Schumacher's efforts helped Ferrari win the Constructors title in 1999. He lost his chance to win the Drivers' Championship at the British Grand Prix at the high-speed "Stowe Corner", his car's rear brake failed, sending him off the track and resulting in a broken leg. During his 98-day absence, he was replaced by Finnish driver Mika Salo. After missing six races he made his return at the inaugural Malaysian Grand Prix, qualifying in pole position by almost a second. He then assumed the role of second driver, assisting team mate Eddie Irvine's bid to win the Drivers' Championship for Ferrari. In the last race of the season, the Japanese Grand Prix, Häkkinen won his second consecutive title. Schumacher would later say that Häkkinen was the opponent he respected the most.
2000–2004: World Championship years.
During this period Schumacher won more races and championships than any other driver in the history of the sport. Schumacher won his third World Championship in 2000 after a year-long battle with Häkkinen. Schumacher won the first three races of the season and five of the first eight. Mid-way through the year, Schumacher's chances suffered with three consecutive non-finishes, allowing Häkkinen to close the gap in the standings. Häkkinen then took another two victories, before Schumacher won at the Italian Grand Prix. At the post race press conference, after equalling the number of wins (41) won by his idol, Ayrton Senna, Schumacher broke into tears. The championship fight would come down to the penultimate race of the season, the Japanese Grand Prix. Starting from pole position, Schumacher lost the lead to Häkkinen at the start. After his second pit-stop, however, Schumacher came out ahead of Häkkinen and went on to win the race and the championship.
In 2001, Schumacher took his fourth drivers' title. Four other drivers won races, but none sustained a season-long challenge for the championship. Schumacher scored a record-tying nine wins and clinched the World Championship with four races yet to run. He finished the championship with 123 points, 58 ahead of runner-up Coulthard. Season highlights included the Canadian Grand Prix, where Schumacher finished 2nd to his brother Ralf, thus scoring the first ever 1–2 finish by brothers in Formula One; and the Belgian Grand Prix in which Schumacher scored his 52nd career win, breaking Alain Prost's record for most career wins.
In 2002, Schumacher used the Ferrari F2002 to retain his Drivers' Championship.
There was again some controversy, however, at the Austrian Grand Prix, where his teammate, Rubens Barrichello was leading, but in the final metres of the race, under team orders, slowed down to allow Schumacher to win the race. The crowd broke into outraged boos at the result and Schumacher tried to make amends by allowing Barrichello to stand on the top step of the podium. At the United States Grand Prix later that year, Schumacher dominated the race and was set for a close finish with Barrichello. At the end he slowed down to create a formation finish with Barrichello, but slowed too much allowing Barrichello to take the victory.
In winning the Drivers' Championship he equalled the record set by Juan Manuel Fangio of five World Championships. Ferrari won 15 out of 17 races, and Schumacher won the title with six races remaining in the season, which is still the earliest point in the season for a driver to be crowned World Champion. Schumacher broke his own record, shared with Nigel Mansell, of nine race wins in a season, by winning eleven times and finishing every race on the podium. He finished with 144 points, a record-breaking 67 points ahead of the runner-up, his teammate Rubens Barrichello. This pair finished nine of the 17 races in the first two places.
Schumacher broke Juan Manuel Fangio's record of five World Drivers' Championships by winning the drivers' title for the sixth time in 2003, a closely contested season. The biggest competition came once again from the McLaren Mercedes and Williams BMW teams. In the first race, Schumacher ran off track, and in the following two, was involved in collisions. He fell 16 points behind Kimi Räikkönen. Schumacher won the San Marino Grand Prix and the next two races, and closed within two points of Räikkönen. Aside from Schumacher's victory in Canada, and Barrichello's victory in Britain, the mid-season was dominated by Williams drivers Ralf Schumacher and Juan Pablo Montoya, who each claimed two victories. After the Hungarian Grand Prix, Michael Schumacher led Montoya and Kimi Räikkönen by only one and two points, respectively. Ahead of the next race, the FIA announced changes to the way tyre widths were to be measured: this forced Michelin, supplier to Williams and McLaren among others, to rapidly redesign their tyres before the Italian Grand Prix. Schumacher, running on Bridgestone tyres, won the next two races. After Montoya was penalised in the United States Grand Prix, only Schumacher and Räikkönen remained in contention for the title. At the final round, the Japanese Grand Prix, Schumacher needed only one point whilst Räikkönen needed to win. By finishing the race in eighth place, Schumacher took one point and assured his sixth World Drivers' title, ending the season two points ahead of Räikkönen.
In 2004, Schumacher won a record twelve of the first thirteen races of the season, only failing to finish in Monaco after an accident with Juan Pablo Montoya during a safety car period when he briefly locked his car's brakes. He clinched a record seventh drivers' title at the Belgian Grand Prix. He finished that season with a record 148 points, 34 points ahead of the runner-up, teammate Rubens Barrichello, and set a new record of 13 race wins out of a possible 18, surpassing his previous best of 11 wins from the 2002 season.
2005–2006.
Rule changes for the 2005 season required tyres to last an entire race, tipping the overall advantage to teams using Michelins over teams such as Ferrari that relied on Bridgestone tyres. The rule changes were partly in an effort to dent Ferrari's dominance and make the series more interesting. The most notable moment of the early season for Schumacher was his battle with Fernando Alonso in San Marino, where he started 13th and finished only 0.2 seconds behind the Spanish driver. Less than half-way through the season, Schumacher said "I don't think I can count myself in this battle any more. It was like trying to fight with a blunted weapon... If your weapons are weak you don't have a chance." Schumacher's sole win in 2005 came at the United States Grand Prix. Prior to that race, the Michelin tyres were found to have significant safety issues. When no compromise between the teams and the FIA could be reached, all but the six drivers using Bridgestone tyres dropped out of the race after the formation lap. Schumacher retired in six of the 19 races. He finished the season in third with 62 points, fewer than half the points of World Champion Alonso.
2006 became the last season of Schumacher's Ferrari career. After three races, Schumacher had just 11 points and was already 17 points behind Alonso. He won the following two races. His pole position at San Marino was his 66th, breaking Ayrton Senna's 12-year-old record.
Schumacher was stripped of pole position at the Monaco Grand Prix and started the race at the back of the grid. This was due to his stopping his car and blocking part of the circuit while Alonso was on his qualifying lap; he still managed to work his way up to 5th place on the notoriously cramped Monaco circuit.
By the Canadian Grand Prix, the ninth race of the season, Schumacher was 25 points behind Alonso, but he then won the following three races to reduce his disadvantage to 11. After his victories in Italy (in which Alonso had an engine failure) and China, in which Alonso had tyre problems, Schumacher led in the championship standings for the first time during the season. Although he and Alonso had the same point total, Schumacher was in front because he had won more races.
The Japanese Grand Prix was led by Schumacher with only 16 laps to go, when, for the first time since the 2000 French Grand Prix, Schumacher's car suffered an engine failure. Alonso won the race, giving himself a ten-point championship lead. With only one race left in the season, Schumacher could only win the championship if he won the season finale and Alonso scored no points.
Before the Brazilian Grand Prix, Schumacher conceded the title to Alonso. In pre-race ceremonies, football legend Pelé presented a trophy to Schumacher for his years of dedication to Formula One. During the race's qualifying session, Schumacher had one of the quickest times during the first session and was fastest in the second session; but a fuel pressure problem prevented him from completing a single lap during the third session, forcing him to start the race in tenth position. Early in the race Schumacher moved up to sixth place. However, in overtaking Alonso's teammate, Giancarlo Fisichella, Schumacher experienced a tyre puncture caused by the front wing of Fisichella's car. Schumacher pitted and consequently fell to 19th place, 70 seconds behind teammate and race leader Felipe Massa. Schumacher recovered and overtook both Fisichella and Räikkönen to secure fourth place. His performance was classified in the press as "heroic", an "utterly breath-taking drive", and a "performance that ... sums up his career".
2007–2009: First retirement.
While Schumacher was on the podium after winning the 2006 Italian Grand Prix, Ferrari issued a press release stating that he would retire from racing at the end of the 2006 season. Schumacher confirmed his retirement. The press release stated that Schumacher would continue working for Ferrari. It was revealed on 29 October 2006 that Ferrari wanted Schumacher to act as assistant to the newly appointed CEO Jean Todt. This would involve selecting the team's future drivers. After Schumacher's announcement, leading Formula One figures such as Niki Lauda and David Coulthard hailed Schumacher as the greatest all-round racing driver in the history of Formula One. The tifosi and the Italian press, who did not always take to Schumacher's relatively cold public persona, displayed an affectionate response after he announced his retirement.
2007: Advisor at Ferrari.
He attended several Grands Prix during the season. Schumacher drove the Ferrari F2007 for the first time on 24 October at Ferrari's home track in Fiorano, Italy. He ran no more than five laps and no lap times were recorded. A Ferrari spokesman said the short drive was done for the Fiat board of directors who were holding their meeting in Maranello.
During the 2007 season Schumacher acted as Ferrari's advisor and Jean Todt's 'super assistant'.
On 13 November 2007 Schumacher, who had not driven a Formula One car since he had retired a year earlier, undertook a formal test session for the first time aboard the F2007. He returned in December 2007 to continue helping Ferrari with their development program at Jerez circuit. He focused on testing electronics and tyres for the 2008 Formula One season.
2008: Car development.
In 2007, former Ferrari top manager Ross Brawn said that Schumacher was very likely and also happy to continue testing in 2008 Schumacher later explained his role further saying that he would "deal with the development of the car inside Gestione Sportiva" and as part of that "I'd like to drive, but not too often.".
During 2008 Schumacher also competed in motorcycle racing in the IDM Superbike-series, but stated that he had no intention of a second competitive career in this sport. He was quoted as saying that riding a Ducati was the most exhilarating thing he had done in his life, the second most being sky diving.
2009: Planned substitution for injured Massa.
In his capacity as racing advisor to Ferrari, Schumacher was present in Budapest for the Hungarian Grand Prix when Ferrari driver Felipe Massa was seriously injured after being struck by a suspension spring during qualifying. As it became clear that Massa would be unable to compete in the next race at Valencia Schumacher was chosen as a replacement for the Brazilian driver and on 29 July 2009, Ferrari announced that they planned to draft in Schumacher for the European Grand Prix and subsequent Grands Prix until Massa was able to race again. Schumacher tested in a modified F2007 to prepare himself as he had been unable to test the 2009 car due to testing restrictions. Ferrari appealed for special permission for Schumacher to test in a 2009 spec car, but Williams, Red Bull and Toro Rosso were against this test. Schumacher was forced to call off his return due to the severity of the neck injury he had received in a motorcycle accident earlier in the year. Massa's place at Ferrari was instead filled by Luca Badoer and Giancarlo Fisichella.
Mercedes (2010–2012).
In December 2009 it was announced that Schumacher would be returning to Formula One in the 2010 season alongside fellow German driver Nico Rosberg in the new Mercedes GP team. The new Mercedes team was their first majority involvement in an F1 team since 1955. Schumacher stated that his preparations to replace the injured Massa for Ferrari had initiated a renewed interest in F1, which, combined with the opportunity to fulfil a long-held ambition to drive for Mercedes and to be working again with team principal Ross Brawn, led Schumacher to accept the offer once he was passed fit. After a period of intensive training medical tests, it was confirmed that the neck injury that had prevented him driving for Ferrari the year before had fully healed. Schumacher signed a three-year contract, reportedly worth £20m.
Schumacher's surprise return to F1 was compared to Niki Lauda's in 1982 aged 33 and Nigel Mansell's return in 1994 at age 41. Schumacher turned 41 in January 2010 and his prospects with Mercedes were compared with the record set by the oldest F1 champion Juan Manuel Fangio who was 46 when he won his fifth championship.
2010: Return to Formula One.
Schumacher's first drive of the 2010 Mercedes car – the Mercedes MGP W01 – was at an official test in February 2010 in Valencia. He finished sixth in the first race of the season at the Bahrain Grand Prix. After the Malaysian race former driver Stirling Moss suggested that Schumacher, who had finished behind his team-mate in each of the first four qualifying sessions and races, might be "past it." Many other respected former Formula One drivers thought otherwise, including former rival Damon Hill, who warned "you should never write Schumacher off." GrandPrix.com identified the inherent understeer of the Mercedes car, exacerbated by the narrower front tyres introduced for the 2010 season, as contributing to Schumacher's difficulties. Jenson Button would later claim that Mercedes 2010 car was designed for him, and that their differing driving styles may have contributed to Schumacher's difficulties.
Mercedes upgraded their car for the Spanish Grand Prix where Schumacher finished fourth. At the Monaco Grand Prix Schumacher finished sixth after passing Ferrari's Fernando Alonso on the final corner of the race when the safety car returned to the pits. However, he was penalised 20 seconds after the race by the race stewards dropping him to 12th. The stewards judged the pass to be in breach of the FIA's sporting code. Mercedes differing interpretation of the regulation would later lead to it being clarified by the FIA.
In Turkey, Schumacher qualified fifth, and finished fourth in the race, both his best results since his return. In European Grand Prix in Valencia, Schumacher finished 15th, the lowest recorded finish in his career. In Hungary, Schumacher finished outside the points in eleventh, but was found guilty of dangerous driving at 180 mph while unsuccessfully defending tenth position against Rubens Barrichello. As a result he was demoted ten places on the grid for the following race, the Belgian Grand Prix, where he finished seventh, despite starting 21st after his grid penalty.
At the season finale in Abu Dhabi, Schumacher was involved in a major accident on the first lap, which occurred after a spin initiated by Rosberg. In recovering from the incident Vitantonio Liuzzi's collided with Schumacher, barely missing his head. Nobody was hurt in the crash, but Schumacher said the crash had been "frightening."
It was Schumacher's first season since his début in 1991 that he finished without a win, pole position, podium or fastest lap. He finished the season 9th with 72 points.
2011.
Schumacher's first points of 2011 were scored in Malaysia, he later came sixth in Spain and had a strong race at the Canadian Grand Prix finishing fourth, after running as high as second in a wet race. Schumacher was passed late in the race by eventual winner Jenson Button.
Schumacher clashed with Vitaly Petrov in Valencia, and with Kamui Kobayashi in Britain, and marked the 20th anniversary of his Formula One début at the Belgian Grand Prix. Despite starting last in Belgium, Schumacher raced well and finished fifth. Schumacher again raced well in Italy, duelling with Lewis Hamilton for fourth place. The Japanese Grand Prix saw Schumacher lead three laps during the race, marking the first time he had led a race since 2006. In doing so, he became the oldest driver to lead a race since Jack Brabham in 1970.
At the Indian Grand Prix Schumacher started well and finished fifth after overtaking Rosberg at the end of the race. Schumacher diced again with Rosberg in Abu Dhabi Grand Prix, battling over sixth position on the first lap. Schumacher finished the season in eighth place in the Drivers' Championship, with 76 points.
2012.
Schumacher was again partnered by Rosberg at Mercedes for the 2012 season. Schumacher retired from the inaugural race of the season Australian Grand Prix, and scored a point in the second race in Malaysia. In China Schumacher started on the front row alongside Rosberg on pole, but retired due to a loose wheel after a mechanics error during a pit stop.
After causing a collision with Bruno Senna in Spain, Schumacher received a five-place grid penalty for the Monaco Grand Prix. Schumacher was fastest in qualifying in Monaco; but started sixth owing to his penalty. He later retired from seventh place in the race.
At the European Grand Prix, Schumacher finished third in the race, his only podium finish since his return to F1 with Mercedes. At the age of 43 years and 173 days, he became the oldest driver to achieve a podium since Jack Brabham's second-place finish at the 1970 British Grand Prix. Further records were set by Schumacher in Germany, where he set the fastest lap in a Grand Prix for the 77th time in his career, and in Belgium where he became the second driver in history to race in 300 Grands Prix.
Schumacher's indecision over his future plans in F1 led to him being replaced by Lewis Hamilton at Mercedes for the 2013 season. In October 2012, Schumacher announced he would retire for a second time at the conclusion of the season. The following week he was quoted as saying: "There were times in the past few months in which I didn't want to deal with Formula One or prepare for the next Grand Prix." The season concluded with the 2012 Brazilian Grand Prix, in which Schumacher finished seventh. He placed 13th in the 2012 Drivers' Championship.
Helmet.
Schumacher, in conjunction with Schuberth, helped develop the first lightweight carbon helmet. In 2004, a prototype was publicly tested by being driven over by a tank; it survived intact. The helmet keeps the driver cool by funneling directed airflow through fifty holes.
Schumacher's original helmet sported the colours of the German flag and his sponsor's decals. On the top was a blue circle with white astroids. When Jos Verstappen was his team-mate, Schumacher added four red diagonal strokes over the visor to differentiate his helmet from his team-mate. After Schumacher joined Ferrari a prancing horse was added on the back. From the 2000 Monaco Grand Prix, in order to differentiate his colours from his new teammate Rubens Barrichello, Schumacher changed the upper blue colour and some of the white areas to red. Since 2004, the helmet sported a white diagonal line with a white vertical line in the zone of the German Flag colors (originally to accommodate sponsor AMD), but then these lines remained in the Suzuka 2006 design.
He sported one-off helmet designs four times. For the 1998 Japanese Grand Prix, a title decider with Mika Häkkinen, he replaced the German flag with a chequered flag motif and reflective silver replacing the white areas. At the 2004 Italian Grand Prix, the German flag design was replaced with an Italian flag in honour of his team's home race. For the Brazilian Grand Prix race of 2006 (at the time intended to be his final Grand Prix), he wore an all-red helmet that included the names of his ninety-one Grand Prix victories. For the 2011 Belgian Grand Prix, Schumacher's 20th anniversary in Formula One, he wore a commemorative gold-leafed helmet. The helmet, very similar to his current helmet, included the year of his début to the present, and the years of his seven World titles. For the 2012 Belgian Grand Prix, Schumacher's 300th Grand Prix appearance, he wore a special platinum-leafed helmet with a message of his achievement.
Honours.
Schumacher was honoured many times during his career. In April 2002, for his contributions to sport and his contributions in raising awareness of child education, he was named as one of the UNESCO Champions for sport, joining the other eight, which include Pelé, Sergey Bubka and Justine Henin. He won the Laureus World Sportsman of the Year award twice, in 2002 and 2004 for his performances in the 2001 and 2003 seasons respectively. He also received nominations for the 2001, 2003, 2005 and 2007 awards. He shares the record for having the second-most nominations for the award with Roger Federer with six nominations, and is eclipsed only by Tiger Woods who has been nominated seven times. He holds the distinction of having the most nominations for a motorsport athlete, (Fernando Alonso has been nominated only twice, Sebastian Vettel three times, and Valentino Rossi five times) and being the only motorsport athlete to have won the award.
In honour of Schumacher's racing career and his efforts to improve safety and the sport, he was awarded an FIA Gold Medal for Motor Sport in 2006. In 2007, in recognition of his contribution to Formula One racing, the Nürburgring racing track renamed turns 8 and 9 (the Audi and Shell Kurves) as the "Schumacher S", and a month later he presented A1 Team Germany with the A1 World Cup at the A1GP World Cup of Motorsport 2007 awards ceremony. He was nominated for the Prince of Asturias Award for Sport for 2007, which he won both for sporting prowess and for his humanitarian record.
In 2008, the Swiss Football Association appointed Schumacher as the Swiss ambassador for the 2008 European football championship.
On 30 April 2010, Schumacher was honored with the Officier of Légion d'honneur title from French prime minister François Fillon.
On 13 November 2014, Schumacher was awarded the Millenium Trophy at the Bambi Awards.
Racing controversies.
Championship-deciding collisions.
Going into the 1994 Australian Grand Prix, the final race of the 1994 season, Schumacher led Damon Hill by a single point in the Drivers' Championship. Schumacher led the race from the beginning, but on lap 35 he went off track and hit the wall with his right side wheels, returning to the track at reduced speed, and with car damage, but still leading the race. At the next corner Hill attempted to pass on the inside, but Schumacher turned in sharply and they collided. Both cars were eliminated from the race and, as neither driver scored, Schumacher took the title. The race stewards judged it a racing accident and took no action against either driver, but public opinion is divided over the incident, and Schumacher was vilified in the British media.
At the 1997 European Grand Prix at Jerez, the last race of the season, Schumacher led Williams's Jacques Villeneuve by one point in the Drivers' Championship. As Villeneuve attempted to pass Schumacher at the Dry Sack corner on lap 48, Schumacher turned in and the right-front wheel of Schumacher's Ferrari hit the left sidepod of Villeneuve's car. Schumacher retired from the race as a result, but Villeneuve finished in third place, taking four points and so becoming the World Champion. The race stewards did not award any penalty, but two weeks after the race Schumacher was disqualified from the entire 1997 Drivers' Championship after an FIA disciplinary hearing found that his "manoeuvre was an instinctive reaction and although deliberate not made with malice or premeditation. It was a serious error." Schumacher accepted the decision and admitted having made a mistake. Schumacher's actions were widely condemned in British, German, and Italian newspapers. This made Schumacher the only driver in the history of the sport, as of 2014[ [update]], to be disqualified from a Driver's World Championship, although the McLaren team was disqualified from the 2007 Constructor's Championship and fined $100m for illegal possession of Ferrari technical information.
Team orders.
Historically, team orders have always been an accepted part of Formula One. However, in the final metres of the 2002 Austrian Grand Prix, Schumacher's teammate, Rubens Barrichello, slowed his car under orders from Ferrari to allow Schumacher to pass and win the race. Although the switching of positions did not break any actual sporting or technical regulation, it angered fans and it was claimed that the team's actions showed a lack of sportsmanship and respect to the spectators. Many argued that Schumacher did not need to be "given" wins in only the 6th race of the season, particularly given that he had already won four of the previous five grands prix, and that Barrichello had dominated the race weekend up to that point. At the podium ceremony, Schumacher pushed Barrichello onto the top step, and for this disturbance, the Ferrari team incurred a US$1 million fine. Later in the season at the end of the 2002 United States Grand Prix, Schumacher slowed down within sight of the finishing line, allowing Barrichello to win by 0.011 seconds, the 2nd closest margin in F1 history. Schumacher's explanation varied between it being him "returning the favour" for Austria (now that Schumacher's title was secure), or trying to engineer a dead-heat (a feat derided as near-impossible in a sport where timings are taken to within a thousandth of a second). The FIA subsequently banned "team orders which interfere with the race result", but the ban was lifted for the 2011 season because the ruling was difficult to enforce.
Dangerous driving.
During his spell in Sauber, in the 1991 Sportscar World Championship, Schumacher was involved in a serious incident with Derek Warwick in that year's 430 km of Nürburgring. While trying to set his flying lap in qualifying, Schumacher encountered Warwick's Jaguar on a slow lap resulting in lost time for Schumacher. As retaliation for being in his way, Schumacher swerved the Sauber into Warwick's car, hitting the Jaguar's nose and front wheel. Enraged by the German's attitude, Warwick drove to the pits and chased a fleeing Schumacher on foot through the Sauber pits. He eventually caught up with Schumacher, and it took intervention from Jochen Mass to prevent Warwick physically assaulting Schumacher.
Toward the end of the 2010 Hungarian Grand Prix, Rubens Barrichello attempted to pass Schumacher down the inside on the main straight. Schumacher closed the inside line to force Barrichello onto the outside, but Barrichello persisted on the inside at 180 mph, despite the close proximity of a concrete wall and Schumacher leaving him only inches to spare. Barrichello said "It is the most dangerous thing that I have been through", and "There is not a rule for that, but between ourselves we should take a line, stick to it and that's it." Schumacher said that "Obviously there was space enough to go through. We didn't touch, so I guess I just left enough space for him to come through." Ross Brawn said "at the end of the day he gave him enough space. You can argue that it was marginal, but it was just tough – tough racing." A range of ex-drivers and commentators were highly critical of Schumacher. Although there was no accident, the race steward, the same Derek Warwick of the 1991 Nürburgring incident, wanted to black flag Schumacher since that "would have shown a better example to our young drivers". The Hungaroring incident was ruled to be dangerous and Schumacher received a 10 place grid penalty for the next race. Schumacher accepted the decision, and apologised.
Other incidents.
In 1995, Schumacher and Williams driver David Coulthard were disqualified for fuel irregularities, after a switch to Renault engines and Elf oils. On appeal, both drivers had their results and points reinstated, but both teams lost the points the results would normally have earned in the Constructors' Championship.
The 1998 Canadian Grand Prix saw Schumacher accused of dangerous driving when his exit from the pit-lane forced Heinz-Harald Frentzen off the track and into retirement. Despite receiving a 10-second penalty, Schumacher recovered and won the race.
Two laps from the finish of the 1998 British Grand Prix, Schumacher was leading the race when he was issued a stop-and-go penalty for overtaking a lapped car (Alexander Wurz) during the early moments of a Safety Car period. This penalty involves going into the pit lane and stopping for 10 seconds, and the rules state that a driver must serve his penalty within three laps of the penalty being issued. On the third lap after receiving the penalty, Schumacher turned into the pit lane to serve his penalty, but as this was the last lap of the race, and as Ferrari's pit box was located after the start/finish line, Schumacher technically finished the race before serving the penalty. The stewards initially resolved that problem by adding 10 seconds to Schumacher's race time, then later rescinded the penalty completely due to the irregularities in how the penalty had been issued.
In the 1998 Belgian Grand Prix, Schumacher was involved in a race-ending collision whilst trying to lap David Coulthard in heavy spray. After the race he stormed into the McLaren garage and accused Coulthard of trying to kill him, and McLaren and Ferrari team members had to separate the drivers. Coulthard admitted some five years later that the accident had been his mistake.
During qualifying for the 2006 Monaco Grand Prix Schumacher set the fastest time, but stopped his car in the Rascasse corner on the racing line, leaving the corner partially blocked, while his main contender for the season title, Fernando Alonso, was on his final qualifying lap. Schumacher stated that he simply locked up the wheels going into the corner and that the car then stalled while he attempted to reverse out. Alonso believed he would have been on pole if the incident had not happened, and Schumacher was stripped of pole position by the race stewards and started the race at the back of the grid. In the same qualifying session, Giancarlo Fisichella was similarly found to have blocked David Coulthard from improving his time, but Fisichella was only demoted five places on the grid. At the 2010 Monaco Grand Prix, the safety car was deployed after an accident, involving Karun Chandhok and Jarno Trulli, and pulled into the pits on the last lap. Schumacher passed Alonso before the finish line. Mercedes held that "the combination of the race control messages 'Safety Car in this lap' and 'Track Clear' and the green flags and lights shown by the marshals after safety car line one indicated that the race was not finishing under the safety car and all drivers were free to race. This opinion appears to have been shared by the majority of the teams with cars in the top ten positions who also gave their drivers instructions to race to the finish line." However an FIA investigation found Schumacher guilty of breaching Safety Car regulations and awarded him a 20-seconds penalty, which cost him six places.
Personal life.
Schumacher's younger brother Ralf was a Formula One driver until the end of 2007. Their stepbrother Sebastian Stahl has also been a racing driver.
In August 1995, Michael married Corinna Betsch. They have two children, a daughter Gina-Marie, born in 1997 and a son Mick, born March 22, 1999. He has always been very protective of his private life and is known to dislike the celebrity spotlight, preferring a simple life. The family moved to a house near Gland, Switzerland in 2007, a 650 m2 mansion with its own underground garage and petrol station, situated on a private beach on Lake Geneva. The family has two dogs – one stray that Corinna fell in love with in Brazil, and an Australian Shepherd named "Ed" whose entrance to the family made headlines. Schumacher personally drove a taxi through the Bavarian town of Coburg after collecting the dog, enabling the family to catch their return flight to Switzerland. Both Schumacher and the taxi driver were reprimanded by local police.
One of his main hobbies is horse riding, and he plays football for his local team FC Echichens. He has appeared in several charity football games and organised games between Formula One drivers. He is a fan of Köln and Newcastle United.
On 23 June 2003, Schumacher was appointed as an Ambassador at Large for the Most Serene Republic of San Marino.
Schumacher is a special ambassador to UNESCO and has donated 1.5 million euros to the organization. Additionally, he paid for the construction of a school for poor children and for area improvements in Dakar, Senegal. He supports a hospital for child victims of war in Sarajevo, which specialises in caring for amputees. In Lima, Peru he funded the "Palace for the Poor", a centre for helping homeless street children obtain an education, clothing, food, medical attention, and shelter. He stated his interest in these various efforts was piqued both by his love for children and the fact that these causes had received little attention. While an exact figure for the amount of money he has donated throughout his life is unknown, it is known that in his last four years as a driver, he donated at least $50 million. In 2008, it was revealed that he had donated between $5M and $10M to the William J. Clinton Presidential Center and Park of Bill Clinton.
Since his participation in an FIA European road safety campaign, as part of his punishment after the collision at the 1997 European Grand Prix, Schumacher continued to support other campaigns, such as Make Roads Safe, which is led by the FIA Foundation and calls on G8 countries and the UN to recognise global road deaths as a major global health issue. In 2008, Schumacher was the figurehead of an advertising campaign by Bacardi to raise awareness about responsible drinking, with a focus on communicating an international message 'drinking and driving don't mix'. He featured in an advertising campaign for television, cinema and online media, supported by consumer engagements, public relations and digital media across the World.
On the eve of the 2002 British Grand Prix, on behalf of Fiat, Schumacher presented a Ferrari 360 Modena to the Indian cricketer Sachin Tendulkar at Silverstone.
On 21 June 2009, Schumacher appeared on the BBC's motoring programme "Top Gear" as the Stig. Presenter Jeremy Clarkson hinted later in the programme that Schumacher was not the regular Stig, and the BBC has since confirmed that this is the case. Schumacher was there on that occasion because Ferrari would not allow anyone else to drive the one-of-a-kind black Ferrari FXX that was featured in the show. The FXX was presented to Schumacher upon his retirement at Monza in 2006.
During his interview with Clarkson, Schumacher stated that his road cars are a Fiat 500 Abarth, and a Fiat Croma, which is his family car.
Finance and sponsorship.
In 2004, "Forbes" magazine listed him as the 2nd highest paid athlete in the World. In 2005, "Eurobusiness" magazine identified Schumacher as the World's first billionaire athlete. His 2004 salary was reported to be around US$80 million. "Forbes" magazine ranked him 17th in its "The World's Most Powerful Celebrities" list. A significant share of his income came from advertising. For example, Deutsche Vermögensberatung paid him $8 million over three years from 1999 for wearing a 10 by 8 centimetre advertisement on his post-race cap. The deal was extended until 2010. He donated $10 million for aid after the 2004 Indian Ocean earthquake. His donation surpassed that of any other sports person, most sports leagues, many worldwide corporations and even some countries. Schumacher's bodyguard Burkhard Cramer and Cramer's two sons were killed in the tsunami.
In 2010, his personal fortune was estimated at £515 million. He reportedly received a salary of £21 million each year from the Mercedes team, plus a further £9 million in endorsements.
2013 skiing accident.
On 29 December 2013, Schumacher was skiing with his 14-year-old son Mick descending the Combe de Saulire below the Dent de Burgin above Méribel in the French Alps. While crossing an unsecured off-piste area between Piste Chamois and Piste Mauduit he fell and hit his head on a rock, sustaining a head injury despite wearing a ski helmet. According to his physicians, Schumacher would most likely have died had he not been wearing a helmet.
Schumacher was put into a medically induced coma because of having suffered a traumatic brain injury; his doctors reported on 7 March 2014, that his condition was stable. On 4 April 2014, Schumacher's agent reported that he was showing "moments of consciousness" as he was gradually withdrawn from the medically induced coma, adding to reports by relatives of "small encouraging signs" over the preceding month. In mid-June he was moved from intensive care into a rehabilitation ward. By 16 June 2014, Schumacher had regained consciousness and left Grenoble Hospital for further rehabilitation at the University Hospital (CHUV) in Lausanne, Switzerland. On 9 September 2014, Schumacher left CHUV and was brought back to his home for further rehabilitation. In November 2014, it was reported that Schumacher was "paralysed and in a wheelchair"; he "cannot speak and has memory problems".
Racing record.
Complete Formula One results.
‡ Schumacher was disqualified from the 1997 World Drivers' Championship due to dangerous driving in the European Grand Prix, where he caused an avoidable accident with Jacques Villeneuve. His points tally would have placed him in second place in that year's standings.
† Driver did not finish the Grand Prix, but was classified as he completed over 90% of the race distance.
Formula One records.
Schumacher holds the following records in Formula One.
Books and films.
Schumacher had a voice role in the Disney/Pixar film "Cars". His character is himself as a car (Ferrari F430). The French film "Asterix and Obelix at the Olympic Games" features Schumacher in a cameo role, also as himself.
References and notes.
All race and championship results (1991–2006) are taken from the Official Formula 1 Website. www.formula1.com. Retrieved 23 May 2007
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="20398" url="http://en.wikipedia.org/wiki?curid=20398" title="Muonium">
Muonium

Muonium is an exotic atom made up of an antimuon and an electron, which was discovered in 1960 and is given the chemical symbol Mu. During the muon's lifetime, muonium can enter into compounds such as muonium chloride (MuCl) or sodium muonide (NaMu). Due to the mass difference between the antimuon and the electron, muonium (#redirect #redirect ) is more similar to atomic hydrogen (#redirect #redirect ) than positronium (#redirect #redirect ). Its Bohr radius and ionization energy are within 0.5% of hydrogen, deuterium, and tritium.
Although muonium is short-lived, physical chemists use it in a modified form of electron spin resonance spectroscopy for the analysis of chemical transformations and the structure of compounds with novel or potentially valuable electronic properties. (This form of electron spin resonance (eSR) is called muon spin resonance (μSR).) There are variants of μSR, e.g. muon spin rotation, which is affected by the presence of a magnetic field applied transverse to the muon beam direction (since muons are typically produced in a spin-polarized state from the decay of pions), and avoided level crossing (ALC), which is also called level crossing resonance (LCR). The latter employs a magnetic field applied longitudinally to the beam direction, and monitors the relaxation of muon spins caused by magnetic oscillations with another magnetic nucleus.
Because the muon is a lepton, the atomic energy levels of muonium can be calculated with great precision from quantum electrodynamics (QED), unlike the case of hydrogen, where the precision is limited by uncertainies related to the internal structure of the proton. For this reason, muonium is an ideal system for studying bound-state QED and also for searching for physics beyond the standard model.
True muonium.
What is called "true muonium", a bound state of a muon and an antimuon, is a theoretical exotic atom which has never been observed. It may have been generated in the collision of electron and positron beams but has not been searched for in the particle debris.

</doc>
<doc id="20401" url="http://en.wikipedia.org/wiki?curid=20401" title="Medicine man">
Medicine man

A medicine man or medicine woman is a traditional healer and spiritual leader among Native Americans in the United States and First Nations in Canada. The terms are also used to refer to traditional healers of other indigenous peoples.
The medicine man and woman in North America.
Role in native society.
The primary function of these "medicine elders" is to secure the help of the spirit world, including the Great Spirit (Wakan Tanka in the language of the Lakota people), for the benefit of the entire community.
Sometimes the help sought may be for the sake of healing disease, sometimes it may be for the sake of healing the psyche, sometimes the goal is to promote harmony between human groups or between humans & nature. 
So the term "medicine man/woman" is not entirely inappropriate, but it greatly oversimplifies and also skews the depiction of the people whose role in society complements that of the chief. These people are not the Native American equivalent of the Chinese "barefoot doctors", herbalists, nor of the emergency medical technicians who ride rescue vehicles.
To be recognized as the one who performs this function of bridging between the natural world and the spiritual world for the benefit of the community, an individual must be validated in his role by that community. Medicine men and women study through a medicine society or from a single teacher.
Cultural context.
The term "medicine people" is commonly used in Native American communities, for example, when Arwen Nuttall (Cherokee) of the National Museum of the American Indian writes, "The knowledge possessed by medicine people is privileged, and it often remains in particular families."
Native Americans tend to be quite reluctant to discuss issues about medicine or medicine people with non-Indians. In some cultures, the people will not even discuss these matters with Indians from other tribes. In most tribes medicine elders are not expected to advertise or introduce themselves as such. As Nuttall writes, "An inquiry to a Native person about religious beliefs or ceremonies is often viewed with suspicion. One example of this was the Apache medicine cord or "Izze-kloth", whose purpose and use by Apache medicine elders was a mystery to nineteenth century ethnologists because ""the Apache look upon these cords as so sacred that strangers are not allowed to see them, much less handle them or talk about them"."
The 1954 version of "Webster's New World Dictionary of the American Language," reflects the poorly grounded perceptions of the people whose use of the term effectively defined it for the people of that time: "a man supposed to have supernatural powers of curing disease and controlling spirits." In effect, such definitions were not explanations of what these "medicine people" were to their own communities, but instead reported on the consensus of socially and psychologically remote observers when they tried to categorize these individuals. The term "medicine man/woman," like the term "shaman", has been criticized by Native Americans, as well as other specialists in the fields of religion and anthropology.
The term "medicine man/woman" was also frequently used by Europeans to refer to African traditional healers, also known as "witch doctors" or "fetish men/women".
While anthropologists sometimes use the term "shaman," for Indigenous healers, "shaman" is the specific name for a spiritual mediator from the Tungusic peoples of Siberia, and is not used in Native American or First Nations communities.
Effectiveness.
Some people believe native American healing can be used to treat a variety of human ailments including heart disease, diabetes and cancer. However, according to the American Cancer Society while its supportive community aspects might improve general well-being, "available scientific evidence does not support claims that Native American healing can cure cancer or any other disease".

</doc>
<doc id="20403" url="http://en.wikipedia.org/wiki?curid=20403" title="Malay Peninsula">
Malay Peninsula

The Malay Peninsula (Malay: "Semenanjung Tanah Melayu", Thai: คาบสมุทรมลายู) is a peninsula in Southeast Asia. The land mass runs approximately north-south and, at its terminus, is the southernmost point of the Asian mainland. The area contains the southernmost tip of Myanmar, Peninsular Malaysia, and Southern Thailand.
The Titiwangsa Mountains are part of the Tenasserim Hills system, and form the backbone of the Peninsula. They form the southernmost section of the central cordillera which runs from Tibet through the Kra Isthmus (the Peninsula's narrowest point) into the Malay peninsula. The Strait of Malacca separates the Malay Peninsula from the Indonesian island of Sumatra while the south coast is separated from the island of Singapore by the Straits of Johor.
Etymology.
The Malay term "Tanah Melayu" is derived from the word "Tanah" (land) and "Melayu" (Malays), thus it means "the Malay land". The term can be found in various pre-modern Malay texts, of which the oldest dating back to the early 17th century. It is frequently mentioned in the "Hikayat Hang Tuah", a well known classical work that began as oral tales associated with the legendary heroes of Melaka Sultanate. "Tanah Melayu" in the text is consistently employed to refer to the area under Melakan dominance. In the early 16th century, Tomé Pires, a Portuguese apothecary who stayed in Melaka from 1512 to 1515, writes an almost identical term, "Terra de Tana Malaio" which he referred to the southeastern part of Sumatra, where the deposed Sultan of Melaka, Mahmud Shah established his exiled government. The 17th century's account of Portuguese historian, Emanuel Godinho de Erédia, noted on the region of "Malaios" surrounded by the Andaman Sea in the north, the entire Malacca Strait in the centre, a part of Sunda Strait in the south, and the western part of South China Sea in the east.
Prior to the foundation of Melaka, reference to Malay peninsula was made in different terms from various foreign sources. According to several Indian scholars, the word "Malayadvipa" ("mountain-insular continent"), mentioned in the ancient Indian text, "Vayu Purana", may possibly refer to the Malay peninsula. Another Indian source, an inscription on the south wall of the Brihadeeswarar Temple, recorded the word "Malaiur", referring to a kingdom in Malay peninsula that had "a strong mountain for its rampart". The Greek source, "Geographia", written by Ptolemy, labelled a geographical part of "Golden Chersonese" as "Maleu-kolon", a term thought to derive from Sanskrit "malayakolam" or "malaikurram". While the Chinese chronicle of Yuan Dynasty mentioned the word "Ma-li-yu-er", referring to a nation of Malay peninsula that threatened by the southward expansion of Sukhothai Kingdom under King Ram Khamhaeng. During the same era, Marco Polo made a reference to "Malauir" in his travelogue, as a kingdom located in the Malay peninsula, possibly similar to the one mentioned in Yuan chronicle.
In the early 20th century, the term "Tanah Melayu" was generally used by the Malays of the peninsula during the rise of Malay nationalism to describe uniting all Malay states on the peninsula under one Malay nation, although this ambition was largely realised with the formation of "Persekutuan Tanah Melayu" (Malay for "Federation of Malaya") in 1948.

</doc>
<doc id="20405" url="http://en.wikipedia.org/wiki?curid=20405" title="Miles Davis">
Miles Davis

Miles Dewey Davis III (May 26, 1926 – September 28, 1991) was an American jazz musician, trumpeter, bandleader, and composer. Widely considered one of the most influential musicians of the 20th century, Miles Davis was, together with his musical groups, at the forefront of several major developments in jazz music, including bebop, cool jazz, hard bop, modal jazz, and jazz fusion.
In 2006, Davis was inducted into the Rock and Roll Hall of Fame, which recognized him as "one of the key figures in the history of jazz". In 2008, his 1959 album "Kind of Blue" received its fourth platinum certification from the Recording Industry Association of America (RIAA), for shipments of at least four million copies in the United States. On December 15, 2009, the U.S. House of Representatives passed a symbolic resolution recognizing and commemorating the album "Kind of Blue" on its 50th anniversary, "honoring the masterpiece and reaffirming jazz as a national treasure".
Life and career.
Early life (1926–44).
Miles Dewey Davis III was born on May 26, 1926, to an affluent African American family in Alton, Illinois. His father, Miles Dewey Davis, Jr., was a dentist. In 1927 the family moved to East St. Louis, Illinois. They also owned a substantial ranch in the Delta region of Arkansas near the city of Pine Bluff, Arkansas, where Davis's father and grandfather were from. It was in both East St. Louis, Illinois and near Pine Bluff, Arkansas that young Davis developed his earliest appreciation for music listening to the gospel music of the black church.
Davis' mother, Cleota Mae (Henry) Davis, wanted her son to learn the piano; she was a capable blues pianist but kept this fact hidden from her son. His musical studies began at 13, when his father gave him a trumpet and arranged lessons with local musician Elwood Buchanan. Davis later suggested that his father's instrument choice was made largely to irk his wife, who disliked the trumpet's sound. Against the fashion of the time, Buchanan stressed the importance of playing without vibrato; he was reported to have slapped Davis' knuckles every time he started using heavy vibrato. Davis would carry his clear signature tone throughout his career. He once remarked on its importance to him, saying, "I prefer a round sound with no attitude in it, like a round voice with not too much tremolo and not too much bass. Just right in the middle. If I can’t get that sound I can’t play anything." Clark Terry was another important early influence.
By age 16, Davis was a member of the music society and, when not at school, playing professionally first at the local Elks Club. At 17, he spent a year playing in Eddie Randle's band, the Blue Devils. During this time, Sonny Stitt tried to persuade him to join the Tiny Bradshaw band, then passing through town, but Davis' mother insisted that he finish his final year of high school. He graduated from East St. Louis Lincoln High School in 1944.
In 1944, the Billy Eckstine band visited East St. Louis. Dizzy Gillespie and Charlie Parker were members of the band, and Davis was brought in on third trumpet for a couple of weeks because the regular player, Buddy Anderson, was out sick. Even after this experience, once Eckstine's band left town, Davis' parents were still keen for him to continue formal academic studies.
New York and the bebop years (1944–48).
In the fall of 1944, following graduation from high school, Davis moved to New York City to study at the Juilliard School of Music. Upon arriving in New York, he spent most of his first weeks in town trying to get in contact with Charlie Parker, despite being advised against doing so by several people he met during his quest, including saxophonist Coleman Hawkins.
Finally locating his idol, Davis became one of the cadre of musicians who held nightly jam sessions at two of Harlem's nightclubs, Minton's Playhouse and Monroe's. The group included many of the future leaders of the bebop revolution: young players such as Fats Navarro, Freddie Webster, and J. J. Johnson. Established musicians including Thelonious Monk and Kenny Clarke were also regular participants.
Davis dropped out of Juilliard after asking permission from his father. In his autobiography, Davis criticized the Juilliard classes for centering too much on the classical European and "white" repertoire. However, he also acknowledged that, in addition to greatly improving his trumpet playing technique, Juilliard helped give him a grounding in music theory that would prove valuable in later years.
Davis began playing professionally, performing in several 52nd Street clubs with Coleman Hawkins and Eddie "Lockjaw" Davis. In 1945, he entered a recording studio for the first time, as a member of Herbie Fields's group. This was the first of many recordings Davis contributed to in this period, mostly as a sideman. He finally got the chance to record as a leader in 1946, with an occasional group called the Miles Davis Sextet plus Earl Coleman and Ann Hathaway—one of the rare occasions when Davis, by then a member of the groundbreaking Charlie Parker Quintet, can be heard accompanying singers. In these early years, recording sessions where Davis was the leader were the exception rather than the rule; his next date as leader would not come until 1947.
Around 1945, Dizzy Gillespie parted ways with Parker, and Davis was hired as Gillespie's replacement in his quintet, which also featured Max Roach on drums, Al Haig (replaced later by Sir Charles Thompson and Duke Jordan) on piano, and Curley Russell (later replaced by Tommy Potter and Leonard Gaskin) on bass.
With Parker's quintet, Davis went into the studio several times, already showing hints of the style he would become known for. On an oft-quoted take of Parker's signature song, "Now's the Time", Davis takes a melodic solo, whose unbop-like quality anticipates the "cool jazz" period that followed. The Parker quintet also toured widely. During a stop in Los Angeles, Parker had a nervous breakdown that landed him in the Camarillo State Mental Hospital for several months, and Davis found himself stranded. He roomed and collaborated for some time with bassist Charles Mingus, before getting a job on Billy Eckstine's California tour, which eventually brought him back to New York. In 1948, Parker returned to New York, and Davis rejoined his group.
However, the relationships within the quintet were growing tense. Parker was behaving erratically due to his well-known drug addiction. Davis and Roach caused friction in the group by objecting to having Duke Jordan as a pianist and would have preferred Bud Powell. By December of 1948, Davis' claims that he was not being paid began to strain the relationship even further. Davis finally left the group following a confrontation with Parker at the Royal Roost.
For Davis, his departure from Parker's group marked the beginning of a period when he worked mainly as a freelancer and sideman in some of the most important combos on the New York jazz scene.
Birth of the Cool (1948–49).
In 1948 Davis grew close to the Canadian composer and arranger Gil Evans. Evans' basement apartment had become the meeting place for several young musicians and composers such as Davis, Roach, pianist John Lewis, and baritone sax player Gerry Mulligan who were unhappy with the increasingly virtuoso instrumental techniques that dominated the bebop scene. Evans had been the arranger for the Claude Thornhill orchestra, and it was the sound of this group, as well as Duke Ellington's example, that suggested the creation of an unusual line-up: a nonet including a French horn and a tuba (this accounts for the "tuba band" moniker that became associated with the combo).
Davis took an active role in the project, so much so that it soon became "his project". The objective was to achieve a sound similar to the human voice, through carefully arranged compositions and by emphasizing a relaxed, melodic approach to the improvisations.
The nonet debuted in the summer of 1948, with a two-week engagement at the Royal Roost. The sign announcing the performance gave a surprising prominence to the role of the arrangers: "Miles Davis Nonet. Arrangements by Gil Evans, John Lewis and Gerry Mulligan." It was, in fact, so unusual that Davis had to persuade the Roost's manager, Ralph Watkins, to word the sign this way. He prevailed only with the help of Monte Kay, the club's artistic director.
The nonet was active until the end of 1949, along the way undergoing several changes in personnel: Roach and Davis were constantly featured, along with Mulligan, tuba player Bill Barber, and alto saxophonist Lee Konitz, who had been preferred to Sonny Stitt (whose playing was considered too bop-oriented). Over the months, John Lewis alternated with Al Haig on piano, Mike Zwerin with Kai Winding on trombone (Johnson was touring at the time), Junior Collins with Sandy Siegelstein and Gunther Schuller on French horn, and Al McKibbon with Joe Shulman on bass. Singer Kenny Hagood was added for one track during the recording.
The presence of white musicians in the group angered some black jazz players, many of whom were unemployed at the time, but Davis rebuffed their criticisms.
A contract with Capitol Records granted the nonet several recording sessions between January 1949 and April 1950. The material they recorded was released in 1956 on an album whose title, "Birth of the Cool", gave its name to the "cool jazz" movement that developed at the same time and partly shared the musical direction begun by Davis' group.
For his part, Davis was fully aware of the importance of the project, which he pursued to the point of turning down a job with Duke Ellington's orchestra.
The importance of the nonet experience would become clear to critics and the larger public only in later years, but, at least commercially, the nonet was not a success. The liner notes of the first recordings of the Davis Quintet for Columbia Records call it one of the most spectacular failures of the jazz club scene. This was bitterly noted by Davis, who claimed the invention of the cool style and resented the success that was later enjoyed—in large part because of the media's attention—by white "cool jazz" musicians (Mulligan and Dave Brubeck in particular).
This experience also marked the beginning of the lifelong friendship between Davis and Gil Evans, an alliance that would bear important results in the years to follow.
Hard bop and the "Blue Period" (1950–54).
The first half of the 1950s was, for Davis, a period of great personal difficulty. At the end of 1949, he went on tour in Paris with a group including Tadd Dameron, Kenny Clarke (who remained in Europe after the tour), and James Moody. Davis was fascinated by Paris and its cultural environment, where black jazz musicians, and African Americans in general, often felt better respected than they did in their homeland. While in Paris, Davis began a relationship with French actress and singer Juliette Gréco.
Many of his new and old friends (Davis, in his autobiography, mentions Clarke) tried to persuade him to stay in France, but Davis decided to return to New York. Back in the States, he began to feel deeply depressed. He attributed the depression to his separation from Gréco, his feeling under-appreciated by the critics (who hailed his former collaborators as leaders of the cool jazz movement)—and to the unraveling of his liaison with a former St. Louis schoolmate who lived with him in New York, with whom he had two children.
Davis blames these factors for the heroin habit that deeply affected him for the next four years. During this period, Davis supported his habit partly with his music and partly by living the life of a hustler. By 1953, his drug addiction began to impair his playing ability. Heroin had killed some of his friends (Navarro and Freddie Webster). He had been arrested for drug possession while on tour in Los Angeles, and his drug habit became public in a "Down Beat" interview of Cab Calloway.
Realizing his precarious condition, Davis tried several times to end his drug addiction, finally succeeding in 1954 after returning to his father's home in St. Louis for several months and locking himself in a room until he had gone through a painful withdrawal. During this period, he avoided New York and played mostly in Detroit and other Midwestern towns, where drugs were then harder to come by. A widely related story, attributed to Richard (Prophet) Jennings was that Davis -- while in Detroit playing at the Blue Bird club as a guest soloist in Billy Mitchell's house band along with Tommy Flanagan, Elvin Jones, Betty Carter, Yusef Lateef, Barry Harris, Thad Jones, Curtis Fuller and Donald Byrd -- stumbled into Baker's Keyboard Lounge out of the rain, soaking wet and carrying his trumpet in a paper bag under his coat, walked to the bandstand and interrupted Max Roach and Clifford Brown in the midst of performing "Sweet Georgia Brown" by beginning to play "My Funny Valentine", and then, after finishing the song, stumbled back into the rainy night. Davis was supposedly embarrassed into getting clean by this incident. In his autobiography, Davis disputed this account, stating that Roach had requested that Davis play with him that night, and that the details of the incident, such as carrying his horn in a paper bag and interrupting Roach and Brown, were fictional and that his decision to quit heroin was unrelated to the incident.
Despite all the personal turmoil, the 1950–54 period was actually quite fruitful for Davis artistically. He made quite a number of recordings and had several collaborations with other important musicians. He got to know the music of Chicago pianist Ahmad Jamal, whose elegant approach and use of space influenced him deeply. He also definitively severed his stylistic ties with bebop.
In 1951, Davis met Bob Weinstock, the owner of Prestige Records, and signed a contract with the label. Between 1951 and 1954, he released many records on Prestige, with several different combos. While the personnel of the recordings varied, the lineup often featured Sonny Rollins and Art Blakey. Davis was particularly fond of Rollins and tried several times, in the years that preceded his meeting with John Coltrane, to recruit him for a regular group. He never succeeded, however, mostly because Rollins was prone to make himself unavailable for months at a time. In spite of the casual occasions that generated these recordings, their quality is almost always quite high, and they document the evolution of Davis' style and sound. During this time he began using the Harmon mute, held close to the microphone, in a way that became his signature, and his phrasing, especially in ballads, became spacious, melodic, and relaxed. This sound became so characteristic that the use of the Harmon mute by any jazz trumpet player since immediately conjures up Miles Davis.
The most important Prestige recordings of this period ("Dig", "Blue Haze", "Bags' Groove", "Miles Davis and the Modern Jazz Giants", and "Walkin"') originated mostly from recording sessions in 1951 and 1954, after Davis' recovery from his addiction. Also of importance are his five Blue Note recordings, collected in the "Miles Davis Volume 1" album.
With these recordings, Davis assumed a central position in what is known as hard bop. In contrast with bebop, hard bop used slower tempos and a less radical approach to harmony and melody, often adopting popular tunes and standards from the American songbook as starting points for improvisation. Hard bop also distanced itself from cool jazz by virtue of a harder beat and by its constant reference to the blues, both in its traditional form and in the form made popular by rhythm and blues. A few critics go as far as to call "Walkin"' the album that created hard bop, but the point is debatable, given the number of musicians who were working along similar lines at the same time (and of course many of them recorded or played with Davis).
In this period, Davis gained a reputation for being distant, cold, and withdrawn, and for having a quick temper. Factors that contributed to this reputation included his contempt for the critics and specialized press, and some well-publicized confrontations with the public and with fellow musicians.
A near fight with Thelonious Monk during the recording of "Bags' Groove", received wide exposure in the specialized press.
Davis had an operation to remove polyps from his larynx in October 1955. Even though he was not supposed to speak at all for ten days, he had an argument with somebody and raised his voice. This outburst damaged his vocal cords forever, giving him the characteristic raspy voice that came to be associated with him. "[...] in February or March 1956, that I had my first throat operation and had to disband the group while recovering. During the course of the conversation I raised my voice to make a point and f***ed up my voice. I wasn't even supposed to talk for at least ten days, and here I was not only talking, but talking loudly. After that incident my voice had this whisper that has been with me ever since."
The "nocturnal" quality of Davis' playing and his somber reputation, along with his whispering voice, earned him the lasting moniker of "prince of darkness", adding a patina of mystery to his public persona.
First great quintet and sextet (1955–58).
Back in New York and in better health, in 1955 Davis attended the Newport Jazz Festival, where his performance (and especially his solo on "'Round Midnight") was greatly admired and prompted the critics to hail the "return of Miles Davis". At the same time, Davis recruited the players for a formation that became known as his "first great quintet": John Coltrane on tenor saxophone, Red Garland on piano, Paul Chambers on bass, and Philly Joe Jones on drums.
None of these musicians, with the exception of Davis, had received a great deal of exposure before that time; Chambers, in particular, was very young (19 at the time), a Detroit player who had been on the New York scene for only about a year, working with the bands of Bennie Green, Paul Quinichette, George Wallington, J. J. Johnson, and Kai Winding. Coltrane was little known at the time, in spite of earlier collaborations with Dizzy Gillespie, Earl Bostic, and Johnny Hodges. Davis hired Coltrane as a replacement for Sonny Rollins, after unsuccessfully trying to recruit alto saxophonist Julian "Cannonball" Adderley.
The repertoire included many bebop mainstays, standards from the Great American Songbook and the pre-bop era, and some traditional tunes. The prevailing style of the group was a development of the Davis experience in the previous years—Davis playing long, legato, and essentially melodic lines, while Coltrane, who during these years emerged as a leading figure on the musical scene, contrasted by playing high-energy solos.
With the new formation also came a new recording contract. In Newport, Rhode Island, Davis had met Columbia Records producer George Avakian, who persuaded him to sign with his label. The quintet made its debut on record with the extremely well received "'Round About Midnight". Before leaving Prestige, however, Davis had to fulfill his obligations during two days of recording sessions in 1956. Prestige released these recordings in the following years as four albums: "Relaxin' with the Miles Davis Quintet", "Steamin' with the Miles Davis Quintet", "Workin' with the Miles Davis Quintet", and "Cookin' with the Miles Davis Quintet". While the recording took place in a studio, each record of this series has the structure and feel of a live performance, with several first takes on each album. The records became almost instant classics and were instrumental in establishing Davis' quintet as one of the best on the jazz scene.
The quintet was disbanded for the first time in 1957, following a series of personal problems that Davis blames on the drug addiction of the other musicians. Davis played some gigs at the "Cafe Bohemia" with a short-lived formation that included Sonny Rollins and drummer Art Taylor, and then traveled to France, where he recorded the score to Louis Malle's film "Ascenseur pour l'échafaud". With the aid of French session musicians Barney Wilen, Pierre Michelot, and René Urtreger, and expatriate American drummer Kenny Clarke, he recorded the entire soundtrack with an innovative procedure, without relying on written material: starting from sparse indication of the harmony and a general feel of a given piece, the group played by watching the movie on a screen in front of them and improvising.
A performance of the Ballets Africains from Guinea in 1958 sparked Davis's interest in modal music. This music, featuring the kalimba, stayed for long periods of time on a single chord, weaving in and out of consonance and dissonance. It was a very new concept in jazz at the time, then dominated by the chord-change based music of bebop.
Returning to New York in 1958, Davis successfully recruited Cannonball Adderley for his standing group. Coltrane, who in the meantime had freed himself from his drug habits, was available after a highly fruitful experience with Thelonious Monk and was hired back, as was Philly Joe Jones. With the quintet re-formed as a sextet, Davis recorded "Milestones", an album anticipating the new directions he was preparing to give to his music.
Almost immediately after the recording of "Milestones," Davis fired Garland and, shortly afterward, Jones, again for behavioral problems; he replaced them with Bill Evans—a young white pianist with a strong classical background—and drummer Jimmy Cobb. With this revamped formation, Davis began a year during which the sextet performed and toured extensively and produced a record ("1958 Miles", also known as "58 Sessions"). Evans had a unique, impressionistic approach to the piano, and his musical ideas had a strong influence on Davis. But after only eight months on the road with the group, he was burned out and left. He was soon replaced by Wynton Kelly, a player who brought to the sextet a swinging, bluesy approach that contrasted with Evans' more delicate playing.
Recordings with Gil Evans (1957–63).
In the late 1950s and early 1960s, Davis recorded a series of albums with Gil Evans, often playing flugelhorn as well as trumpet. The first, "Miles Ahead" (1957), showcased his playing with a jazz big band and a horn section arranged by Evans. Songs included Dave Brubeck's "The Duke," as well as Léo Delibes's "The Maids of Cadiz," the first piece of European classical music Davis had recorded. Another distinctive feature of the album was the orchestral passages that Evans had devised as transitions between the different tracks, which were joined together with the innovative use of editing in the post-production phase, turning each side of the album into a seamless piece of music.
In 1958, Davis and Evans were back in the studio to record "Porgy and Bess", an arrangement of pieces from George Gershwin's opera of the same name. The lineup included three members of the sextet: Paul Chambers, Philly Joe Jones, and Julian "Cannonball" Adderley. Davis called the album one of his favorites.
Also in 1958, he married his first wife Frances Taylor. Their marriage lasted 10 years, despite his persistent domestic violence.
"Sketches of Spain" (1959–1960) featured songs by contemporary Spanish composer Joaquin Rodrigo and also Manuel de Falla, as well as Gil Evans originals with a Spanish flavor. "Miles Davis at Carnegie Hall" (1961) includes Rodrigo's "Concierto de Aranjuez", along with other compositions recorded in concert with an orchestra under Evans' direction.
Sessions with Davis and Evans in 1962 resulted in the album "Quiet Nights", a short collection of bossa novas that was released against the wishes of both artists: Evans stated it was only half an album, and blamed the record company; Davis blamed producer Teo Macero, to whom he did not speak for more than two years. This was the last time Evans and Davis made a full album together; despite the professional separation, however, Davis noted later that "my best friend is Gil Evans."
"Kind of Blue" (1959–64).
In March and April 1959, Davis re-entered the studio with his working sextet to record what is widely considered his "magnum opus", "Kind of Blue." He called back Bill Evans, months away from forming what would become his own seminal trio, for the album sessions, as the music had been planned around Evans' piano style. Both Davis and Evans were acquainted with the ideas of pianist George Russell regarding modal jazz; Davis from discussions with Russell and others before the "Birth of the Cool" sessions, and Evans from study with Russell in 1956. Davis, however, had neglected to inform current pianist Kelly of Evans' role in the recordings; Kelly subsequently played only on the track "Freddie Freeloader" and was not present at the April dates for the album. "So What" and "All Blues" had been played by the sextet at performances prior to the recording sessions, but for the other three compositions, Davis and Evans prepared skeletal harmonic frameworks that the other musicians saw for the first time on the day of recording, to allow a fresher approach to their improvisations. The resulting album has proven both highly popular and enormously influential. According to the RIAA, "Kind of Blue" is the best-selling jazz album of all time, having been certified as quadruple platinum (4 million copies sold). In December 2009, the US House of Representatives voted 409–0 to pass a resolution honoring the album as a national treasure.
The trumpet Davis used on the recording is currently displayed in the music building on the campus of the University of North Carolina at Greensboro. It was donated to the school by Arthur "Buddy" Gist, who met Davis in 1949 and became a close friend. The gift was the reason why the jazz program at UNCG is named the "Miles Davis Jazz Studies Program."
In August 1959, the Miles Davis Quintet was appearing at the famous Birdland nightclub in New York City. After finishing a recording for the armed services, Davis took a break outside the club. As he was escorting an attractive blonde woman across the sidewalk to a taxi, Davis was told by a patrolman to "move on." Davis explained that he worked at the nightclub and refused to move. The officer said that he would arrest Davis and grabbed him as Davis protected himself. Witnesses said that the patrolman punched Davis in the stomach with his nightstick without provocation. While two detectives held the crowd back, a third detective approached Davis from behind and beat him about the head. Davis was arrested and taken to jail where he was charged with feloniously assaulting an officer. He was then taken to St. Clary Hospital where he received five stitches for a wound on his head. The following October, he was acquitted of the charge of disorderly conduct and was likewise acquitted the following January of the charge of third-degree assault.
Davis tried to pursue the case by bringing a suit against the New York City Police Department, but eventually dropped the proceedings in a plea bargain so he could recover his suspended cabaret card – entertainers awaiting trial were automatically deprived of their cards – and return to work in New York clubs. In his autobiography, Davis stated that the incident "changed my whole life and whole attitude again, made me feel bitter and cynical again when I was starting to feel good about the things that had changed in this country."
Davis persuaded Coltrane to play with the group on one final European tour in the spring of 1960. Coltrane then departed to form his classic quartet, although he returned for some of the tracks on Davis' 1961 album "Someday My Prince Will Come." After Coltrane, Davis tried various saxophonists, including Jimmy Heath, Sonny Stitt, and Hank Mobley. The quintet with Hank Mobley was recorded in the studio and on several live engagements at Carnegie Hall and the Black Hawk jazz club in San Francisco. Stitt's playing with the group is found on a recording made in Olympia, Paris (where Davis and Coltrane had played a few months before) and the "Live in Stockholm" album.
In 1963, Davis' longtime rhythm section of Kelly, Chambers, and Cobb departed. He quickly got to work putting together a new group, including tenor saxophonist George Coleman and bassist Ron Carter. Davis, Coleman, Carter and a few other musicians recorded half the tracks for an album in the spring of 1963. A few weeks later, seventeen-year-old drummer Tony Williams and pianist Herbie Hancock joined the group, and soon afterward Davis, Coleman, and the new rhythm section recorded the rest of "Seven Steps to Heaven."
The rhythm players melded together quickly as a section and with the horns. The group's rapid evolution can be traced through the "Seven Steps to Heaven" album, "In Europe" (July 1963), "My Funny Valentine" (February 1964), and "Four and More" (also February 1964). The quintet played essentially the same repertoire of bebop tunes and standards that earlier Davis bands had played, but they tackled them with increasing structural and rhythmic freedom and, in the case of the up-tempo material, breakneck speed.
Coleman left in the spring of 1964, to be replaced by avant-garde saxophonist Sam Rivers, on the suggestion of Tony Williams. Rivers remained in the group only briefly, but was recorded live with the quintet in Japan; this configuration can be heard on "Miles in Tokyo!" (July 1964).
By the end of the summer, Davis had persuaded Wayne Shorter to leave Art Blakey's Jazz Messengers and join the quintet. Shorter became the group's principal composer, and some of his compositions of this era (including "Footprints" and "Nefertiti") have become standards. While on tour in Europe, the group quickly made their first official recording, "Miles in Berlin" (September 1964). On returning to the United States later that year, ever the musical entrepreneur, Davis (at Jackie DeShannon's urging) was instrumental in getting the Byrds signed to Columbia Records.
Second great quintet (1964–68).
By the time of "E.S.P." (1965), Davis's lineup consisted of Wayne Shorter (saxophone), Herbie Hancock (piano), Ron Carter (bass), and Tony Williams (drums). The last of his acoustic bands, this group is often referred to as the "second great quintet".
A two-night Chicago performance in late 1965 is captured on "The Complete Live at the Plugged Nickel 1965", released in 1995. Unlike their studio albums, the live engagement shows the group still playing primarily standards and bebop tunes. Although some of the titles remain the same as the tunes played by the 1950s quintet, the quick tempos and musical departure from the framework of the tune are dramatic. It could be said that these live performances of standards are as radical as the studio recordings of new compositions on the albums listed below.
The recording of "Live at the Plugged Nickel" was not issued anywhere in the 1960s, first appearing as a Japan-only partial issue in the late 1970s, then as a double-LP in the U.S. and Europe in 1982. It was followed by a series of studio recordings: "Miles Smiles" (1966), "Sorcerer" (1967), "Nefertiti" (1967), "Miles in the Sky" (1968), and "Filles de Kilimanjaro" (1968). The quintet's approach to improvisation came to be known as "time no changes" or "freebop," because they abandoned the more conventional chord-change-based approach of bebop for a modal approach. Through "Nefertiti," the studio recordings consisted primarily of originals composed by Shorter, with occasional compositions by the other sidemen. In 1967, the group began to play their live concerts in continuous sets, each tune flowing into the next, with only the melody indicating any sort of demarcation. Davis's bands would continue to perform in this way until his retirement in 1975.
"Miles in the Sky" and "Filles de Kilimanjaro,"—which tentatively introduced electric bass, electric piano, and electric guitar on some tracks—pointed the way to the subsequent fusion phase of Davis's career. Davis also began experimenting with more rock-oriented rhythms on these records. By the time the second half of "Filles de Kilimanjaro" was recorded, bassist Dave Holland and pianist Chick Corea had replaced Carter and Hancock in the working band, though both Carter and Hancock occasionally contributed to future recording sessions. Davis soon began to take over the compositional duties of his sidemen.
Electric Miles (1968–75).
The guru-manipulator shifted gears at will in his early-'70s music, orchestrating moods and settings to subjugate the individual musical inspirations of his young close-enough-for-funk subgeniuses to the life of a single palpitating organism that would have perished without them—no arrangements, little composition, and not many solos either, although at any moment a player could find himself left to fly off on his own.
 — Robert Christgau, review of "Dark Magus" (1977)
Davis's influences included 1960s rock and funk artists such as Sly and the Family Stone and Parliament/Funkadelic, many of whom he met through Betty Mabry (later Betty Davis), a young model and songwriter Davis married in September 1968 and divorced a year later. The musical transition required that Davis and his band adapt to electric instruments in both live performances and the studio. By the time "In a Silent Way" had been recorded in February 1969, Davis had augmented his quintet with additional players. At various times Hancock or Joe Zawinul was brought in to join Corea on electric keyboards, and guitarist John McLaughlin made the first of his many appearances with Davis. By this point, Shorter was also doubling on soprano saxophone. After recording this album, Williams left to form his group Lifetime and was replaced by Jack DeJohnette.
Six months later, an even larger group of musicians, including Jack DeJohnette, Airto Moreira, and Bennie Maupin, recorded the double LP "Bitches Brew", which became a huge seller, reaching gold status by 1976. This album and "In a Silent Way" were among the first fusions of jazz and rock that were commercially successful, building on the groundwork laid by Charles Lloyd, Larry Coryell, and others who pioneered a genre that would become known as jazz fusion. Throughout 1969, Davis' touring band included Shorter, Corea, Holland, and DeJohnette; as the group never completed a studio recording, it has been subsequently characterized as the "lost quintet" by many critics. The quintet's repertoire included material from "Bitches Brew", "In a Silent Way", and the 1960s quintet albums, along with an occasional standard. 
Both "Bitches Brew" and "In a Silent Way" feature "extended" (more than 20 minutes each) compositions that were never actually "played straight through" by the musicians in the studio. Instead, Davis and producer Teo Macero selected musical motifs of various lengths from recorded extended improvisations and edited them together into a musical whole that exists only in the recorded version. "Bitches Brew" made use of such electronic effects as multi-tracking, tape loops, and other editing techniques. Both records, especially "Bitches Brew", were big sellers. Starting with "Bitches Brew", Davis's albums began to often feature cover art much more in line with psychedelic art or black power movements than that of his earlier albums. He took significant cuts in his usual performing fees in order to open for rock groups like the Steve Miller Band, Grateful Dead, Neil Young, and Santana. Several live albums (with a transitional sextet/septet including Corea, DeJohnette, Holland, percussionist Airto Moreira, and saxophonist Steve Grossman that expanded to encompass Keith Jarrett on electronic organ by June 1970) were recorded at these performances: ' (March 1970)," " (April 1970), and ' (June 1970).
By the time of "Live-Evil" in December 1970, Davis's ensemble—though retaining the exploratory imperative of "Bitches Brew"—had transformed into a much more funk-oriented group. Davis began experimenting with wah-wah effects on his horn. A new sextet including DeJohnette, Jarrett, Moreira, Gary Bartz and erstwhile Stevie Wonder bassist Michael Henderson—often referred to as the "Cellar Door band" (the live portions of "Live-Evil" were recorded at a Washington, DC, club by that name)—is documented in the six-CD box set "The Cellar Door Sessions," which was recorded over four nights in December 1970; however, the ensemble disbanded before recording a studio album. Earlier in 1970, Davis contributed extensively to the soundtrack of a documentary about the African-American boxer heavyweight champion Jack Johnson. Himself a devotee of boxing, Davis drew parallels between Johnson, whose career had been defined by the fruitless search for a Great White Hope to dethrone him, and Davis's own career, in which he felt the musical establishment of the time had prevented him from receiving the acclaim and rewards that were due him. The resulting album, 1971's "Jack Johnson", contained two long pieces that featured musicians (some of whom were not credited on the record) including guitarists John McLaughlin and Sonny Sharrock, Herbie Hancock on a Farfisa organ, and drummer Billy Cobham. McLaughlin and Cobham went on to become founding members of the Mahavishnu Orchestra in 1971. In 1972, Davis was introduced to the music of Karlheinz Stockhausen by Paul Buckmaster, leading to a period of new creative exploration. Biographer J. K. Chambers wrote that "the effect of Davis' study of Stockhausen could not be repressed for long... Davis' own 'space music' shows Stockhausen's influence compositionally." His recordings and performances during this period were described as "space music" by fans, by music critic Leonard Feather, and by Buckmaster, who described it as "a lot of mood changes—heavy, dark, intense—definitely space music."
During this period, Davis was committed to making music for the young African-American audience drawn to the more commercial, groove-oriented idioms of popular music that dominated the epoch; by November 1971, DeJohnette and Moreira had been replaced in the touring ensemble by drummer Leon "Ndugu" Chancler and percussionists James Mtume & Don Alias. "On the Corner" (1972) blended the incipient influence of Stockhausen with funk elements in a trenchantly improvisatory milieu. The album was highlighted by the appearance of saxophonist Carlos Garnett. Critics were not kind to the album; in his autobiography, Davis stated that critics could not figure out how to categorize it, and he complained that the album was not promoted to the right crowd. Columbia tried selling the album to the old jazz generation who didn't really understand it instead of the younger crowd that Miles intended the album for. After recording "On the Corner", Davis put together a new group, with only Henderson and Mtume returning from the Jarrett-era band. It included Garnett, guitarist Reggie Lucas, organist Lonnie Liston Smith, tabla player Badal Roy, sitarist Khalil Balakrishna, and drummer Al Foster. It was unusual in that only Smith was a major jazz instrumentalist; as a result, the music emphasized rhythmic density and shifting textures instead of individual solos. This group, which recorded in Philharmonic Hall for the album "" (1972), was unsatisfactory to Davis. Through the first half of 1973, he dropped the tabla and sitar, took over keyboard duties, and added guitarist Pete Cosey. The Davis/Cosey/Lucas/Henderson/Mtume/Foster ensemble would remain virtually intact over the next two years. Initially, Dave Liebman played saxophones and flute with the band; in 1974, he was replaced by Sonny Fortune, who was eventually supplanted by Sam Morrison during the band's final American engagements in 1975.
"Big Fun" (1974) was a double album containing four long improvisations, recorded between 1969 and 1972. Similarly, "Get Up With It" (1974) collected recordings from May 1970 to October 1974. Notably, the album included "He Loved Him Madly", a tribute to Duke Ellington, as well as one of Davis's most lauded pieces from this era, "Calypso Frelimo". It was his last studio album of the 1970s. In 1974 and 1975, Columbia recorded three double-LP live Davis albums: "Dark Magus", "Agharta", and "Pangaea". "Dark Magus" captures a 1974 New York concert; the latter two are recordings of consecutive concerts from the same February 1975 day in Osaka. At the time, only "Agharta" was available in the US; "Pangaea" and "Dark Magus" were initially released only by CBS/Sony Japan. All three feature at least two electric guitarists (Reggie Lucas and Pete Cosey, deploying an array of Hendrix-inspired electronic distortion devices; Dominique Gaumont is a third guitarist on "Dark Magus"), electric bass, drums, reeds, and Davis on electric trumpet and organ. These albums were the last he recorded for five years. Davis was troubled by osteoarthritis (which led to a hip replacement operation in 1976, the first of several), sickle-cell anemia, depression, bursitis, ulcers, and a renewed dependence on alcohol and drugs (primarily cocaine), and his performances were routinely panned by critics throughout late 1974 and early 1975. By the time the group reached Japan in February 1975, Davis was nearing a physical breakdown and required copious amounts of alcohol and narcotics to make it through his engagements. Nonetheless, as noted by Richard Cook and Brian Morton, during these concerts his trumpet playing "is of the highest and most adventurous order."
 This was music that polarized audiences, provoking boos and walk-outs amid the ecstasy of others. The length, density, and unforgiving nature of it mocked those who said that Miles was interested only in being trendy and popular. Some have heard in this music the feel and shape of a musician's late work, an egoless music that precedes its creator's death. As Theodor Adorno said of the late Beethoven, the disappearance of the musician into the work is a bow to mortality. It was as if Miles were testifying to all that he had been witness to for the past thirty years, both terrifying and joyful.
 — John Szwed, on "Agharta" (1975) and "Pangaea" (1976) 
Although the Japanese performances have been lauded as the apogee of Davis' experimental period, Pete Cosey would later assert that "the band really advanced after the Japanese tour." Davis undertook an arduous tour of the American Midwest (opening for Herbie Hancock) following his return from Japan, culminating in a series of club performances at the Bottom Line in New York and Pall's Mall in Boston throughout the spring and summer. However, his precarious health was compounded by an ulcer-related hospitalization in March 1975 and the diagnosis of a hernia in August 1975. After a hometown performance at New York's Schaefer Music Festival on September 5, 1975, Davis withdrew almost completely from the public eye for six years, enabled by an unprecedented special retainer issued by Columbia Records. As Gil Evans said, "His organism is tired. And after all the music he's contributed for 35 years, he needs a rest." In his memoirs, Davis is characteristically candid about his wayward mental state during this period, describing himself as a hermit, his Upper West Side apartment as a wreck, and detailing his drug and sex addictions. In 1976, "Rolling Stone" reported rumors of his imminent demise. Although he stopped practicing trumpet on a regular basis, Davis continued to compose intermittently and made three attempts at recording during his self-imposed exile from performing; these sessions (one with the assistance of Paul Buckmaster and Gil Evans, who left after not receiving promised compensation) bore little fruit and remain unreleased. In 1979, he placed in the yearly top-ten trumpeter poll of "Down Beat". Columbia continued to issue compilation albums and records of unreleased vault material to fulfill contractual obligations. During his period of inactivity, Davis saw the fusion music that he had spearheaded over the past decade enter into the mainstream. When he emerged from retirement, Davis's musical descendants—most notably Prince—would be in the realm of new wave rock.
Later years and death.
By 1979, Davis had rekindled his relationship with actress Cicely Tyson, with whom he overcame his cocaine addiction and regained his enthusiasm for music. As he had not played trumpet for the better part of three years, regaining his famed embouchure proved particularly arduous. While recording "The Man with the Horn" at a leisurely pace throughout 1980–1981, Davis played mostly wahwah with a younger, larger band.
The initial large band was eventually abandoned in favor of a smaller combo featuring saxophonist Bill Evans (not to be confused with pianist Bill Evans of the 1958–59 sextet), and bass player Marcus Miller, both of whom would be among Davis's most regular collaborators throughout the decade. He married Tyson in 1981; they would divorce in 1988. "The Man with the Horn" was finally released in 1981 and received a poor critical reception despite selling fairly well. In May, the new band played two dates as part of the Newport Jazz Festival. The concerts, as well as the live recording "We Want Miles" from the ensuing tour, received positive reviews.
By late 1982, Davis's band included French percussionist Mino Cinelu and guitarist John Scofield, with whom he worked closely on the album "Star People." In mid-1983, while working on the tracks for "Decoy", an album mixing soul music and electronica that was released in 1984, Davis brought in producer, composer and keyboardist Robert Irving III, who had earlier collaborated with him on "The Man with the Horn." With a seven-piece band, including Scofield, Evans, keyboardist and music director Irving, drummer Al Foster and bassist Darryl Jones (later of the Rolling Stones), Davis played a series of European gigs to positive receptions. While in Europe, he took part in the recording of "Aura", an orchestral tribute to Davis composed by Danish trumpeter Palle Mikkelborg.
"You're Under Arrest", Davis' next album, was released in 1985 and included another brief stylistic detour. Included on the album were his interpretations of Cyndi Lauper's ballad "Time After Time", and Michael Jackson's pop hit "Human Nature". Davis considered releasing an entire album of pop songs and recorded dozens of them, but the idea was scrapped. Davis noted that many of today's accepted jazz standards were in fact pop songs from Broadway theater, and that he was simply updating the "standards" repertoire with new material. 1985 also saw Davis guest-star on the TV show "Miami Vice" as pimp and minor criminal Ivory Jones in the episode titled "Junk Love" (first aired November 8, 1985).
"You're Under Arrest" was Davis' final album for Columbia. Trumpeter Wynton Marsalis publicly dismissed Davis' more recent fusion recordings as not being "'true' jazz," comments Davis initially shrugged off, calling Marsalis "a nice young man, only confused." This changed after Marsalis appeared, unannounced, onstage in the midst of Davis' performance at the inaugural Vancouver International Jazz Festival in 1986. Marsalis whispered into Davis' ear that "someone" had told him to do so. Davis responded by ordering him off the stage.
Davis grew irritated at Columbia's delay releasing "Aura". The breaking point in the label-artist relationship appears to have come when a Columbia jazz producer requested Davis place a goodwill birthday call to Marsalis. Davis signed with Warner Bros. Records shortly thereafter.
Davis collaborated with a number of figures from the British post-punk and new wave movements during this period, including Scritti Politti. At the invitation of producer Bill Laswell, Davis recorded some trumpet parts during sessions for Public Image Ltd.'s "Album", according to Public Image's John Lydon in the liner notes of their "Plastic Box" box set. In Lydon's words, however, "strangely enough, we didn't use [his contributions]." According to Lydon in the "Plastic Box" notes, Davis favorably compared Lydon's singing voice to his trumpet sound during these sessions.
Having first taken part in the Artists United Against Apartheid recording, Davis signed with Warner Brothers records and reunited with Marcus Miller. The resulting record, "Tutu" (1986), was his first to use modern studio tools—programmed synthesizers, samples and drum loops—to create an entirely new setting for his playing. The album was described as the modern counterpart of "Sketches of Spain" and won a Grammy in 1987. He was featured on the instrumental Toto track "Don't Stop Me Now" from their "Fahrenheit" album (1986).
He followed "Tutu" with "Amandla", another collaboration with Miller and George Duke, plus the soundtracks to four movies: "Street Smart", "Siesta", "The Hot Spot" (with bluesman John Lee Hooker), and "Dingo." He continued to tour with a band of constantly rotating personnel and a critical stock at a level higher than it had been for 15 years. His last recordings, both released posthumously, were the hip hop-influenced studio album "Doo-Bop" and "Miles & Quincy Live at Montreux", a collaboration with Quincy Jones for the 1991 Montreux Jazz Festival. For the first time in three decades, Davis returned to the songs arranged by Gil Evans on such 1950s albums as "Miles Ahead", "Porgy and Bess" and "Sketches of Spain". This album was also the last album recorded by Davis. It left a lot of people who had been disappointed with his newer, more experimental works happy that he had ended his career in such a way.
In 1988 he had a small part as a street musician in the film "Scrooged", starring Bill Murray. In 1989, Davis was interviewed on "60 Minutes" by Harry Reasoner. Davis received the Grammy Lifetime Achievement Award in 1990.
In early 1991, he appeared in the Rolf de Heer film "Dingo" as a jazz musician. In the film's opening sequence, Davis and his band unexpectedly land on a remote airstrip in the Australian outback and proceed to perform for the surprised locals. The performance was one of Davis's last on film and one of the first released after his death in September.
During the last years of Miles Davis's life, there were rumors that he had AIDS, something that he and his manager Peter Shukat vehemently denied. According to Quincy Troupe by that time Davis was taking azidothymidine (AZT), a type of antiretroviral drug used for the treatment of HIV/AIDS.
Davis died on September 28, 1991, from the combined effects of a stroke, pneumonia and respiratory failure in Santa Monica, California, at the age of 65. He is buried in Woodlawn Cemetery in the Bronx.
Views on his earlier work.
Late in his life, from the "electric period" onwards, Davis repeatedly explained his reasons for not wishing to perform his earlier works, such as "Birth of the Cool" or "Kind of Blue". In his view, remaining stylistically static was the wrong option. He commented: " "So What" or "Kind of Blue", they were done in that era, the right hour, the right day, and it happened. It's over [...] What I used to play with Bill Evans, all those different modes, and substitute chords, we had the energy then and we liked it. But I have no feel for it anymore, it's more like warmed-over turkey." When Shirley Horn insisted in 1990 that Miles reconsider playing the ballads and modal tunes of his "Kind of Blue" period, he demurred. "Nah, it hurts my lip," was the reason he gave.
Other musicians regretted Davis's change of style, for example, Bill Evans, who was instrumental in creating "Kind of Blue", said: "I would like to hear more of the consummate melodic master, but I feel that big business and his record company have had a corrupting influence on his material. The rock and pop thing certainly draws a wider audience. It happens more and more these days, that unqualified people with executive positions try to tell musicians what is good and what is bad music."
Legacy and influence.
Miles Davis is regarded as one of the most innovative, influential and respected figures in the history of music. He has been described as “one of the great innovators in jazz”. "The Rolling Stone Encyclopedia of Rock & Roll" noted "Miles Davis played a crucial and inevitably controversial role in every major development in jazz since the mid-'40s, and no other jazz musician has had so profound an effect on rock. Miles Davis was the most widely recognized jazz musician of his era, an outspoken social critic and an arbiter of style—in attitude and fashion—as well as music". His album "Kind of Blue" is the best-selling album in the history of jazz music. On November 5, 2009, Rep. John Conyers of Michigan sponsored a measure in the United States House of Representatives to recognize and commemorate the album on its 50th anniversary. The measure also affirms jazz as a national treasure and "encourages the United States government to preserve and advance the art form of jazz music." It passed, unanimously, with a vote of 409–0 on December 15, 2009.
As an innovative bandleader and composer, Miles Davis has influenced many notable musicians and bands from diverse genres. Many well-known musicians rose to prominence as members of Davis's ensembles, including saxophonists Gerry Mulligan, John Coltrane, Cannonball Adderley, George Coleman, Wayne Shorter, Dave Liebman, Branford Marsalis and Kenny Garrett; trombonist J. J. Johnson; pianists Horace Silver, Red Garland, Wynton Kelly, Bill Evans, Herbie Hancock, Joe Zawinul, Chick Corea, Keith Jarrett and Kei Akagi; guitarists John McLaughlin, Pete Cosey, John Scofield and Mike Stern; bassists Paul Chambers, Ron Carter, Dave Holland, Marcus Miller and Darryl Jones; and drummers Elvin Jones, Philly Joe Jones, Jimmy Cobb, Tony Williams, Billy Cobham, Jack DeJohnette, and Al Foster. Miles' influence on the people who played with him has been described by music writer and author Christopher Smith as follows:
His approach, owing largely to the African American performance tradition that focused on individual expression, emphatic interaction, and creative response to shifting contents, had a profound impact on generations of jazz musicians.
In 1986, the New England Conservatory awarded Miles Davis an Honorary Doctorate for his extraordinary contributions to music. Since 1960 the National Academy of Recording Arts and Sciences (NARAS) has honored him with eight Grammy Awards, a Grammy Lifetime Achievement Award, and three Grammy Hall of Fame Awards.
In 2010, Moldejazz premiered a play called "Driving Miles", which focused on a landmark concert Davis performed in Molde, Norway, in 1984.

</doc>
<doc id="20406" url="http://en.wikipedia.org/wiki?curid=20406" title="M-theory">
M-theory

M-theory is a theory in physics that unifies all consistent versions of superstring theory. The existence of such a theory was first conjectured by Edward Witten at a string theory conference at the University of Southern California in the spring of 1995. Witten's announcement initiated a flurry of research activity known as the second superstring revolution.
Prior to Witten's announcement, string theorists had identified five versions of superstring theory. Although these theories appeared at first to be very different, work by several physicists showed that the theories were related in intricate and nontrivial ways. In particular, physicists found that apparently distinct theories could be identified by mathematical transformations called S-duality and T-duality. Witten's conjecture was based in part on the existence of these dualities and in part on the relationship of the string theories to a field theory called eleven-dimensional supergravity.
Although a complete formulation of M-theory is not known, the theory should describe two- and five-dimensional objects called branes and should be approximated by eleven-dimensional supergravity at low energies. Modern attempts to formulate M-theory are typically based on matrix theory or the AdS/CFT correspondence. According to Witten, M should stand for “magic”, “mystery”, or “membrane” according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.
Investigations of the mathematical structure of M-theory have spawned important theoretical results in physics and mathematics. More speculatively, M-theory may provide a framework for developing a unified theory of all of the fundamental forces of nature. Attempts to connect M-theory to experiment typically focus on compactifying its extra dimensions to construct models of our four-dimensional world.
Background.
Quantum gravity and strings.
One of the deepest problems in modern physics is the problem of quantum gravity. Our current understanding of gravity is based on Albert Einstein's general theory of relativity, which is formulated within the framework of classical physics. However, nongravitational forces are described within the framework of quantum mechanics, a radically different formalism for describing physical phenomena based on probability. A quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, but difficulties arise when one attempts to apply the usual prescriptions of quantum theory to the force of gravity.
String theory is a theoretical framework that attempts to reconcile gravity and quantum mechanics. In string theory, the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how strings propagate through space and interact with each other. In a given version of string theory, there is only one kind of string, which may look like a small loop or segment of ordinary string, and it can vibrate in different ways. On distance scales larger than the string scale, a string will look just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In this way, all of the different elementary particles may be viewed as vibrating strings. One of the vibrational states of a string gives rise to the graviton, a quantum mechanical particle that carries gravitational force.
There are several versions of string theory: type I, type IIA, type IIB, and two flavors of heterotic string theory ("SO"(32) and "E"8×"E"8). The different theories allow different types of strings, and the particles that arise at low energies exhibit different symmetries. For example, the type I theory includes both open strings (which are segments with endpoints) and closed strings (which form closed loops), while types IIA and IIB include only closed strings. Each of these five string theories arises as a special limiting case of M-theory. This theory, like its string theory predecessors, is an example of a quantum theory of gravity. It describes a force just like the familiar gravitational force subject to the rules of quantum mechanics.
Number of dimensions.
In everyday life, there are three familiar dimensions of space: height, width and length. Einstein's general theory of relativity treats time as a dimension on par with the three spatial dimensions; in general relativity, space and time are not modeled as separate entities but are instead unified to a four-dimensional spacetime. In this framework, the phenomenon of gravity is viewed as a consequence of the geometry of spacetime.
In spite of the fact that the universe is well described by four-dimensional spacetime, there are several reasons why physicists consider theories in other dimensions. In some cases, by modeling spacetime in a different number of dimensions, a theory becomes more mathematically tractable, and one can perform calculations and gain general insights more easily. There are also situations where theories in two or three spacetime dimensions are useful for describing phenomena in condensed matter physics. Finally, there exist scenarios in which there could actually be more than four dimensions of spacetime which have nonetheless managed to escape detection.
One notable feature of string theory and M-theory is that these theories require extra dimensions of spacetime for their mathematical consistency. In string theory, spacetime is ten-dimensional, while in M-theory it is eleven-dimensional. In order to describe real physical phenomena using these theories, one must therefore imagine scenarios in which these extra dimensions would not be observed in experiments.
Compactification is one way of modifying the number of dimensions in a physical theory. In compactification, some of the extra dimensions are assumed to "close up" on themselves to form circles. In the limit where these curled up dimensions become very small, one obtains a theory in which spacetime has effectively a lower number of dimensions. A standard analogy for this is to consider a multidimensional object such as a garden hose. If the hose is viewed from a sufficient distance, it appears to have only one dimension, its length. However, as one approaches the hose, one discovers that it contains a second dimension, its circumference. Thus, an ant crawling on the surface of the hose would move in two dimensions.
Dualities.
Theories that arise as different limits of M-theory turn out to be related in highly nontrivial ways. One of the relationships that can exist between these different physical theories is called S-duality. This is a relationship which says that a collection of strongly interacting particles in one theory can, in some cases, be viewed as a collection of weakly interacting particles in a completely different theory. Roughly speaking, a collection of particles is said to be strongly interacting if they combine and decay often and weakly interacting if they do so infrequently. Type I string theory turns out to be equivalent by S-duality to the "SO"(32) heterotic string theory. Similarly, type IIB string theory is related to itself in a nontrivial way by S-duality.
Another relationship between different string theories is T-duality. Here one considers strings propagating around a circular extra dimension. T-duality states that a string propagating around a circle of radius "R" is equivalent to a string propagating around a circle of radius 1/"R" in the sense that all observable quantities in one description are identified with quantities in the dual description. For example, a string has momentum as it propagates around a circle, and it can also wind around the circle one or more times. The number of times the string winds around a circle is called the winding number. If a string has momentum "p" and winding number "n" in one description, it will have momentum "n" and winding number "p" in the dual description. For example, type IIA string theory is equivalent to type IIB string theory via T-duality, and the two versions of heterotic string theory are also related by T-duality.
In general, the term "duality" refers to a situation where two seemingly different physical systems turn out to be equivalent in a nontrivial way. If two theories are related by a duality, it means that one theory can be transformed in some way so that it ends up looking just like the other theory. The two theories are then said to be "dual" to one another under the transformation. Put differently, the two theories are mathematically different descriptions of the same phenomena.
Supersymmetry.
Another important theoretical idea that plays a role in M-theory is supersymmetry. This is a mathematical relation that exists in certain physical theories between a class of particles called bosons and a class of particles called fermions. Roughly speaking, fermions are the constituents of matter, while bosons mediate interactions between particles. In theories with supersymmetry, each boson has a counterpart which is a fermion, and vice versa. When supersymmetry is imposed as a local symmetry, one automatically obtains a quantum mechanical theory that includes gravity. Such a theory is called a supergravity theory.
A theory of strings that incorporates the idea of supersymmetry is called a superstring theory. There are several different versions of superstring theory which are all subsumed within the M-theory framework. At low energies, the superstring theories are approximated by supergravity in ten spacetime dimensions. Similarly, M-theory is approximated at low energies by supergravity in eleven dimensions.
Branes.
In string theory and related theories such as supergravity theories, a brane is a physical object that generalizes the notion of a point particle to higher dimensions. For example, a point particle can be viewed as a brane of dimension zero, while a string can be viewed as a brane of dimension one. It is also possible to consider higher-dimensional branes. In dimension "p", these are called "p"-branes. Branes are dynamical objects which can propagate through spacetime according to the rules of quantum mechanics. They can have mass and other attributes such as charge. A "p"-brane sweeps out a ("p"+1)-dimensional volume in spacetime called its "worldvolume". Physicists often study fields analogous to the electromagnetic field which live on the worldvolume of a brane. The word brane comes from the word "membrane" which refers to a two-dimensional brane.
In string theory, the fundamental objects that give rise to elementary particles are the one-dimensional strings. Although the physical phenomena described by M-theory are still poorly understood, physicists know that the theory describes two- and five-dimensional branes. Much of the current research in M-theory attempts to better understand the properties of these branes.
History and development.
Kaluza–Klein theory.
In the early 20th century, physicists and mathematicians including Albert Einstein and Hermann Minkowski pioneered the use of four-dimensional geometry for describing the physical world. These efforts culminated in the formulation of Einstein's general theory of relativity, which relates gravity to the geometry of four-dimensional spacetime.
The success of general relativity led to efforts to apply higher dimensional geometry to explain other forces. In 1919, work by Theodor Kaluza showed that by passing to five-dimensional spacetime, one can unify gravity and electromagnetism into a single force. This idea was improved by physicist Oskar Klein, who suggested that the additional dimension proposed by Kaluza could take the form of a circle with radius around 10−30 cm.
The Kaluza–Klein theory and subsequent attempts by Einstein to develop unified field theory were never completely successful. In part this was because Kaluza–Klein theory predicted a particle that has never been shown to exist, and in part because it was unable to correctly predict the ratio of an electron's mass to its charge. In addition, these theories were being developed just as other physicists were beginning to discover quantum mechanics, which would ultimately prove successful in describing known forces such as electromagnetism, as well as new nuclear forces that were being discovered throughout the middle part of the century. Thus it would take almost fifty years for the idea of new dimensions to be taken seriously again.
Early work on supergravity.
New concepts and mathematical tools provided fresh insights into general relativity, giving rise to a period in the 1960s and 70s now known as the golden age of general relativity. In the mid-1970s, physicists began studying higher-dimensional theories combining general relativity with supersymmetry, the so-called supergravity theories.
General relativity does not place any limits on the possible dimensions of spacetime. Although the theory is typically formulated in four dimensions, one can write down the same equations for the gravitational field in any number of dimensions. Supergravity is more restrictive because it places an upper limit on the number of dimensions. In 1978, work by Werner Nahm showed that the maximum spacetime dimension in which one can formulate a consistent supersymmetric theory is eleven. In the same year, Eugene Cremmer, Bernard Julia, and Joel Scherk of the École Normale Supérieure showed that supergravity not only permits up to eleven dimensions but is in fact most elegant in this maximal number of dimensions.
Initially, many physicists hoped that by compactifying eleven-dimensional supergravity, it might be possible to construct realistic models of our four-dimensional world. The hope was that such models would provide a unified description of the four fundamental forces of nature: electromagnetism, the strong and weak nuclear forces, and gravity. Interest in eleven-dimensional supergravity soon waned as various flaws in this scheme were discovered. One of the problems was that the laws of physics appear to distinguish between clockwise and counterclockwise, a phenomenon known as chirality. Edward Witten and others observed this chirality property cannot be readily derived by compactifying from eleven dimensions.
In the first superstring revolution in 1984, many physicists turned to string theory as a unified theory of particle physics and quantum gravity. Unlike supergravity theory, string theory was able to accommodate the chirality of the standard model, and it provided a theory of gravity consistent with quantum effects. Another feature of string theory that many physicists were drawn to in the 1980s and 1990s was its high degree of uniqueness. In ordinary particle theories, one can consider any collection of elementary particles whose classical behavior is described by an arbitrary Lagrangian. In string theory, the possibilities are much more constrained: by the 1990s, physicists had argued that there were only five consistent supersymmetric versions of the theory.
Relationships between string theories.
Although there were only a handful of consistent superstring theories, it remained a mystery why there was not just one consistent formulation. However, as physicists began to examine string theory more closely, they realized that these theories are related in intricate and nontrivial ways.
In the late 1970s, Claus Montonen and David Olive had conjectured a special property of certain physical theories. A sharpened version of their conjecture concerns a theory called supersymmetric Yang–Mills theory, which describes particles similar to the quarks and gluons that make up atomic nuclei. The strength with which the particles of this theory interact is measured by a number called the coupling constant. The result of Montonen and Olive, now known as Montonen–Olive duality, states that supersymmetric Yang–Mills theory with coupling constant "g" is equivalent to the same theory with coupling constant 1/"g". In other words, a system of strongly interacting particles (large coupling constant) has an equivalent description as a system of weakly interacting particles (small coupling constant) and vice versa.
In the 1990s, several theorists generalized Montonen–Olive duality to the S-duality relationship, which connects different string theories. Ashoke Sen studied S-duality in the context of heterotic strings in four dimensions. Chris Hull and Paul Townsend showed that type IIB string theory with a large coupling constant is equivalent via S-duality to the same theory with small coupling constant. Theorists also found that different string theories may be related by T-duality. This duality implies that strings propagating on completely different spacetime geometries may be physically equivalent.
Membranes and fivebranes.
String theory extends ordinary particle physics by promoting zero-dimensional point particles to one-dimensional objects called strings. In the late 1980s, it was natural for theorists to attempt to formulate other extensions in which particles are replaced by two-dimensional supermembranes or by higher-dimensional objects called branes. Such objects had been considered as early as 1962 by Paul Dirac, and they were reconsidered by a small but enthusiastic group of physicists in the 1980s.
Supersymmetry severely restricts the possible number of dimensions of a brane. In 1987, Eric Bergshoeff, Ergin Sezgin, and Paul Townsend showed that eleven-dimensional supergravity includes two-dimensional branes. Intuitively, these objects look like sheets or membranes propagating through the eleven-dimensional spacetime. Shortly after this discovery, Michael Duff, Paul Howe, Takeo Inami, and Kellogg Stelle considered a particular compactification of eleven-dimensional supergravity with one of the dimensions curled up into a circle. In this setting, one can imagine the membrane wrapping around the circular dimension. If the radius of the circle is sufficiently small, then this membrane looks just like a string in ten-dimensional spacetime. In fact, Duff and his collaborators showed that this construction reproduces exactly the strings appearing in type IIA superstring theory.
In 1990, Andrew Strominger published a similar result which suggested that strongly interacting strings in ten dimensions might have an equivalent description in terms of weakly interacting five-dimensional branes. Initially, physicists were unable to prove this relationship for two important reasons. On the one hand, the Montonen–Olive duality was still unproven, and so Strominger's conjecture was even more tenuous. On the other hand, there were many technical issues related to the quantum properties of five-dimensional branes. The first of these problems was solved in 1993 when Ashoke Sen established that certain physical theories require the existence of objects with both electric and magnetic charge which were predicted by the work of Montonen and Olive.
In spite of this progress, the relationship between strings and five-dimensional branes remained conjectural because theorists were unable to quantize the branes. Starting in 1991, a team of researchers including Michael Duff, Ramzi Khuri, Jianxin Lu, and Ruben Minasian considered a special compactification of string theory in which four of the ten dimensions curl up. If one considers a five-dimensional brane wrapped around these extra dimensions, then the brane looks just like a one-dimensional string. In this way, the conjectured relationship between strings and branes was reduced to a relationship between strings and strings, and the latter could be tested using already established theoretical techniques.
Second superstring revolution.
Speaking at the string theory conference at the University of Southern California in 1995, Edward Witten of the Institute for Advanced Study made the surprising suggestion that all five superstring theories were in fact just different limiting cases of a single theory in eleven spacetime dimensions. Witten's announcement drew together all of the previous results on S- and T-duality and the appearance of two- and five-dimensional branes in string theory. In the months following Witten's announcement, hundreds of new papers appeared on the Internet confirming that the new theory involved membranes in an important way. Today this flurry of work is known as the second superstring revolution.
One of the important developments following Witten's announcement was Witten's work in 1996 with string theorist Petr Hořava. Witten and Hořava studied M-theory on a special spacetime geometry with two ten-dimensional boundary components. Their work shed light on the mathematical structure of M-theory and suggested possible ways of connecting M-theory to real world physics.
Origin of the term.
Initially, some physicists suggested that the new theory was a fundamental theory of membranes, but Witten was skeptical of the role of membranes in the theory. In a paper from 1996, Hořava and Witten wrote
As it has been proposed that the eleven-dimensional theory is a supermembrane theory but there are some reasons to doubt that interpretation, we will non-committally call it the M-theory, leaving to the future the relation of M to membranes.
In the absence of an understanding of the true meaning and structure of M-theory, Witten has suggested that the "M" should stand for "magic", "mystery", or "membrane" according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.
Matrix theory.
BFSS matrix model.
In mathematics, a matrix is a rectangular array of numbers or other data. In physics, a matrix model is a particular kind of physical theory whose mathematical formulation involves the notion of a matrix in an important way. A matrix model describes how a set of matrices evolves in time according to the rules of quantum mechanics.
One important example of a matrix model is the BFSS matrix model proposed by Tom Banks, Willy Fischler, Stephen Shenker, and Leonard Susskind in 1997. This theory describes the behavior of a set of nine large matrices. In their original paper, these authors showed, among other things, that the low energy limit of this matrix model is described by eleven-dimensional supergravity. These calculations led them to propose that the BFSS matrix model is exactly equivalent to M-theory. The BFSS matrix model can therefore be used as a prototype for a correct formulation of M-theory and a tool for investigating the properties of M-theory in a relatively simple setting.
Noncommutative geometry.
In geometry, it is often useful to introduce coordinates. For example, in order to study the geometry of the Euclidean plane, one defines the coordinates "x" and "y" as the distances between any point in the plane and a pair of axes. In ordinary geometry, the coordinates of a point are numbers, so they can be multiplied, and the product of two coordinates does not depend on the order of multiplication. That is, . This property of multiplication is known as the commutative law, and this relationship between geometry and the commutative algebra of coordinates is the starting point for much of modern geometry.
Noncommutative geometry is a branch of mathematics that attempts to generalize this situation. Rather than working with ordinary numbers, one considers some similar objects, such as matrices, whose multiplication does not satisfy the commutative law (that is, objects for which "xy" is not necessarily equal to "yx"). One imagines that these noncommuting objects are coordinates on some more general notion of "space" and proves theorems about these generalized spaces by exploiting the analogy with ordinary geometry.
In a paper from 1998, Alain Connes, Michael R. Douglas, and Albert Schwarz showed that some aspects of matrix models and M-theory are described by a noncommutative quantum field theory, a special kind of physical theory in which the coordinates on spacetime do not satisfy the commutativity property. This established a link between matrix models and M-theory on the one hand, and noncommutative geometry on the other hand. It quickly led to the discovery of other important links between noncommutative geometry and various physical theories.
AdS/CFT correspondence.
Overview.
The application of quantum mechanics to physical objects such as the electromagnetic field, which are extended in space and time, is known as quantum field theory. In particle physics, quantum field theories form the basis for our understanding of elementary particles, which are modeled as excitations in the fundamental fields. Quantum field theories are also used throughout condensed matter physics to model particle-like objects called quasiparticles.
One approach to formulating M-theory and studying its properties is provided by the anti-de Sitter/conformal field theory (AdS/CFT) correspondence. Proposed by Juan Maldacena in late 1997, the AdS/CFT correspondence is a theoretical result which implies that M-theory is in some cases equivalent to a quantum field theory. In addition to providing insights into the mathematical structure of string and M-theory, the AdS/CFT correspondence has shed light on many aspects of quantum field theory in regimes where traditional calculational techniques are ineffective.
In the AdS/CFT correspondence, the geometry of spacetime is described in terms of a certain vacuum solution of Einstein's equation called anti-de Sitter space. In very elementary terms, anti-de Sitter space is a mathematical model of spacetime in which the notion of distance between points (the metric) is different from the notion of distance in ordinary Euclidean geometry. It is closely related to hyperbolic space, which can be viewed as a disk as illustrated on the left. This image shows a tessellation of a disk by triangles and squares. One can define the distance between points of this disk in such a way that all the triangles and squares are the same size and the circular outer boundary is infinitely far from any point in the interior.
Now imagine a stack of hyperbolic disks where each disk represents the state of the universe at a given time. The resulting geometric object is three-dimensional anti-de Sitter space. It looks like a solid cylinder in which any cross section is a copy of the hyperbolic disk. Time runs along the vertical direction in this picture. The surface of this cylinder plays an important role in the AdS/CFT correspondence. As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.
This construction describes a hypothetical universe with only two space dimensions and one time dimension, but it can be generalized to any number of dimensions. Indeed, hyperbolic space can have more than two dimensions and one can "stack up" copies of hyperbolic space to get higher-dimensional models of anti-de Sitter space.
An important feature of anti-de Sitter space is its boundary (which looks like a cylinder in the case of three-dimensional anti-de Sitter space). One property of this boundary is that, within a small region on the surface around any given point, it looks just like Minkowski space, the model of spacetime used in nongravitational physics. One can therefore consider an auxiliary theory in which "spacetime" is given by the boundary of anti-de Sitter space. This observation is the starting point for AdS/CFT correspondence, which states that the boundary of anti-de Sitter space can be regarded as the "spacetime" for a quantum field theory. The claim is that this quantum field theory is equivalent to the gravitational theory on the bulk anti-de Sitter space in the sense that there is a "dictionary" for translating entities and calculations in one theory into their counterparts in the other theory. For example, a single particle in the gravitational theory might correspond to some collection of particles in the boundary theory. In addition, the predictions in the two theories are quantitatively identical so that if two particles have a 40 percent chance of colliding in the gravitational theory, then the corresponding collections in the boundary theory would also have a 40 percent chance of colliding.
6D (2,0) superconformal field theory.
One particular realization of the AdS/CFT correspondence states that M-theory on the product space "AdS"7×"S"4 is equivalent to the so-called (2,0)-theory on the six-dimensional boundary. Here "(2,0)" refers to the particular type of supersymmetry that appears in the theory. In this example, the spacetime of the gravitational theory is effectively seven-dimensional (hence the notation "AdS"7), and there are four additional "compact" dimensions (encoded by the "S"4 factor). In the real world, spacetime is four-dimensional, at least macroscopically, so this version of the correspondence does not provide a realistic model of gravity. Likewise, the dual theory is not a viable model of any real-world system since it describes a world with six spacetime dimensions.
Nevertheless, the (2,0)-theory has proven to be important for studying the general properties of quantum field theories. Indeed, this theory subsumes many mathematically interesting effective quantum field theories and points to new dualities relating these theories. For example, Luis Alday, Davide Gaiotto, and Yuji Tachikawa showed that by compactifying this theory on a surface, one obtains a four-dimensional quantum field theory, and there is a duality known as the AGT correspondence which relates the physics of this theory to certain physical concepts associated with the surface itself. More recently, theorists have extended these ideas to study the theories obtained by compactifying down to three dimensions.
In addition to its applications in quantum field theory, the (2,0)-theory has spawned important results in pure mathematics. For example, the existence of the (2,0)-theory was used by Witten to give a "physical" explanation for a conjectural relationship in mathematics called the geometric Langlands correspondence. In subsequent work, Witten showed that the (2,0)-theory could be used to understand a concept in mathematics called Khovanov homology. Developed by Mikhail Khovanov around 2000, Khovanov homology provides a tool in knot theory, the branch of mathematics that studies and classifies the different shapes of knots. Another application of the (2,0)-theory in mathematics is the work of Davide Gaiotto, Greg Moore, and Andrew Neitzke, which used physical ideas to derive new results in hyperkähler geometry.
ABJM superconformal field theory.
Another realization of the AdS/CFT correspondence states that M-theory on "AdS"4×"S"7 is equivalent to a quantum field theory called the ABJM theory in three dimensions. In this version of the correspondence, seven of the dimensions of M-theory are curled up, leaving four non-compact dimensions. Since the spacetime of our universe is four-dimensional, this version of the correspondence provides a somewhat more realistic description of gravity.
The ABJM theory appearing in this version of the correspondence is also interesting for a variety of reasons. Introduced by Aharony, Bergman, Jafferis, and Maldacena, it is closely related to another quantum field theory called Chern-Simons theory. The latter theory was popularized by Witten in the late 1980s because of its applications to knot theory. In addition, the ABJM theory serves as a semi-realistic simplified model for solving problems that arise in condensed matter physics.
Phenomenology.
Overview.
In addition to being an idea of considerable theoretical interest, M-theory provides a framework for constructing models of real world physics that combine general relativity with the standard model of particle physics. Phenomenology is the branch of theoretical physics in which physicists construct realistic models of nature from more abstract theoretical ideas. String phenomenology is the part of string theory that attempts to construct realistic models of particle physics based on string and M-theory.
Typically, such models are based on the idea of compactification. Starting with the ten- or eleven-dimensional spacetime of string or M-theory, physicists postulate a shape for the extra dimensions. By choosing this shape appropriately, they can construct models roughly similar to the standard model of particle physics, together with additional undiscovered particles. One popular way of deriving realistic physics from string theory is to start with the heterotic theory in ten dimensions and assume that the six extra dimensions of spacetime are shaped like a six-dimensional Calabi–Yau manifold. This is a special kind of geometric object named after mathematicians Eugenio Calabi and Shing-Tung Yau. Calabi–Yau manifolds offer many ways of extracting realistic physics from string theory. Other similar methods can be used to construct realistic models of our four-dimensional world based on M-theory.
Partly because of theoretical and mathematical difficulties and partly because of the extremely high energies needed to test these theories experimentally, there is so far no experimental evidence that would unambiguously point to any of these models being a correct fundamental description of nature. This has led some in the community to criticize these approaches to unification and question the value of continued research on these problems.
Compactification on "G"2 manifolds.
In one approach to M-theory phenomenology, theorists assume that the seven extra dimensions of M-theory are shaped like a "G"2 manifold. This is a special kind of seven-dimensional shape constructed by mathematician Dominic Joyce of the University of Oxford. These "G"2 manifolds are still poorly understood mathematically, and this fact has made it difficult for physicists to fully develop this approach to phenomenology.
For example, physicists and mathematicians often assume that space has a mathematical property called smoothness, but this property cannot be assumed in the case of a "G"2 manifold if one wishes to recover the physics of our four-dimensional world. Another problem is that "G"2 manifolds are not complex manifolds, so theorists are unable to use tools from the branch of mathematics known as complex analysis. Finally, there are many open questions about the existence, uniqueness, and other mathematical properties of "G"2 manifolds, and mathematicians lack a systematic way of searching for these manifolds.
Heterotic M-theory.
Because of the difficulties with "G"2 manifolds, most attempts to construct realistic theories of physics based on M-theory have taken a more indirect approach to compactifying eleven-dimensional spacetime. One approach, pioneered by Witten, Hořava, Burt Ovrut, and others, is known as heterotic M-theory. In this approach, one imagines that one of the eleven dimensions of M-theory is shaped like a circle. If this circle is very small, then the spacetime becomes effectively ten-dimensional. One then assumes that six of the ten dimensions form a Calabi–Yau manifold. If this Calabi–Yau manifold is also taken to be small, one is left with a theory in four-dimensions.
Heterotic M-theory has been used to construct models of brane cosmology in which the observable universe is thought to exist on a brane in a higher dimensional ambient space. It has also spawned alternative theories of the early universe that do not rely on the theory of cosmic inflation.
References.
Bibliography.
</dl>

</doc>
<doc id="20407" url="http://en.wikipedia.org/wiki?curid=20407" title="Multicast">
Multicast

In computer networking, multicast (one-to-many or many-to-many distribution) is group communication where information is addressed to a group of destination computers simultaneously. Multicast should not be confused with physical layer point-to-multipoint communication.
Group communication may either be "application layer multicast" or "network assisted multicast", where the latter makes it possible for the source to efficiently send to the group in a single transmission. Copies are automatically created in other network elements, such as routers, switches and cellular network base stations, but only to network segments that currently contain members of the group.
Network assisted multicast may be implemented at the Internet layer using IP multicast, which is often employed in Internet Protocol (IP) applications of streaming media, such as Internet television scheduled content (but not media-on-demand) and multipoint videoconferencing, but also for ghost distribution of backup disk images to multiple computers simultaneously. In IP multicast the implementation of the multicast concept occurs at the IP routing level, where routers create optimal distribution paths for datagrams sent to a multicast destination address.
Network assisted multicast may also be implemented at the Data Link Layer using one-to-many addressing and switching such as Ethernet multicast addressing, Asynchronous Transfer Mode (ATM) point-to-multipoint virtual circuits (P2MP) or Infiniband multicast.
IP multicast.
IP multicast is a technique for one-to-many communication over an IP infrastructure in a network. The destination nodes send join and leave messages, for example in the case of Internet television when the user changes from one TV channel to another. IP multicast scales to a larger receiver population by not requiring prior knowledge of who or how many receivers there are. Multicast uses network infrastructure efficiently by requiring the source to send a packet only once, even if it needs to be delivered to a large number of receivers. The nodes in the network take care of replicating the packet to reach multiple receivers only when necessary.
The most common transport layer protocol to use multicast addressing is User Datagram Protocol (UDP). By its nature, UDP is not reliable—messages may be lost or delivered out of order. By adding loss detection and retransmission mechanisms, reliable multicast has been implemented on top of UDP or IP by various middleware products, e.g. those that implement the Real-Time Publish-Subscribe (RTPS) Protocol of the OMG Data Distribution Service (DDS) standard, as well as by special transport protocols such as Pragmatic General Multicast (PGM).
IP multicast is widely deployed in enterprises, commercial stock exchanges, and multimedia content delivery networks. A common enterprise use of IP multicast is for IPTV applications such as distance learning and televised company meetings.
s of 2006[ [update]], most effort at scaling multicast up to large networks have concentrated on the simpler case of single-source multicast, which seems to be more computationally tractable.
Still, the large state requirements in routers make applications using a large number of trees unable to work while using IP multicast. Take presence information as an example where each person needs to keep at least one tree of its subscribers, if not several. No mechanism has yet been demonstrated that would allow the IP multicast model to scale to millions of senders and millions of multicast groups and, thus, it is not yet possible to make fully general multicast applications practical. For these reasons, and also reasons of economics, IP multicast is not, in general, used in commercial Internet backbones.
Application layer multicast.
Application layer multicast-over-unicast overlay services (not based on IP multicast or datalink layer multicast) for application level group communication are widely used. Notably the Internet Relay Chat (IRC), which is more pragmatic and scales better for large numbers of small groups. IRC implements a single spanning tree across its overlay network for all conference groups. However, this leads to suboptimal routing for some of these groups. Additionally, IRC keeps a large amount of distributed states that limit growth of an IRC network, leading to fractioning into several non-interconnected networks. The lesser known PSYC technology uses custom multicast strategies per conference. Also some peer-to-peer technologies employ the multicast concept when distributing content to multiple recipients, known as peercasting.
Explicit Multi-Unicast (Xcast) is an alternate multicast strategy that provides reception addresses of all destinations with each packet. As such, since the IP packet size is limited in general, Xcast cannot be used for multicast groups with many destinations. The Xcast model generally assumes that stations participating in the communication are known ahead of time, so that distribution trees can be generated and resources allocated by network elements in advance of actual data traffic.
Multicast over wireless networks and cable-TV.
Wireless communications (with exception to point-to-point radio links) as well as cable TV bus networks are inherently broadcasting media, i.e. multipoint channels, especially if the antennas are omni-directional and radio/TV transmitters covering a region form a broadcasting network that send the same content. However, the communication service provided may be unicast, multicast as well as broadcast, depending on if the data is addressed to one, to a group or to all receivers in the covered network, respectively.
In digital-TV, the concept of multicast service sometimes is used to refer to content protection by broadcast encryption, i.e. encrypted content over a simplex broadcast channel only addressed to paying viewers (pay television). In this case, data is broadcast (or distributed) to all receivers, but only addressed to a specific group.
The concept of "interactive multicast", for example using IP multicast, may be used over TV broadcast networks to improve efficiency, offer more TV programs, or reduce the required spectrum. Interactive multicast implies that TV programs are sent only over transmitters where there are viewers, and that only the most popular programs are transmitted. It relies on an additional interaction channel (a back-channel or return channel), where user equipment may send join and leave messages when the user changes TV channel. Interactive multicast has been suggested as an efficient transmission scheme in DVB-H and DVB-T2 terrestrial digital television systems, A similar concept is "switched broadcast" over cable-TV networks, where only the currently most popular content is delivered in the cable-TV network.
TV gateways converts Satellite: DVB-S, DVB-S2, Cable: DVB-C, DVB-C2 and Terrestrial television: DVB-T, DVB-T2 to IP for distribution using unicast and multicast in home, hospitality and enterprise applications
Another similar concept is Cell-TV, i.e. implies TV distribution over 3G cellular networks using the network-assisted multicasting offered by the Multimedia Broadcast Multicast Service (MBMS) service, or over 4G/LTE cellular networks with the eMBMS (enhanced MBMS) service.
Scalable video multicast in an application of interactive multicast, where a subset of the viewers receive additional data for high-resolution video.
Other multicast technologies.
In an optical mesh network, protecting multicast lightpaths is one of the key concerns. The most straight forward approach to protect a multicast tree is to establish a link-disjoint backup tree which establishes dedicated protection. It is much easier to find an arc-disjoint path for each leaf node in a light tree. The essence of protecting a multicast session is to find a backup path for each destination node when a link on the working path to that node fails.

</doc>
<doc id="20408" url="http://en.wikipedia.org/wiki?curid=20408" title="Marie Curie">
Marie Curie

Marie Skłodowska-Curie (; ]; 7 November 1867 – 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person and only woman to win twice, the only person to win twice in multiple sciences, and was part of the Curie family legacy of five Nobel Prizes. She was also the first woman to become a professor at the University of Paris, and in 1995 became the first woman to be entombed on her own merits in the Panthéon in Paris.
She was born Maria Salomea Skłodowska (]) in Warsaw, in what was then the Kingdom of Poland, part of the Russian Empire. She studied at Warsaw's clandestine Floating University and began her practical scientific training in Warsaw. In 1891, aged 24, she followed her older sister Bronisława to study in Paris, where she earned her higher degrees and conducted her subsequent scientific work. She shared the 1903 Nobel Prize in Physics with her husband Pierre Curie and with physicist Henri Becquerel. She won the 1911 Nobel Prize in Chemistry.
Her achievements included a theory of "radioactivity" (a term that she coined), techniques for isolating radioactive isotopes, and the discovery of two elements, polonium and radium. Under her direction, the world's first studies were conducted into the treatment of neoplasms, using radioactive isotopes. She founded the Curie Institutes in Paris and in Warsaw, which remain major centres of medical research today. During World War I, she established the first military field radiological centres.
While a French citizen, Marie Skłodowska Curie (she used both surnames) never lost her sense of Polish identity. She taught her daughters the Polish language and took them on visits to Poland. She named the first chemical element that she discovered – polonium, which she isolated in 1898 – after her native country.
Curie died in 1934 at the sanatorium of Sancellemoz (Haute-Savoie), France, due to aplastic anemia brought on by exposure to radiation – including carrying test tubes of radium in her pockets during research and her service during World War I in mobile X-ray units created by her.
Biography.
Early years.
Maria Skłodowska was born in Warsaw, in the Russian partition of Poland, on 7 November 1867, as the fifth and youngest child of well-known teachers Bronisława, "née" Boguska, and Władysław Skłodowski. Maria's older siblings were Zofia (born 1862), (1863), Bronisława (1865) and (1866).
On both the paternal and maternal sides, the family had lost their property and fortunes through patriotic involvements in Polish national uprisings aimed at restoring Poland's independence (the most recent had been the January Uprising of 1863–65). This condemned the subsequent generation, including Maria, her elder sisters and her brother, to a difficult struggle to get ahead in life.
Maria's paternal grandfather, , had been a respected teacher in Lublin, where he taught the young Bolesław Prus, who would become a leading figure in Polish literature. Her father, Władysław Skłodowski, taught mathematics and physics, subjects that Maria was to pursue, and was also director of two Warsaw "gymnasia" for boys. After Russian authorities eliminated laboratory instruction from the Polish schools, he brought much of the laboratory equipment home, and instructed his children in its use.
The father was eventually fired by his Russian supervisors for pro-Polish sentiments, and forced to take lower-paying posts; the family also lost money on a bad investment, and eventually chose to supplement their income by lodging boys in the house. Maria's mother Bronisława operated a prestigious Warsaw boarding school for girls; she resigned from the position after Maria was born. She died of tuberculosis in May 1878, when Maria was ten years old. Less than three years earlier, Maria's oldest sibling, Zofia, had died of typhus contracted from a boarder. Maria's father was an atheist; her mother a devout Catholic. The deaths of Maria's mother and sister caused her to give up Catholicism and become agnostic.
When she was ten years old, Maria began attending the boarding school of J. Sikorska; next she attended a "gymnasium" for girls, from which she graduated on 12 June 1883 with a gold medal. After a collapse, possibly due to depression, she spent the following year in the countryside with relatives of her father, and the next year with her father in Warsaw, where she did some tutoring. Unable to enroll in a regular institution of higher education because she was a woman, she and her sister Bronisława became involved with the clandestine Flying University, a Polish patriotic institution of higher learning that admitted women students.
Maria made an agreement with her sister, Bronisława, that she would give her financial assistance during Bronisława's medical studies in Paris, in exchange for similar assistance two years later. In connection with this, Maria took a position as governess: first as a home tutor in Warsaw; then for two years as a governess in Szczuki with a landed family, the Żorawskis, who were relatives of her father. While working for the latter family, she fell in love with their son, Kazimierz Żorawski, a future eminent mathematician. His parents rejected the idea of his marrying the penniless relative, and Kazimierz was unable to oppose them. Maria's loss of the relationship with Żorawski was tragic for both. He soon earned a doctorate and pursued an academic career as a mathematician, becoming a professor and rector of Kraków University. Still, as an old man and a mathematics professor at the Warsaw Polytechnic, he would sit contemplatively before the statue of Maria Skłodowska which had been erected in 1935 before the Radium Institute that she had founded in 1932.
At the beginning of 1890, Bronisława — who a few months earlier had married Kazimierz Dłuski, a Polish physician and social and political activist — invited Maria to join them in Paris. Maria declined because she could not afford the university tuition; it would take her a year and a half longer to gather the necessary funds. She was helped by her father, who was able to secure a more lucrative position again. All that time she continued to educate herself, reading books, exchanging letters, and being tutored herself. In early 1889 she returned home to her father in Warsaw. She continued working as a governess, and remained there till late 1891. She tutored, studied at the Flying University, and began her practical scientific training (1890–91) in a chemical laboratory at the Museum of Industry and Agriculture at "Krakowskie Przedmieście 66", near Warsaw's Old Town. The laboratory was run by her cousin Józef Boguski, who had been an assistant in Saint Petersburg to the Russian chemist Dmitri Mendeleev.
New life in Paris.
In late 1891 she left Poland for France. In Paris, Maria (or Marie, as she would be known in France) briefly found shelter with her sister and brother-in-law before renting a garret closer to the university, in the Latin Quarter, and proceeding with her studies of physics, chemistry and mathematics at the University of Paris, where she enrolled in late 1891. She subsisted on her meager resources, suffering from cold winters and occasionally fainting from hunger.
Marie studied during the day and tutored evenings, barely earning her keep. In 1893 she was awarded a degree in physics and began work in an industrial laboratory of Professor Gabriel Lippmann. Meanwhile she continued studying at the University of Paris, and with the aid of a fellowship she was able to earn a second degree in 1894.[b]
Marie had begun her scientific career in Paris with an investigation of the magnetic properties of various steels, commissioned by the The Society for the Encouragement of National Industry ("Société d'encouragement pour l'industrie nationale" ). That same year Pierre Curie entered her life; it was their mutual interest in natural sciences that drew them together. Pierre was an instructor at the School of Physics and Chemistry, the "École supérieure de physique et de chimie industrielles de la ville de Paris" (ESPCI). They were introduced by the Polish physicist, Professor , who had learned that Marie was looking for a larger laboratory space, something that Kowalski-Wierusz thought Pierre had access to. Though Pierre did not have a large laboratory, he was able to find some space for Marie where she was able to begin work.
Their mutual passion for science brought them increasingly closer, and they began to develop feelings for one another. Eventually Pierre proposed marriage, but at first Marie did not accept as she was still planning to go back to her native country. Pierre, however, declared that he was ready to move with her to Poland, even if it meant being reduced to teaching French. Meanwhile, for the 1894 summer break, Marie returned to Warsaw, where she visited her family. She was still laboring under the illusion that she would be able to work in her chosen field in Poland, but she was denied a place at Kraków University because she was a woman. A letter from Pierre convinced her to return to Paris to pursue a PhD. At Marie's insistence, Pierre had written up his research on magnetism and received his own doctorate in March 1895; he was also promoted to professor at the School. A contemporary quip would call Marie, "Pierre's biggest discovery." On 26 July 1895 they were married in Sceaux (Seine); neither wanted a religious service. Marie's dark blue outfit, worn instead of a bridal grown, would serve her for many years as a laboratory outfit. They shared two pastimes: long bicycle trips, and journeys abroad, which brought them even closer. In Pierre, Marie had found a new love, a partner, and a scientific collaborator on whom she could depend.
New elements.
In 1895 Wilhelm Roentgen discovered the existence of X-rays, though the mechanism behind their production was not yet understood. In 1896 Henri Becquerel discovered that uranium salts emitted rays that resembled X-rays in their penetrating power. He demonstrated that this radiation, unlike phosphorescence, did not depend on an external source of energy but seemed to arise spontaneously from uranium itself. Influenced by these two important discoveries, Marie decided to look into uranium rays as a possible field of research for a thesis.
She used an innovative technique to investigate samples. Fifteen years earlier, her husband and his brother had developed a version of the electrometer, a sensitive device for measuring electric charge. Using Pierre's electrometer, she discovered that uranium rays caused the air around a sample to conduct electricity. Using this technique, her first result was the finding that the activity of the uranium compounds depended only on the quantity of uranium present. She hypothesized that the radiation was not the outcome of some interaction of molecules but must come from the atom itself. This hypothesis was an important step in disproving the ancient assumption that atoms were indivisible.
In 1897 her daughter Irène was born. To support her family, Curie began teaching at the École Normale Supérieure. The Curies did not have a dedicated laboratory; most of their research was carried out in a converted shed next to the School of Physics and Chemistry. The shed, formerly a medical school dissecting room, was poorly ventilated and not even waterproof. They were unaware of the deleterious effects of radiation exposure attendant on their continued unprotected work with radioactive substances. The School did not sponsor her research, but she would receive subsidies from metallurgical and mining companies and from various organizations and governments.
Curie's systematic studies included two uranium minerals, pitchblende and torbernite (also known as chalcolite). Her electrometer showed that pitchblende was four times as active as uranium itself, and chalcolite twice as active. She concluded that, if her earlier results relating the quantity of uranium to its activity were correct, then these two minerals must contain small quantities of another substance that was far more active than uranium. She began a systematic search for additional substances that emit radiation, and by 1898 she discovered that the element thorium was also radioactive.
Pierre was increasingly intrigued by her work. By mid-1898 he was so invested in it that he decided to drop his work on crystals and to join her.
The [research] idea [writes Reid] was her own; no one helped her formulate it, and although she took it to her husband for his opinion she clearly established her ownership of it. She later recorded the fact twice in her biography of her husband to ensure there was no chance whatever of any ambiguity. It [is] likely that already at this early stage of her career [she] realized that... many scientists would find it difficult to believe that a woman could be capable of the original work in which she was involved.
She was acutely aware of the importance of promptly publishing her discoveries and thus establishing her priority. Had not Becquerel, two years earlier, presented his discovery to the "Académie des Sciences" the day after he made it, credit for the discovery of radioactivity, and even a Nobel Prize, would instead have gone to Silvanus Thompson. Curie chose the same rapid means of publication. Her paper, giving a brief and simple account of her work, was presented for her to the "Académie" on 12 April 1898 by her former professor, Gabriel Lippmann. Even so, just as Thompson had been beaten by Becquerel, so Curie was beaten in the race to tell of her discovery that thorium gives off rays in the same way as uranium; two months earlier, Gerhard Carl Schmidt had published his own finding in Berlin.
At that time, no one else in the world of physics had noticed what Curie recorded in a sentence of her paper, describing how much greater were the activities of pitchblende and chalcolite than uranium itself: "The fact is very remarkable, and leads to the belief that these minerals may contain an element which is much more active than uranium." She later would recall how she felt "a passionate desire to verify this hypothesis as rapidly as possible." On 14 April 1898 the Curies optimistically weighed out a 100-gram sample of pitchblende and ground it with a pestle and mortar. They did not realize at the time that what they were searching for was present in such minute quantities that they would eventually have to process tons of the ore.
In July 1898 Curie and her husband published a joint paper announcing the existence of an element which they named "polonium", in honour of her native Poland, which would for another twenty years remain partitioned among three empires. On 26 December 1898, the Curies announced the existence of a second element, which they named "radium", from the Latin word for "ray". In the course of their research, they also coined the word "radioactivity".
To prove their discoveries beyond any doubt, the Curies sought to isolate polonium and radium in pure form. Pitchblende is a complex mineral; the chemical separation of its constituents was an arduous task. The discovery of polonium had been relatively easy; chemically it resembles the element bismuth, and polonium was the only bismuth-like substance in the ore. Radium, however, was more elusive; it is closely related chemically to barium, and pitchblende contains both elements. By 1898 the Curies had obtained traces of radium, but appreciable quantities, uncontaminated with barium, were still beyond reach. The Curies undertook the arduous task of separating out radium salt by differential crystallization. From a ton of pitchblende, one-tenth of a gram of radium chloride was separated in 1902. In 1910 Marie Curie isolated pure radium metal. She never succeeded in isolating polonium, which has a half-life of only 138 days.
Between 1898 and 1902 the Curies published, jointly or separately, a total of 32 scientific papers, including one that announced that, when exposed to radium, diseased, tumor-forming cells were destroyed faster than healthy cells.
In 1900 Curie became the first woman faculty member at the École Normale Supérieure, and her husband joined the faculty of the University of Paris. In 1902 she visited Poland on the occasion of her father's death.
In June 1903, supervised by Gabriel Lippmann, Curie was awarded her doctorate from the University of Paris. That month the couple were invited to the Royal Institution in London to give a speech on radioactivity; being a woman, she was prevented from speaking, and Pierre alone was allowed to. Meanwhile a new industry began developing, based on radium. The Curies did not patent their discovery and benefited little from this increasingly profitable business.
Nobel Prizes.
In December 1903, the Royal Swedish Academy of Sciences awarded Pierre Curie, Marie Curie, and Henri Becquerel the Nobel Prize in Physics, "in recognition of the extraordinary services they have rendered by their joint researches on the radiation phenomena discovered by Professor Henri Becquerel." At first, the Committee intended to honour only Pierre and Becquerel, but one of the committee members and an advocate of woman scientists, Swedish mathematician Magnus Goesta Mittag-Leffler, alerted Pierre to the situation, and after his complaint, Marie's name was added to the nomination. Marie was the first woman to be awarded a Nobel Prize.
Curie and her husband declined to go to Stockholm to receive the prize in person; they were too busy with their work, and Pierre, who disliked public ceremonies, was feeling increasingly ill. As Nobel laureates were required to deliver a lecture, the Curies finally undertook the trip in 1905. The award money allowed the Curies to hire their first laboratory assistant. Following the award of the Nobel Prize, and galvanized by an offer from the University of Geneva, which offered Pierre a position, the University of Paris gave Pierre a professorship and the chair of physics, although the Curies still did not have a proper laboratory. Upon Pierre's complaint, the University of Paris relented and agreed to furnish a new laboratory, but it would not be ready until 1906.
In December 1904, Curie gave birth to their second daughter, Ève. She later hired Polish governesses to teach her daughters her native language, and sent or took them on visits to Poland.
On 19 April 1906, Pierre was killed in a road accident. Walking across the Rue Dauphine in heavy rain, he was struck by a horse-drawn vehicle and fell under its wheels, causing his skull to fracture. Curie was devastated by her husband's death. On 13 May 1906 the physics department of the University of Paris decided to retain the chair that had been created for Pierre and to offer it to Marie. She accepted it hoping to create a world-class laboratory as a tribute to Pierre. She was the first woman to become a professor at the University of Paris.
Curie's quest to create a new laboratory did not end with the University of Paris, however. In her later years, she headed the Radium Institute ("Institut du radium", now Curie Institute, "Institut Curie"), a radioactivity laboratory created for her by the Pasteur Institute and the University of Paris. The initiative for creating the Radium Institute had come in 1909 from Pierre Paul Émile Roux, director of the Pasteur Institute, who had been disappointed that the University of Paris was not giving Curie a proper laboratory and had suggested that she move to the Pasteur Institute. Only then, with the threat of Curie leaving, did the University of Paris relent, and eventually the Curie Pavilion became a joint initiative of the University of Paris and the Pasteur Institute.
In 1910 Curie succeeded in isolating radium; she also defined an international standard for radioactive emissions that was eventually named for her and Pierre: the curie. Nevertheless, in 1911 the French Academy of Sciences did not elect her to be a member by one or two votes. Elected instead was Édouard Branly, an inventor who had helped Guglielmo Marconi develop the wireless telegraph. A doctoral student of Curie, Marguerite Perey, became the first woman elected to membership in the Academy – over half a century later, in 1962. Despite Curie's fame as a scientist working for France, the public's attitude tended toward xenophobia—the same that had led to the Dreyfus affair–which also fuelled false speculation that Curie was Jewish. During the French Academy of Sciences elections, she was vilified by the right wing press who criticised her for being a foreigner and an atheist. Her daughter later remarked on the public hypocrisy as the French press often portrayed Curie as an unworthy foreigner when she was nominated for a French honour, but would portray her as a French hero when she received a foreign one such as her Nobel Prizes.
In 1911 it was revealed that in 1910–11 Curie had conducted an affair of about a year's duration with physicist Paul Langevin, a former student of Pierre's—a married man who was estranged from his wife. This resulted in a press scandal that was exploited by her academic opponents. Curie (then in her mid-40s) was five years older than Langevin and was misrepresented in the tabloids as a foreign Jewish home-wrecker. When the scandal broke, she was away at a conference in Belgium; on her return, she found an angry mob in front of her house and had to seek refuge, with her daughters, in the home of a friend.
International recognition for her work had been growing to new heights, and the Royal Swedish Academy of Sciences, overcoming opposition prompted by the Langevin scandal, honored her a second time, with the 1911 Nobel Prize in Chemistry. This award was "in recognition of her services to the advancement of chemistry by the discovery of the elements radium and polonium, by the isolation of radium and the study of the nature and compounds of this remarkable element." She was the first person to win or share two Nobel Prizes, and remains alone with Linus Pauling as Nobel laureates in two fields each. A delegation of celebrated Polish men of learning, headed by novelist Henryk Sienkiewicz, encouraged her to return to Poland and continue her research in her native country. Curie's second Nobel Prize enabled her to persuade the French government into supporting the Radium Institute, built in 1914, where research was conducted in chemistry, physics, and medicine. A month after accepting her 1911 Nobel Prize, she was hospitalised with depression and a kidney ailment. For most of 1912 she avoided public life but did spend time in England with her friend and fellow physicist, Hertha Ayrton. She returned to her laboratory only in December, after a break of about 14 months.
In 1912 the Warsaw Scientific Society offered her the directorship of a new laboratory in Warsaw but she declined, focusing on the developing Radium Institute to be completed in August 1914, and on a new street named Rue Pierre-Curie. She visited Poland in 1913 and was welcomed in Warsaw but the visit was mostly ignored by the Russian authorities. The Institute's development was interrupted by the coming war, as most researchers were drafted into the French Army, and it fully resumed its activities in 1919.
World War I.
During World War I, Curie saw a need for field radiological centres near the front lines to assist battlefield surgeons. After a quick study of radiology, anatomy, and automotive mechanics she procured X-ray equipment, vehicles, auxiliary generators, and developed mobile radiography units, which came to be popularly known as "petites Curies" ("Little Curies"). She became the director of the Red Cross Radiology Service and set up France's first military radiology centre, operational by late 1914. Assisted at first by a military doctor and by her 17-year-old daughter Irène, Curie directed the installation of 20 mobile radiological vehicles and another 200 radiological units at field hospitals in the first year of the war. Later, she began training other women as aides.
In 1915 Curie produced hollow needles containing 'radium emanation', a colorless, radioactive gas given off by radium, later identified as radon, to be used for sterilizing infected tissue. She provided the radium from her own one-gram supply. It is estimated that over a million wounded soldiers were treated with her X-ray units. Busy with this work, she carried out very little scientific research during that period. In spite of all her humanitarian contributions to the French war effort, Curie never received any formal recognition of it from the French government.
Also, promptly after the war started, she attempted to donate her gold Nobel Prize medals to the war effort but the French National Bank refused to accept them. She did buy war bonds, using her Nobel Prize money. She was also an active member in committees of Polonia in France dedicated to the Polish cause. After the war, she summarized her war time experiences in a book "Radiology in War" (1919).
Postwar years.
In 1920, for the 25th anniversary of the discovery of radium, the French government established a stipend for her; its previous recipient was Louis Pasteur (1822–95). In 1921, Marie was welcomed triumphantly when she toured the United States to raise funds for research on radium. Mrs. William Brown Meloney, after interviewing Marie, created a "Marie Curie Radium Fund" and raised money to buy radium, publicising her trip. In 1921, US President Warren G. Harding received her at the White House to present her with the 1 gram of radium collected in the United States. Before the meeting, recognising her growing fame abroad, and embarrassed by the fact that she had no French official distinctions to wear in public, the French government offered her a Legion of Honour award, but she refused. In 1922 she became a fellow of the French Academy of Medicine. She also travelled to other countries, appearing publicly and giving lectures in Belgium, Brazil, Spain, and Czechoslovakia.
Led by Curie, the Institute produced four more Nobel Prize winners, including her daughter Irène Joliot-Curie and her son-in-law, Frédéric Joliot-Curie. Eventually, it became one of four major radioactivity research laboratories, the others being the Cavendish Laboratory, with Ernest Rutherford; the Institute for Radium Research, Vienna, with Stefan Meyer; and the Kaiser Wilhelm Institute for Chemistry, with Otto Hahn and Lise Meitner.
In August 1922, Marie Curie became a member of the newly created International Commission for Intellectual Cooperation of the League of Nations. In 1923, she wrote a biography of Pierre, entitled "Pierre Curie". In 1925, she visited Poland, to participate in the ceremony that laid foundations for the Radium Institute in Warsaw. Her second American tour, in 1929, succeeded in equipping the Warsaw Radium Institute with radium; it was opened in 1932 and her sister Bronisława became its director. These distractions from her scientific labours and the attendant publicity caused her much discomfort but provided resources needed for her work. In 1930, she was elected a member of the International Atomic Weights Committee where she served until her death.
Death.
Curie visited Poland for the last time in early 1934. A few months later, on 4 July 1934, she died at the Sancellemoz Sanatorium in Passy, in Haute-Savoie, from aplastic anemia believed to have been contracted from her long-term exposure to radiation. The damaging effects of ionising radiation were not known at the time of her work, which had been carried out without the safety measures later developed. She had carried test tubes containing radioactive isotopes in her pocket, and she stored them in her desk drawer, remarking on the faint light that the substances gave off in the dark. Curie was also exposed to X-rays from unshielded equipment while serving as a radiologist in field hospitals during the war. Although her many decades of exposure to radiation caused chronic illnesses (including near blindness due to cataracts) and ultimately her death, she never really acknowledged the health risks of radiation exposure.
She was interred at the cemetery in Sceaux, alongside her husband Pierre. Sixty years later, in 1995, in honour of their achievements, the remains of both were transferred to the Panthéon, Paris. She became the first—and so far the only—woman to be honoured with interment in the Panthéon on her own merits.
Because of their levels of radioactive contamination, her papers from the 1890s are considered too dangerous to handle. Even her cookbook is highly radioactive. Her papers are kept in lead-lined boxes, and those who wish to consult them must wear protective clothing.
In her last year she worked on a book, "Radioactivity", which was published posthumously in 1935.
Legacy.
The physical and societal aspects of the Curies' work contributed substantially to shaping the world of the twentieth and twenty-first centuries. Cornell University professor L. Pearce Williams observes:
The result of the Curies' work was epoch-making. Radium's radioactivity was so great that it could not be ignored. It seemed to contradict the principle of the conservation of energy and therefore forced a reconsideration of the foundations of physics. On the experimental level the discovery of radium provided men like Ernest Rutherford with sources of radioactivity with which they could probe the structure of the atom. As a result of Rutherford's experiments with alpha radiation, the nuclear atom was first postulated. In medicine, the radioactivity of radium appeared to offer a means by which cancer could be successfully attacked.
If Marie Curie's work helped overturn established ideas in physics and chemistry, it has had an equally profound effect in the societal sphere. To attain her scientific achievements, she had to overcome barriers that were placed in her way because she was a woman, in both her native and her adoptive country. This aspect of her life and career is highlighted in Françoise Giroud's "Marie Curie: A Life", which emphasizes Marie's role as a feminist precursor.
She was known for her honesty and moderate life style. Having received a small scholarship in 1893, she returned it in 1897 as soon as she began earning her keep. She gave much of her first Nobel Prize money to friends, family, students, and research associates. In an unusual decision, Marie intentionally refrained from patenting the radium-isolation process, so that the scientific community could do research unhindered. She insisted that monetary gifts and awards be given to the scientific institutions she was affiliated with rather than to her. She and her husband often refused awards and medals. Albert Einstein reportedly remarked that she was probably the only person who could not be corrupted by fame.
Awards, honours, and tributes.
As one of the most famous female scientists to date, Marie Curie has become an icon in the scientific world and has received tributes from across the globe, even in the realm of pop culture. In a 2009 poll carried out by "New Scientist", Marie Curie was voted the "most inspirational woman in science". Curie received 25.1 per cent of all votes cast, nearly twice as many as second-place Rosalind Franklin (14.2 per cent).
Poland and France declared 2011 the Year of Marie Curie, and the United Nations declared that this would be the International Year of Chemistry. An artistic installation celebrating "Madame Curie" filled the Jacobs Gallery at San Diego's Museum of Contemporary Art. On 7 November, Google celebrated the anniversary of her birth with a special Google Doodle. On 10 December, the New York Academy of Sciences celebrated the centenary of Marie Curie's second Nobel prize in the presence of Princess Madeleine of Sweden.
Marie Curie was the first woman to win a Nobel prize, the first person to win two Nobel Prizes, the only woman to win in two fields, and the only person to win in multiple sciences. Awards that she received include:
In 1995, she became the first woman to be entombed on her own merits in the Panthéon, Paris.
The curie (symbol Ci), a unit of radioactivity, is named in honour of her and Pierre (although the commission which agreed on the name never clearly stated whether the standard was named after Pierre, Marie or both of them). The element with atomic number 96 was named curium. Three radioactive minerals are also named after the Curies: curite, sklodowskite, and cuprosklodowskite. She received numerous honorary degrees from universities across the world. The Marie Curie Actions fellowship program of the European Union for young scientists wishing to work in a foreign country is named after her. In Poland, she had received honorary doctorates from the Lwów Polytechnic (1912), Poznań University (1922), Kraków's Jagiellonian University (1924), and the Warsaw Polytechnic (1926). In 1921, she was awarded the Iota Sigma Pi National Honorary Member for her significant contribution.
Numerous locations around the world are named after her. In 2007, a metro station in Paris was renamed to honour both of the Curies. Polish nuclear research reactor Maria is named after her. The 7000 Curie asteroid is also named after her. A KLM McDonnell Douglas MD-11 (registration PH-KCC) is named in her honour.
Several institutions bear her name, starting with the two Curie institutes – the Maria Skłodowska–Curie Institute of Oncology, in Warsaw; and the "Institut Curie" in Paris. She is the patron of Maria Curie-Skłodowska University, in Lublin, founded in 1944; and of Pierre and Marie Curie University (Paris VI), France's pre-eminent science university. In Britain, Marie Curie Cancer Care was organized in 1948 to care for the terminally ill.
Two museums are devoted to Marie Curie. In 1967, the Maria Skłodowska-Curie Museum was established in Warsaw's "New Town", at her birthplace on "ulica Freta" (Freta Street). Her Paris laboratory is preserved as the Musée Curie, open since 1992.
Several works of art bear her likeness. In 1935, Michalina Mościcka, wife of Polish President Ignacy Mościcki, unveiled a statue of Marie Curie before Warsaw's Radium Institute. During the 1944 Second World War Warsaw Uprising against the Nazi German occupation, the monument was damaged by gunfire; after the war it was decided to leave the bullet marks on the statue and its pedestal. In 1955 Jozef Mazur created a stained glass panel of her, the Maria Skłodowska-Curie Medallion, featured in the University at Buffalo Polish Room.
A number of biographies are devoted to her. In 1938 her daughter, Ève Curie, published "Madame Curie". In 1987 Françoise Giroud wrote "Marie Curie: A Life". In 2005 Barbara Goldsmith wrote "Obsessive Genius: The Inner World of Marie Curie". In 2011 Lauren Redniss published "Radioactive: Marie and Pierre Curie, a Tale of Love and Fallout ".
Greer Garson and Walter Pidgeon starred in the 1943 U.S. Oscar-nominated film, "Madame Curie", based on her life. More recently, in 1997, a French film about Pierre and Marie Curie was released, "Les Palmes de M. Schutz". It was adapted from a play of the same name. In the film, Marie Curie was played by Isabelle Huppert.
Curie is the subject of the play "False Assumptions" by Lawrence Aronovitch, in which the ghosts of three other female scientists observe events in her life. Curie has also been portrayed by Susan Marie Frontczak in her play "Manya: The Living History of Marie Curie", a one-woman show performed in 30 US states and nine countries, by 2014.
Curie's likeness also has appeared on bills, stamps and coins around the world. She was featured on the Polish late-1980s 20,000-"złoty" banknote as well as on the last French 500-franc note, before the franc was replaced by the euro. Interestingly, Marie Curie themed postage stamps from Mali, the Republic of Togo, Zambia, and the Republic of Guinea actually show a picture of Susan Marie Frontczak portraying Curie in a 2001 picture by Paul Schroeder.
On the 2011 centenary of Marie Curie's second Nobel Prize (1911), an allegorical mural was painted on the façade of her Warsaw birthplace. It depicts an infant Maria Skłodowska holding a test tube from which emanate the elements that she would discover as an adult: polonium and radium.
Also in 2011, a new Warsaw bridge over the Vistula was named after her.
Notes.
a. ^ Poland had been partitioned in the 18th century among Russia, Prussia and Austria, and it was Maria Skłodowska Curie's hope that naming the element after her native country would bring world attention to Poland's lack of independence as a sovereign state. Polonium may have been the first chemical element named to highlight a political question.
b. ^ Sources vary concerning the field of her second degree. Tadeusz Estreicher, in the 1938 "Polski słownik biograficzny" entry, writes that, while many sources state she earned a degree in mathematics, this is incorrect, and that her second degree was in chemistry.

</doc>
<doc id="20412" url="http://en.wikipedia.org/wiki?curid=20412" title="MATLAB">
MATLAB

MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and fourth-generation programming language. Developed by MathWorks, MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, Java, Fortran and Python.
Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine, allowing access to symbolic computing capabilities. An additional package, Simulink, adds graphical multi-domain simulation and Model-Based Design for dynamic and embedded systems.
In 2004, MATLAB had around one million users across industry and academia. MATLAB users come from various backgrounds of engineering, science, and economics. MATLAB is widely used in academic and research institutions as well as industrial enterprises.
History.
Cleve Moler, the chairman of the computer science department at the University of New Mexico, started developing MATLAB in the late 1970s. He designed it to give his students access to LINPACK and EISPACK without them having to learn Fortran. It soon spread to other universities and found a strong audience within the applied mathematics community. Jack Little, an engineer, was exposed to it during a visit Moler made to Stanford University in 1983. Recognizing its commercial potential, he joined with Moler and Steve Bangert. They rewrote MATLAB in C and founded MathWorks in 1984 to continue its development. These rewritten libraries were known as JACKPAC. In 2000, MATLAB was rewritten to use a newer set of libraries for matrix manipulation, LAPACK.
MATLAB was first adopted by researchers and practitioners in control engineering, Little's specialty, but quickly spread to many other domains. It is now also used in education, in particular the teaching of linear algebra, numerical analysis, and is popular amongst scientists involved in image processing.
Syntax.
The MATLAB application is built around the MATLAB scripting language. Common usage of the MATLAB application involves using the Command Window as an interactive mathematical shell or executing text files containing MATLAB code.
Variables.
Variables are defined using the assignment operator, codice_1. MATLAB is a weakly typed programming language because types are implicitly converted. It is an inferred typed language because variables can be assigned without declaring their type, except if they are to be treated as symbolic objects, and that their type can change. Values can come from constants, from computation involving values of other variables, or from the output of a function. For example:
Vectors and matrices.
A simple array is defined using the colon syntax: "init"codice_2"increment"codice_2"terminator". For instance:
defines a variable named codice_4 (or assigns a new value to an existing variable with the name codice_4) which is an array consisting of the values 1, 3, 5, 7, and 9. That is, the array starts at 1 (the "init" value), increments with each step from the previous value by 2 (the "increment" value), and stops once it reaches (or to avoid exceeding) 9 (the "terminator" value).
the "increment" value can actually be left out of this syntax (along with one of the colons), to use a default value of 1.
assigns to the variable named codice_6 an array with the values 1, 2, 3, 4, and 5, since the default value of 1 is used as the incrementer.
Indexing is one-based, which is the usual convention for matrices in mathematics, although not for some programming languages such as C, C++, and Java.
Matrices can be defined by separating the elements of a row with blank space or comma and using a semicolon to terminate each row. The list of elements should be surrounded by square brackets: []. Parentheses: () are used to access elements and subarrays (they are also used to denote a function argument list).
Sets of indices can be specified by expressions such as "2:4", which evaluates to [2, 3, 4]. For example, a submatrix taken from rows 2 through 4 and columns 3 through 4 can be written as:
A square identity matrix of size "n" can be generated using the function "eye", and matrices of any size with zeros or ones can be generated with the functions "zeros" and "ones", respectively.
Most MATLAB functions can accept matrices and will apply themselves to each element. For example, codice_7 will multiply every element in "J" by 2, and then reduce each element modulo "n". MATLAB does include standard "for" and "while" loops, but (as in other similar applications such as R), using the vectorized notation often produces code that is faster to execute. This code, excerpted from the function "magic.m", creates a magic square "M" for odd values of "n" (MATLAB function codice_8 is used here to generate square matrices I and J containing 1:n).
Structures.
MATLAB has structure data types. Since all variables in MATLAB are arrays, a more adequate name is "structure array", where each element of the array has the same field names. In addition, MATLAB supports dynamic field names (field look-ups by name, field manipulations, etc.). Unfortunately, MATLAB JIT does not support MATLAB structures, therefore just a simple bundling of various variables into a structure will come at a cost.
Functions.
When creating a MATLAB function, the name of the file should match the name of the first function in the file. Valid function names begin with an alphabetic character, and can contain letters, numbers, or underscores.
Function handles.
MATLAB supports elements of lambda calculus by introducing function handles, or function references, which are implemented either in .m files or anonymous/nested functions.
Classes and Object-Oriented Programming.
MATLAB's support for object-oriented programming includes classes, inheritance, virtual dispatch, packages, pass-by-value semantics, and pass-by-reference semantics. However, the syntax and calling conventions are significantly different from other languages. MATLAB has value classes and reference classes, depending on whether the class has "handle" as a super-class (for reference classes) or not (for value classes).
Method call behavior is different between value and reference classes. For example, a call to a method
can alter any member of "object" only if "object" is an instance of a reference class.
An example of a simple class is provided below.
When put into a file named hello.m, this can be executed with the following commands:
Graphics and graphical user interface programming.
MATLAB supports developing applications with graphical user interface features. MATLAB includes GUIDE (GUI development environment) for graphically designing GUIs.
It also has tightly integrated graph-plotting features. For example the function "plot" can be used to produce a graph from two vectors "x" and "y". The code:
produces the following figure of the sine function:
A MATLAB program can produce three-dimensional graphics using the functions "surf", "plot3" or "mesh".
In MATLAB, graphical user interfaces can be programmed with the GUI design environment (GUIDE) tool.
Interfacing with other languages.
MATLAB can call functions and subroutines written in the C programming language or Fortran. A wrapper function is created allowing MATLAB data types to be passed and returned. The dynamically loadable object files created by compiling such functions are termed "MEX-files" (for MATLAB executable).
Libraries written in Perl, Java, ActiveX or .NET can be directly called from MATLAB, and many MATLAB libraries (for example XML or SQL support) are implemented as wrappers around Java or ActiveX libraries. Calling MATLAB from Java is more complicated, but can be done with a MATLAB toolbox which is sold separately by MathWorks, or using an undocumented mechanism called JMI (Java-to-MATLAB Interface), (which should not be confused with the unrelated Java Metadata Interface that is also called JMI).
As alternatives to the MuPAD based Symbolic Math Toolbox available from MathWorks, MATLAB can be connected to Maple or Mathematica.
Libraries also exist to import and export MathML.
License.
MATLAB is a proprietary product of MathWorks, so users are subject to vendor lock-in. Although MATLAB Builder products can deploy MATLAB functions as library files which can be used with .NET or Java application building environment, future development will still be tied to the MATLAB language.
Each toolbox is purchased separately. If an evaluation license is requested, the MathWorks sales department requires detailed information about the project for which MATLAB is to be evaluated. If granted (which it often is), the evaluation license is valid for two to four weeks. A student version of MATLAB is available as is a home-use license for MATLAB, SIMULINK, and a subset of Mathwork's Toolboxes at substantially reduced prices.
It has been reported that EU competition regulators are investigating whether MathWorks refused to sell licenses to a competitor.
Alternatives.
MATLAB has a number of competitors. Commercial competitors include Mathematica, TK Solver, Maple, and IDL. There are also free open source alternatives to MATLAB, in particular GNU Octave, Scilab, FreeMat, Julia, and Sage which are intended to be mostly compatible with the MATLAB language. Among other languages that treat arrays as basic entities (array programming languages) are APL, Fortran 90 and higher, S-Lang, as well as the statistical languages R and S. There are also libraries to add similar functionality to existing languages, such as IT++ for C++, Perl Data Language for Perl, ILNumerics for .NET, NumPy/SciPy for Python, and for JavaScript.
GNU Octave is unique from other alternatives because it treats incompatibility with MATLAB as a bug (see MATLAB Compatibility of GNU Octave). Therefore, GNU Octave attempts to provide a software clone of MATLAB.
Release history.
The number (or Release number) is the version reported by Concurrent License Manager program FLEXlm.
For a complete list of changes of both MATLAB and official toolboxes, consult the MATLAB release notes.
Easter eggs.
Several easter eggs exist in MATLAB. These include hidden pictures, and jokes. For example, typing in "spy" will generate a picture of the spies from Spy vs Spy. "Spy" was changed to an image of a dog in recent releases (R2011B). Typing in "why" randomly outputs a philosophical answer. Other commands include "penny", "toilet", "image", and "life". Not every Easter egg appears in every version of MATLAB.
References.
</dl>

</doc>
<doc id="20414" url="http://en.wikipedia.org/wiki?curid=20414" title="Meuse (river)">
Meuse (river)

The Meuse (; ]; Walloon "Mouze" [muːs]) or Maas (Dutch: "Maas"; ]) is a major European river, rising in France and flowing through Belgium and the Netherlands before draining into the North Sea. It has a total length of 925 km. The Meuse is one of the five oldest rivers in the world.
History.
From 1301 the upper Meuse roughly marked the western border of the Holy Roman Empire with the Kingdom of France, after Count Henry III of Bar had to receive the western part of the County of Bar ("Barrois mouvant") as a French fief from the hands of King Philip IV. The border remained stable until the annexation of the Three Bishoprics Metz, Toul and Verdun by King Henry II in 1552 and the occupation of the Duchy of Lorraine by the forces of King Louis XIII in 1633. Its lower Belgian (Walloon) portion, part of the sillon industriel, was the first fully industrialized area in continental Europe. The Meuse and its crossings were a key objective of the last major German WWII counter-offensive on the Western Front, the Battle of the Bulge (Battle of the Ardennes) in the winter of 1944/45.
The Meuse River is represented in the documentary "The River People" released in 2012 by Xavier Istasse.
Etymology.
The name "Meuse" is derived from the French name of the river, which evolved from the Latin name "Mosa". The Dutch name "Maas" descends from Middle Dutch "Mase", which comes from the presumed but unattested Old Dutch form *"Masa", from Proto-Germanic *"Masō". Only modern Dutch preserves this Germanic form, however.
Despite its appearance, the Germanic name is not derived from the Latin name, judging from the change from earlier "o" into "a", which is characteristic of the Germanic languages. Therefore, both the Latin and Germanic names were probably derived from a Celtic source, which would have been *"Mosā".
Geography.
The Meuse rises in Pouilly-en-Bassigny, commune of Le Châtelet-sur-Meuse on the Langres plateau in France from where it flows northwards past Sedan (the head of navigation) and Charleville-Mézières into Belgium.
At Namur it is joined by the River Sambre. Beyond Namur the Meuse winds eastwards, skirting the Ardennes, and passes Liège before turning north. The river then forms part of the Belgian-Dutch border, except that at Maastricht the border lies further to the west. In the Netherlands it continues northwards through Venlo closely along the border to Germany, then turns towards the west, where it runs parallel to the Waal and forms part of the extensive Rhine–Meuse–Scheldt delta, together with the Scheldt river in its south and the Rhine in the north. The river has been divided near Heusden into the Afgedamde Maas on the right and the Bergse Maas on the left. The Bergse Maas continues under the name of Amer, which is part of De Biesbosch. The Afgedamde Maas joins the Waal, the main stem of the Rhine at Woudrichem, and then flows under the name of Boven Merwede to Hardinxveld-Giessendam, where it splits into Nieuwe Merwede and Beneden Merwede. Near Lage Zwaluwe, the Nieuwe Merwede joins the Amer, forming the Hollands Diep, which splits into Grevelingen and Haringvliet, before finally flowing into the North Sea.
The Meuse is crossed by railway bridges between the following stations (on the left and right banks respectively):
There are also numerous road bridges and around 32 ferry crossings.
The Meuse is navigable over a substantial part of its total length: In the Netherlands and Belgium, the river is part of the major inland navigation infrastructure, connecting the Rotterdam-Amsterdam-Antwerp port areas to the industrial areas upstream: Hertogenbosch, Venlo, Maastricht, Liège, Namur. Between Maastricht and Maasbracht, an unnavigable section of the Meuse is bypassed by the 36 km Juliana Canal. South of Namur, further upstream, the river can only carry more modest vessels, although a barge as long as 100 m. can still reach the French border town of Givet.
From Givet, the river is canalized over a distance of 272 kilometres. The canalized Meuse used to be called the "Canal de l'Est — Branche Nord" but was recently rebaptized into "Canal de la Meuse". The waterway can be used by the smallest barges that are still in use commercially (almost 40 metres long and just over 5 metres wide). Just upstream of the town of Commercy, the Canal de la Meuse connects with the Marne–Rhine Canal by means of a short diversion canal.
The Cretaceous sea reptile Mosasaur is named after the river Meuse. The first fossils of it were discovered outside Maastricht 1780.
A view of the Meuse in the French Ardennes at Laifour.
Basin area.
An international agreement was signed in 2002 in Ghent, Belgium about the management of the river amongst France, Germany, Luxembourg, the Netherlands, and Belgium. Also participating in the agreement were the Belgian regional governments of Flanders, Wallonia, and Brussels (which is not in the basin of the Meuse but pumps running water into the Meuse).
Most of the basin area (approximately 36,000 km2) is in Wallonia (12,000 km2), followed by France (9,000 km2), the Netherlands (8,000 km2), Germany (2,000 km2), Flanders (2,000 km2) and Luxembourg (a few km2).
An International Commission on the Meuse has the responsibility of the implementation of the treaty.
The costs of this Commission are met by all these countries, in proportion of their own territory into the basin of the Meuse: Netherlands and Wallonia 30%, France 15%, Germany 14.5%, Flanders 5%, Brussels 4.5%, Kingdom of Belgium and Luxemburg 0.5%.
The map of the basin area of Meuse was joined to the text of the treaty.
On the cultural plan, the river Meuse, as a major communication route, is the origin of the Mosan art, principally (Wallonia and France).
The first landscape painted in the Middle-Age was the landscape of Meuse. For instance Joachim Patinir He was likely the uncle of Henri Blès who is sometimes defined as a Mosan landscape painter active during the second third of the 16th century (i.e., second generation of landscape painters) 
Tributaries.
The main tributaries of the river Meuse are listed below in downstream-upstream order, with the town where the tributary meets the river:
Distributaries.
The mean annual discharge rate of the Meuse has been relatively stable over the last few thousand years. One recent study estimates that average flow has increased about 10% since 2000 BC. The hydrological distribution of the Meuse changed during the later Middle Ages, when a major flood forced it to shift its main course northwards towards the Merwede river. From then on, several stretches of the original Merwede were named "Maas" (i.e. Meuse) instead and served as the primary outflow of that river. Those branches are currently known as the Nieuwe Maas and Oude Maas.
However, during another series of severe floods the Meuse found an additional path towards the sea, resulting in the creation of the Biesbosch wetlands and Hollands Diep estuaries. Thereafter, the Meuse split near Heusden into two main distributaries, one flowing north to join the Merwede, and one flowing directly to the sea. The branch of the Meuse leading directly to the sea eventually silted up, (and now forms the Oude Maasje stream), but in 1904 the canalised Bergse Maas was dug to take over the functions of the silted-up branch. At the same time, the branch leading to the Merwede was dammed at Heusden, (and has since been known as the Afgedamde Maas) so that little water from the Meuse entered the old Maas courses, or the Rhine distributaries. The resulting separation of the rivers Rhine and Meuse is considered to be the greatest achievement in Dutch hydraulic engineering before the completion of the Zuiderzee Works and Delta Works. In 1970 the Haringvlietdam has been finished. Since then the reunited Rhine and Meuse waters reach the North Sea either at this site or, during times of lower discharges of the Rhine, at Hoek van Holland.
A 2008 study notes that the difference between summer and winter flow volumes has increased significantly in the last 100–200 years. These workers point out that the frequency of serious floods ("i.e." flows > 1000% of normal) has increased markedly. They predict that winter flooding of the Meuse may become a recurring problem in the coming decades.
Départements, provinces and towns.
The Meuse flows through the following departments of France, provinces of Belgium, provinces of the Netherlands and towns:
Deutschlandlied.
The Meuse ("Maas") is mentioned in the (nowadays not sung) first stanza of the "Deutschlandlied" German national anthem. The lyrics written in 1841 describe a then–disunited Germany with the river as its western boundary, where King William I of the Netherlands had joined the German Confederation with his Duchy of Limburg in 1839. Though the duchy's territory officially became an integral part of the Netherlands by the 1867 Treaty of London, the text passage remained unchanged when the "Deutschlandlied" was declared the national anthem of the Weimar Republic in 1922.

</doc>
<doc id="20418" url="http://en.wikipedia.org/wiki?curid=20418" title="Michael Bentine">
Michael Bentine

Michael Bentine, CBE (born Michael James Bentin; 26 January 1922 – 26 November 1996) was an English comedian, comic actor and founding member of the Goons. He was a Peruvian Briton. In 1971, Bentine received the Order of Merit of Peru following his fund-raising work for the 1970 Great Peruvian Earthquake.
Biography.
Bentine was born in Watford, Hertfordshire, to a Peruvian father, Adam Bentin, and a British mother, Florence Dawkins, and grew up in Folkestone, Kent. He was educated at Eton College. With the help of speech trainer, Harry Burgess, he overcame a stammer and subsequently developed an interest in amateur theatricals, along with the Tomlinson family, including the young David Tomlinson. He spoke fluent Spanish and French. His father was an early aeronautical engineer for Sopwith aircraft during and after World War I and invented a tensometer for setting the tension on aircraft rigging wires.
In World War II, he volunteered for all services when the war broke out (the RAF was his first choice owing to the influence of his father's experience), but was initially rejected because of his father's nationality.
He started his acting career in 1940, in a touring company in Cardiff playing a juvenile lead in "Sweet Lavender". He went on to join Robert Atkin's Shakespearean company in Regent's Park, London, until he was called up for service in the RAF. He was appearing in a Shakespearean play in doublet and hose in the open-air theatre in London's Hyde Park when two RAF MPs marched on stage and arrested him for desertion. Unknown to him, an RAF conscription notice had been following him for a month as his company toured.
Once in the RAF he went through flying training. He was the penultimate man going through a medical line receiving inoculations for typhoid with the other flight candidates in his class (they were going to Canada to receive new aircraft) when the vaccine ran out. They refilled the bottle to inoculate him and the other man as well. By mistake they loaded a pure culture of typhoid. The other man died immediately, and Bentine was in a coma for six weeks. When he regained consciousness his eyesight was ruined, leaving him myopic for the rest of his life. Since he was no longer physically qualified for flying, he was transferred to RAF Intelligence and seconded to MI9, a unit that was dedicated to supporting resistance movements and helping prisoners escape. His immediate superior was the Colditz escapee Airey Neave.
At the end of the war, he took part in the liberation of Bergen-Belsen concentration camp. He said about this experience:
Comedy career.
After the war he decided to become a comedian and worked in the Windmill Theatre where he met Harry Secombe. He specialised in off-the-wall humour, often involving cartoons and other types of animation. His acts included giving lectures in an invented language called Slobodian, "Imaginative Young Man with a Walking Stick" and "The Chairback", with a broken chairback having a number of uses from comb to machine gun and taking on a demonical life of its own. Peter Sellers told him this was the inspiration for the prosthetic arm routine in "Dr Strangelove". This act led to his engagement by Val Parnell to appear in the Starlight Roof revues starring Vic Oliver, where he met and married his second wife Clementina, with whom he had four children. Also on the bill were Fred Emney and a young Julie Andrews.
He co-founded "The Goon Show" radio show with Spike Milligan, Peter Sellers and Harry Secombe, but appeared in only the first 38 shows on the BBC Light Programme from 1951 to 1953. The first of these shows were actually called "Crazy People" and subtitled "The Junior Crazy Gang"; the term "Goon" was used as the headline of a review of Bentine's act by "Picture Post" dated 5 November 1948. Only one of this first series (and very few of the following three in which he did not appear) has survived, the rest of the original disc recordings having apparently been destroyed or discarded as no longer usable, so there is almost no record of his work as a radio "Goon". He also appeared in the "Goon Show" film "Down Among the Z Men".
In 1951 Bentine was invited to the United States to appear on "The Ed Sullivan Show". On his return he parted amicably from his partners and continued touring in variety, remaining close to Secombe and Sellers for the rest of his life. In 1972, Secombe and Sellers told Michael Parkinson that Bentine was "always calling everyone a genius" and, since he was the only one of the four with a "proper education", they always believed him.
His first appearances on television were as presenter on a 13-part children's series featuring remote controlled puppets, "The Bumblies", which he also devised, designed and wrote. These were three small creatures from outer space who slept on "Professor Bentine's" ceiling and who had come to Earth to learn the ways of Earthling children. Angelo de Calferta modelled the puppets from Bentine's designs and Richard Dendy moulded them in latex rubber. He sold the series to the BBC for less than they had cost to make. He then spent two years touring in Australia (1954–55).
On his return to Britain in 1954, he found he faced hostility for having left the Goons to the extent that his picture had been excised from early pictures and wasn't replaced until Bentine complained to BBC Director-General Michael Checkland in the 1990s. He worked as a scriptwriter for Peter Sellers and then on 39 episodes of his own radio show "Round the Bend in 30 Minutes", which has also been wiped from the BBC archive. He then teamed up with Dick Lester to devise a series of six TV programmes "Before Midnight" for Associated British Corporation (ABC) in Birmingham in 1958. This led to a 13-programme series called "After Hours" in which he appeared alongside Dick Emery, Clive Dunn, David Lodge, Joe Gibbons and Benny Lee. The show featured the "olde English sport of drats, later known as nurdling". Some of the sketches were adapted into a stage revue, "Don't Shoot, We're British". He also appeared in the film comedy "Raising a Riot", starring Kenneth More, which featured his five-year-old daughter "Fusty". He joked that she got better billing.
From 1960 to 1964, he had a television series, "It's a Square World", which won a BAFTA award in 1962 and Grand Prix de la Presse at Montreux in 1963. A prominent feature of the series was the imaginary flea circus where plays were enacted on tiny sets using nothing but special effects to show the movement of things too small to see and sounds with Bentine's commentary. The plays were not serious. One, titled "The Beast of the Black Bog Tarn", was set in a (miniature) haunted house.
In 1969-70 he was presenter of "The Golden Silents" on BBC TV, which attempted authentic showings of silent films, without the commentaries with which they were usually shown on television before then.
From 1974 to 1980 he wrote, designed, narrated and presented the children's television programme "Michael Bentine's Potty Time" and made one-off comedy specials.
He was also the best-selling writer of 16 novels, comedies and non-fiction books. Four of his books, "The Long Banana Skin" (1975), "The Door Marked Summer" (1981), "Doors to the Mind" and "The Reluctant Jester" (1992) are autobiographical.
Other interests.
In 1968, travelling on the British Hovercraft Corporation (BHC) SR.N6, "GH2012", he took part in the first hovercraft expedition up the River Amazon.
In 1995, Bentine received a CBE from Queen Elizabeth II "for services to entertainment". He was also a holder of the Peruvian Order of Merit, as was his grandfather, Don Antonio Bentin Palamero, for his work leading the fundraising for the Peruvian Earthquake Appeal.
Bentine was a crack pistol shot and helped to start the idea of a counter-terrorist wing within 22 SAS Regiment. In doing so, he became the first non-SAS person ever to fire a gun inside the close-quarters battle training house at Hereford.
His interests included parapsychology. This was as a result of his and his family's extensive research into the paranormal, which resulted in his writing "The Door Marked Summer" and "The Doors of the Mind". He was, for the final years of his life, president of the Association for the Scientific Study of Anomalous Phenomena.
Bentine was also interested in science. On 14 December 1977, he appeared with Arthur C. Clarke on Patrick Moore's BBC "The Sky At Night" programme. The broadcast was entitled "Suns, Spaceships and Bug-Eyed Monsters" - a light-hearted look at how science fiction had become science fact, as well as how ideas of space travel had become reality through the 20th century. In the opening of the programme, Patrick Moore introduces Bentine with Bentine confirming that he was the possessor of a "Readers Digest Degree." This remark was typical of Bentine's comic overtones to most things in life whilst ensuring that he hid his undoubted talent and knowledge of science. Bentine appeared in a subsequent broadcast on a similar theme with Patrick Moore in 1980. Following the death of Arthur C. Clarke, BBC Sky At Night magazine released a copy of the 1977 archive programme on the cover of their May 2008 edition.
Family and health.
He was married twice, remaining with his second wife, Clementina Stuart, a Royal Ballet dancer, for over fifty years. He had a child from his first marriage, Elaine, from whom he had a granddaughter, Marie Laurence, and three great-grandsons, William, Arthur and Nicholas. His children from his second marriage were better known by their family nicknames than their birth names: Gus (real name Stewart), Fusty (real name Marylla), Suki (real name Serena) and Peski (real name Richard). Two of his five children, his eldest daughters, died from cancer (breast cancer and lymphoma), while his elder son, Gus, was killed when a Piper PA-18 (Super Cub, registration G-AYPN) crashed into a hillside at Ditcham Park Woods near Petersfield, Hampshire, on 28 August 1971. His body, together with that of the pilot and the aircraft, was found on 31 October 1971. Bentine's subsequent investigation into regulations governing private airfields resulted in his writing a report for Special Branch into the use of personal aircraft in smuggling operations. He fictionalised much of the material in his novel "Lords of the Levels".
When his son Richard's first boy Elliot was born, he tried to give him an MG 08 machine gun, which his daughter-in-law refused to accept. When Richard's second son Harry was born, Michael bought him a train set.
From 1975 until his death in 1996, he and his wife spent their winters at a second home in Palm Springs, California, USA.
Shortly before his death from prostate cancer at the age of 74, he was visited in hospital by Prince Charles, who was a close personal friend, as well as a devoted fan of the Goons.
Programmes.
Some of the programmes Bentine appeared in were:

</doc>
<doc id="20419" url="http://en.wikipedia.org/wiki?curid=20419" title="Mania">
Mania

Mania is the mood of an abnormally elevated arousal energy level. Elevated irritability is common along with behavior that seems on the surface to be the opposite of depression. Mania is a necessary symptom for certain psychiatric diagnoses. The word derives from the Greek μανία ("mania"), "madness, frenzy" and that from the verb μαίνομαι ("mainomai"), "to be mad, to rage, to be furious".
In addition to mood disorders, persons may exhibit manic behavior increased by drug intoxication (notably stimulants, such as cocaine and methamphetamine), medication side effects (notably SSRIs), and malignancy (the worsening of a condition). Mania is most often associated with bipolar disorder, where episodes of mania may alternate unpredictably with episodes of depression or periods of euthymia. Gelder, Mayou, and Geddes (2005) suggest that it is vital that mania be predicted in the early stages because otherwise the patient becomes reluctant to comply with the treatment. Those who never experience depression also experience cyclical changes in mood. These cycles are often affected by changes in sleep cycle (too much or too little), diurnal rhythms, and environmental stressors.
Mania varies in intensity, from mild mania (hypomania) to full mania with extreme and frenzied energy, florid psychosis and incomprehensibly rapid speech. Standardized tools such as Altman Self-Rating Mania Scale and Young Mania Rating Scale can be used to measure severity of manic episodes. Because mania and hypomania have also been associated with creativity and artistic talent, it is not always the case that the clearly manic bipolar person needs or wants medical help; such persons often either retain sufficient self-control to function normally or are unaware that they have "gone manic" severely enough to be committed or to commit themselves. Manic persons often can be mistaken for being on drugs or other mind-altering substances.
Classification.
Mixed states.
In a mixed state the individual has co-occurring manic and depressive features. Dysphoric mania is primarily manic and agitated depression is primarily depressed. This has caused speculation amongst doctors that mania and depression are two independent axes in a bipolar spectrum, rather than opposites.
The mixed state can put a patient at greater suicide risk. Feeling depressed on its own is a risk factor, but when coupled with increased energy, agitation, and impulsivity, the patient is more likely to engage in dangerous behaviour, including self-injury or suicide.
Hypomania.
Hypomania is a lowered state of mania that does little to impair function or decrease quality of life. In hypomania, there is less need for sleep and both goal-motivated behaviour and metabolism increase. Though the elevated mood and energy level typical of hypomania could be seen as a benefit, mania itself generally has many undesirable consequences including suicidal tendencies.
Associated disorders.
A single manic episode is sufficient to diagnose bipolar I disorder. Hypomania may be indicative of bipolar II disorder or cyclothymia. However, if prominent psychotic symptoms are present for a duration significantly longer than the mood episode, a diagnosis of schizoaffective disorder is more appropriate. Several types of "mania" such as kleptomania and pyromania are related more closely to obsessive-compulsive disorder than to bipolar disorder, depending on the severity of these disorders. For instance, someone with kleptomania who suffers from impulses to steal things such as pencils, pens, and paperclips is better diagnosed with a form of OCD.
B12 deficiency can also cause characteristics of mania and psychosis.
Hyperthyroidism can produce similar symptoms to those of mania such as agitation, elevated mood, increased energy, hyperactivity, sleep disturbances and sometimes, especially in severe cases, psychosis.
Signs and symptoms.
"A manic episode" is defined in the American Psychiatric Association's diagnostic manual as a period of seven or more days (or any period if admission to hospital is required) of unusually and continuously effusive and open elated or irritable mood, where the mood is not caused by drugs/medication or a medical illness (e.g., hyperthyroidism), and (a) is causing obvious difficulties at work or in social relationships and activities, or (b) requires admission to hospital to protect the person or others, or (c) the person is suffering psychosis.
To be classed as a manic episode, while the disturbed mood is present at least three (or four if only irritability is present) of the following must have been consistently prominent: grand or extravagant style, or expanded self-esteem; pressured speech; reduced need of sleep (e.g. three hours may be sufficient); talks more often and feels the urge to talk longer; ideas flit through the mind in quick succession, or thoughts race and preoccupy the person; over indulgence in enjoyable behaviours with high risk of a negative outcome (e.g., extravagant shopping, sexual adventures or improbable commercial schemes). Though the activities one participates in while in a manic state are not "always" negative, those with the potential to have negative outcomes are far more likely to.
If the person is concurrently depressed, they are said to be having a mixed episode.
The World Health Organization's classification system defines "a manic episode" as one where mood is higher than the person's situation warrants and may vary from relaxed high spirits to barely controllable exuberance, accompanied by hyperactivity, a compulsion to speak, a reduced sleep requirement, difficulty sustaining attention and, often, increased distractibility. Frequently, confidence and self-esteem are excessively enlarged, and grand, extravagant ideas are expressed. Behavior that is out of character and risky, foolish or inappropriate may result from a loss of normal social restraint.
Some people also have physical symptoms, such as sweating, pacing, and weight loss. In full-blown mania, often the manic person will feel as though his or her goal(s) trump all else, that there are no consequences or that negative consequences would be minimal, and that they need not exercise restraint in the pursuit of what they are after. Hypomania is different, as it may cause little or no impairment in function. The hypomanic person's connection with the external world, and its standards of interaction, remain intact, although intensity of moods is heightened. But those who suffer from prolonged unresolved hypomania do run the risk of developing full mania, and indeed may cross that "line" without even realizing they have done so.
One of the most signature symptoms of mania (and to a lesser extent, hypomania) is what many have described as racing thoughts. These are usually instances in which the manic person is excessively distracted by objectively unimportant stimuli. This experience creates an absentmindedness where the manic individual's thoughts totally preoccupy him or her, making him or her unable to keep track of time, or be aware of anything besides the flow of thoughts. Racing thoughts also interfere with the ability to fall asleep.
Mania is always relative to the normal rate of intensity of the person being diagnosed with it; therefore, an easily angered person may exhibit mania by getting even angrier even more quickly, and an intelligent person may adopt seemingly "genius" characteristics and an ability to perform and to articulate thought beyond what they can do in a normal mood. A very simple indicator of mania would be if a noticeably clinically depressed person becomes suddenly and inordinately energetic, cheerful, aggressive, or "happy". Other often-less-obvious elements of mania include delusions (of grandeur, potential, persecution, arrogance), hypersensitivity, hypervigilance, hypersexuality, hyper-religiosity, hyperactivity, impulsiveness, compulsion to over explain (keep talking with rapid speech), grandiose ideas and plans, and difficulty falling asleep or decreased need for sleep (e.g. feeling rested after 3 or 4 hours of sleep). The afflicted person's eyes may look, as well as feel abnormally "wide" or "open", rarely blinking; this sometimes contributes to clinicians' misconception that a manic patient is under the influence of a stimulant drug when the patient is either not on any mind-altering substances, or is in fact under the influence of a depressant drug in a misguided effort to stave off destructive and unwanted manic impulses. In manic and hypomanic cases, the afflicted person may engage in out-of-character behaviour, such as questionable business transactions, wasteful expenditures of money (e.g., excessive shopping sprees or unnecessary purchases), risky sexual activity, recreational drug abuse, excessive gambling, reckless behavior (such as fast, reckless driving or daredevil activity), abnormal social interaction, or highly vocal arguments uncharacteristic of previous behaviours. These behaviours may increase stress in personal relationships, lead to problems at work and increase the risk of altercations with law enforcement. There is a high risk of impulsively taking part in activities potentially harmful to self and others.
Although "severely elevated mood" sounds somewhat desirable and enjoyable, the experience of mania is ultimately often quite unpleasant and sometimes disturbing, if not frightening, for the person involved and for those close to them, and it may lead to impulsive behaviour that may later be regretted. It can also often be complicated by the sufferer's lack of judgment and insight regarding periods of exacerbation of characteristic states. Manic patients are frequently grandiose, obsessive, impulsive, irritable, belligerent, and frequently deny anything is wrong with them. Because mania frequently encourages high energy and decreased perception of need or ability to sleep, within a few days of a manic cycle, sleep-deprived psychosis may appear, further complicating the ability to think clearly. Racing thoughts and misperceptions lead to frustration and decreased ability to communicate with others.
There are different "stages" or "states" of mania. A minor state is essentially hypomania and, like hypomania's characteristics, may involve increased creativity, wit, gregariousness, and ambition. Full-blown mania will make a person feel elated, but perhaps also irritable, frustrated, and even disconnected from reality.
Cause.
The biological mechanism by which mania occurs is not yet known. Based on the mechanism of action of antimanic agents (such as antipsychotics, valproate, tamoxifen, lithium, carbamazepine, etc.) and abnormalities seen in patients experiencing a manic episode the following is theorised to be involved in the pathophysiology of mania:
Imaging studies have shown that the left amygdala is more active in women who are manic and the orbitofrontal cortex is less active.
Treatment.
Before beginning treatment for mania, careful differential diagnosis must be performed to rule out non-psychiatric causes.
Acute mania in bipolar disorder is typically treated with an atypical antipsychotic medication as these medications tend to produce the most rapid improvement in manic symptoms.
When the manic behaviours have gone, long-term treatment then focuses on prophylactic treatment to try to stabilize the patient's mood, typically through a combination of pharmacotherapy and psychotherapy. The likelihood of having a relapse is very high for those who have experienced two or more episodes of mania or depression. While medication for bipolar disorder is important to manage symptoms of mania and depression, studies show relying on medications alone is not the most effective method of treatment. Medication is most effective when used in combination with other bipolar disorder treatments, including psychotherapy, self-help coping strategies, and healthy lifestyle choices.
Lithium is the classic mood stabilizer to prevent further manic and depressive episodes. A systematic review found that long term lithium treatment substantially reduces the risk of bipolar manic relapse, by 42%. Anticonvulsants such as valproic acid, oxcarbazepine and carbamazepine are also used for prophylaxis. More recent drug solutions include lamotrigine, which is another anticonvulsant. Clonazepam (Klonopin) is also used. Sometimes atypical antipsychotics are used in combination with the previous mentioned medications as well, including olanzapine (Zyprexa) which helps treat hallucinations or delusions, Asenapine (Saphris, Sycrest), aripiprazole (Abilify), risperidone, ziprasidone, and clozapine which is often used for people who do not respond to lithium or anticonvulsants.
Verapamil, a calcium-channel blocker, is useful in the treatment of hypomania and in those cases where lithium and mood stabilizers are contraindicated or ineffective. Verapamil is effective for both short-term and long-term treatment.
Antidepressant monotherapy is not recommended for the treatment of depression in patients with bipolar disorders I or II, and no benefit has been demonstrated by combining antidepressants with mood stabilizers in these patients.
Society and culture.
In "Electroboy: A Memoir of Mania" by Andy Behrman, he describes his experience of mania as "the most perfect prescription glasses with which to see the world...life appears in front of you like an oversized movie screen". Behrman indicates early in his memoir that he sees himself not as a person suffering from an uncontrollable disabling illness, but as a director of the movie that is his vivid and emotionally alive life. "When I'm manic, I'm so awake and alert, that my eyelashes fluttering on the pillow sound like thunder" . Many people who are artistic and do art in various forms have mania.
Winston Churchill had periods of "manic symptoms" that may have been both an asset and a liability. 

</doc>
<doc id="20420" url="http://en.wikipedia.org/wiki?curid=20420" title="Multimedia">
Multimedia

Multimedia refers to content that uses a combination of different content forms. This contrasts with media that use only rudimentary computer displays such as text-only or traditional forms of printed or hand-produced material. Multimedia includes a combination of text, audio, still images, animation, video, or interactivity content forms.
Multimedia is usually recorded and played, displayed, or accessed by information content processing devices, such as computerized and electronic devices, but can also be part of a live performance. Multimedia devices are electronic media devices used to store and experience multimedia content. Multimedia is distinguished from mixed media in fine art; by including audio, for example, it has a broader scope. The term "rich media" is synonymous for interactive multimedia. Hypermedia can be considered one particular multimedia application.
Categorization of multimedia.
Multimedia may be broadly divided into linear and non-linear categories. Linear active content progresses often without any navigational control for the viewer such as a cinema presentation. Non-linear uses interactivity to control progress as with a video game or self-paced computer based training. Hypermedia is an example of non-linear content.
Multimedia presentations can be live or recorded. A recorded presentation may allow interactivity via a navigation system. A live multimedia presentation may allow interactivity via an interaction with the presenter or performer.
Major characteristics of multimedia.
Multimedia presentations may be viewed by person on stage, projected, transmitted, or played locally with a media player. A broadcast may be a live or recorded multimedia presentation. Broadcasts and recordings can be either analog or digital electronic media technology. Digital online multimedia may be downloaded or streamed. Streaming multimedia may be live or on-demand.
Multimedia games and simulations may be used in a physical environment with special effects, with multiple users in an online network, or locally with an offline computer, game system, or simulator.
The various formats of technological or digital multimedia may be intended to enhance the users' experience, for example to make it easier and faster to convey information. Or in entertainment or art, to transcend everyday experience.
Enhanced levels of interactivity are made possible by combining multiple forms of media content. Online multimedia is increasingly becoming object-oriented and data-driven, enabling applications with collaborative end-user innovation and personalization on multiple forms of content over time. Examples of these range from multiple forms of content on Web sites like photo galleries with both images (pictures) and title (text) user-updated, to simulations whose co-efficients, events, illustrations, animations or videos are modifiable, allowing the multimedia "experience" to be altered without reprogramming. In addition to seeing and hearing, Haptic technology enables virtual objects to be felt. Emerging technology involving illusions of taste and smell may also enhance the multimedia experience.
Terminology.
History of the term.
The term "multimedia" was coined by singer and artist Bob Goldstein (later 'Bobb Goldsteinn') to promote the July 1966 opening of his "LightWorks at L'Oursin" show at Southampton, Long Island. Goldstein was perhaps aware of an American artist named Dick Higgins, who had two years previously discussed a new approach to art-making he called "intermedia."
On August 10, 1966, Richard Albarino of "Variety" borrowed the terminology, reporting: "Brainchild of songscribe-comic Bob ('Washington Square') Goldstein, the 'Lightworks' is the latest "multi-media" music-cum-visuals to debut as discothèque fare." Two years later, in 1968, the term "multimedia" was re-appropriated to describe the work of a political consultant, David Sawyer, the husband of Iris Sawyer—one of Goldstein’s producers at L’Oursin.
 In the intervening forty years, the word has taken on different meanings. In the late 1970s, the term referred to presentations consisting of multi-projector slide shows timed to an audio track. However, by the 1990s 'multimedia' took on its current meaning.
In the 1993 first edition of McGraw-Hill’s "Multimedia: Making It Work", Tay Vaughan declared “Multimedia is any combination of text, graphic art, sound, animation, and video that is delivered by computer. When you allow the user – the viewer of the project – to control what and when these elements are delivered, it is "interactive multimedia". When you provide a structure of linked elements through which the user can navigate, interactive multimedia becomes "hypermedia".” 
The German language society, Gesellschaft für deutsche Sprache, decided to recognize the word's significance and ubiquitousness in the 1990s by awarding it the title of 'Word of the Year' in 1995. The institute summed up its rationale by stating "[Multimedia] has become a central word in the wonderful new media world"
In common usage, "multimedia" refers to an electronically delivered combination of media including video, still images, audio, text in such a way that can be accessed interactively. Much of the content on the web today falls within this definition as understood by millions. Some computers which were marketed in the 1990s were called "multimedia" computers because they incorporated a CD-ROM drive, which allowed for the delivery of several hundred megabytes of video, picture, and audio data. That era saw also a boost in the production of educational multimedia CD-ROMs.
Word usage and context.
Since media is the plural of medium, the term "multimedia" is used to describe multiple occurrences of only one form of media such as a collection of audio CDs. This is why it's important that the word "multimedia" is used exclusively to describe multiple forms of media and content.
The term "multimedia" is also ambiguous. Static content (such as a paper book) may be considered multimedia if it contains both pictures and text or may be considered interactive if the user interacts by turning pages at will. Books may also be considered non-linear if the pages are accessed non-sequentially. The term "video", if not used exclusively to describe motion photography, is ambiguous in multimedia terminology. "Video" is often used to describe the file format, delivery format, or presentation format instead of "footage" which is used to distinguish motion photography from "animation" of rendered motion imagery. Multiple forms of information content are often not considered modern forms of presentation such as audio or video. Likewise, single forms of information content with single methods of information processing (e.g. non-interactive audio) are often called multimedia, perhaps to distinguish static media from active media. In the Fine arts, for example, Leda Luss Luyken's ModulArt brings two key elements of musical composition and film into the world of painting: variation of a theme and movement of and within a picture, making "ModulArt" an interactive multimedia form of art. Performing arts may also be considered multimedia considering that performers and props are multiple forms of both content and media.
The "Gesellschaft für deutsche Sprache" chose "Multimedia" as German Word of the Year 1995.
Usage / Application.
Multimedia finds its application in various areas including, but not limited to, advertisements, art, education, entertainment, engineering, medicine, mathematics, business, scientific research and spatial temporal applications. Several examples are as follows:
Creative industries.
Creative industries use multimedia for a variety of purposes ranging from fine arts, to entertainment, to commercial art, to journalism, to media and software services provided for any of the industries listed below. An individual multimedia designer may cover the spectrum throughout their career. Request for their skills range from technical, to analytical, to creative.
Commercial uses.
Much of the electronic old and new media used by commercial artists is multimedia. Exciting presentations are used to grab and keep attention in advertising. Business to business, and interoffice communications are often developed by creative services firms for advanced multimedia presentations beyond simple slide shows to sell ideas or liven-up training. Commercial multimedia developers may be hired to design for governmental services and nonprofit services applications as well.
Entertainment and fine arts.
In addition, multimedia is heavily used in the entertainment industry, especially to develop special effects in movies and animations(VFX, 3D animation, etc.). Multimedia games are a popular pastime and are software programs available either as CD-ROMs or online. Some video games also use multimedia features. 
Multimedia applications that allow users to actively participate instead of just sitting by as passive recipients of information are called "Interactive Multimedia". 
In the Arts there are multimedia artists, whose minds are able to blend techniques using different media that in some way incorporates interaction with the viewer. One of the most relevant could be Peter Greenaway who is melding Cinema with Opera and all sorts of digital media. Another approach entails the creation of multimedia that can be displayed in a traditional fine arts arena, such as an art gallery. Although multimedia display material may be volatile, the survivability of the content is as strong as any traditional media. Digital recording material may be just as durable and infinitely reproducible with perfect copies every time.
Education.
In Education, multimedia is used to produce computer-based training courses (popularly called CBTs) and reference books like encyclopedia and almanacs. A CBT lets the user go through a series of presentations, text about a particular topic, and associated illustrations in various information formats. Edutainment is the combination of education with entertainment, especially multimedia entertainment.
Learning theory in the past decade has expanded dramatically because of the introduction of multimedia. Several lines of research have evolved (e.g. Cognitive load, Multimedia learning, and the list goes on). The possibilities for learning and instruction are nearly endless.
The idea of media convergence is also becoming a major factor in education, particularly higher education. Defined as separate technologies such as voice (and telephony features), data (and productivity applications) and video that now share resources and interact with each other, synergistically creating new efficiencies, media convergence is rapidly changing the curriculum in universities all over the world. Likewise, it is changing the availability, or lack thereof, of jobs requiring this savvy technological skill.
The English education in middle school in China is well invested and assisted with various equipments. In contrast, the original objective has not been achieved at the desired effect. The government, schools, families, and students spend a lot of time working on improving scores, but hardly gain practical skills. English education today has gone into the vicious circle. Educators need to consider how to perfect the education system to improve students’ practical ability of English. 
Therefore an efficient way should be used to make the class vivid. Multimedia teaching will bring students into a class where they can interact with the teacher and the subject. Multimedia teaching is more intuitive than old ways; teachers can simulate situations in real life. In many circumstances teachers do not have to be there, students will learn by themselves in the class. More importantly, teachers will have more approaches to stimulating students’ passion of learning.
Journalism.
Newspaper companies all over are also trying to embrace the new phenomenon by implementing its practices in their work. While some have been slow to come around, other major newspapers like "The New York Times", "USA Today" and "The Washington Post" are setting the precedent for the positioning of the newspaper industry in a globalized world.
News reporting is not limited to traditional media outlets. Freelance journalists can make use of different new media to produce multimedia pieces for their news stories. It engages global audiences and tells stories with technology, which develops new communication techniques for both media producers and consumers. Common Language Project is an example of this type of multimedia journalism production.
Multimedia reporters who are mobile (usually driving around a community with cameras, audio and video recorders, and wifi-equipped laptop computers) are often referred to as Mojos, from "mo"bile "jo"urnalist.
Engineering.
Software engineers may use multimedia in Computer Simulations for anything from entertainment to training such as military or industrial training. Multimedia for software interfaces are often done as a collaboration between creative professionals and software engineers.
Industry.
In the Industrial sector, multimedia is used as a way to help present information to shareholders, superiors and coworkers. Multimedia is also helpful for providing employee training, advertising and selling products all over the world via virtually unlimited web-based technology.
Mathematical and scientific research.
In mathematical and scientific research, multimedia is mainly used for modeling and simulation. For example, a scientist can look at a molecular model of a particular substance and manipulate it to arrive at a new substance. Representative research can be found in journals such as the Journal of Multimedia.
Medicine.
In Medicine, doctors can get trained by looking at a virtual surgery or they can simulate how the human body is affected by diseases spread by viruses and bacteria and then develop techniques to prevent it.
Document imaging.
Document imaging is a technique that takes hard copy of an image/document and converts it into a digital format (for example, scanners).
Disabilities.
Ability Media allows those with disabilities to gain qualifications in the multimedia field so they can pursue careers that give them access to a wide array of powerful communication forms.
Miscellaneous.
In Europe, the reference organisation for Multimedia industry is the European Multimedia Associations Convention (EMMAC).
Structuring information in a multimedia form.
Multimedia represents the convergence of text, pictures, video and sound into a single form. The power of multimedia and the Internet lies in the way in which information is linked.
Multimedia and the Internet require a completely new approach to writing. The style of writing that is appropriate for the 'on-line world' is highly optimized and designed to be able to be quickly scanned by readers.
A good site must be made with a specific purpose in mind and a site with good interactivity and new technology can also be useful for attracting visitors. The site must be attractive and innovative in its design, function in terms of its purpose, easy to navigate, frequently updated and fast to download.
When users view a page, they can only view one page at a time. As a result, multimedia users must create a "mental model" of information structure.
Conferences.
There is a , the two main scholarly scientific conferences being:

</doc>
<doc id="20421" url="http://en.wikipedia.org/wiki?curid=20421" title="Max Headroom (TV series)">
Max Headroom (TV series)

Max Headroom is a British-produced American satirical science fiction television series by Chrysalis Visual Programming and Lakeside Productions for Lorimar-Telepictures that aired in the United States on ABC from March 1987 to May 1988. The series was based on the Channel 4 British TV pilot produced by Chrysalis, "". The series is often mistaken as an American-produced show due to the setting and its use of an almost entirely American cast along with being broadcast in the United States on the ABC network. Cinemax aired the UK pilot followed by a six-week run of highlights from "The Max Headroom Show", a music video show where Headroom appears between music videos. ABC took an interest in the pilot and asked Chrysalis/Lakeside to produce the series for American audiences.
The show went into production in late 1986 and ran for six episodes in the first season with eight being produced in season two.
Television series.
In 1987, the story told in "", a made-for-television movie shot in 1985, formed the basis of a drama television series. The film was re-shot as a pilot program for a new series broadcast by the U.S.-based ABC television network. The pilot featured plot changes and some minor visual touches, but retained the same basic storyline. The only original cast retained for the U.S. version series were Matt Frewer (Max Headroom/Edison Carter) and Amanda Pays (Theora Jones); a third original cast member, W. Morgan Sheppard, joined the series as "Blank Reg" in later episodes. Among the non-original cast, Jeffrey Tambor co-starred as "Murray", Edison Carter's neurotic producer.
The series is set in a futuristic dystopia ruled by an oligarchy of television networks. Even the government functions primarily as a puppet state of the network executives, serving mainly to pass laws — such as banning "off" switches on televisions — that protect and consolidate the networks' power. Television technology has advanced to the point that viewers' physical movements and thoughts can be monitored through their television sets; however, almost all non-television technology has been discontinued or destroyed. The only real check on the power of the networks is Edison Carter, a crusading investigative journalist who regularly exposes the unethical practices of his own employer, and the team of allies both inside and outside the system who assist him in getting his reports to air and protecting him from the forces that wish to silence or kill him.
The series began as a mid-season replacement in spring of 1987, and did well enough to be renewed for the fall television season, but the viewer ratings could not be sustained, due perhaps to direct competition with CBS's Top 20 hit "Dallas" (also produced by Lorimar) and NBC's Top 30 hit "Miami Vice." "Max Headroom" was canceled part-way into its second season; the entire series, along with two leftover episodes, was rerun in spring 1988, during the Writers Guild of America strike. A cinema spin-off titled "Max Headroom for President" was announced with production intended to start in early 1988 in order to capitalize on that year's U.S. presidential election, but it was never made.
Comico comics also had plans to publish a graphic novel based on the story, but never fulfilled them. A few posters were produced for comic shops, with a picture of Max Headroom saying "comics will never be the same again".
Characters.
Edison Carter.
Edison Carter (Matt Frewer) was a hard-hitting reporter for Network 23, who sometimes uncovered things that his superiors in the network would have preferred kept private. Eventually, one of these instances required him to flee his workspace, upon which he was injured in a motorcycle accident in a parking lot. Bryce Lynch uploaded a copy of his mind into a computer, giving birth to the character Max Headroom, as the last words seen by Edison Carter before impact were "Max Headroom," specifying vehicle clearance height in the parking lot.
The series depicted very little of the past described by Edison, though he did meet a female televangelist (whom he once dated when his reporting put him at odds with the Vu Age Church that she now headed) and a former colleague/rival who later died (and whose death sent Edison off on close to a rampage to avenge that death).
Edison cares about his co-workers, especially Theora Jones and Bryce Lynch, and he has a deep respect for his producer Murray (although he rarely shows it).
According to a personal statistics file displayed on a computer screen in the series, Edison is 6'2", weighing 180 pounds.
Theora Jones.
Theora Jones was played by Amanda Pays and first appeared in the British-made television pilot film for the series. Along with Matt Frewer and W. Morgan Sheppard, Pays was one of only three cast members to also appear in the American-made series that followed.
Theora was Network 23's star controller ("stolen" from the World One Network by Murray) and, working with the network's star reporter, Edison Carter, she often helped save the day for everyone. She was also the pseudo-love-interest of Edison Carter, but that subplot was not explored fully on the show before it was cancelled.
Network 23's personnel files list her father as unknown, her mother as deceased, and her brother as Shawn Jones; Shawn is the focus on the second episode broadcast, "Rakers".
Bryce Lynch.
Bryce Lynch (Chris Young), a child prodigy and computer hacker, is Network 23's one-man technology research department.
His birthdate is shown on-screen to be October 7, 1988.
In the stereotypical hacker ethos, Bryce has few principles and fewer loyalties. He seems to accept any task, even morally questionable ones, as long as he is allowed to have the freedom to play with technology however he sees fit. This, in turn, makes him a greater asset to the technological needs and demands of the network, and the whims of its executives and stars. However, he also generally does not hurt or infringe on others, making him an uncannily neutral character in the Max Headroom universe.
In the pilot episode of the series, Bryce is enlisted by evil network CEO Ned Grossberg (Charles Rocket, another non-original cast member) to investigate the mental patterns of unconscious reporter Edison Carter, to determine whether or not Carter has discovered the secrets of the "Blipverts" scandal. Bryce uploads the contents of Carter's memory into the Network 23 computer system, and manages to boot them as a computer program. The resulting personality, an unhinged and unrepressed version of Carter's personality, is dubbed "Max Headroom" after his first words (the last words seen by Carter before being knocked unconscious by a parking-garage security gate). Ironically, it had been Bryce, following orders from Grossberg, who fought a hacking battle of sorts (a la the opening scene to "Hackers") with Theora Jones that led to Edison hitting his head on a traffic barrier and falling unconscious.
After the first episode, Bryce is generally recruited by Carter and his controller, Theora Jones, to provide technical aid to their investigative reporting efforts.
Bryce is seen outside of his laboratory in just six episodes.
Blank Reg.
Blank Reg was played by W. Morgan Sheppard, one of only three cast members to also appear in the American-made series that followed.
Reg is a "blank", a person not indexed in the government's database. He broadcasts the underground Big Time Television Network from his bus. He is a good friend of Edison Carter, and saves him on more than one occasion. With cohort Dominique, he operates and is the onscreen voice of Big Time television, "All day every day, making tomorrow seem like yesterday."
He dresses in a punk style and has a Mohawk haircut. His personality could be considered energetic. He also has a strong nostalgic streak, defending antiquated music videos and printed books in equal measure.
Ned Grossberg.
Ned Grossberg is a recurring villain on the series, played by former "Saturday Night Live" cast member Charles Rocket.
In the UK telefilm "Max Headroom: 20 Minutes Into the Future" upon which the American series was based, the character was called Grosman and was played by Nickolas Grace. Rocket portrayed Grossberg as an American yuppie with a characteristic facial and neck-stretching twitch.
In the pilot episode, Grossberg is the chairman of Network 23, a major city television station with the highest rated investigative news show in town, hosted by Edison Carter. In the Max Headroom world, real-time ratings equal advertising dollars, and advertisements have replaced stocks as the measure of corporate worth.
Grossberg, with his secret prodigy Bryce Lynch, develops a high-speed advertising delivery method known as Blipverts, which condenses full advertisements into a few seconds. When Carter discovers that Blipverts are killing people, Grossberg orders Lynch to prevent Carter from getting out of the building. Knocked unconscious, Carter's memories are extracted into a computer by Lynch in order to determine whether Carter uncovered Grossberg's knowledge of the danger of Blipverts. The resulting computer file of the memory-extraction process becomes Max Headroom, making Grossberg directly responsible for the creation of the character.
In the end, Grossberg is publicly exposed as responsible for the Blipverts scandal, and is removed as chairman of Network 23.
A few episodes later, in "Grossberg's Return", Grossberg reappears as a board member of Network 66. Again, he invents a dubious advertising medium and convinces the chairman of the network to adopt it. When the advertising method is shown to be a complete fraud, the resulting public reaction against the network leads to the chairman being removed, and Grossberg manages to assume the chairmanship.
When under stress, Grossberg exhibits a tic of slightly stretching his neck in his suit's collar, first seen in episode 1 when he confronts Lynch in his lab regarding Max retaining Carter's memory about the blipverts.
Impact on society.
"Max Headroom" was the first cyberpunk series to run in the United States on one of the main broadcast networks in prime time, although it was not tagged with that label until some time after its cancellation. Like other science fiction, the series introduced the general public to new ideas in the form of cyberpunk themes and social issues. The series portrayed the Blanks, a counter-culture group of people who lived without any official numbers or documentation for the sake of privacy. Various episodes delved into issues like literacy and the lack thereof in a TV-dominated culture (for example, in the episode "Body Banks", Blank Reg says: "It's a book. It's a non-volatile storage medium. It's very rare. You should 'ave one." This statement also anticipates the mid-2000s controversy over the replacement of print by online and e-book sources.)
Of Max Headroom himself, actor Matt Frewer told "Rolling Stone" Magazine that "The cool thing about playing Max is that you can say virtually anything because theoretically the guy's not real, right? Can't sue a computer!"
The Max Headroom broadcast signal intrusion incident involved someone dressed as Max Headroom interrupting the signals of Chicago television stations WGN-TV and WTTW. The person or persons responsible were never identified.
In the late 1990s, U.S. cable TV channels Bravo and the Sci-Fi Channel re-ran the series. Reruns also briefly appeared on TechTV in 2001.
References in pop culture.
Max Headroom has inspired many imitations and spoofs. In the '80s, Garry Trudeau created the character Ron Headrest for his political comic strip "Doonesbury". The character combined the concepts of Max Headroom and then US President Ronald Reagan. "Back to the Future Part II" also featured a Max Headroom inspired Reagan, as well as computer generated versions of Michael Jackson and the Ayatollah Khomeini as waiters at the fictitious "Cafe '80s". There is an homage to Max Headroom in the 1997 film "Batman & Robin" when Barbara encounters her uncle Alfred Pennyworth in the batcave. He has programmed his brain algorithms into the batcomputer and created a virtual simulation. He appears and speaks (stutteringly) like Max Headroom.
Nickelodeon's "ME:TV" made a "You're watching ME:TV" clip with Ryan Knowles impersonating Max Headroom on the webwall. In the clip, Ryan's hair was combed back like Max's, and he stutters occasionally and the background panned vertically with purple and blue neon stripes. In episode 7, "John Quixote", of "Farscape"'s season 4, John Crichton enters virtual reality where he encounters a Max Headroom-like version of himself.
There is also a song by rock band Sum 41 called "Second Chance for Max Headroom".
In the Ernest Cline novel "Ready Player One", protagonist Wade Watts uses the name Bryce Lynch as his alias. He also has a Max Headroom AI in his ship.
Eminem's "Rap God" video features himself portrayed as Max Headroom.
DVD release.
The original British version of the movie was released to the Japanese DVD rental market on September 2, 2005.
"Shout! Factory" released "Max Headroom: The Complete Series" on DVD in the United States and Canada on August 10, 2010. The set includes a roundtable discussion with most of the major cast members (other than Matt Frewer), and interviews with the writers and producers.

</doc>
<doc id="20423" url="http://en.wikipedia.org/wiki?curid=20423" title="Malaria">
Malaria

Malaria is a mosquito-borne infectious disease of humans and other animals caused by parasitic protozoans (a group of single-celled microorganism) belonging to the genus "Plasmodium". Malaria causes symptoms that typically include fever, fatigue, vomiting and headaches. In severe cases it can cause yellow skin, seizures, coma or death. The disease is transmitted by the biting of mosquitos, and the symptoms usually begin ten to fifteen days after being bitten. If not appropriately treated, people may have recurrences of the disease months later. In those who have recently survived an infection, re-infection typically causes milder symptoms. This partial resistance disappears over months to years if the person has no continuing exposure to malaria.
The disease is transmitted most commonly by an infected female "Anopheles" mosquito. The mosquito bite introduces the parasites from the mosquito's saliva into a person's blood. The parasites travel to the liver where they mature and reproduce. Five species of "Plasmodium" can infect and be spread by humans. Most deaths are caused by "P. falciparum" because "P. vivax", "P. ovale", and "P. malariae" generally cause a milder form of malaria. The species "P. knowlesi" rarely causes disease in humans. Malaria is typically diagnosed by the microscopic examination of blood using blood films, or with antigen-based rapid diagnostic tests. Methods that use the polymerase chain reaction to detect the parasite's DNA have been developed, but are not widely used in areas where malaria is common due to their cost and complexity.
The risk of disease can be reduced by preventing mosquito bites by using mosquito nets and insect repellents, or with mosquito-control measures such as spraying insecticides and draining standing water. Several medications are available to prevent malaria in travellers to areas where the disease is common. Occasional doses of the medication sulfadoxine/pyrimethamine are recommended in infants and after the first trimester of pregnancy in areas with high rates of malaria. Despite a need, no effective vaccine exists, although efforts to develop one are ongoing. The recommended treatment for malaria is a combination of antimalarial medications that includes an artemisinin. The second medication may be either mefloquine, lumefantrine, or sulfadoxine/pyrimethamine. Quinine along with doxycycline may be used if an artemisinin is not available. It is recommended that in areas where the disease is common, malaria is confirmed if possible before treatment is started due to concerns of increasing drug resistance. Resistance among the parasites has developed to several antimalarial medications; for example, chloroquine-resistant "P. falciparum" has spread to most malarial areas, and resistance to artemisinin has become a problem in some parts of Southeast Asia. 
The disease is widespread in the tropical and subtropical regions that exist in a broad band around the equator. This includes much of Sub-Saharan Africa, Asia, and Latin America. Malaria is commonly associated with poverty and has a major negative effect on economic development. In Africa it is estimated to result in losses of US$12 billion a year due to increased healthcare costs, lost ability to work, and effects on tourism. The World Health Organization reports there were 198 million cases of malaria worldwide in 2013. This resulted in an estimated 584,000 to 855,000 deaths, the majority (90%) of which occurred in Africa.
Signs and symptoms.
The signs and symptoms of malaria typically begin 8–25 days following infection; however, symptoms may occur later in those who have taken antimalarial medications as prevention. Initial manifestations of the disease—common to all malaria species—are similar to flu-like symptoms, and can resemble other conditions such as septicemia, gastroenteritis, and viral diseases. The presentation may include headache, fever, shivering, joint pain, vomiting, hemolytic anemia, jaundice, hemoglobin in the urine, retinal damage, and convulsions.
The classic symptom of malaria is paroxysm—a cyclical occurrence of sudden coldness followed by shivering and then fever and sweating, occurring every two days (tertian fever) in "P. vivax" and "P. ovale" infections, and every three days (quartan fever) for "P. malariae". "P. falciparum" infection can cause recurrent fever every 36–48 hours, or a less pronounced and almost continuous fever.
Severe malaria is usually caused by "P. falciparum" (often referred to as falciparum malaria). Symptoms of falciparum malaria arise 9–30 days after infection. Individuals with cerebral malaria frequently exhibit neurological symptoms, including abnormal posturing, nystagmus, conjugate gaze palsy (failure of the eyes to turn together in the same direction), opisthotonus, seizures, or coma.
Complications.
Malaria has several serious complications. Among these is the development of respiratory distress, which occurs in up to 25% of adults and 40% of children with severe "P. falciparum" malaria. Possible causes include respiratory compensation of metabolic acidosis, noncardiogenic pulmonary oedema, concomitant pneumonia, and severe anaemia. Although rare in young children with severe malaria, acute respiratory distress syndrome occurs in 5–25% of adults and up to 29% of pregnant women. Coinfection of HIV with malaria increases mortality. Renal failure is a feature of blackwater fever, where hemoglobin from lysed red blood cells leaks into the urine.
Infection with "P. falciparum" may result in cerebral malaria, a form of severe malaria that involves encephalopathy. It is associated with retinal whitening, which may be a useful clinical sign in distinguishing malaria from other causes of fever. Splenomegaly, severe headache, hepatomegaly (enlarged liver), hypoglycemia, and hemoglobinuria with renal failure may occur. Complications may include spontaneous bleeding and coagulopathy. May cause shock (algid malaria).
Malaria in pregnant women is an important cause of stillbirths, infant mortality, abortion and low birth weight, particularly in "P. falciparum" infection, but also with "P. vivax".
Cause.
Malaria parasites belong to the genus "Plasmodium" (phylum Apicomplexa). In humans, malaria is caused by "P. falciparum", "P. malariae", "P. ovale", "P. vivax" and "P. knowlesi". Among those infected, "P. falciparum" is the most common species identified (~75%) followed by "P. vivax" (~20%). Although "P. falciparum" traditionally accounts for the majority of deaths, recent evidence suggests that "P. vivax" malaria is associated with potentially life-threatening conditions about as often as with a diagnosis of "P. falciparum" infection. "P. vivax " proportionally is more common outside Africa. There have been documented human infections with several species of "Plasmodium" from higher apes; however, except for "P. knowlesi"—a zoonotic species that causes malaria in macaques—these are mostly of limited public health importance.
Global warming is likely to affect malaria transmission, but the severity and geographic distribution of such effects is uncertain.
Life cycle.
In the life cycle of "Plasmodium", a female "Anopheles" mosquito (the definitive host) transmits a motile infective form (called the sporozoite) to a vertebrate host such as a human (the secondary host), thus acting as a transmission vector. A sporozoite travels through the blood vessels to liver cells (hepatocytes), where it reproduces asexually (tissue schizogony), producing thousands of merozoites. These infect new red blood cells and initiate a series of asexual multiplication cycles (blood schizogony) that produce 8 to 24 new infective merozoites, at which point the cells burst and the infective cycle begins anew.
Other merozoites develop into immature gametocytes, which are the precursors of male and female gametes. When a fertilised mosquito bites an infected person, gametocytes are taken up with the blood and mature in the mosquito gut. The male and female gametocytes fuse and form an ookinete—a fertilized, motile zygote. Ookinetes develop into new sporozoites that migrate to the insect's salivary glands, ready to infect a new vertebrate host. The sporozoites are injected into the skin, in the saliva, when the mosquito takes a subsequent blood meal.
Only female mosquitoes feed on blood; male mosquitoes feed on plant nectar, and do not transmit the disease. The females of the "Anopheles" genus of mosquito prefer to feed at night. They usually start searching for a meal at dusk, and will continue throughout the night until taking a meal. Malaria parasites can also be transmitted by blood transfusions, although this is rare.
Recurrent malaria.
Symptoms of malaria can recur after varying symptom-free periods. Depending upon the cause, recurrence can be classified as either recrudescence, relapse, or reinfection. Recrudescence is when symptoms return after a symptom-free period. It is caused by parasites surviving in the blood as a result of inadequate or ineffective treatment. Relapse is when symptoms reappear after the parasites have been eliminated from blood but persist as dormant hypnozoites in liver cells. Relapse commonly occurs between 8–24 weeks and is commonly seen with "P. vivax" and "P. ovale" infections. "P. vivax" malaria cases in temperate areas often involve overwintering by hypnozoites, with relapses beginning the year after the mosquito bite. Reinfection means the parasite that caused the past infection was eliminated from the body but a new parasite was introduced. Reinfection cannot readily be distinguished from recrudescence, although recurrence of infection within two weeks of treatment for the initial infection is typically attributed to treatment failure. People may develop some immunity when exposed to frequent infections.
Pathophysiology.
Malaria infection develops via two phases: one that involves the liver (exoerythrocytic phase), and one that involves red blood cells, or erythrocytes (erythrocytic phase). When an infected mosquito pierces a person's skin to take a blood meal, sporozoites in the mosquito's saliva enter the bloodstream and migrate to the liver where they infect hepatocytes, multiplying asexually and asymptomatically for a period of 8–30 days.
After a potential dormant period in the liver, these organisms differentiate to yield thousands of merozoites, which, following rupture of their host cells, escape into the blood and infect red blood cells to begin the erythrocytic stage of the life cycle. The parasite escapes from the liver undetected by wrapping itself in the cell membrane of the infected host liver cell.
Within the red blood cells, the parasites multiply further, again asexually, periodically breaking out of their host cells to invade fresh red blood cells. Several such amplification cycles occur. Thus, classical descriptions of waves of fever arise from simultaneous waves of merozoites escaping and infecting red blood cells.
Some "P. vivax" sporozoites do not immediately develop into exoerythrocytic-phase merozoites, but instead produce hypnozoites that remain dormant for periods ranging from several months (7–10 months is typical) to several years. After a period of dormancy, they reactivate and produce merozoites. Hypnozoites are responsible for long incubation and late relapses in "P. vivax" infections, although their existence in "P. ovale" is uncertain.
The parasite is relatively protected from attack by the body's immune system because for most of its human life cycle it resides within the liver and blood cells and is relatively invisible to immune surveillance. However, circulating infected blood cells are destroyed in the spleen. To avoid this fate, the "P. falciparum" parasite displays adhesive proteins on the surface of the infected blood cells, causing the blood cells to stick to the walls of small blood vessels, thereby sequestering the parasite from passage through the general circulation and the spleen. The blockage of the microvasculature causes symptoms such as in placental malaria. Sequestered red blood cells can breach the blood–brain barrier and cause cerebral malaria.
Genetic resistance.
According to a 2005 review, due to the high levels of mortality and morbidity caused by malaria—especially the "P. falciparum" species—it has placed the greatest selective pressure on the human genome in recent history. Several genetic factors provide some resistance to it including sickle cell trait, thalassaemia traits, glucose-6-phosphate dehydrogenase deficiency, and the absence of Duffy antigens on red blood cells.
The impact of sickle cell trait on malaria immunity illustrates some evolutionary trade-offs that have occurred because of endemic malaria. Sickle cell trait causes a defect in the hemoglobin molecule in the blood. Instead of retaining the biconcave shape of a normal red blood cell, the modified hemoglobin S molecule causes the cell to sickle or distort into a curved shape. Due to the sickle shape, the molecule is not as effective in taking or releasing oxygen. Infection causes red cells to sickle more, and so they are removed from circulation sooner. This reduces the frequency with which malaria parasites complete their life cycle in the cell. Individuals who are homozygous (with two copies of the abnormal hemoglobin beta allele) have sickle-cell anaemia, while those who are heterozygous (with one abnormal allele and one normal allele) experience resistance to malaria. Although the shorter life expectancy for those with the homozygous condition would not sustain the trait's survival, the trait is preserved because of the benefits provided by the heterozygous form.
Liver dysfunction.
Liver dysfunction as a result of malaria is uncommon and usually only occurs in those with other liver condition such as viral hepatitis or chronic liver disease. The syndrome is sometimes called "malarial hepatitis". While it has been considered a rare occurrence, malarial hepatopathy has seen an increase, particularly in Southeast Asia and India. Liver compromise in people with malaria correlates with a greater likelihood of complications and death.
Diagnosis.
Owing to the non-specific nature of the presentation of symptoms, diagnosis of malaria in non-endemic areas requires a high degree of suspicion, which might be elicited by any of the following: recent travel history, enlarged spleen, fever, low number of platelets in the blood, and higher-than-normal levels of bilirubin in the blood combined with a normal level of white blood cells.
Malaria is usually confirmed by the microscopic examination of blood films or by antigen-based rapid diagnostic tests (RDT). Microscopy is the most commonly used method to detect the malarial parasite—about 165 million blood films were examined for malaria in 2010. Despite its widespread usage, diagnosis by microscopy suffers from two main drawbacks: many settings (especially rural) are not equipped to perform the test, and the accuracy of the results depends on both the skill of the person examining the blood film and the levels of the parasite in the blood. The sensitivity of blood films ranges from 75–90% in optimum conditions, to as low as 50%. Commercially available RDTs are often more accurate than blood films at predicting the presence of malaria parasites, but they are widely variable in diagnostic sensitivity and specificity depending on manufacturer, and are unable to tell how many parasites are present.
In regions where laboratory tests are readily available, malaria should be suspected, and tested for, in any unwell person who has been in an area where malaria is endemic. In areas that cannot afford laboratory diagnostic tests, it has become common to use only a history of fever as the indication to treat for malaria—thus the common teaching "fever equals malaria unless proven otherwise". A drawback of this practice is overdiagnosis of malaria and mismanagement of non-malarial fever, which wastes limited resources, erodes confidence in the health care system, and contributes to drug resistance. Although polymerase chain reaction-based tests have been developed, they are not widely used in areas where malaria is common as of 2012, due to their complexity.
Classification.
Malaria is classified into either "severe" or "uncomplicated" by the World Health Organization (WHO). It is deemed severe when "any" of the following criteria are present, otherwise it is considered uncomplicated.
Cerebral malaria is defined as a severe "P. falciparum"-malaria presenting with neurological symptoms, including coma (with a Glasgow coma scale less than 11, or a Blantyre coma scale greater than 3), or with a coma that lasts longer than 30 minutes after a seizure.
Prevention.
Methods used to prevent malaria include medications, mosquito elimination and the prevention of bites. There is no vaccine for malaria. The presence of malaria in an area requires a combination of high human population density, high anopheles mosquito population density and high rates of transmission from humans to mosquitoes and from mosquitoes to humans. If any of these is lowered sufficiently, the parasite will eventually disappear from that area, as happened in North America, Europe and parts of the Middle East. However, unless the parasite is eliminated from the whole world, it could become re-established if conditions revert to a combination that favours the parasite's reproduction. Furthermore, the cost per person of eliminating anopheles mosquitoes rises with decreasing population density, making it economically unfeasible in some areas.
Prevention of malaria may be more cost-effective than treatment of the disease in the long run, but the initial costs required are out of reach of many of the world's poorest people. There is a wide difference in the costs of control (i.e. maintenance of low endemicity) and elimination programs between countries. For example, in China—whose government in 2010 announced a strategy to pursue malaria elimination in the Chinese provinces—the required investment is a small proportion of public expenditure on health. In contrast, a similar program in Tanzania would cost an estimated one-fifth of the public health budget.
Mosquito control.
Vector control refers to methods used to decrease malaria by reducing the levels of transmission by mosquitoes. For individual protection, the most effective insect repellents are based on DEET or picaridin. Insecticide-treated mosquito nets (ITNs) and indoor residual spraying (IRS) have been shown to be highly effective in preventing malaria among children in areas where malaria is common. Prompt treatment of confirmed cases with artemisinin-based combination therapies (ACTs) may also reduce transmission.
Mosquito nets help keep mosquitoes away from people and reduce infection rates and transmission of malaria. Nets are not a perfect barrier and are often treated with an insecticide designed to kill the mosquito before it has time to find a way past the net. Insecticide-treated nets are estimated to be twice as effective as untreated nets and offer greater than 70% protection compared with no net. Between 2000 and 2008, the use of ITNs saved the lives of an estimated 250,000 infants in Sub-Saharan Africa. About 13% of households in Sub-Saharan countries own ITNs. In 2000, 1.7 million (1.8%) African children living in areas of the world where malaria is common were protected by an ITN. That number increased to 20.3 million (18.5%) African children using ITNs in 2007, leaving 89.6 million children unprotected. An increased percentage of African households (31%) are estimated to own at least one ITN in 2008. Most nets are impregnated with pyrethroids, a class of insecticides with low toxicity. They are most effective when used from dusk to dawn. It is recommended to hang a large "bed net" above the center of a bed and either tuck the edges under the mattress or make sure it is large enough such that it touches the ground.
Indoor residual spraying is the spraying of insecticides on the walls inside a home. After feeding, many mosquitoes rest on a nearby surface while digesting the bloodmeal, so if the walls of houses have been coated with insecticides, the resting mosquitoes can be killed before they can bite another person and transfer the malaria parasite. As of 2006, the World Health Organization recommends 12 insecticides in IRS operations, including DDT and the pyrethroids cyfluthrin and deltamethrin. This public health use of small amounts of DDT is permitted under the Stockholm Convention, which prohibits its agricultural use. One problem with all forms of IRS is insecticide resistance. Mosquitoes affected by IRS tend to rest and live indoors, and due to the irritation caused by spraying, their descendants tend to rest and live outdoors, meaning that they are less affected by the IRS.
There are a number of other methods to reduce mosquito bites and slow the spread of malaria. Efforts to decrease mosquito larva by decreasing the availability of open water in which they develop or by adding substances to decrease their development is effective in some locations. Electronic mosquito repellent devices which make very high frequency sounds that are supposed to keep female mosquitoes away, do not have supporting evidence.
Other methods.
Community participation and health education strategies promoting awareness of malaria and the importance of control measures have been successfully used to reduce the incidence of malaria in some areas of the developing world. Recognizing the disease in the early stages can stop the disease from becoming fatal. Education can also inform people to cover over areas of stagnant, still water, such as water tanks that are ideal breeding grounds for the parasite and mosquito, thus cutting down the risk of the transmission between people. This is generally used in urban areas where there are large centers of population in a confined space and transmission would be most likely in these areas. Intermittent preventive therapy is another intervention that has been used successfully to control malaria in pregnant women and infants, and in preschool children where transmission is seasonal.
Medications.
There are a number of drugs that can help prevent malaria while travelling in areas where it exists. Most of these drugs are also sometimes used in treatment. Chloroquine may be used where the parasite is still sensitive. Because most "Plasmodium" is resistant to one or more medications, one of three medications—mefloquine ("Lariam"), doxycycline (available generically), or the combination of atovaquone and proguanil hydrochloride ("Malarone")—is frequently needed. Doxycycline and the atovaquone and proguanil combination are the best tolerated; mefloquine is associated with death, suicide, and neurological and psychiatric symptoms.
The protective effect does not begin immediately, and people visiting areas where malaria exists usually start taking the drugs one to two weeks before arriving and continue taking them for four weeks after leaving (except for atovaquone/proguanil, which only needs to be started two days before and continued for seven days afterward). The use of preventative drugs is often not practical for those who live in areas where malaria exists, and their use is usually only in short-term visitors and travellers. This is due to the cost of the drugs, side effects from long-term use, and the difficulty in obtaining anti-malarial drugs outside of wealthy nations. The use of preventative drugs where malaria-bearing mosquitoes are present may encourage the development of partial resistance. An exception to this is during pregnancy when taking medication to prevent malaria has been found to improve the weight of the baby at birth and decrease the risk of anemia in the mother.
Treatment.
Malaria is treated with antimalarial medications; the ones used depends on the type and severity of the disease. While medications against fever are commonly used, their effects on outcomes are not clear.
Uncomplicated malaria may be treated with oral medications. The most effective treatment for "P. falciparum" infection is the use of artemisinins in combination with other antimalarials (known as artemisinin-combination therapy, or ACT), which decreases resistance to any single drug component. These additional antimalarials include: amodiaquine, lumefantrine, mefloquine or sulfadoxine/pyrimethamine. Another recommended combination is dihydroartemisinin and piperaquine. ACT is about 90% effective when used to treat uncomplicated malaria. To treat malaria during pregnancy, the WHO recommends the use of quinine plus clindamycin early in the pregnancy (1st trimester), and ACT in later stages (2nd and 3rd trimesters). In the 2000s (decade), malaria with partial resistance to artemisins emerged in Southeast Asia.
Infection with "P. vivax", "P. ovale" or "P. malariae" is usually treated without the need for hospitalization. Treatment of "P. vivax" requires both treatment of blood stages (with chloroquine or ACT) and clearance of liver forms with primaquine.
Recommended treatment for severe malaria is the intravenous use of antimalarial drugs. For severe malaria, artesunate is superior to quinine in both children and adults. Treatment of severe malaria involves supportive measures that are best done in a critical care unit. This includes the management of high fevers and the seizures that may result from it. It also includes monitoring for poor breathing effort, low blood sugar, and low blood potassium.
Resistance.
Drug resistance poses a growing problem in 21st-century malaria treatment. Resistance is now common against all classes of antimalarial drugs save the artemisinins. Treatment of resistant strains became increasingly dependent on this class of drugs. The cost of artemisinins limits their use in the developing world. Malaria strains found on the Cambodia–Thailand border are resistant to combination therapies that include artemisinins, and may therefore be untreatable. Exposure of the parasite population to artemisinin monotherapies in subtherapeutic doses for over 30 years and the availability of substandard artemisinins likely drove the selection of the resistant phenotype. Resistance to artemisinin has been detected in Cambodia, Myanmar, Thailand, and Vietnam, and there has been emerging resistance in Laos.
Prognosis.
When properly treated, people with malaria can usually expect a complete recovery. However, severe malaria can progress extremely rapidly and cause death within hours or days. In the most severe cases of the disease, fatality rates can reach 20%, even with intensive care and treatment. Over the longer term, developmental impairments have been documented in children who have suffered episodes of severe malaria. Chronic infection without severe disease can occur in an immune-deficiency syndrome associated with a decreased responsiveness to "Salmonella" bacteria and the Epstein–Barr virus.
During childhood, malaria causes anemia during a period of rapid brain development, and also direct brain damage resulting from cerebral malaria. Some survivors of cerebral malaria have an increased risk of neurological and cognitive deficits, behavioural disorders, and epilepsy. Malaria prophylaxis was shown to improve cognitive function and school performance in clinical trials when compared to placebo groups.
Epidemiology.
The WHO estimates that in 2010 there were 219 million cases of malaria resulting in 660,000 deaths. Others have estimated the number of cases at between 350 and 550 million for falciparum malaria and deaths in 2010 at 1.24 million up from 1.0 million deaths in 1990. The majority of cases (65%) occur in children under 15 years old. About 125 million pregnant women are at risk of infection each year; in Sub-Saharan Africa, maternal malaria is associated with up to 200,000 estimated infant deaths yearly. There are about 10,000 malaria cases per year in Western Europe, and 1300–1500 in the United States. About 900 people died from the disease in Europe between 1993 and 2003. Both the global incidence of disease and resulting mortality have declined in recent years. According to the WHO, deaths attributable to malaria in 2010 were reduced by over a third from a 2000 estimate of 985,000, largely due to the widespread use of insecticide-treated nets and artemisinin-based combination therapies. In 2012, there were 207 million cases of malaria. That year, the disease is estimated to have killed between 473,000 and 789,000 people, many of whom were children in Africa.
Malaria is presently endemic in a broad band around the equator, in areas of the Americas, many parts of Asia, and much of Africa; in Sub-Saharan Africa, 85–90% of malaria fatalities occur. An estimate for 2009 reported that countries with the highest death rate per 100,000 of population were Ivory Coast (86.15), Angola (56.93) and Burkina Faso (50.66). A 2010 estimate indicated the deadliest countries per population were Burkina Faso, Mozambique and Mali. The Malaria Atlas Project aims to map global endemic levels of malaria, providing a means with which to determine the global spatial limits of the disease and to assess disease burden. This effort led to the publication of a map of "P. falciparum" endemicity in 2010. As of 2010, about 100 countries have endemic malaria. Every year, 125 million international travellers visit these countries, and more than 30,000 contract the disease.
The geographic distribution of malaria within large regions is complex, and malaria-afflicted and malaria-free areas are often found close to each other. Malaria is prevalent in tropical and subtropical regions because of rainfall, consistent high temperatures and high humidity, along with stagnant waters in which mosquito larvae readily mature, providing them with the environment they need for continuous breeding. In drier areas, outbreaks of malaria have been predicted with reasonable accuracy by mapping rainfall. Malaria is more common in rural areas than in cities. For example, several cities in the Greater Mekong Subregion of Southeast Asia are essentially malaria-free, but the disease is prevalent in many rural regions, including along international borders and forest fringes. In contrast, malaria in Africa is present in both rural and urban areas, though the risk is lower in the larger cities.
History.
Although the parasite responsible for "P. falciparum" malaria has been in existence for 50,000–100,000 years, the population size of the parasite did not increase until about 10,000 years ago, concurrently with advances in agriculture and the development of human settlements. Close relatives of the human malaria parasites remain common in chimpanzees. Some evidence suggests that the "P. falciparum" malaria may have originated in gorillas.
References to the unique periodic fevers of malaria are found throughout recorded history, beginning in 2700 BC in China. Hippocrates described periodic fevers, labelling them tertian, quartan, subtertian and quotidian. The Roman Columella associated the disease with insects from swamps. Malaria may have contributed to the decline of the Roman Empire, and was so pervasive in Rome that it was known as the "Roman fever". Several regions in ancient Rome were considered at-risk for the disease because of the favourable conditions present for malaria vectors. This included areas such as southern Italy, the island of Sardinia, the Pontine Marshes, the lower regions of coastal Etruria and the city of Rome along the Tiber River. The presence of stagnant water in these places was preferred by mosquitoes for breeding grounds. Irrigated gardens, swamp-like grounds, runoff from agriculture, and drainage problems from road construction led to the increase of standing water.
The term malaria originates from Medieval Italian: "mala aria" — "bad air"; the disease was formerly called "ague" or "marsh fever" due to its association with swamps and marshland. The term first appeared in the English literature about 1829. Malaria was once common in most of Europe and North America, where it is no longer endemic, though imported cases do occur.
Scientific studies on malaria made their first significant advance in 1880, when Charles Louis Alphonse Laveran—a French army doctor working in the military hospital of Constantine in Algeria—observed parasites inside the red blood cells of infected people for the first time. He therefore proposed that malaria is caused by this organism, the first time a protist was identified as causing disease. For this and later discoveries, he was awarded the 1907 Nobel Prize for Physiology or Medicine. A year later, Carlos Finlay, a Cuban doctor treating people with yellow fever in Havana, provided strong evidence that mosquitoes were transmitting disease to and from humans. This work followed earlier suggestions by Josiah C. Nott, and work by Sir Patrick Manson, the "father of tropical medicine", on the transmission of filariasis.
In April 1894, a Scottish physician Sir Ronald Ross visited Sir Patrick Manson at his house on Queen Anne Street, London. This visit was the start of four years of collaboration and fervent research that culminated in 1898 when Ross, who was working in the Presidency General Hospital in Calcutta, proved the complete life-cycle of the malaria parasite in mosquitoes. He thus proved that the mosquito was the vector for malaria in humans by showing that certain mosquito species transmit malaria to birds. He isolated malaria parasites from the salivary glands of mosquitoes that had fed on infected birds. For this work, Ross received the 1902 Nobel Prize in Medicine. After resigning from the Indian Medical Service, Ross worked at the newly established Liverpool School of Tropical Medicine and directed malaria-control efforts in Egypt, Panama, Greece and Mauritius. The findings of Finlay and Ross were later confirmed by a medical board headed by Walter Reed in 1900. Its recommendations were implemented by William C. Gorgas in the health measures undertaken during construction of the Panama Canal. This public-health work saved the lives of thousands of workers and helped develop the methods used in future public-health campaigns against the disease.
The first effective treatment for malaria came from the bark of cinchona tree, which contains quinine. This tree grows on the slopes of the Andes, mainly in Peru. The indigenous peoples of Peru made a tincture of cinchona to control fever. Its effectiveness against malaria was found and the Jesuits introduced the treatment to Europe around 1640; by 1677, it was included in the London Pharmacopoeia as an antimalarial treatment. It was not until 1820 that the active ingredient, quinine, was extracted from the bark, isolated and named by the French chemists Pierre Joseph Pelletier and Joseph Bienaimé Caventou.
Quinine become the predominant malarial medication until the 1920s, when other medications began to be developed. In the 1940s, chloroquine replaced quinine as the treatment of both uncomplicated and severe malaria until resistance supervened, first in Southeast Asia and South America in the 1950s and then globally in the 1980s. Artemisinins, discovered by Chinese scientist Tu Youyou and colleagues in the 1970s from the plant "Artemisia annua", became the recommended treatment for "P. falciparum" malaria, administered in combination with other antimalarials as well as in severe disease.
"Plasmodium vivax" was used between 1917 and the 1940s for malariotherapy—deliberate injection of malaria parasites to induce fever to combat certain diseases such as tertiary syphilis. In 1917, the inventor of this technique, Julius Wagner-Jauregg, received the Nobel Prize in Physiology or Medicine for his discoveries. The technique was dangerous, killing about 15% of patients, so it is no longer in use.
The first pesticide used for indoor residual spraying was DDT. Although it was initially used exclusively to combat malaria, its use quickly spread to agriculture. In time, pest control, rather than disease control, came to dominate DDT use, and this large-scale agricultural use led to the evolution of resistant mosquitoes in many regions. The DDT resistance shown by "Anopheles" mosquitoes can be compared to antibiotic resistance shown by bacteria. During the 1960s, awareness of the negative consequences of its indiscriminate use increased, ultimately leading to bans on agricultural applications of DDT in many countries in the 1970s. Before DDT, malaria was successfully eliminated or controlled in tropical areas like Brazil and Egypt by removing or poisoning the breeding grounds of the mosquitoes or the aquatic habitats of the larva stages, for example by applying the highly toxic arsenic compound Paris Green to places with standing water.
Malaria vaccines have been an elusive goal of research. The first promising studies demonstrating the potential for a malaria vaccine were performed in 1967 by immunizing mice with live, radiation-attenuated sporozoites, which provided significant protection to the mice upon subsequent injection with normal, viable sporozoites. Since the 1970s, there has been a considerable effort to develop similar vaccination strategies within humans.
Society and culture.
Economic impact.
Malaria is not just a disease commonly associated with poverty: some evidence suggests that it is also a cause of poverty and a major hindrance to economic development. Although tropical regions are most affected, malaria's furthest influence reaches into some temperate zones that have extreme seasonal changes. The disease has been associated with major negative economic effects on regions where it is widespread. During the late 19th and early 20th centuries, it was a major factor in the slow economic development of the American southern states.
A comparison of average per capita GDP in 1995, adjusted for parity of purchasing power, between countries with malaria and countries without malaria gives a fivefold difference ($1,526 USD versus $8,268 USD). In the period 1965 to 1990, countries where malaria was common had an average per capita GDP that increased only 0.4% per year, compared to 2.4% per year in other countries.
Poverty can increase the risk of malaria, since those in poverty do not have the financial capacities to prevent or treat the disease. In its entirety, the economic impact of malaria has been estimated to cost Africa US$12 billion every year. The economic impact includes costs of health care, working days lost due to sickness, days lost in education, decreased productivity due to brain damage from cerebral malaria, and loss of investment and tourism. The disease has a heavy burden in some countries, where it may be responsible for 30–50% of hospital admissions, up to 50% of outpatient visits, and up to 40% of public health spending.
Cerebral malaria is one of the leading causes of neurological disabilities in African children. Studies comparing cognitive functions before and after treatment for severe malarial illness continued to show significantly impaired school performance and cognitive abilities even after recovery. Consequently, severe and cerebral malaria have far-reaching socioeconomic consequences that extend beyond the immediate effects of the disease.
Counterfeit and substandard drugs.
Sophisticated counterfeits have been found in several Asian countries such as Cambodia, China, Indonesia, Laos, Thailand, and Vietnam, and are an important cause of avoidable death in those countries. The WHO said that studies indicate that up to 40% of artesunate-based malaria medications are counterfeit, especially in the Greater Mekong region and have established a rapid alert system to enable information about counterfeit drugs to be rapidly reported to the relevant authorities in participating countries. There is no reliable way for doctors or lay people to detect counterfeit drugs without help from a laboratory. Companies are attempting to combat the persistence of counterfeit drugs by using new technology to provide security from source to distribution.
Another clinical and public health concern is the proliferation of substandard antimalarial medicines resulting from inappropriate concentration of ingredients, contamination with other drugs or toxic impurities, poor quality ingredients, poor stability and inadequate packaging. A 2012 study demonstrated that roughly one-third of antimalarial medications in Southeast Asia and Sub-Saharan Africa failed chemical analysis, packaging analysis, or were falsified.
War.
Throughout history, the contraction of malaria has played a prominent role in the fates of government rulers, nation-states, military personnel, and military actions. In 1910, Nobel Prize in Medicine-winner Ronald Ross (himself a malaria survivor), published a book titled "The Prevention of Malaria" that included a chapter titled "The Prevention of Malaria in War." The chapter's author, Colonel C. H. Melville, Professor of Hygiene at Royal Army Medical College in London, addressed the prominent role that malaria has historically played during wars: "The history of malaria in war might almost be taken to be the history of war itself, certainly the history of war in the Christian era. ... It is probably the case that many of the so-called camp fevers, and probably also a considerable proportion of the camp dysentery, of the wars of the sixteenth, seventeenth and eighteenth centuries were malarial in origin."
Malaria was the most important health hazard encountered by U.S. troops in the South Pacific during World War II, where about 500,000 men were infected. According to Joseph Patrick Byrne, "Sixty thousand American soldiers died of malaria during the African and South Pacific campaigns."
Significant financial investments have been made to procure existing and create new anti-malarial agents. During World War I and World War II, inconsistent supplies of the natural anti-malaria drugs cinchona bark and quinine prompted substantial funding into research and development of other drugs and vaccines. American military organizations conducting such research initiatives include the Navy Medical Research Center, Walter Reed Army Institute of Research, and the U.S. Army Medical Research Institute of Infectious Diseases of the US Armed Forces.
Additionally, initiatives have been founded such as Malaria Control in War Areas (MCWA), established in 1942, and its successor, the Communicable Disease Center (now known as the Centers for Disease Control and Prevention, or CDC) established in 1946. According to the CDC, MCWA "was established to control malaria around military training bases in the southern United States and its territories, where malaria was still problematic".
Eradication efforts.
Several notable attempts are being made to eliminate the parasite from sections of the world, or to eradicate it worldwide. In 2006, the organization Malaria No More set a public goal of eliminating malaria from Africa by 2015, and the organization plans to dissolve if that goal is accomplished. Several malaria vaccines are in clinical trials, which are intended to provide protection for children in endemic areas and reduce the speed of transmission of the disease. s of 2012[ [update]], The Global Fund to Fight AIDS, Tuberculosis and Malaria has distributed 230 million insecticide-treated nets intended to stop mosquito-borne transmission of malaria. The U.S.-based Clinton Foundation has worked to manage demand and stabilize prices in the artemisinin market. Other efforts, such as the Malaria Atlas Project, focus on analysing climate and weather information required to accurately predict the spread of malaria based on the availability of habitat of malaria-carrying parasites. The Malaria Policy Advisory Committee (MPAC) of the World Health Organization (WHO) was formed in 2012, "to provide strategic advice and technical input to WHO on all aspects of malaria control and elimination". In November 2013, WHO and the malaria vaccine funders group set a goal to develop vaccines designed to interrupt malaria transmission with the long-term goal of malaria eradication.
Malaria has been successfully eliminated or greatly reduced in certain areas. Malaria was once common in the United States and southern Europe, but vector control programs, in conjunction with the monitoring and treatment of infected humans, eliminated it from those regions. Several factors contributed, such as the draining of wetland breeding grounds for agriculture and other changes in water management practices, and advances in sanitation, including greater use of glass windows and screens in dwellings. Malaria was eliminated from most parts of the USA in the early 20th century by such methods, and the use of the pesticide DDT and other means eliminated it from the remaining pockets in the South in the 1950s. (see National Malaria Eradication Program) In Suriname, the disease has been cleared from its capital city and coastal areas through a three-pronged approach initiated by the Global Malaria Eradication program in 1955, involving: vector control through the use of DDT and IRS; regular collection of blood smears from the population to identify existing malaria cases; and providing chemotherapy to all affected individuals. Bhutan is pursuing an aggressive malaria elimination strategy, and has achieved a 98.7% decline in microscopy-confirmed cases from 1994 to 2010. In addition to vector control techniques such as IRS in high-risk areas and thorough distribution of long-lasting ITNs, factors such as economic development and increasing access to health services have contributed to Bhutan's successes in reducing malaria incidence.
Research.
Vaccine.
Immunity (or, more accurately, tolerance) to "P. falciparum" malaria does occur naturally, but only in response to years of repeated infection. An individual can be protected from a "P. falciparum" infection if they receive about a thousand bites from mosquitoes that carry a version of the parasite rendered non-infective by a dose of X-ray irradiation. An effective vaccine is not yet available for malaria, although several are under development. The highly polymorphic nature of many "P. falciparum" proteins results in significant challenges to vaccine design. Vaccine candidates that target antigens on gametes, zygotes, or ookinetes in the mosquito midgut aim to block the transmission of malaria. These transmission-blocking vaccines induce antibodies in the human blood; when a mosquito takes a blood meal from a protected individual, these antibodies prevent the parasite from completing its development in the mosquito. Other vaccine candidates, targeting the blood-stage of the parasite's life cycle, have been inadequate on their own. For example, SPf66 was tested extensively in areas where the disease is common in the 1990s, but trials showed it to be insufficiently effective. Several potential vaccines targeting the pre-erythrocytic stage of the parasite's life cycle are being developed, with RTS,S as a leading candidate; it is expected to be licensed in 2015. A US biotech company, Sanaria, is developing a pre-erythrocytic attenuated vaccine called PfSPZ that uses whole sporozoites to induce an immune response. In 2006, the Malaria Vaccine Advisory Committee to the WHO outlined a "Malaria Vaccine Technology Roadmap" that has as one of its landmark objectives to "develop and license a first-generation malaria vaccine that has a protective efficacy of more than 50% against severe disease and death and lasts longer than one year" by 2015.
Medications.
Malaria parasites contain apicoplasts, organelles usually found in plants, complete with their own genomes. These apicoplasts are thought to have originated through the endosymbiosis of algae and play a crucial role in various aspects of parasite metabolism, such as fatty acid biosynthesis. Over 400 proteins have been found to be produced by apicoplasts and these are now being investigated as possible targets for novel anti-malarial drugs.
With the onset of drug-resistant "Plasmodium" parasites, new strategies are being developed to combat the widespread disease. One such approach lies in the introduction of synthetic pyridoxal-amino acid adducts, which are taken up by the parasite and ultimately interfere with its ability to create several essential B vitamins. Antimalarial drugs using synthetic metal-based complexes are attracting research interest.
Other.
A non-chemical vector control strategy involves genetic manipulation of malaria mosquitoes. Advances in genetic engineering technologies make it possible to introduce foreign DNA into the mosquito genome and either decrease the lifespan of the mosquito, or make it more resistant to the malaria parasite. Sterile insect technique is a genetic control method whereby large numbers of sterile males mosquitoes are reared and released. Mating with wild females reduces the wild population in the subsequent generation; repeated releases eventually eliminate the target population.
Genomics is central to malaria research. With the sequencing of "P. falciparum", one of its vectors "Anopheles gambiae", and the human genome, the genetics of all three organisms in the malaria lifecycle can be studied. Another new application of genetic technology is the ability to produce genetically modified mosquitoes that do not transmit malaria, potentially allowing biological control of malaria transmission.
Other animals.
Nearly 200 parasitic "Plasmodium" species have been identified that infect birds, reptiles, and other mammals, and about 30 species naturally infect non-human primates. Some malaria parasites that affect non-human primates (NHP) serve as model organisms for human malarial parasites, such as "P. coatneyi" (a model for "P. falciparum") and "P. cynomolgi" ("P. vivax"). Diagnostic techniques used to detect parasites in NHP are similar to those employed for humans. Malaria parasites that infect rodents are widely used as models in research, such as "P. berghei". Avian malaria primarily affects species of the order Passeriformes, and poses a substantial threat to birds of Hawaii, the Galapagos, and other archipelagoes. The parasite "P. relictum" is known to play a role in limiting the distribution and abundance of endemic Hawaiian birds. Global warming is expected to increase the prevalence and global distribution of avian malaria, as elevated temperatures provide optimal conditions for parasite reproduction.

</doc>
<doc id="20424" url="http://en.wikipedia.org/wiki?curid=20424" title="Lunar phase">
Lunar phase

<br>
The lunar phase or phase of the moon is the shape of the illuminated (sunlit) portion of the Moon as seen by an observer on Earth. The lunar phases change cyclically as the Moon orbits the Earth, according to the changing positions of the Moon and Sun relative to the Earth. The Moon and the Earth are tidally locked, therefore the same lunar surface always faces Earth. This face is variously sunlit depending on the position of the Moon in its orbit. Therefore, the portion of this hemisphere that is visible to an observer on Earth can vary from about 100% (full moon) to 0% (new moon). The lunar terminator is the boundary between the illuminated and darkened hemispheres. Aside from some craters near the lunar poles such as Shoemaker, all parts of the Moon see around 14.77 days of sunlight followed by 14.77 days of "night" (the "dark side" of the Moon is a reference to radio darkness, not visible light darkness).
The phases of the moon.
In Western culture, the four principal lunar phases are first quarter, full moon, last quarter (also known as third quarter), and new moon. These are the instants when the Moon's apparent geocentric celestial longitude minus the Sun's apparent geocentric celestial longitude is 0°, 90°, 180° and 270°, respectively. Each of these phases is roughly 7.38 days long, but the durations vary slightly because the Moon's orbit is slightly elliptical, and thus its speed in orbit isn't constant.
The eight principal and intermediate phases are given the following names, in sequential order:
When the Sun and Moon are aligned on the same side of the Earth, the moon is "new", and the side of the Moon facing Earth is not illuminated by the Sun. As the moon "waxes" (the amount of illuminated surface as seen from Earth is increasing), the lunar phases progress through new moon, crescent moon, first-quarter moon, gibbous moon, and full moon. The moon is then said to "wane" as it passes through the gibbous moon, last-quarter moon, crescent moon and back to new moon. The terms "old moon" and "new moon" are interchangeable, although new moon is more common. Half moon is often used to mean the first- and last-quarter moons, while the term 'quarter' refers to the extent of the moon's cycle around the Earth, not its shape.
When a sphere is illuminated on one hemisphere and viewed from a different angle, the portion of the illuminated area that is visible will have a two-dimensional shape defined by the intersection of an ellipse and circle (where the major axis of the ellipse coincides with a diameter of the circle). If the half-ellipse is convex with respect to the half-circle, then the shape will be gibbous (bulging outwards, Origin: 1350–1400; Middle English < Latin gibbōsus humped, equivalent to gibb ( a ) hump + -ōsus -ous ), whereas if the half-ellipse is concave with respect to the half-circle, then the shape will be a crescent. When a crescent Moon occurs, the phenomenon of Earthshine may be apparent, where the night side of the Moon faintly reflects light from the Earth.
In the northern hemisphere, if the left side of the Moon is dark then the light part is growing, and the Moon is referred to as waxing (moving toward a full moon). If the right side of the Moon is dark then the light part is shrinking, and the Moon is referred to as waning (past full and moving toward a new moon). Assuming that the viewer is in the northern hemisphere, the right portion of the Moon is the part that is always growing (i.e., if the right side is dark, the Moon is growing darker; if the right side is lit, the Moon is growing lighter). In the southern hemisphere the Moon is observed from a perspective inverted to that of the northern hemisphere, so the opposite sides appear to grow (wax) and shrink (wane).
The above descriptions of the lunar phases apply for observers at temperate or high latitudes on the Earth. Observers in tropical latitudes see the Moon with its terminator apparently horizontal during the morning and evening. The crescent Moon can open upward or downward, with the "horns" of the crescent pointing up or down, respectively. When the Sun appears above the Moon in the sky, the crescent opens downward; when the Moon is above the Sun, the crescent opens upward. The crescent Moon is most clearly and brightly visible when the Sun is below the horizon, which implies that the Moon must be above the Sun, and the crescent must open upward. This is therefore the orientation in which the crescent Moon is most often seen from the Earth's tropics. The waxing and waning crescents look very similar. The waxing crescent appears in the western sky in the evening, and the waning crescent in the east, in the morning.
When the Moon, as seen from Earth, is a narrow crescent, the Earth as seen from the Moon is almost fully lit by the Sun. Often, the part of the Moon that is not directly lit by the Sun is sufficiently brightly lit by light reflected from the Earth to be easily visible from Earth. This phenomenon is called "earthshine", and is sometimes picturesquely described as "the old moon in the new moon's arms".
Non-Western cultures may use a different number of Moon phases, for example traditional Hawaiian culture has a total of 30 different Moon phases.
The calendar.
The average calendrical month, which is 1⁄12 of a year, is about 30.44 days, while the Moon's phase (synodic) cycle repeats on average every 29.53 days. Therefore the timing of the Moon's phases shifts by an average of almost one day for each successive month. Photographing the Moon's phase every day for a month, starting in the evening after sunset, and repeating approximately 25 minutes later each successive day, ending in the morning before sunrise, would create a composite image like the example calendar from May 8, 2005, to June 6, 2005. There is no picture on May 20 since a picture would be taken before midnight on May 19, and after midnight on May 21. Similarly, on a calendar listing moon rise or set times, some days will appear to be skipped. When the Moon rises just before midnight one night it will rise just after midnight the next (so too with setting). The 'skipped day' is just a calendar artifact and not the Moon behaving strangely. The moon has a predictable orbit every month.
Calculating phase.
Each of the 4 lunar phases lasts approximately 7 days (~7.4 days), but varies slightly due to lunar apogee and perigee.
The approximate age of the moon, and hence the approximate phase, can be calculated for any date by calculating the number of days since a known new moon (such as January 1, 1900 or August 11, 1999) and reducing this modulo 29.530588853 (the length of a synodic month). The difference between two dates can be calculated by subtracting the Julian Day Number of one from that of the other, or there are simpler formulae giving (for instance) the number of days since December 31, 1899. However, this calculation assumes a perfectly circular orbit and therefore may be incorrect by several hours (it also becomes less accurate the larger the difference between the required date and the reference date); it is accurate enough to use in a novelty clock application showing moon phase, but specialist usage taking account of lunar apogee and perigee requires a more elaborate calculation. 
Effect of parallax.
The Earth subtends an angle of about two degrees, when seen from the Moon. This means that an observer on Earth who sees the Moon when it is close to the eastern horizon sees it from an angle that is about two degrees different from the line of sight of an observer who sees the Moon on the western horizon. The Moon moves about 12 degrees around its orbit per day, so, if these observers were stationary, they would see the phases of the Moon at times that differ by about one-sixth of a day, or four hours. But in reality the observers are on the surface of the rotating Earth, so someone who sees the Moon on the eastern horizon at one moment sees it on the western horizon about 12 hours later. This adds an oscillation to the apparent progression of the lunar phases. They appear to occur more slowly when the Moon is high in the sky than when it is below the horizon. The Moon appears to move jerkily, and the phases do the same. The amplitude of this oscillation is never more than about four hours, which is a small fraction of a month. It does not have any obvious effect on the appearance of the Moon. However, it does affect accurate calculations of the times of lunar phases.
Misconceptions.
It might be expected that once every month, when the Moon passes between Earth and the Sun during a new moon, its shadow would fall on Earth causing a solar eclipse, but this does not happen every month. Nor is it true that during every full moon, the Earth's shadow falls on the Moon, causing a lunar eclipse. Solar and lunar eclipses are not observed "every" month because the plane of the Moon's orbit around the Earth is tilted by about five degrees with respect to the plane of Earth's orbit around the Sun (the plane of the ecliptic). Thus, when new and full moons occur, the Moon usually lies to the north or south of a direct line through the Earth and Sun. Although an eclipse can only occur when the Moon is either new (solar) or full (lunar), it must also be positioned very near the intersection of Earth's orbit plane about the Sun and the Moon's orbit plane about the Earth (that is, at one of its nodes). This happens about twice per year, and so there are between four and seven eclipses in a calendar year. Most of these events are quite insignificant; major eclipses of the Moon or Sun are less frequent.

</doc>
<doc id="20426" url="http://en.wikipedia.org/wiki?curid=20426" title="Metonic cycle">
Metonic cycle

For astronomy and calendar studies, the Metonic cycle or Enneadecaeteris (from Ancient Greek: ἐννεακαιδεκαετηρίς, "nineteen years") is a period of very close to 19 years that is remarkable for being nearly a common multiple of the solar year and the synodic (lunar) month. The Greek astronomer Meton of Athens (fifth century BC) observed that a period of 19 years is almost exactly equal to 235 synodic months and, rounded to full days, counts 6,940 days. The difference between the two periods (of 19 years and 235 synodic months) is only a few hours, depending on the definition of the year.
Considering a year to be 1⁄19 of this 6,940-day cycle gives a year length of 365 + 1⁄4 + 1⁄76 days (the unrounded cycle is much more accurate), which is slightly more than 12 synodic months. To keep a 12-month lunar year in pace with the solar year, an intercalary 13th month would have to be added on seven occasions during the nineteen-year period (235 = 19 × 12 + 7). When Meton introduced the cycle around 432 BC, it was already known by Babylonian astronomers.
A mechanical computation of the cycle is built into the Antikythera mechanism.
The cycle was used in the Babylonian calendar, ancient Chinese calendar systems (the 'Rule Cycle' 章) and the medieval computus (i.e. the calculation of the date of Easter). It regulates the 19-year cycle of intercalary months of the Hebrew calendar.
Mathematical basis.
At the time of Meton, axial precession had not yet been discovered, and he could not distinguish between sidereal years (currently: 365.256363 days) and tropical years (currently: 365.242190 days). Most calendars, like the commonly used Gregorian calendar, are based on the tropical year and maintain the seasons at the same calendar times each year. Nineteen tropical years are about two hours shorter than 235 synodic months. The Metonic cycle's error is, therefore, one full day every 219 years, or 12.4 parts per million.
Note that the 19-year cycle is also close (to somewhat more than half a day) to 255 draconic months, so it is also an eclipse cycle, which lasts only for about 4 or 5 recurrences of eclipses. The Octon is 1⁄5 of a Metonic cycle (47 synodic months, 3.8 years), and it recurs about 20 to 25 cycles.
This cycle seems to be a coincidence. The periods of the Moon's orbit around the Earth and the Earth's orbit around the Sun are believed to be independent, and not to have any known physical resonance except with each other. An example of a non-coincidental cycle is the orbit of Mercury, with its 3:2 spin-orbit resonance.
A lunar year of 12 synodic months is about 354 days, approximately 11 days short of the "365-day" solar year. Therefore, for a lunisolar calendar, every 2 to 3 years there is a difference of more than a full lunar month between the lunar and solar years, and an extra ("embolismic") month needs to be inserted (intercalation). The Athenians initially seem not to have had a regular means of intercalating a 13th month; instead, the question of when to add a month was decided by an official. Meton's discovery made it possible to propose a regular intercalation scheme. The Babylonians seem to have introduced this scheme around 500 BC, thus well before Meton.
Application in traditional calendars.
Traditionally, for the Babylonian and Hebrew lunisolar calendars, the years 3, 6, 8, 11, 14, 17, and 19 are the long (13-month) years of the Metonic cycle. This cycle, which can be used to predict eclipses, forms the basis of the Greek and Hebrew calendars, and is used for the computation of the date of Easter each year.
The Babylonians applied the 19-year cycle since the late sixth century BC. As they measured the moon's motion against the stars, the 235:19 relationship may originally have referred to sidereal years, instead of tropical years as it has been used for various calendars.
Apollo was said to have visited the Hyperboreans once every 19 years, presumably at the high point of the cycle. 
The Runic calendar is a perpetual calendar based on the 19-year-long Metonic cycle. Also known as a Rune staff or Runic Almanac, it appears to have been a medieval Swedish invention. This calendar does not rely on knowledge of the duration of the tropical year or of the occurrence of leap years. It is set at the beginning of each year by observing the first full moon after the winter solstice. The oldest one known, and the only one from the Middle Ages, is the Nyköping staff, which is believed to date from the 13th century.
The Bahá'í calendar, established during the middle of the 19th century, is also based on cycles of 19 years.
Further details.
The Metonic cycle is related to two less accurate subcycles:
By combining appropriate numbers of 11-year and 19-year periods, it is possible to generate ever more accurate cycles. For example, simple arithmetic shows that:
This gives an error of only about half an hour in 687 years (2.5 seconds a year), although this is subject to secular variation in the length of the tropical year and the lunation.
Meton of Athens approximated the cycle to a whole number (6,940) of days, obtained by 125 long months of 30 days and 110 short months of 29 days. During the next century, Callippus developed the Callippic cycle of four 19-year periods for a 76-year cycle with a mean year of exactly 365.25 days.

</doc>
<doc id="20427" url="http://en.wikipedia.org/wiki?curid=20427" title="March 26">
March 26

March 26 is the day of the year in the Gregorian calendar.

</doc>
<doc id="20428" url="http://en.wikipedia.org/wiki?curid=20428" title="Marcello Malpighi">
Marcello Malpighi

Marcello Malpighi (March 10, 1628 – November 29, 1694) was an Italian physician and biologist regarded as the father of microscopical anatomy and histology. Malpighi gave his name to several physiological features related to the biological excretory system, such as the Malpighian corpuscles and Malpighian pyramids of the kidneys and the Malpighian tubule system of insects. The splenic lymphoid nodules are often called the "Malpighian bodies of the spleen" or Malpighian corpuscles. The botanical family Malpighiaceae is also named after him.
Early years.
Malpighi was born on March 10, 1628 at Crevalcore near Bologna, Italy. The son of well-to-do parents, Malpighi was educated in his native city, entering the University of Bologna at the age of 17. In a posthumous work delivered and dedicated to the Royal Society in London in 1697, Malpighi says he completed his grammatical studies in 1645, at which point he began to apply himself to the study of peripatetic philosophy. He completed these studies about 1649, where at the persuasion of his mother Frances Natalis, he began to study physics. When his parents and grandmother became ill, he returned to his family home near Bologna to care for them.
Career.
In 1653, his father, mother, and grandmother being dead, Malpighi left his family villa and returned to the University of Bologna to study anatomy. In 1656, he was made a reader at Bologna, and then a professor of physics at Pisa, where he began to abandon the disputative method of learning and apply himself to a more experimental method of research. Based on this research, he wrote some "Dialogues against the Peripatetics and Galenists" (those who followed the precepts of Galen), which were destroyed when his house burned down. Weary of philosophical disputation, in 1660, Malpighi returned to Bologna and dedicated himself to the study of anatomy. He subsequently discovered a new structure of the lungs which led him to several disputes with the learned medical men of the times. In 1662, he was made a professor of Physic at the Academy of Messina.
Retiring from university life to his villa in the country near Bologna in 1663, he worked as a physician while continuing to conduct experiments on the plants and insects he found on his estate. There he made discoveries of the structure of plants which he published in his "Observations". At the end of 1666, Malpighi was invited to return to the public academy at Messina, which he did in 1667. Although he accepted temporary chairs at the universities of Pisa and Messina, throughout his life he continuously returned to Bologna to practice medicine, a city that repaid him by erecting a monument in his memory after his death. 
In 1668, Malpighi received a letter from Mr. Oldenburg of the Royal Society in London, inviting him to correspond. Malpighi wrote his history of the silkworm in 1668, and sent the manuscript to Mr. Oldenburg. As a result, Malpighi was made a member of the Royal Society in 1669. In 1671, Malpighi’s "Anatomy of Plants" was published in London by the Royal Society, and he simultaneously wrote to Mr. Oldenburg, telling him of his recent discoveries regarding the lungs, fibers of the spleen and testicles, and several other discoveries involving the brain and sensory organs. He also shared more information regarding his research on plants. At that time, he related his disputes with some younger physicians who were strenuous supporters of the Galenic principles and opposed to all new discoveries. Following many other discoveries and publications, in 1691, Malpighi was uprooted from his beloved home in Bologna and summoned to Rome by Pope Innocent XII 
Marcello Malpighi is buried in the church of the Santi Gregorio e Siro, in Bologna, where nowadays can be seen a marble monument to the scientist with an inscription in Latin remembering - among other things - his "SUMMUM INGENIUM / INTEGERRIMAM VITAM / FORTEM STRENUAMQUE MENTEM / AUDACEM SALUTARIS ARTIS AMOREM" (great genius, honest life, strong and tough mind, daring love for the medical art).
Research.
Around the age of 38, and with a remarkable academic career behind him, Malpighi decided to dedicate his free time to anatomical studies. Although he conducted some of his studies using vivisection and others through the dissection of corpses, his most illustrative efforts appear to have been based on the use of the microscope. Because of this work, many microscopic anatomical structures are named after Malpighi, including a skin layer (Malpighi layer) and two different Malpighian corpuscles in the kidneys and the spleen, as well as the Malpighian tubules in the excretory system of insects.
See Timeline of microscope technology for more information.
Although a Dutch spectacle maker created the compound lens and inserted it in a microscope around the turn of the 17th century, and Galileo had applied the principle of the compound lens to the making of his microscope patented in 1609, its possibilities as a microscope had remained unexploited for half a century, until Robert Hooke improved the instrument. Following this, Marcello Malpighi, Hooke, and two other early investigators associated with the Royal Society, Nehemiah Grew and Antoine van Leeuwenhoek were fortunate to have a virtually untried tool in their hands as they began their investigations. 
Working on frogs and extrapolating to humans, Malpighi demonstrated the structure of the lungs, previously thought to be a homogeneous mass of flesh, and he offered an explanation for how air and blood mixed in the lungs. Malpighi also used the microscope for his studies of the skin, kidneys, and liver. For example, after he dissected a black male, Malpighi made some groundbreaking headway into the discovery of the origin of black skin. He found that the black pigment was associated with a layer of mucus just beneath the skin.
He was the first to see capillaries in animals, and he discovered the link between arteries and veins that had eluded William Harvey. He may have been the first person to see red blood cells under a microscope. His treatise "De polypo cordis" (1666) was important for understanding blood composition, as well as how blood clots. In it, Malpighi described how the form of a blood clot differed in the right vs. the left sides of the heart.
The use of the microscope enabled Malpighi to discover that insects (particularly, the silk worm) do not use lungs to breathe, but small holes in their skin called tracheae. Malpighi also studied the anatomy of the brain and concluded this organ is a gland. In terms of modern endocrinology, this deduction is correct because the hypothalamus of the brain has long been recognized for its hormone-secreting capacity. 
Because Malpighi had a wide knowledge of both plants and animals, he made contributions to the scientific study of both. The Royal Society in London published two volumes of his botanical and zoological works in 1675 and 1679. Another edition followed in 1687, and a supplementary volume in 1697. In his autobiography, Malpighi speaks of his "Anatome Plantarum", decorated with the engravings of Robert White (1645–1703) as "the most elegant format in the whole literate world." 
His study of plants led him to conclude that plants had tubules similar to those he saw in insects like the silk worm (using his microscope, he probably saw the stomata, through which plants exchange carbon dioxide with oxygen). Malpighi observed that when a ring-like portion of bark was removed on a trunk a swelling occurred in the tissues above the ring, and he correctly interpreted this as growth stimulated by food coming down from the leaves, and being blocked above the ring. 
A talented sketch artist, Malpighi seems to have been the first author to have made detailed drawings of individual organs of flowers. In his "Anatome plantarum" is a longitudinal section of a flower of "Nigella" (his Melanthi, literally honey-flower) with details of the nectariferous organs. He adds that it is strange that nature has produced on the leaves of the flower shell-like organs in which honey is produced. 
Malpighi had success in tracing the ontogeny of plant organs, and the serial development of the shoot owing to his instinct shaped in the sphere of animal embryology. He specialized in seedling development, and in 1679, he published a volume containing a series of exquisitely drawn and engraved images of the stages of development of Legumeninosae (beans) and Cucurbitae (squash, melons). Later, he published material depicting the development of the date palm. The great Swedish botanist Linnaeus named the genus "Malpighia" in honor of Malpighi’s work with plants; "Malpighia" is the type genus for the Malpighiaceae, a family of tropical and subtropical flowering plants.
Because Malpighi was concerned with teratology (the scientific study of the visible conditions caused by the interruption or alteration of normal development) he expressed grave misgivings about the view of his contemporaries that the galls of trees and herbs gave birth to insects. He conjectured that the creatures in question arose from eggs previously laid in the plant tissue. 
Malpighi’s investigations of the lifecycle of plants and animals led him into the topic of reproduction. He created detailed drawings of his studies of chick embryo development, seed development in plants (such as the lemon tree), and the transformation of caterpillars into insects. His discoveries helped to illuminate philosophical arguments surrounding the topics of "emboîtment", pre-existence, preformation, epigenesis, and metamorphosis.
Years in Rome.
In 1691 Pope Innocent XII invited him to Rome as papal physician. He taught medicine in the Papal Medical School and wrote a long treatise about his studies which he donated to the Royal Society of London.
Marcello Malpighi died of apoplexy (an old-fashioned term for a stroke or stroke-like symptoms) in Rome on September 29, 1694, at the age of 66. In accordance with his wishes, an autopsy was performed. The Royal Society published his studies in 1696.
Memories of Malpighi in Bologna.
Malpighi is buried in the church of the Santi Gregorio e Siro, in Bologna, where nowadays can be seen a marble monument to the scientist with an inscription in Latin remembering - among other things - his "SUMMUM INGENIUM / INTEGERRIMAM VITAM / FORTEM STRENUAMQUE MENTEM / AUDACEM SALUTARIS ARTIS AMOREM" (great genius, honest life, strong and tough mind, daring love for the medical art).

</doc>
<doc id="20431" url="http://en.wikipedia.org/wiki?curid=20431" title="Momentum">
Momentum

In classical mechanics, linear momentum or translational momentum (pl. momenta; SI unit kg m/s, or equivalently, N s) is the product of the mass and velocity of an object. For example, a heavy truck moving rapidly has a large momentum—it takes a large or prolonged force to get the truck up to this speed, and it takes a large or prolonged force to bring it to a stop afterwards. If the truck were lighter, or moving more slowly, then it would have less momentum.
Like velocity, linear momentum is a vector quantity, possessing a direction as well as a magnitude:
Linear momentum is also a "conserved" quantity, meaning that if a closed system is not affected by external forces, its total linear momentum cannot change. In classical mechanics, conservation of linear momentum is implied by Newton's laws; but it also holds in special relativity (with a modified formula) and, with appropriate definitions, a (generalized) linear momentum conservation law holds in electrodynamics, quantum mechanics, quantum field theory, and general relativity.
Newtonian mechanics.
Momentum has a direction as well as magnitude. Quantities that have both a magnitude and a direction are known as vector quantities. Because momentum has a direction, it can be used to predict the resulting direction of objects after they collide, as well as their speeds. Below, the basic properties of momentum are described in one dimension. The vector equations are almost identical to the scalar equations (see multiple dimensions).
Single particle.
The momentum of a particle is traditionally represented by the letter "p". It is the product of two quantities, the mass (represented by the letter "m") and velocity ("v"):
The units of momentum are the product of the units of mass and velocity. In SI units, if the mass is in kilograms and the velocity in meters per second, then the momentum is in kilogram meters/second (kg m/s). Being a vector, momentum has magnitude and direction. For example, a model airplane of 1 kg, traveling due north at 1 m/s in straight and level flight, has a momentum of 1 kg m/s due north measured from the ground.
Many particles.
The momentum of a system of particles is the sum of their momenta. If two particles have masses "m"1 and "m"2, and velocities "v"1 and "v"2, the total momentum is
The momenta of more than two particles can be added in the same way.
A system of particles has a center of mass, a point determined by the weighted sum of their positions:
If all the particles are moving, the center of mass will generally be moving as well (unless the system is in pure rotation around it). If the center of mass is moving at velocity "v"cm, the momentum is:
This is known as Euler's first law.
Relation to force.
If a force "F" is applied to a particle for a time interval Δ"t", the momentum of the particle changes by an amount
In differential form, this gives Newton's second law: the rate of change of the momentum of a particle is equal to the force "F" acting on it:
If the force depends on time, the change in momentum (or impulse) between times "t"1 and "t"2 is
The second law only applies to a particle that does not exchange matter with its surroundings, and so it is equivalent to write
so the force is equal to mass times acceleration.
"Example": A model airplane of 1 kg accelerates from rest to a velocity of 6 m/s due north in 2 s. The net force required to produce this acceleration is 3 newtons due north. The change in momentum is 6 kg m/s. The rate of change of momentum is 3 (kg m/s)/s = 3 N.
Conservation.
In a closed system (one that does not exchange any matter with its surroundings and is not acted on by external forces) the total momentum is constant. This fact, known as the "law of conservation of momentum", is implied by Newton's laws of motion. Suppose, for example, that two particles interact. Because of the third law, the forces between them are equal and opposite. If the particles are numbered 1 and 2, the second law states that and . Therefore
or
If the velocities of the particles are "u"1 and "u"2 before the interaction, and afterwards they are "v"1 and "v"2, then
This law holds no matter how complicated the force is between particles. Similarly, if there are several particles, the momentum exchanged between each pair of particles adds up to zero, so the total change in momentum is zero. This conservation law applies to all interactions, including collisions and separations caused by explosive forces. It can also be generalized to situations where Newton's laws do not hold, for example in the theory of relativity and in electrodynamics.
Dependence on reference frame.
Momentum is a measurable quantity, and the measurement depends on the motion of the observer. For example, if an apple is sitting in a glass elevator that is descending, an outside observer looking into the elevator sees the apple moving, so to that observer the apple has a nonzero momentum. To someone inside the elevator, the apple does not move, so it has zero momentum. The two observers each have a frame of reference in which they observe motions, and if the elevator is descending steadily they will see behavior that is consistent with the same physical laws.
Suppose a particle has position "x" in a stationary frame of reference. From the point of view of another frame of reference moving at a uniform speed "u", the position (represented by a primed coordinate) changes with time as
This is called a Galilean transformation. If the particle is moving at speed in the first frame of reference, in the second it is moving at speed
Since "u" does not change, the accelerations are the same:
Thus, momentum is conserved in both reference frames. Moreover, as long as the force has the same form in both frames, Newton's second law is unchanged. Forces such as Newtonian gravity, which depend only on the scalar distance between objects, satisfy this criterion. This independence of reference frame is called Newtonian relativity or Galilean invariance.
A change of reference frame can often simplify calculations of motion. For example, in a collision of two particles a reference frame can be chosen where one particle begins at rest. Another commonly used reference frame is the center of mass frame, one that is moving with the center of mass. In this frame, the total momentum is zero.
Application to collisions.
By itself, the law of conservation of momentum is not enough to determine the motion of particles after a collision. Another property of the motion, kinetic energy, must be known. This is not necessarily conserved. If it is conserved, the collision is called an "elastic collision"; if not, it is an "inelastic collision".
Elastic collisions.
An elastic collision is one in which no kinetic energy is lost. Perfectly elastic "collisions" can occur when the objects do not touch each other, as for example in atomic or nuclear scattering where electric repulsion keeps them apart. A slingshot maneuver of a satellite around a planet can also be viewed as a perfectly elastic collision from a distance. A collision between two pool balls is a good example of an "almost" totally elastic collision, due to their high rigidity; but when bodies come in contact there is always some dissipation.
A head-on elastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are "u"1 and "u"2 before the collision and "v"1 and "v"2 after, the equations expressing conservation of momentum and kinetic energy are:
A change of reference frame can often simplify the analysis of a collision. For example, suppose there are two bodies of equal mass "m", one stationary and one approaching the other at a speed "v" (as in the figure). The center of mass is moving at speed "v"/2 and both bodies are moving towards it at speed "v"/2. Because of the symmetry, after the collision both must be moving away from the center of mass at the same speed. Adding the speed of the center of mass to both, we find that the body that was moving is now stopped and the other is moving away at speed "v". The bodies have exchanged their velocities. Regardless of the velocities of the bodies, a switch to the center of mass frame leads us to the same conclusion. Therefore, the final velocities are given by
In general, when the initial velocities are known, the final velocities are given by
If one body has much greater mass than the other, its velocity will be little affected by a collision while the other body will experience a large change.
Inelastic collisions.
In an inelastic collision, some of the kinetic energy of the colliding bodies is converted into other forms of energy such as heat or sound. Examples include traffic collisions, in which the effect of lost kinetic energy can be seen in the damage to the vehicles; electrons losing some of their energy to atoms (as in the Franck–Hertz experiment); and particle accelerators in which the kinetic energy is converted into mass in the form of new particles.
In a perfectly inelastic collision (such as a bug hitting a windshield), both bodies have the same motion afterwards. If one body is motionless to begin with, the equation for conservation of momentum is
so
In a frame of reference moving at the speed "v"), the objects are brought to rest by the collision and 100% of the kinetic energy is converted.
One measure of the inelasticity of the collision is the coefficient of restitution "C"R, defined as the ratio of relative velocity of separation to relative velocity of approach. In applying this measure to ball sports, this can be easily measured using the following formula:
The momentum and energy equations also apply to the motions of objects that begin together and then move apart. For example, an explosion is the result of a chain reaction that transforms potential energy stored in chemical, mechanical, or nuclear form into kinetic energy, acoustic energy, and electromagnetic radiation. Rockets also make use of conservation of momentum: propellant is thrust outward, gaining momentum, and an equal and opposite momentum is imparted to the rocket.
Multiple dimensions.
Real motion has both direction and magnitude and must be represented by a vector. In a coordinate system with "x, y, z" axes, velocity has components "v"x in the "x" direction, "v"y in the "y" direction, "v"z in the "z" direction. The vector is represented by a boldface symbol:
Similarly, the momentum is a vector quantity and is represented by a boldface symbol:
The equations in the previous sections work in vector form if the scalars "p" and "v" are replaced by vectors p and v. Each vector equation represents three scalar equations. For example,
represents three equations:
The kinetic energy equations are exceptions to the above replacement rule. The equations are still one-dimensional, but each scalar represents the magnitude of the vector, for example,
Each vector equation represents three scalar equations. Often coordinates can be chosen so that only two components are needed, as in the figure. Each component can be obtained separately and the results combined to produce a vector result.
A simple construction involving the center of mass frame can be used to show that if a stationary elastic sphere is struck by a moving sphere, the two will head off at right angles after the collision (as in the figure).
Objects of variable mass.
The concept of momentum plays a fundamental role in explaining the behavior of variable-mass objects such as a rocket ejecting fuel or a star accreting gas. In analyzing such an object, one treats the object's mass as a function that varies with time: "m"("t"). The momentum of the object at time "t" is therefore . One might then try to invoke Newton's second law of motion by saying that the external force "F" on the object is related to its momentum "p"("t") by , but this is incorrect, as is the related expression found by applying the product rule to "d"("mv")/"dt":
This equation does not correctly describe the motion of variable-mass objects. The correct equation is
where "u" is the velocity of the ejected/accreted mass "as seen in the object's rest frame". This is distinct from "v", which is the velocity of the object itself as seen in an inertial frame.
This equation is derived by keeping track of both the momentum of the object as well as the momentum of the ejected/accreted mass. When considered together, the object and the mass constitute a closed system in which total momentum is conserved.
Relativistic mechanics.
Lorentz invariance.
Newtonian physics assumes that absolute time and space exist outside of any observer; this gives rise to the Galilean invariance described earlier. It also results in a prediction that the speed of light can vary from one reference frame to another. This is contrary to observation. In the special theory of relativity, Einstein keeps the postulate that the equations of motion do not depend on the reference frame, but assumes that the speed of light "c" is invariant. As a result, position and time in two reference frames are related by the Lorentz transformation instead of the Galilean transformation.
Consider, for example, a reference frame moving relative to another at velocity "v" in the "x" direction. The Galilean transformation gives the coordinates of the moving frame as
while the Lorentz transformation gives
where "γ" is the Lorentz factor:
Newton's second law, with mass fixed, is not invariant under a Lorentz transformation. However, it can be made invariant by making the "inertial mass" "m" of an object a function of velocity:
"m"0 is the object's invariant mass.
The modified momentum,
obeys Newton's second law:
Within the domain of classical mechanics, relativistic momentum closely approximates Newtonian momentum: at low velocity, "γm"0v is approximately equal to "m"0v, the Newtonian expression for momentum.
Four-vector formulation.
In the theory of relativity, physical quantities are expressed in terms of four-vectors that include time as a fourth coordinate along with the three space coordinates. These vectors are generally represented by capital letters, for example R for position. The expression for the "four-momentum" depends on how the coordinates are expressed. Time may be given in its normal units or multiplied by the speed of light so that all the components of the four-vector have dimensions of length. If the latter scaling is used, an interval of proper time, "τ", defined by
is invariant under Lorentz transformations (in this expression and in what follows the (+ − − −) metric signature has been used, different authors use different conventions). Mathematically this invariance can be ensured in one of two ways: by treating the four-vectors as Euclidean vectors and multiplying time by the square root of -1; or by keeping time a real quantity and embedding the vectors in a Minkowski space. In a Minkowski space, the scalar product of two four-vectors and is defined as
In all the coordinate systems, the (contravariant) relativistic four-velocity is defined by
and the (contravariant) four-momentum is
where "m"0 is the invariant mass. If (in Minkowski space), then
Using Einstein's mass-energy equivalence, , this can be rewritten as
Thus, conservation of four-momentum is Lorentz-invariant and implies conservation of both mass and energy.
The magnitude of the momentum four-vector is equal to "m"0"c":
and is invariant across all reference frames.
The relativistic energy–momentum relationship holds even for massless particles such as photons; by setting it follows that
In a game of relativistic "billiards", if a stationary particle is hit by a moving particle in an elastic collision, the paths formed by the two afterwards will form an acute angle. This is unlike the non-relativistic case where they travel at right angles.
Quantum mechanics.
In quantum mechanics, momentum is defined as an operator on the wave function. The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables.
For a single particle described in the position basis the momentum operator can be written as
where ∇ is the gradient operator, "ħ" is the reduced Planck constant, and "i" is the imaginary unit. This is a commonly encountered form of the momentum operator, though the momentum operator in other bases can take other forms. For example, in momentum space the momentum operator is represented as
where the operator p acting on a wave function "ψ"("p") yields that wave function multiplied by the value "p", in an analogous fashion to the way that the position operator acting on a wave function "ψ"("x") yields that wave function multiplied by the value "x".
For both massive and massless objects, relativistic momentum is related to the de Broglie wavelength "λ" by
Electromagnetic radiation (including visible light, ultraviolet light, and radio waves) is carried by photons. Even though photons (the particle aspect of light) have no mass, they still carry momentum. This leads to applications such as the solar sail. The calculation of the momentum of light within dielectric media is somewhat controversial (see Abraham–Minkowski controversy).
Generalized coordinates.
Newton's laws can be difficult to apply to many kinds of motion because the motion is limited by "constraints". For example, a bead on an abacus is constrained to move along its wire and a pendulum bob is constrained to swing at a fixed distance from the pivot. Many such constraints can be incorporated by changing the normal Cartesian coordinates to a set of "generalized coordinates" that may be fewer in number. Refined mathematical methods have been developed for solving mechanics problems in generalized coordinates. They introduce a "generalized momentum", also known as the "canonical" or "conjugate momentum", that extends the concepts of both linear momentum and angular momentum. To distinguish it from generalized momentum, the product of mass and velocity is also referred to as "mechanical", "kinetic" or "kinematic momentum". The two main methods are described below.
Lagrangian mechanics.
In Lagrangian mechanics, a Lagrangian is defined as the difference between the kinetic energy "T" and the potential energy "V":
If the generalized coordinates are represented as a vector and time differentiation is represented by a dot over the variable, then the equations of motion (known as the Lagrange or Euler–Lagrange equations) are a set of "N" equations:
If a coordinate "q""i" is not a Cartesian coordinate, the associated generalized momentum component "p""i" does not necessarily have the dimensions of linear momentum. Even if "q"i is a Cartesian coordinate, "p""i" will not be the same as the mechanical momentum if the potential depends on velocity. Some sources represent the kinematic momentum by the symbol Π.
In this mathematical framework, a generalized momentum is associated with the generalized coordinates. Its components are defined as
Each component "p""j" is said to be the "conjugate momentum" for the coordinate "q""j".
Now if a given coordinate "q""i" does not appear in the Lagrangian (although its time derivative might appear), then
This is the generalization of the conservation of momentum.
Even if the generalized coordinates are just the ordinary spatial coordinates, the conjugate momenta are not necessarily the ordinary momentum coordinates. An example is found in the section on electromagnetism.
Hamiltonian mechanics.
In Hamiltonian mechanics, the Lagrangian (a function of generalized coordinates and their derivatives) is replaced by a Hamiltonian that is a function of generalized coordinates and momentum. The Hamiltonian is defined as
where the momentum is obtained by differentiating the Lagrangian as above. The Hamiltonian equations of motion are
As in Lagrangian mechanics, if a generalized coordinate does not appear in the Hamiltonian, its conjugate momentum component is conserved.
Symmetry and conservation.
Conservation of momentum is a mathematical consequence of the homogeneity (shift symmetry) of space (position in space is the canonical conjugate quantity to momentum). That is, conservation of momentum is a consequence of the fact that the laws of physics do not depend on position; this is a special case of Noether's theorem.
Electromagnetism.
In Newtonian mechanics, the law of conservation of momentum can be derived from the law of action and reaction, which states that every force has a reciprocating equal and opposite force. Under some circumstances one moving charged particle can exert a force on another without any return force. Moreover, Maxwell's equations, the foundation of classical electrodynamics, are Lorentz-invariant. Nevertheless, the combined momentum of the particles and the electromagnetic field is conserved.
Vacuum.
In Maxwell's equations, the forces between particles are mediated by electric and magnetic fields. The electromagnetic force ("Lorentz force") on a particle with charge "q" due to a combination of electric field E and magnetic field (as given by the "B-field" B) is
This force imparts a momentum to the particle, so by Newton's second law the particle must impart a momentum to the electromagnetic fields.
In a vacuum, the momentum per unit volume is
where μ0 is the vacuum permeability and "c" is the speed of light. The momentum density is proportional to the Poynting vector S which gives the directional rate of energy transfer per unit area:
If momentum is to be conserved in a volume "V", changes in the momentum of matter through the Lorentz force must be balanced by changes in the momentum of the electromagnetic field and outflow of momentum. If Pmech is the momentum of all the particles in a volume "V", and the particles are treated as a continuum, then Newton's second law gives
The electromagnetic momentum is
and the equation for conservation of each component "i" of the momentum is
The term on the right is an integral over the surface "S" representing momentum flow into and out of the volume, and "n""j" is a component of the surface normal of "S". The quantity "T""ij" is called the Maxwell stress tensor, defined as
Media.
The above results are for the "microscopic" Maxwell equations, applicable to electromagnetic forces in a vacuum (or on a very small scale in media). It is more difficult to define momentum density in media because the division into electromagnetic and mechanical is arbitrary. The definition of electromagnetic momentum density is modified to
where the H-field H is related to the B-field and the magnetization M by
The electromagnetic stress tensor depends on the properties of the media.
Particle in field.
If a charged particle "q" moves in an electromagnetic field, its kinetic momentum "m"v is not conserved. However, it has a canonical momentum that is conserved.
Lagrangian and Hamiltonian formulation.
The "kinetic momentum" p is different from the "canonical momentum" P (synonymous with the generalized momentum) conjugate to the ordinary position coordinates r, because P includes a contribution from the electric potential "φ"(r, "t") and vector potential A(r, "t"):
where ṙ = v is the velocity (see time derivative), "q" is the electric charge of the particle and "γ" = (1 − ṙ·ṙ/"c"2)−1/2 is the Lorentz factor. See also Electromagnetism (momentum). If neither "φ" nor A depends on position, P is conserved.
The classical Hamiltonian formula_62 for a particle in any field equals the total energy of the system – the kinetic energy "T" = p2/2"m" (where p2 = p · p, see dot product) plus the potential energy "V". For a particle in an electromagnetic field, the potential energy is "V" = "eφ", and since the kinetic energy "T" always corresponds to the kinetic momentum p, replacing the kinetic momentum by the above equation (p = P − "q"A) leads to the Hamiltonian in the table.
These Lagrangian and Hamiltonian expressions can derive the Lorentz force.
Canonical commutation relations.
The kinetic momentum (p above) satisfies the commutation relation:
where: "j, k, ℓ" are indices labelling vector components, "Bℓ" is a component of the magnetic field, and "εkjℓ" is the Levi-Civita symbol, here in 3-dimensions.
Deformable bodies and fluids.
Conservation in a continuum.
In fields such as fluid dynamics and solid mechanics, it is not feasible to follow the motion of individual atoms or molecules. Instead, the materials must be approximated by a continuum in which there is a particle or fluid parcel at each point that is assigned the average of the properties of atoms in a small region nearby. In particular, it has a density "ρ" and velocity v that depend on time "t" and position r. The momentum per unit volume is "ρ"v.
Consider a column of water in hydrostatic equilibrium. All the forces on the water are in balance and the water is motionless. On any given drop of water, two forces are balanced. The first is gravity, which acts directly on each atom and molecule inside. The gravitational force per unit volume is "ρ"g, where g is the gravitational acceleration. The second force is the sum of all the forces exerted on its surface by the surrounding water. The force from below is greater than the force from above by just the amount needed to balance gravity. The normal force per unit area is the pressure "p". The average force per unit volume inside the droplet is the gradient of the pressure, so the force balance equation is
If the forces are not balanced, the droplet accelerates. This acceleration is not simply the partial derivative "∂"v/"∂t" because the fluid in a given volume changes with time. Instead, the material derivative is needed:
Applied to any physical quantity, the material derivative includes the rate of change at a point and the changes due to advection as fluid is carried past the point. Per unit volume, the rate of change in momentum is equal to "ρD"v/"Dt". This is equal to the net force on the droplet.
Forces that can change the momentum of a droplet include the gradient of the pressure and gravity, as above. In addition, surface forces can deform the droplet. In the simplest case, a shear stress "τ", exerted by a force parallel to the surface of the droplet, is proportional to the rate of deformation or strain rate. Such a shear stress occurs if the fluid has a velocity gradient because the fluid is moving faster on one side than another. If the speed in the "x" direction varies with "z", the tangential force in direction "x" per unit area normal to the "z" direction is
where "μ" is the viscosity. This is also a flux, or flow per unit area, of x-momentum through the surface.
Including the effect of viscosity, the momentum balance equations for the incompressible flow of a Newtonian fluid are
These are known as the Navier–Stokes equations.
The momentum balance equations can be extended to more general materials, including solids. For each surface with normal in direction "i" and force in direction "j", there is a stress component "σ""ij". The nine components make up the Cauchy stress tensor σ, which includes both pressure and shear. The local conservation of momentum is expressed by the Cauchy momentum equation:
where f is the body force.
The Cauchy momentum equation is broadly applicable to deformations of solids and liquids. The relationship between the stresses and the strain rate depends on the properties of the material (see Types of viscosity).
Acoustic waves.
A disturbance in a medium gives rise to oscillations, or waves, that propagate away from their source. In a fluid, small changes in pressure "p" can often be described by the acoustic wave equation:
where "c" is the speed of sound. In a solid, similar equations can be obtained for propagation of pressure (P-waves) and shear (S-waves).
The flux, or transport per unit area, of a momentum component "ρvj" by a velocity "vi" is equal to "ρ vjvj". In the linear approximation that leads to the above acoustic equation, the time average of this flux is zero. However, nonlinear effects can give rise to a nonzero average. It is possible for momentum flux to occur even though the wave itself does not have a mean momentum.
History of the concept.
In about 530 A.D., working in Alexandria, Byzantine philosopher John Philoponus developed a concept of momentum in his commentary to Aristotle's "Physics".
Aristotle claimed that everything that is moving must be kept moving by something. For example, a thrown ball must be kept moving by motions of the air. Most writers continued to accept Aristotle's theory until the time of Galileo, but a few were skeptical. Philoponus pointed out the absurdity in Aristotle's claim that motion of an object is promoted by the same air that is resisting its passage. He proposed instead that an impetus was imparted to the object in the act of throwing it. Ibn Sīnā (also known by his Latinized name Avicenna) read Philoponus and published his own theory of motion in "The Book of Healing" in 1020. He agreed that an impetus is imparted to a projectile by the thrower; but unlike Philoponus, who believed that it was a temporary virtue that would decline even in a vacuum, he viewed it as a persistent, requiring external forces such as air resistance to dissipate it. 
The work of Philoponus, and possibly that of Ibn Sīnā, was read and refined by the European philosophers Peter Olivi and Jean Buridan. Buridan, who in about 1350 was made rector of the University of Paris, referred to impetus being proportional to the weight times the speed. Moreover, Buridan's theory was different from his predecessor's in that he did not consider impetus to be self-dissipating, asserting that a body would be arrested by the forces of air resistance and gravity which might be opposing its impetus.
René Descartes believed that the total "quantity of motion" in the universe is conserved, where the quantity of motion is understood as the product of size and speed. This should not be read as a statement of the modern law of momentum, since he had no concept of mass as distinct from weight and size, and more importantly he believed that it is speed rather than velocity that is conserved. So for Descartes if a moving object were to bounce off a surface, changing its direction but not its speed, there would be no change in its quantity of motion. Galileo, later, in his "Two New Sciences", used the Italian word "impeto".
Leibniz, in his "Discourse on Metaphysics", gave an argument against Descartes' construction of the conservation of the "quantity of motion" using an example of dropping blocks of different sizes different distances. He points out that force is conserved but quantity of motion, construed as the product of size and speed of an object, is not conserved.
The first correct statement of the law of conservation of momentum was by English mathematician John Wallis in his 1670 work, "Mechanica sive De Motu, Tractatus Geometricus": "the initial state of the body, either of rest or of motion, will persist" and "If the force is greater than the resistance, motion will result". Wallis uses "momentum" and "vis" for force. Newton's "Philosophiæ Naturalis Principia Mathematica", when it was first published in 1687, showed a similar casting around for words to use for the mathematical momentum. His Definition II defines "quantitas motus", "quantity of motion", as "arising from the velocity and quantity of matter conjointly", which identifies it as momentum. Thus when in Law II he refers to "mutatio motus", "change of motion", being proportional to the force impressed, he is generally taken to mean momentum and not motion. It remained only to assign a standard term to the quantity of motion. The first use of "momentum" in its proper mathematical sense is not clear but by the time of Jenning's "Miscellanea" in 1721, four years before the final edition of Newton's "Principia Mathematica", momentum M or "quantity of motion" was being defined for students as "a rectangle", the product of Q and V, where Q is "quantity of material" and V is "velocity", s/t.
Further reading.
</dl>

</doc>
<doc id="20432" url="http://en.wikipedia.org/wiki?curid=20432" title="Mood stabilizer">
Mood stabilizer

A mood stabilizer is a psychiatric medication used to treat mood disorders characterized by intense and sustained mood shifts, typically bipolar disorder.
Uses.
Used to treat bipolar disorder, mood stabilizers suppress swings between mania and depression. Mood-stabilizing drugs are also used in borderline personality disorder and schizoaffective disorder.
Examples.
The term "mood stabilizer" does not describe a mechanism, but rather an effect. More precise terminology is used to classify these agents.
Drugs commonly classed as mood stabilizers include:
Anticonvulsants.
Many agents described as "mood stabilizers" are also categorized as anticonvulsants. The term "anticonvulsant mood stabilizers" is sometimes used to describe these as a class. Although this group is also defined by effect rather than mechanism, there is at least a preliminary understanding of the mechanism of most of the anticonvulsants used in the treatment of mood disorders.
There is insufficient evidence to support the use of various other anticonvulsants, such as gabapentin and topiramate, as mood stabilizers.
Combination therapy.
In routine practice, monotherapy is often not sufficiently effective for acute and/or maintenance therapy and thus most patients are given combination therapies. Combination therapy (atypical antipsychotic with lithium or valproate) shows better efficacy over monotherapy in the manic phase in terms of efficacy and prevention of relapse. However, side effects are more frequent and discontinuation rates due to adverse events are higher with combination therapy than with monotherapy.
Relationship to antidepressants.
Most mood stabilizers are primarily antimanic agents, meaning that they are effective at treating mania and mood cycling and shifting, but are not effective at treating acute depression. The principal exceptions to that rule, because they treat both manic and depressive symptoms, are lamotrigine, lithium carbonate and quetiapine.
Nevertheless, antidepressants are still often prescribed in addition to mood stabilizers during depressive phases. This brings some risks, however, as antidepressants can induce mania, psychosis, and other disturbing problems in people with bipolar disorder—in particular, when taken alone. The risk of antidepressant-induced mania when given to patients concomitantly on antimanic agents is not known for certain but may still exist. The majority of antidepressants appear ineffective in treating bipolar depression.
Antidepressants cause several risks when given to bipolar patients. They are ineffective in treating acute bipolar depression, preventing relapse, and can cause rapid cycling. Studies have been shown that antidepressants have no benefit versus a placebo or other treatment. Antidepressants can also lead to a higher rate of non-lethal suicidal behavior. Relapse can also be related to treatment with antidepressants. This is less likely to occur if a mood stabilizer is combined with an antidepressant, rather than an antidepressant being used alone. Evidence from previous studies shows that rapid cycling is linked to use of antidepressants. Rapid cycling is defined as the presence of four or more mood episodes within a year's time. Evidence suggests that rapid cycling and mixed symptoms have become more common since antidepressant medication has come into widespread use. There is a need for caution when treating bipolar patients with antidepressant medication due to the risks that they pose. 
Use of mood stabilizers and anticonvulsants such as lamotrigine, carbamazapine, valproate and others may lead to chronic folate deficiency, potentiating depression. Also, "Folate deficiency may increase the risk of depression and reduce the action of antidepressants." L-methylfolate (also formally known as 5-MTHF or Levofolinic acid), a centrally acting trimonoamine modulator, boosts the synthesis of three CNS neurotransmitters: dopamine, norepinephrine and serotonin. Mood stabilizers and anticonvulsants may interfere with folic acid absorption and L-methylfolate formation. Augmentation with the medical food L-methylfolate may improve antidepressant effects of these medicines, including lithium and antidepressants themselves, by boosting the synthesis of antidepressant neurotransmitters. However, the U.S. National Institutes of Health issued a warning caution about the use of L-methylfolate for patients with bipolar disease. 
Mechanism.
The precise mechanism of action of lithium is still unknown, and it is suspected that it acts at various points of the neuron between the nucleus and the synapse. Lithium is known to inhibit the enzyme GSK-3B. This has the effect relieving pressure on the circadian clock - which is thought to be often malfunctioning in people with bipolar disorder - and positively modulates gene transcription of brain-derived neurotrophic factor (BDNF). The resulting increase in neural plasticity may be central to lithium's therapeutic effects. Lithium may also increase the synthesis of serotonin.
All of the anticonvulsants routinely used to treat bipolar disorder are blockers of voltage-gated sodium channels, affecting the brain's glutamate system. For valproic acid, carbamazepine and oxcarbazepine, however, their mood-stabilizing effects may be more related to effects on the GABAergic system. Lamotrigine is known to decrease the patient's cortisol response to stress. 
One possible downstream target of several mood stabilizers such as lithium, valproate, and carbamazepine is the arachidonic acid cascade.

</doc>
<doc id="20433" url="http://en.wikipedia.org/wiki?curid=20433" title="Mere Christianity">
Mere Christianity

Mere Christianity is a theological book by C. S. Lewis, adapted from a series of BBC radio talks made between 1942 and 1944, while Lewis was at Oxford during World War II. Considered a classic of Christian apologetics, the transcripts of the broadcasts originally appeared in print as three separate pamphlets: "The Case for Christianity" (1942), "Christian Behaviour" (1943), and "Beyond Personality" (1944). Lewis was invited to give the talks by Rev. James Welch, the BBC Director of Religious Broadcasting, who had read his 1940 book, "The Problem of Pain".
Thesis.
Lewis, an Anglican, intended to describe the Christian common ground. In "Mere Christianity", he aims at avoiding controversies to explain fundamental teachings of Christianity, for the sake of those basically educated as well as the intellectuals of his generation, for whom the jargon of formal Christian theology did not retain its original meaning.
"The Case for Christianity".
Lewis spends most of his defense of the Christian faith on an argument from morality, a point which persuaded him from atheism to Christianity. He bases his case on a moral law, a "rule about right and wrong" commonly known to all human beings, citing the example of Nazism; both Christians and atheists believed that Hitler's actions were morally wrong. On a more mundane level, it is generally accepted that stealing is violating this moral law. Lewis argues that the moral law is like the law of nature in that it was not contrived by humans. However, it is unlike natural laws in that it can be broken or ignored, and it is known intuitively, rather than through observation. After introducing the moral law, Lewis argues that thirst reflects the fact that people naturally need water, and there is no other substance which satisfies that need. Lewis points out that earthly experience does not satisfy the human craving for "joy" and that only God could fit the bill; humans cannot know to yearn for something if it does not exist.
After providing reasons for his conversion to theism, Lewis goes over rival conceptions of God to Christianity. Pantheism, he argues, is incoherent, and atheism too simple. Eventually he arrives at Jesus Christ, and invokes a well-known argument now known as the "Lewis trilemma". Lewis, arguing that Jesus was claiming to be God, uses logic to advance three possibilities: either he really was God, was deliberately lying, or was not God but thought himself to be (which would make him delusional and likely insane). The book goes on to say that the latter two possibilities are not consistent with Jesus' character and it was most likely that he was being truthful.
Lewis claims that to understand Christianity, one must understand the moral law, which is the underlying structure of the universe and is "hard as nails." Unless one grasps the dismay which comes from humanity's failure to keep the moral law, one cannot understand the coming of Christ and his work. The eternal God who is the law's source takes primacy over the created Satan whose rebellion undergirds all evil. The death and resurrection of Christ is introduced as the only way in which our inadequate human attempts to redeem humanity's sins could be made adequate in God's eyes.
God "became a man" in Christ, Lewis says, so that mankind could be "amalgamated with God's nature" and make full atonement possible. Lewis offers several analogies to explain this abstract concept: that of Jesus "paying the penalty" for a crime, "paying a debt," or helping humanity out of a hole. His main point, however, is that redemption is so incomprehensible that it cannot be fully appreciated, and he attempts to explain that the method by which God atones for the sins of humanity is not nearly as important as the fact that he does so.
"Christian Behaviour".
The next third of the book explores the ethics resulting from Christian belief. He cites the four cardinal virtues: prudence, justice, temperance, and fortitude. After touching on these, he goes into the three theological virtues: hope, faith, and charity. Lewis also explains morality as being composed of three "layers": relationships between man and man, the motivations and attitudes of the man himself, and contrasting worldviews.
Lewis also covers such topics as social relations and forgiveness, sexual ethics and the tenets of Christian marriage, and the relationship between morality and psychoanalysis. He also writes about "the great sin": pride, which he argues to be the root cause of all evil and rebellion.
His most important point is that Christianity mandates that one "love your neighbor as yourself." He points out that all persons unconditionally love themselves. Even if one does not "like" oneself, one would still love oneself. Christians, he writes, must also apply this attitude to others, even if they do not like them. Lewis calls this one of the "great secrets": when one acts as if he loves others, he will presently come to love them.
Cultural impact.
In 2006, "Mere Christianity" was placed third in "Christianity Today"'s list of the most influential books amongst evangelicals since 1945. The title has influenced "Touchstone Magazine: A Journal of Mere Christianity" and William Dembski's book "Mere Creation". Charles Colson's conversion to Christianity resulted from his reading this book, as did the conversions of Francis Collins, Jonathan Aitken, Josh Caterer and the philosopher C. E. M. Joad.
A passage in the book also influenced the name of contemporary Christian Texan Grammy-nominated pop/rock group Sixpence None the Richer. The phrase, "the hammering process" was used by Christian metal band Living Sacrifice for the name of their album "The Hammering Process". Metalcore band, Norma Jean, derived the title of their song "No Passenger: No Parasite" from the section in the book in which Lewis describes a fully Christian society as having "No passengers or parasites".

</doc>
<doc id="20434" url="http://en.wikipedia.org/wiki?curid=20434" title="Mathematical game">
Mathematical game

A mathematical game is a multiplayer game whose rules, strategies, and outcomes are defined by clear mathematical parameters. Often, such games have simple rules and match procedures, such as Tic-tac-toe and Dots and Boxes. Generally, mathematical games need not be conceptually intricate to involve deeper computational underpinnings. For example, even though the rules of Mancala are relatively basic, the game can be rigorously analyzed through the lens of combinatorial game theory.
Mathematical games differ sharply from mathematical puzzles in that mathematical puzzles require specific mathematical expertise to complete, whereas mathematical games do not require a deep knowledge of mathematics to play. Often, the arithmetic core of mathematical games is not readily apparent to players untrained to note the statistical or mathematical aspects.
Some mathematical games are of deep interest in the field of recreational mathematics.
When studying a game's core mathematics, arithmetic theory is generally of higher utility than actively playing or observing the game itself. To analyze a game numerically, it is particularly useful to study the rules of the game insofar as they can yield equations or relevant formulas. This is frequently done to determine winning strategies or to distinguish if the game has a solution.
Specific mathematical games and puzzles.
Abstract Strategy Games (No chance involved).
Sometimes it is not immediately obvious that a particular game involves chance. Often a card game is described as "pure strategy" and such, but a game with any sort of random shuffling or face-down dealing of cards should not be considered to be "no chance". Several abstract strategy games are listed below:

</doc>
<doc id="20435" url="http://en.wikipedia.org/wiki?curid=20435" title="Martin Gardner">
Martin Gardner

Martin Gardner (October 21, 1914 – May 22, 2010) was an American popular mathematics and popular science writer, with interests also encompassing micromagic, scientific skepticism, philosophy, religion, and literature—especially the writings of Lewis Carroll and G.K. Chesterton.
Gardner was best known for creating and sustaining general interest in recreational mathematics for a large part of the 20th century, principally through his "Scientific American" "Mathematical Games" columns from 1956 to 1981 and his subsequent books collecting them. He was an uncompromising critic of fringe science and was a founding member of CSICOP, an organization devoted to debunking pseudoscience, and wrote a monthly column ("Notes of a Fringe Watcher") from 1983 to 2002 in "Skeptical Inquirer", that organization's monthly magazine. He also wrote a "Puzzle Tale" column for "Asimov's Science Fiction" magazine from 1977 to 1986 and altogether published more than 100 books.
Biography.
Youth and education.
Gardner, son of a petroleum geologist, grew up in and around Tulsa, Oklahoma. He showed an early interest in puzzles and games, and his closest childhood friend, John Bennett Shaw, later became "the greatest of all collectors of Sherlockian memorabilia". He attended the University of Chicago, where he earned his bachelor's degree in philosophy in 1936. Early jobs included reporter on the "Tulsa Tribune", writer at the University of Chicago Office of Press Relations, and case worker in Chicago's Black Belt for the city's Relief Administration. During World War II, he served for four years in the U.S. Navy as a yeoman on board the destroyer escort USS "Pope" in the Atlantic. His ship was still in the Atlantic when the war came to an end with the surrender of Japan in August 1945.
After the war, Gardner returned to the University of Chicago. He attended graduate school for a year there, but he did not earn an advanced degree. In 1950 he published an article in the "Antioch Review" entitled "The Hermit Scientist", a pioneering work on what would later come to be called pseudoscientists. It was Gardner's first publication of a skeptical nature, and two years later it was published in a much-expanded book version: "In the Name of Science", his first book.
Early career.
In the late 1940s, Gardner moved to New York City and became a writer and designer at "Humpty Dumpty" magazine where for eight years he wrote features and stories for it and several other children's magazines. His paper-folding puzzles at that magazine (sister publication to "Children's Digest" at the time, and now sister publication to "Jack and Jill" magazine) led to his first work at "Scientific American." For many decades, Gardner, his wife Charlotte, and their two sons lived in Hastings-on-Hudson, New York, where he earned his living as an independent author, publishing books with several different publishers, and also publishing hundreds of magazine and newspaper articles. Appropriately enough – given his interest in logic and mathematics – they lived on Euclid Avenue. The year 1960 saw the original edition of his best-selling book ever, "The Annotated Alice", various editions of which have sold over a million copies worldwide in several languages.
Gatherings for Gardner.
Gardner was famously shy and declined many honors when he learned that a public appearance would be required if he accepted. (He once told Colm Mulcahy that he "never gave a lecture in his life and that he wouldn't know how to.") However, in 1993 Atlanta puzzle collector Tom Rodgers persuaded Gardner to attend an evening devoted to Gardner's puzzle-solving efforts, called "Gathering for Gardner". The event was repeated in 1996, again with Gardner in attendance, which convinced Rodgers and his friends to make the gathering a regular event. It has been held since then in even-numbered years near Atlanta, and the program consists of any topic which could have been touched by Gardner during his writing career. The event's name is abbreviated to "G4G"n"", with "n" being replaced by the number of the event (the 2010 event thus was "G4G9"). Gardner attended the 1993 and 1996 events.
Retirement and death.
In 1979, Gardner and his wife Charlotte semi-retired and moved to Hendersonville, North Carolina. Gardner never really retired as an author, but rather he continued to do literature research and to write, especially in updating many of his older books, such as "Origami, Eleusis, and the Soma Cube", ISBN 978-0-521-73524-7, published 2008. Charlotte died in 2000 and two years later Gardner returned to Norman, Oklahoma, where his son, James Gardner, was a professor of education at the University of Oklahoma. He died there on May 22, 2010. An autobiography — "Undiluted Hocus-Pocus: The Autobiography of Martin Gardner" — was published posthumously.
Views and interests.
I just play all the time and am fortunate enough to get paid for it.
– Martin Gardner, 1998
Recreational mathematics and "Mathematical Games".
For over a quarter century Gardner wrote a monthly column on the subject of "recreational mathematics" for "Scientific American". It all began with his free-standing article on hexaflexagons which ran in the December 1956 issue. Flexagons became a bit of a fad and soon people all over New York City were making them. Gerry Piel, the "SA" publisher at the time asked Gardner, “Is there enough similar material to this to make a regular feature?” Gardner said he thought so. The January 1957 issue contained his first column, entitled "Mathematical Games". Almost 300 more columns were to follow.
The "Mathematical Games" column ran from 1956 to 1981 and was the first introduction of many subjects to a wider audience, notably:
Ironically, Gardner had problems learning calculus and never took a mathematics course after high school. While editing "Humpty Dumpty's Magazine" he constructed many paper folding puzzles, and this led to his interest in the flexagons invented by British mathematician Arthur H Stone. The subsequent article he wrote on hexaflexagons led directly to the column.
In the 1980s the "Mathematical Games" column began to appear only irregularly. Other authors began to share the column and the June 1986 issue saw the final installment under that title. In 1981, on Gardner's retirement from "Scientific American", the column was replaced by Douglas Hofstadter's "Metamagical Themas", a name that is an anagram of "Mathematical Games".
Many of the games columns were collected in book form starting in 1959 with "The Scientific American Book of Mathematical Puzzles & Diversions". Over the next four decades fourteen more books followed. Donald Knuth called them the .
Pseudoscience and skepticism.
Gardner's uncompromising attitude toward pseudoscience made him one of the foremost anti-pseudoscience polemicists of the 20th century. His book "Fads and Fallacies in the Name of Science" (1952, revised 1957) is a classic and seminal work of the skeptical movement. It explored myriad dubious outlooks and projects, including Fletcherism, creationism, food faddism, Charles Fort, Rudolf Steiner, Scientology, Dianetics, UFOs, dowsing, extra-sensory perception, the Bates method, and psychokinesis. This book and his subsequent efforts ("Science: Good, Bad and Bogus", 1981; "Order and Surprise", 1983, "Gardner's Whys & Wherefores", 1989, etc.) earned him a wealth of detractors and antagonists in the fields of "fringe science" and New Age philosophy, with many of whom he kept up running dialogues (both public and private) for decades.
Gardner was a relentless critic of self-proclaimed Israeli psychic Uri Geller and wrote two satirical exposes of him in the 1970s using the pen name Uriah Fuller.
In 1976 Gardner joined with Carl Sagan, Isaac Asimov and others in founding the Committee for the Scientific Investigation of Claims of the Paranormal (CSICOP). He wrote a column called "Notes of a Fringe Watcher" (originally "Notes of a Psi-Watcher") from 1983 to 2002 for that organization's periodical "Skeptical Inquirer". These have been collected in five books: "New Age: Notes of a Fringe Watcher" (1988), "On the Wild Side" (1992), "Weird Water and Fuzzy Logic" (1996), "Did Adam and Eve Have Navels" (2000), and "Are Universes Thicker than Blackberries" (2003). Gardner was a senior CSICOP fellow and prominent skeptic of the paranormal.
On August 21, 2010, Gardner was posthumously honored with an award recognizing his contributions in the skeptical field from the Independent Investigations Group during its 10th Anniversary Gala.
Religion and philosophy.
Gardner had an abiding fascination with religious belief. He was a fideistic theist, professing belief in one God as Creator, but critical of organized religion. In his autobiography, Gardner stated: "When many of my fans discovered that I believed in God and even hoped for an afterlife, they were shocked and dismayed... I do not mean the God of the Bible, especially the God of the Old Testament, or any other book that claims to be divinely inspired. For me God is a "Wholly Other" transcendent intelligence, impossible for us to understand. He or she is somehow responsible for our universe and capable of providing, how I have no inkling, an afterlife."
I am a philosophical theist. I believe in a personal God, and I believe in an afterlife, and I believe in prayer, but I don’t believe in any established religion. This is called philosophical theism... Philosophical theism is entirely emotional. As Kant said, he destroyed pure reason to make room for faith.
– Martin Gardner, 2008
He described his own belief as philosophical theism inspired by the theology of the philosopher Miguel de Unamuno. While eschewing systematic religious doctrine, Gardner believed in God, asserting that this belief cannot be confirmed or disconfirmed by reason or science. At the same time, he was skeptical of claims that any god has communicated with human beings through spoken or telepathic revelation or through miracles in the natural world.
He has been quoted as saying that he regarded parapsychology and other research into the paranormal as tantamount to "tempting God" and seeking "signs and wonders". He stated that while he would expect tests on the efficacy of prayers to be negative, he would not rule out "a priori" the possibility that as yet unknown paranormal forces may allow prayers to influence the physical world.
Gardner wrote repeatedly about what public figures such as Robert Maynard Hutchins, Mortimer Adler, and William F. Buckley, Jr. believed and whether their beliefs were logically consistent. In some cases, he attacked prominent religious figures such as Mary Baker Eddy on the grounds that their claims are unsupportable. His semi-autobiographical novel "The Flight of Peter Fromm" depicts a traditionally Protestant Christian man struggling with his faith, examining 20th century scholarship and intellectual movements and ultimately rejecting Christianity while remaining a theist.
Gardner said that he suspected that the fundamental nature of human consciousness may not be knowable or discoverable, unless perhaps a physics more profound than ("underlying") quantum mechanics is some day developed. In this regard, he said, he was an adherent of the "New Mysterianism".
Literary criticism and fiction.
Gardner was considered a leading authority on Lewis Carroll. His annotated version of "Alice's Adventures in Wonderland" and "Through the Looking Glass", explaining the many mathematical riddles, wordplay, and literary references found in the Alice books, was first published as "The Annotated Alice" (Clarkson Potter, 1960), a sequel published with new annotations as "More Annotated Alice" (Random House, 1990), and finally as "The Annotated Alice: The Definitive Edition" (Norton, 1999) combining notes from the earlier editions and new material. The book arose when Gardner, who found the Alice books 'sort of frightening' when he was young but found them fascinating as an adult, felt that someone ought to annotate them and suggested to a publisher that Bertrand Russell be asked; when the publisher did not manage to get past Russell's secretary, Gardner was asked to take the project. The book has been Gardner's most successful, selling over half a million copies.
Gardner's interest in wordplay led him to conceive of a magazine on recreational linguistics. In 1967 he pitched the idea to Greenwood Periodicals and nominated Dmitri Borgmann as editor. The resulting journal, "Word Ways", carried many articles from Gardner; as of 2013 it was still publishing his submissions posthumously.
In addition to the 'Alice' books, Gardner produced “Annotated” editions of G. K. Chesterton’s "The Innocence Of Father Brown" and "The Man Who Was Thursday" as well as of celebrated poems including "The Rime of the Ancient Mariner", "Casey at the Bat", "The Night Before Christmas", and "The Hunting of the Snark"; the last also written by Lewis Carroll.
Gardner occasionally tried his hand at fiction of a kind always closely associated with his non-fictional preoccupations. His "roman à clef" novel was "The Flight of Peter Fromm" (1973) and his short stories were collected in "The No-Sided Professor and Other Tales of Fantasy, Humor, Mystery, and Philosophy" (1987).
Gardner published stories about an imaginary numerologist named Dr. Matrix and "Visitors from Oz" (1998), based on L. Frank Baum's Oz books, which reflected his love of Oz. (He was a founding member of the International Wizard of Oz Club, and winner of its 1971 L. Frank Baum Memorial Award.)
Gardner was a member of the all-male literary banqueting club, the Trap Door Spiders, which served as the basis of Isaac Asimov's fictional group of mystery solvers, the Black Widowers.
Philosophy of mathematics.
Gardner was known for his sometimes controversial philosophy of mathematics. He wrote negative reviews of "The Mathematical Experience" by Philip J. Davis and Reuben Hersh and "What Is Mathematics, Really?" by Hersh, both of which were critical of aspects of mathematical Platonism, and the first of which was well received by the mathematical community. While Gardner was often perceived as a hard-core Platonist, his reviews demonstrated some formalist tendencies. Gardner maintained that his views are widespread among mathematicians, but Hersh has countered that in his experience as a professional mathematician and speaker, this is not the case.
Other views.
Over the years Gardner held forth on many contemporary issues, arguing for his points of view in a wide range of fields, from general semantics to fuzzy logic to watching TV (he once wrote a negative review of Jerry Mander's book "Four Arguments for the Elimination of Television"). His philosophical views are described and defended in his book "The Whys of a Philosophical Scrivener" (1983, revised 1999). Under the pseudonym "George Groth", Gardner panned his own book for the "New York Review of Books".

</doc>
<doc id="20436" url="http://en.wikipedia.org/wiki?curid=20436" title="MIDI timecode">
MIDI timecode

MIDI time code (MTC), or MIDI time division, embeds the same timing information as standard SMPTE timecode as a series of small 'quarter-frame' MIDI messages. There is no provision for the user bits in the standard MIDI time code messages, and messages are used to carry this information instead. The quarter-frame messages are transmitted in a sequence of eight messages, thus a complete timecode value is specified every two frames. If the MIDI data stream is running close to capacity, the MTC data may arrive a little behind schedule which has the effect of introducing a small amount of jitter. In order to avoid this it is ideal to use a completely separate MIDI port for MTC data. Larger full-frame messages, which encapsulate a frame worth of timecode in a single message, are used to locate to a time while timecode is not running.
Unlike standard SMPTE timecode, MIDI timecode's quarter-frame and full-frame messages carry a two-bit flag value that identifies the rate of the timecode, specifying it as either:
MTC distinguishes between film speed and video speed only by the rate at which timecode advances, not by the information contained in the timecode messages; thus, 29.97 frame/s dropframe is represented as 30 frame/s dropframe at 0.1% pulldown.
MTC allows the synchronisation of a sequencer or DAW with other devices that can synchronise to MTC or for these devices to 'slave' to a tape machine that is striped with SMPTE. For this to happen a SMPTE to MTC converter needs to be employed. It is possible for a tape machine to synchronise to an MTC signal (if converted to SMPTE), if the tape machine is able to 'slave' to incoming timecode via motor control, which is a rare feature.
Time code format.
The MIDI time code is 32 bits long, of which 24 are used, while 8 bits are unused and always zero. Because the full-time code messages requires that the most significant bits of each byte are zero (valid MIDI data bytes), there are really only 28 available bits and 4 spare bits.
Like most audiovisual timecodes such as SMPTE time code, it encodes only time of day, repeating each 24 hours. Time is given in units of hours, minutes, seconds, and frames. There may be 24, 25, or 30 frames per second.
Each component is assigned one byte:
Full time code.
When there is a jump in the time code, a single full-time code is sent to synchronize attached equipment. This takes the form of a special global system exclusive message:
The manufacturer ID of codice_10 indicates a real-time universal message, the channel of codice_10 indicates it is a global broadcast. The following ID of codice_12 identifies this is a time code type message, and the second codice_12 indicates it is a full-time code message. The 4 bytes of time code follow. Although MIDI is generally little-endian, the 4 time code bytes follow in big-endian order, followed by a codice_14 "end of exclusive" byte.
After a jump, the time clock stops until the first following quarter-frame message is received.
Quarter-frame messages.
When the time is running continuously, the 32-bit time code is broken into 8 4-bit pieces, and one piece is transmitted each quarter frame. I.e. 96—120 times per second, depending on the frame rate. A quarter-frame messages consists of a status byte of 0xF1, followed by a single 7-bit data value: 3 bits to identify the piece, and 4 bits of partial time code. When time is running forward, the piece numbers increment from 0–7; with the time that piece 0 is transmitted is the coded instant, and the remaining pieces are transmitted later.
If the MIDI data stream is being rewound, the time codes count backward. Again, piece 0 is transmitted at the coded moment.
The time code is divided little-endian as follows:

</doc>
<doc id="20437" url="http://en.wikipedia.org/wiki?curid=20437" title="Mass transfer">
Mass transfer

Mass transfer is the net movement of mass from one location, usually meaning a stream, phase, fraction or component, to another. Mass transfer occurs in many processes, such as absorption, evaporation, adsorption, drying, precipitation, membrane filtration, and distillation. Mass transfer is used by different scientific disciplines for different processes and mechanisms. The phrase is commonly used in engineering for physical processes that involve diffusive and convective transport of chemical species within physical systems.
Some common examples of mass transfer processes are the evaporation of water from a pond to the atmosphere, the purification of blood in the kidneys and liver, and the distillation of alcohol. In industrial processes, mass transfer operations include separation of chemical components in distillation columns, absorbers such as scrubbers, adsorbers such as activated carbon beds, and liquid-liquid extraction. Mass transfer is often coupled to additional transport processes, for instance in industrial cooling towers. These towers couple heat transfer to mass transfer by allowing hot water to flow in contact with hotter air and evaporate as it absorbs heat from the air.
Astrophysics.
In astrophysics, mass transfer is the process by which matter gravitationally bound to a body, usually a star, fills its Roche lobe and becomes gravitationally bound to a second body, usually a compact object (white dwarf, neutron star or black hole), and is eventually accreted onto it. It is a common phenomenon in binary systems, and may play an important role in some types of supernovae and pulsars.
Chemical engineering.
Mass transfer finds extensive application in chemical engineering problems. It is used in reaction engineering, separations engineering, heat transfer engineering, and many other sub-disciplines of chemical engineering. 
The driving force for mass transfer is typically a difference in chemical potential, when it can be defined, though other thermodynamic gradients may couple to the flow of mass and drive it as well. A chemical species moves from areas of high chemical potential to areas of low chemical potential. Thus, the maximum theoretical extent of a given mass transfer is typically determined by the point at which the chemical potential is uniform. For single phase-systems, this usually translates to uniform concentration throughout the phase, while for multiphase systems chemical species will often prefer one phase over the others and reach a uniform chemical potential only when most of the chemical species has been absorbed into the preferred phase, as in liquid-liquid extraction. 
While thermodynamic equilibrium determines the theoretical extent of a given mass transfer operation, the actual rate of mass transfer will depend on additional factors including the flow patterns within the system and the diffusivities of the species in each phase. This rate can be quantified through the calculation and application of mass transfer coefficients for an overall process. These mass transfer coefficients are typically published in terms of dimensionless numbers, often including Péclet numbers, Reynolds numbers, Sherwood numbers and Schmidt numbers, among others.
Analogies between heat, mass, and momentum transfer.
There are notable similarities in the commonly used approximate differential equations for momentum, heat, and mass transfer. The molecular transfer equations of Newton's law for fluid momentum at low Reynolds number (Stokes flow), Fourier's law for heat, and Fick's law for mass are very similar, since they are all linear approximations to transport of conserved quantities in a flow field. 
At higher Reynolds number, the analogy between mass and heat transfer and momentum transfer becomes less useful due to the nonlinearity of the Navier-Stokes equation (or more fundamentally, the general momentum conservation equation), but the analogy between heat and mass transfer remains good. A great deal of effort has been devoted to developing analogies among these three transport processes so as to allow prediction of one from any of the others.

</doc>
<doc id="20448" url="http://en.wikipedia.org/wiki?curid=20448" title="Museum of Jurassic Technology">
Museum of Jurassic Technology

The Museum of Jurassic Technology is a museum located at 9341 Venice Boulevard in the Palms district of Los Angeles, California (although it has a postal address of Culver City because it is served by that city's post office). It was founded by David Hildebrand Wilson and Diana Drake Wilson (husband and wife) in 1988.
The museum calls itself "an educational institution dedicated to the advancement of knowledge and the public appreciation of the Lower Jurassic"; the relevance of the term "Lower Jurassic" to the museum's collections is left uncertain and unexplained. The museum's collection includes a mixture of artistic, scientific, ethnographic, and historic, as well as some unclassifiable exhibits, and the diversity of its offerings evokes the cabinets of curiosities that were the 16th-century predecessors of modern natural history museums. The factual claims of many of the museum's exhibits strain credibility, provoking an array of interpretations from commentators. The museum was the subject of a 1995 book by Lawrence Weschler entitled "Mr. Wilson's Cabinet of Wonder: Pronged Ants, Horned Humans, Mice on Toast, And Other Marvels of Jurassic Technology", which describes in detail many of its exhibits. David Hildebrand Wilson received a MacArthur Foundation fellowship in 2001. The museum is also mentioned in the novel "The Museum of Innocence", by Nobel-laureate Orhan Pamuk. 
Overview.
The museum contains an unusual collection of exhibits and objects with varying and uncertain degrees of authenticity. "New York Times" critic Edward Rothstein described it as a "museum about museums", "where the persistent question is: what kind of place is this?" "Smithsonian" magazine called it "a witty, self-conscious homage to private museums of yore . . . when natural history was only barely charted by science, and museums were closer to Renaissance cabinets of curiosity." In a similar vein, "The Economist" said the museum "captures a time chronicled in Richard Holmes's recent book "The Age of Wonder", when science mingled with poetry in its pursuit of answers to life's mysterious questions." 
Lawrence Weschler's book, "Mr. Wilson's Cabinet of Wonder: Pronged Ants, Horned Humans, Mice on Toast, And Other Marvels of Jurassic Technology", attempts to explain the mystery of the Museum of Jurassic Technology. Weschler deeply explores the museum through conversations with its founder, David Wilson, and through outside research on several exhibitions. His investigations into the history of certain exhibits led to various results of authenticity; some exhibits seem to have been created by Wilson's imagination while other exhibits might just be displayed in the Natural History Museum. The Museum of Jurassic Technology at its heart, according to Wilson, is "a museum interested in presenting phenomena that other natural history museums are unwilling to present." 
The museum's introductory slideshow recounts that, "In its original sense, the term, 'museum' meant '"a spot dedicated to the Muses, a place where man's mind could attain a mood of aloofness above everyday affairs"'". In this spirit, the dimly lit atmosphere, wood and glass vitrines, and labyrinthine floorplan lead visitors through an eclectic range of exhibits on art, natural history, history of science, philosophy, and anthropology, with a special focus on the history of museums and the variety of paths to knowledge. The museum attracts approximately 25,000 visitors per year.
Over the years, the museum has expanded both its exhibitions and other public offerings. In 2005, the museum opened its Tula Tea Room, a Russian-style tea room where Georgian tea, cookies, and crackers are served to patrons. This room is a miniature reconstruction of the study of Tsar Nicolas II from the Winter Palace in St. Petersburg, Russia. The Borzoi Kabinet Theater screens a series of poetic documentaries produced by the Museum of Jurassic Technology in collaboration with the St. Petersburg–based arts and science collective Kabinet. The series of films, entitled "A Chain of Flowers", draws its name from the quote by Charles Willson Peale: "The Learner must be led always from familiar objects toward the unfamiliar, guided along, as it were, a chain of flowers into the mysteries of life". The titles of the films are "Levsha: The Cross-eyed Lefty from Tula and the Steel Flea" (2001), "Obshee Delo: The Common Task" (2005), "Bol'shoe Sovietskaia Zatmenie: The Great Soviet Eclipse" (2008), "The Book of Wisdom and Lies" (2011), and "Language of the Birds" (2012).
Publications.
The museum produces a series of leaflets and books about museum exhibits, including:
Many of these books are published in conjunction with the Society for the Diffusion of Useful Information.
Exhibitions.
The museum maintains over 30 permanent exhibits, including:
From 1992 to 2006, the museum's Foundation Collection was on display in its Tochtermuseum at the Karl Ernst Osthaus-Museum in Hagen, Germany. This exhibition was part of the Museum of Museums wing at the KEOM, which came into being under the stewardship of then-director Michael Fehr.

</doc>
<doc id="20451" url="http://en.wikipedia.org/wiki?curid=20451" title="Men at Work">
Men at Work

Men at Work were an Australian rock band, which formed in 1978. Their founding mainstay was Colin Hay on lead vocals; he formed the group with Jerry Speiser on drums and Ron Strykert on lead guitar. They were joined by Greg Ham on flute and keyboards and then John Rees on bass guitar. This line-up achieved national and international success in the early 1980s. In January 1983, they were the first Australian artists to have a simultaneous No. 1 album and No. 1 single in the United States "Billboard" charts – "Business as Usual" (released on 9 November 1981) and "Down Under" (1981), respectively. With the same works, they achieved the same distinction of a simultaneous No. 1 album and No. 1 single on the Australian, New Zealand and United Kingdom charts. Their second album, "Cargo" (2 May 1983) was also No. 1 in Australia, No. 2 in New Zealand, No. 3 in the US, and No. 8 in the UK. Their third album, "Two Hearts" (3 April 1985), reached the top 20 in Australia and top 50 in the US.
At the Grammy Awards of 1983 they won the Best New Artist category; while at the ARIA Music Awards of 1994 they were inducted into the related Hall of Fame. Men at Work have sold over 30 million albums worldwide. According to Australian musicologist, Ian McFarlane, "[i]rrespective of the band's fairytale rise to prominence, [their] phenomenal success inextricably created worldwide interest in Australia and Australian music ... [they] simply opened the floodgates with little more than a clutch of great songs" The group disbanded in 1986 and reformed in 1996 to disband again by 2002.
In May 2001 "Down Under" was listed at No. 4 on the APRA Top 30 Australian songs and "Business as Usual" appeared in the book, "100 Best Australian Albums" (October 2010). In February 2010 Larrikin Music Publishing won a case against Hay and Strykert, their record label (Sony BMG Music Entertainment) and music publishing company (EMI Songs Australia) arising from the uncredited appropriation of "Kookaburra" for the flute line in "Down Under". On 19 April 2012 Greg Ham was found dead at his home of an apparent heart attack.
History.
Origins of the group.
Men at Work formed in Sydney in 1978 by Colin Hay on lead vocals; Jerry Speiser on drums; and Ron Strykert on lead guitar; they were soon joined by Greg Ham on flute and keyboards; and then John Rees on bass guitar. Hay had emigrated to Australia in 1967 from Scotland with his family. In 1978, he formed a duo with Strykert, which expanded by mid-1979 with the addition of Speiser and progressive rocker Greg Sneddon on keyboards (ex-Alroy Band). They formed an unnamed four-piece group. The band's first experience in the recording studio was recording the music to "Riff Raff", a low-budget stage musical on which Sneddon had worked.
Sneddon left and was replaced in late 1979 by Ham, and when Rees joined they adopted the name Men at Work from a construction zone sign near an early venue, The Cricketer's Arms Hotel, Richmond. The band built a "grass roots" reputation as a pub rock band. In 1980 the group issued their debut single, "Keypunch Operator" backed by "Down Under", with both tracks co-written by Hay and Strykert. It was "self-financed" and appeared on their own independent, M. A. W. label. Australian musicologist, Ian McFarlane, felt the A-side was "a fast-paced country-styled rocker with a clean sound and quirky rhythm". Despite not appearing in the top 100 on the Australian Kent Music Report Singles Chart, by the end of that year the group had "grown in stature to become the most in-demand and highly paid, unsigned band of the year".
International success (1981–1983).
Early in 1981 Men at Work signed with the Australian branch of Columbia Records on the recommendation of Peter Karpin, the label's A&R person. Fran of the "Woroni" caught their performance at the Refectory in Canberra in April, she noted that they provided "some reggae-ish type music and the minimum of audience attention. From what I saw of them they were perhaps a little bit boring but quite competent and probably deserving of more notice". The group's second single, "Who Can It Be Now?", was released in June 1981 which reached No. 2 and remained in the chart for 24 weeks. It had been produced by United States-based, Peter McIan, who was also working on their debut album, "Business as Usual".
Their next single was a re-worked version of "Down Under", Ham added an improvised flute solo, and the group had revisited its tempo and arrangement with McIan. It appeared in October that year and reached No. 1 in November, where it remained for six weeks. "Business as Usual" was also released in October and went to No. 1 on the Australian Kent Music Report Albums Chart, spending a total of nine weeks at the top spot. "The Canberra Times"‍ '​ Garry Raffaele opined that it "generally stays at a high level, tight and jerky ... There is a delicacy about this music — and that is not a thing you can say about too many rock groups. The flute and reeds of Greg Ham do much to further that". McFarlane noted that "[a]side from the strength of the music, part of the album's appeal was its economy. The production sound was low-key, but clean and uncluttered. Indeed, the songs stood by themselves with little embellishment save for a bright, melodic, singalong quality".
By February the following year both "Down Under" and "Business as Usual" had reached No. 1 on the respective Official New Zealand Music Charts – the latter was the first Australian album to reach that peak in New Zealand. Despite its strong Australian and New Zealand showing, and having an American producer (McIan), "Business as Usual" was twice rejected by Columbia's US parent company. Thanks to the persistence of the band's management and Karpin, the album was finally released in the US and the United Kingdom in April 1982 – six months after its Australian release. Their next single, "Be Good Johnny", was issued in Australia in April 1982 and reached No. 8 in Australia, and No. 3 in New Zealand.
Men at Work initially broke through to North American audiences in the western provinces of Canada with "Who Can It Be Now?" hitting top 10 on radio stations in Winnipeg by May 1982. It peaked at No. 8 on the Canadian "RPM" Top Singles Chart in July. In August the group toured Canada and the US to promote the album and related singles, supporting Fleetwood Mac. The band became more popular on Canadian radio in the following months and also started receiving top 40 US airplay by August. In October "Who Can It Be Now?" reached No. 1 on the US "Billboard" Hot 100, while Canada was one single ahead with "Down Under" topping the Canadian charts that same month. In the following month "Business as Usual" began a 15-week run at No. 1 on the "Billboard" 200.
While "Who Can It Be Now?" was still in the top ten in the US, "Down Under" was finally released in that market. It entered the US charts at No. 79 and ten weeks later, it was No. 1. By January 1983 Men at Work had the top album and single in both the US and the UK – never previously achieved by an Australian act. "Be Good Johnny" received moderate airplay in the US; it reached the top 20 in Canada.
The band released their second album, "Cargo", in April 1983, which also peaked at No. 1 – for two weeks – on the Australian charts. In New Zealand it reached No.2. It had been finished in mid-1982 with McIan producing again, but was held back due to the success of their debut album on the international market, where "Business as Usual" was still riding high. "Cargo" appeared at No. 3 on the "Billboard" 200, and No. 8 in the UK. The lead single, "Overkill", was issued in Australia ahead of the album in October 1982 and reached No. 6, it peaked at No. 3 in the US. "Dr. Heckyll and Mr. Jive" followed in March 1983 made it to No. 5 in Australia, and No. 28 in the US. "It's a Mistake" reached No. 6 in the US. The band toured the world extensively in 1983.
"Two Hearts" to first break-up (1984–1986).
During 1984 the band took a break as members pursued other interests. Upon reconvening later that year, infighting during rehearsals between Hay and Speiser over songwriting and the band's management led to a split in the band. Both Rees and Speiser were told they were "not required", as Hay, Ham and Strykert used session musicians to record their third album, "Two Hearts" (23 April 1985). Studio musicians included Jeremy Alsop on bass guitar (ex-Ram Band, Pyramid, Broderick Smith Band); and Mark Kennedy on drums (Spectrum, Ayers Rock, Marcia Hines Band). "Two Hearts" was produced by Hay and Ham, and peaked at No. 16 in Australia, and No. 50 on the US chart. Strykert had left during its production.
Four tracks were released as singles, "Everything I Need" (May 1985), "Man with Two Hearts", "Maria" (August), and "Hard Luck Story" (October); only the lead single charted in Australia (No. 37) and the US (No. 47). The album relied heavily on drum programming and synthesisers, and reduced the presence of Ham's saxophone. Hay and Ham hired new band mates, to tour in support of "Two Hearts", with Alsop and Kennedy joined by James Black on guitar and keyboards (Mondo Rock, The Black Sorrows). Soon after a third guitarist, Colin Bayley (Mi-Sex), was added and Kennedy was replaced on drums by Chad Wackerman (Frank Zappa). Australian singers Kate Ceberano and Renée Geyer had also worked on the album and performed live as guest vocalists.
On 13 July 1985 Men at Work performed three tracks for the Oz for Africa concert (part of the global Live Aid program)—"Maria", "Overkill", and an unreleased one, "The Longest Night". They were broadcast in Australia (on both Seven Network and Nine Network) and on MTV in the US. "Maria" and "Overkill" were also broadcast by American Broadcasting Company (ABC) during their Live Aid telecast. Ham left during the band's time touring behind the album. The final Men at Work performances during 1985 had jazz saxophonist, Paul Williamson (The Black Sorrows), replacing Ham. By early 1986 Hay disbanded Men at Work and then he started recording his first solo album, "Looking for Jack" (January 1987), which had Alsop and Wackerman as session musicians.
Reunion to second break-up (1996–2002).
By mid-1996, after a ten-year absence, Hay and Ham reformed Men at Work to tour South America. They had enjoyed strong fan support there during their earlier career and demands for a reunion had persisted. The 1996 line up had Stephen Hadley on bass guitar and backing vocals (ex-The Black Sorrows, Paul Kelly Band); Simon Hosford on guitar and backing vocals (Colin Hay backing band); and John Watson on drums (The Black Sorrows). The tour culminated in a performance in São Paulo, which was recorded for the Brazilian release of a live album, "Brazil '96", in 1997, which was co-produced by Hay and Ham for Sony Records. It was re-released worldwide in 1998 as "Brazil" with a bonus track, "The Longest Night", the first new studio track since "Two Hearts".
The band toured Australia, South America, Europe and the US from 1998 to 2000. Other than Hay and Ham the line up for these tours varied, including Rick Grossman of the Hoodoo Gurus on bass guitar for a Brazilian tour, among other touring musicians. Men at Work performed "Down Under" at the closing ceremony of the 2000 Summer Olympics in Sydney, alongside Paul Hogan of ""Crocodile" Dundee" (1986).
One of their European tours for mid-2000 was cancelled and the group had disbanded by 2002, although Hay and Ham periodically reunited Men at Work with guest musicians (including an appearance in February 2009, when they performed "Down Under" at the Australia Unites Victorian Bushfire Appeal Telethon) until Ham's death; his body was found at his home on April 19, 2012. He had reportedly suffered a fatal heart attack.
Copyright lawsuit and controversy.
In February 2010 Larrikin Music Publishing won a case against Hay and Strykert, their record label (Sony BMG Music Entertainment) and music publishing company (EMI Songs Australia) arising from the uncredited appropriation of "Kookaburra", originally written in 1934 by Marion Sinclair and for which Larrikin owned the publishing rights, as the flute line in the Men at Work song, "Down Under". Back in early 2009 the Australian music-themed TV quiz, "Spicks and Specks", had posed a question which suggested that "Down Under" contained elements of "Kookaburra".
Larrikin, headed by Norman Lurie (now retired), then filed suit after Larrikin was sold to another company and had demanded between 40% and 60% of the previous six years of earnings from the song. In February 2010 the judge ruled that "Down Under" did contain a flute riff based on "Kookaburra" but stipulated that neither was it necessarily the hook nor a substantial part of the hit song (Hay and Strykert had written the track years before the flute riff was added by Ham). In July 2010 a judge ruled that Larrikin should be paid 5% of past (since 2002) and future profits.
Other projects.
Hay maintained a solo career and played with Ringo Starr & His All-Starr Band. Strykert relocated to Hobart in 2009 from Los Angeles, and continued to play music and released his first solo album, "Paradise", in September that year. He expressed resentment towards Hay, mainly over royalties. Ham remained musically active and played sax with the Melbourne-based group The Nudist Funk Orchestra until his death. Rees was a music teacher in Melbourne and also played the violin and bass guitar for the band Beggs 2 Differ. Speiser played drums for the band, The Afterburner.
Awards and nominations.
The group won the 1983 Grammy Award for Best New Artist; the other nominees were Asia, Jennifer Holliday, The Human League and Stray Cats. In August 1983 they ware given a Crystal Globe Award for $100 million worth of record business by their US label. That same year in Canada they were awarded a Juno Award for "International LP of the Year". Men at Work have sold over 30 million albums worldwide.
At the ARIA Music Awards of 1994 they were inducted into the related Hall of Fame. On 28 May 2001 "Down Under" was listed at No. 4 on the APRA Top 30 Australian songs. In October 2010, "Business as Usual" was listed in the book, "100 Best Australian Albums".
References.
</dl> 

</doc>
<doc id="20452" url="http://en.wikipedia.org/wiki?curid=20452" title="Meconium aspiration syndrome">
Meconium aspiration syndrome

Meconium aspiration syndrome (MAS) also known as neonatal aspiration of meconium is a medical condition affecting newborn infants. It occurs when meconium is present in their lungs during or before delivery. Meconium is the first stool of an infant, composed of materials ingested during the time the infant spends in the uterus.
Meconium is normally stored in the infant's intestines until after birth, but sometimes (often in response to fetal distress and hypoxia) it is expelled into the amniotic fluid prior to birth, or during labor. If the baby then inhales the contaminated fluid, respiratory problems may occur.
Signs and symptoms.
The most obvious sign that meconium has been passed during or before labor is the greenish or yellowish appearance of the amniotic fluid. The infant's skin, umbilical cord, or nailbeds may be stained green if the meconium was passed a considerable amount of time before birth. These symptoms alone do not necessarily indicate that the baby has inhaled in the fluid by gasping in utero or after birth. After birth, rapid or labored breathing, cyanosis, slow heartbeat, a barrel-shaped chest or low Apgar score are all signs of the syndrome. Inhalation can be confirmed by one or more tests such as using a stethoscope to listen for abnormal lung sounds (diffuse 'wet' crackles and rhonchi), performing blood gas tests to confirm a severe loss of lung function (respiratory acidosis as a consequence of hypercapnia), and using chest X-rays to look for patchy or streaked areas on the lungs. Infants who have inhaled meconium may develop respiratory distress syndrome often requiring ventilatory support. Complications of MAS include pneumothorax and persistent pulmonary hypertension of the newborn.
Causes.
Fetal distress during labor causes intestinal contractions, as well as relaxation of the anal sphincter, which allows meconium to pass into the amniotic fluid and contaminate the amniotic fluid. Meconium passage into the amniotic fluid occurs in about 5–20 percent of all births and is more common in overdue births. Of the cases where meconium is found in the amniotic fluid, meconium aspiration syndrome develops less than 5 percent of the time. Amniotic fluid is normally clear, but becomes greenish if it is tinted with meconium.
The risk of MAS increases after the 40th week of pregnancy.
Mechanism.
The pathophysiology of MAS is due to a combination of primary surfactant deficiency and surfactant inactivation as a result of plasma proteins leaking into the airways from areas of epithelial disruption and injury.
The leading three causes of MAS are
If an infant inhales this mixture before, during, or after birth, it may be sucked deep into the lungs. Three main problems occur if this happens:
These can lead to significant morbidity and mortality if severe enough. 
Diagnosis.
High risk infants may be identified by fetal tachycardia, bradycardia or absence of fetal accelerations upon CTG in utero, at birth the infant may look cachexic and show signs of yellowish meconium staining on skin, nail and the umbillical cord, these infants usually progress onto Infant Respiratory distress syndrome within 4 hours. Investigations which can confirm the diagnosis are fetal chest x-ray, which will show hyperinflation, diaphragmatic flattening, cardiomegaly, patchy atelectasis and consolidation, and ABG samples, which will show decreased oxygen levels.
Prevention.
MAS is difficult to prevent. Amnioinfusion, a method of thinning thick meconium that has passed into the amniotic fluid through pumping of sterile fluid into the amniotic fluid, has not shown a benefit.
Treatment.
Surfactant appears to improve outcomes when given to infants follow meconium aspiration.
It has been recommended that the throat and nose of the baby be suctioned as soon as the head is delivered. However, this is not really useful and the revised Neonatal Resuscitation Guidelines no longer recommend it. When meconium staining of the amniotic fluid is present and the baby is born depressed, it is recommended that an individual trained in neonatal intubation use a laryngoscope and endotracheal tube to suction meconium from below the vocal cords. If the condition worsens, extracorporeal membrane oxygenation (ECMO) can useful.
Albumin-lavage has not demonstrated to benefit outcomes of MAS. Steroid use has not demonstrated to benefit the outcomes of MAS.
Prognosis.
The mortality rate of meconium-stained infants is considerably higher than that of non-stained infants; meconium aspiration used to account for a significant proportion of neonatal deaths. Residual lung problems are rare but include symptomatic cough, wheezing, and persistent hyperinflation for up to 5–10 yr. The ultimate prognosis depends on the extent of CNS injury from asphyxia and the presence of associated problems such as pulmonary hypertension.
Epidemiology.
In a study conducted between 1995 and 2002, MAS occurred in 1,061 of 2,490,862 live births, reflecting an incidence of 0.43 of 1,000. MAS requiring intubation occurs at higher rates in pregnancies beyond 40 weeks. 34% of all MAS cases born after 40 weeks required intubation compared to 16% prior to 40 weeks.

</doc>
<doc id="20453" url="http://en.wikipedia.org/wiki?curid=20453" title="Meconium">
Meconium

Meconium is the earliest stool of a mammalian infant. Unlike later feces, meconium is composed of materials ingested during the time the infant spends in the uterus: intestinal epithelial cells, lanugo, mucus, amniotic fluid, bile, and water. Meconium, unlike later feces, is viscous and sticky like tar, its color usually being a very dark olive green; it is almost odorless. When diluted in amniotic fluid, it may appear in various shades of green, brown, or yellow. It should be completely passed by the end of the first few days after birth, with the stools progressing toward yellow (digested milk).
Meconium is normally retained in the infant's bowel until after birth, but sometimes it is expelled into the amniotic fluid (also called "amniotic liquor") prior to birth or during labor and delivery. The stained amniotic fluid (called "meconium liquor" or "meconium stained liquor") is recognised by medical staff as a sign of fetal distress, and puts the neonate at risk of meconium aspiration. Medical staff may aspirate the meconium from the nose and mouth of a newborn immediately after delivery in the event the baby shows signs of respiratory distress to decrease the risk of meconium aspiration syndrome.
Meconium had been thought to be sterile until the team of researchers from the University of Valencia in Spain found bacterial communities in it so developed that they seemed to fall into two categories. Around half of the samples appeared to be dominated by bacteria that produce lactic acid, such as lactobacillus, while the other half mostly contained a family of so-called enteric bacteria, such as "Escherichia coli".
The Latin term "meconium" derives from the Greek μηκώνιον, "mēkōnion", a diminutive of μήκων, "mēkōn", i.e. poppy, in reference either to its tarry appearance that may resemble some raw opium preparations, or to Aristotle's belief that it induces sleep in the fetus.
A symptom of both Hirschsprung's disease and cystic fibrosis is the failure to pass meconium.
Meconium can be tested for various drugs, to check for "in utero" exposure. Using meconium, a Canadian research group at the Hospital for Sick Children, University of Toronto, showed that by measuring a by-product of alcohol (FAEE) they could objectively detect babies exposed to excessive maternal drinking of alcohol in pregnancy. In the USA, the results of meconium testing may be used by child protective services and other law enforcement agencies to determine the eligibility of the parents to keep the newborn.
Terminal meconium.
Most of the time that the amniotic fluid is stained with meconium it will be homogeneously distributed throughout the fluid making it brown. This indicates that the fetus passed the meconium some time ago such that sufficient mixing occurred as to establish the homogeneous mixture. Terminal meconium occurs when the fetus passes the meconium a short enough time before birth/caesarean section that the amniotic fluid remains clear, but individual clumps of meconium are in the fluid.
Meconium ileus.
The meconium sometimes becomes thickened and congested in the intestines, a condition known as meconium ileus. Meconium ileus is often the first sign of cystic fibrosis. In cystic fibrosis, the meconium can form a bituminous black-green mechanical obstruction in a segment of the ileum. Beyond this there may be a few separate grey-white globular pellets. Below this level, the bowel is a narrow and empty micro-colon. Above the level of the obstruction, there are several loops of hypertrophied bowel distended with fluid. No meconium is passed, and abdominal distension and vomiting appear soon after birth. About 20% of cases of cystic fibrosis present with meconium ileus, while approximately 20% of one series of cases of meconium ileus did not have cystic fibrosis. The presence of meconium ileus is not related to the severity of the cystic fibrosis. The obstruction can be relieved in a number of different ways.
Meconium ileus should be distinguished from meconium plug syndrome, in which a tenacious mass of mucus prevents the meconium from passing and there is no risk of intestinal perforation. Meconium ileus has a significant risk of intestinal perforation. In barium enema, meconium plug syndrome rather shows normal or dilated colon as compared to micro-colon in meconium ileus.

</doc>
<doc id="20454" url="http://en.wikipedia.org/wiki?curid=20454" title="Montreux Convention Regarding the Regime of the Straits">
Montreux Convention Regarding the Regime of the Straits

The Montreux Convention Regarding the Regime of the Straits is a 1936 agreement that gives Turkey control over the Bosporus Straits and the Dardanelles and regulates the transit of naval warships. The Convention gives Turkey full control over the Straits and guarantees the free passage of civilian vessels in peacetime. It restricts the passage of naval ships not belonging to Black Sea states. The terms of the convention have been the source of controversy over the years, most notably concerning the Soviet Union's military access to the Mediterranean Sea.
Signed on 20 July 1936 at the Montreux Palace in Switzerland, it permitted Turkey to remilitarise the Straits. It went into effect on 9 November 1936 and was registered in "League of Nations Treaty Series" on 11 December 1936. It is still in force today, with some amendments.
The proposed controversial 21st century Kanal İstanbul project may constitute a possible by-pass to the Montreux Convention and force greater Turkish autonomy with respect to the passage of military ships from the Black Sea to the Sea of Marmara.
Background.
The convention was one of a series of agreements in the 19th and 20th centuries that sought to address the long-running "Straits Question" of who should control the strategically vital link between the Black Sea and Mediterranean Sea. In 1923 the Treaty of Lausanne had demilitarised the Dardanelles and opened the Straits to unrestricted civilian and military traffic, under the supervision of the International Straits Commission of the League of Nations.
By the late 1930s, the strategic situation in the Mediterranean had altered with the rise of Fascist Italy, which controlled the Greek-inhabited Dodecanese islands off the west coast of Turkey and had constructed fortifications on Rhodes, Leros and Kos. The Turks feared that Italy would seek to exploit access to the Straits to expand its power into Anatolia and the Black Sea region. There were also fears of Bulgarian rearmament. Although Turkey was not permitted to refortify the Straits, it nonetheless did so secretly.
In April 1935, the Turkish government dispatched a lengthy diplomatic note to the signatories of the Treaty of Lausanne proposing a conference on the agreement of a new regime for the Straits and requested that the League of Nations authorise the reconstruction of the Dardanelles forts. In the note, Turkish foreign minister Tevfik Rüştü Aras explained that the international situation had changed greatly since 1923. At that time, Europe had been moving towards disarmament and an international guarantee to defend the Straits. The Abyssinia Crisis of 1934–35, the denunciation by Germany of the Treaty of Versailles and international moves towards rearmament meant that "the only guarantee intended to guard against the total insecurity of the Straits has just disappeared in its turn." Indeed, Aras said, "the Powers most closely concerned are proclaiming the existence of a threat of general conflagration." The key weaknesses of the present regime were that the machinery for collective guarantees were too slow and ineffective, there was no contingency for a general threat of war and no provision for Turkey to defend itself. Turkey was therefore prepared
to enter into negotiations with a view to arriving in the near future at the conclusion of agreements for regulations of the regime of the Straits under the conditions of security which are indispensable for the inviolability of Turkey's territory, in most liberal spirit, for the constant development of commercial navigation between the Mediterranean and the Black Sea.
The response to the note was generally favourable, and Australia, Bulgaria, France, Germany, Greece, Japan, Romania, the Soviet Union, Turkey, the United Kingdom and Yugoslavia agreed to attend negotiations at Montreux in Switzerland, which began on 22 June 1936. Two major powers were not represented: Italy, whose aggressively expansionist policies had prompted the conference in the first place, refused to attend and the United States declined even to send an observer.
Turkey, the UK and the Soviet Union each put forward their own set of proposals, aimed chiefly at protecting their own interests. The British favoured the continuation of a relatively restrictive approach, while the Turks sought a more liberal regime that reasserted their own control over the Straits and the Soviets proposed a regime that would guarantee absolute freedom of passage. The British, supported by France, sought to exclude the Soviet fleet from the Mediterranean Sea, where it might have threatened the vital shipping lanes to India, Egypt and the Far East. In the end, the British conceded some of their requests while the Soviets succeeded in ensuring that the Black Sea countries – including the USSR – were given some exemptions from the military restrictions imposed on non-Black Sea nations. The agreement was ratified by all of the conference attendees with the exception of Germany, which had not been a signatory to the Treaty of Lausanne, and with reservations by Japan, and came into force on 9 November 1936.
Britain's willingness to make concessions has been attributed to a desire to avoid Turkey being driven to ally itself with, or fall under the influence of Adolf Hitler or Benito Mussolini. It was thus the first in a series of steps by Britain and France to ensure that Turkey would either remain neutral or tilt towards the Western Allies in the event of any future conflict with the Axis.
Terms and consequences of the Convention.
The Convention consists of 29 Articles, four annexes and one protocol. Articles 2–7 consider the passage of merchant ships. Articles 8–22 consider the passage of war vessels. The key principle of freedom of passage and navigation is stated in articles 1 and 2. Article 1 provides that "The High Contracting Parties recognise and affirm the principle of freedom of passage and navigation by sea in the Straits". Article 2 states that "In time of peace, merchant vessels shall enjoy complete freedom of passage and navigation in the Straits, by day and by night, under any flag with any kind of cargo."
The International Straits Commission was abolished, authorising the full resumption of Turkish military control over the Straits and the refortification of the Dardanelles. Turkey was authorised to close the Straits to all foreign warships in wartime or when it was threatened by aggression; additionally, it was authorised to refuse transit from merchant ships belonging to countries at war with Turkey. A number of highly specific restrictions were imposed on what type of warships are allowed passage. Non-Black Sea state warships in the Straits must be under 15,000 tons. No more than nine non-Black Sea state warships, with a total aggregate tonnage of no more than 30,000 tons, may pass at any one time, and they are permitted to stay in the Black Sea for no longer than twenty-one days. 
Although the treaty is often cited as prohibiting aircraft carriers in the straits, there is no explicit prohibition on aircraft carriers in the treaty. However, the tonnage limits in Article 14, which apply to all non-Black Sea powers, would preclude the transit of modern aircraft carrying ships. In the case of non-Black Sea powers, these terms make it impossible for transit any modern ships carrying aircraft through the straits without violating the terms of the convention.
By contrast, Black Sea powers such as the USSR were able to transit aircraft carrying cruisers through the straits under other terms of the convention. As with non-Black Seas powers, the Montreux convention does not explicitly forbid a Black Sea power from transiting aircraft carriers through the straits, and the tonnage limits in Article 14 also apply to Black Sea powers as well as non-Black Sea powers. However, under Article 11, Black Sea states are permitted to transit capital ships of any tonnage through the straits. Annex II specifically excludes aircraft carriers from the definition of capital ships, but limits the definition of carriers to ships that are designed primarily for carrying and operating aircraft at sea and specifically excludes other ships that merely are able to operate aircraft.
The result of this is that by designing its aircraft carrying ships such as the "Kiev" and the "Admiral Kuznetsov" to have roles other than aircraft operation and by designating those ships as "aircraft carrying cruisers" rather than "aircraft carriers" the Soviet Union was able to transit its aircraft carrying ships through the straits in compliance with the convention, while at the same time the Convention denied access to NATO aircraft carriers, which are not covered by the exemption in Article 11. 
Under Article 12, Black Sea states are also allowed to send submarines through the Straits, with prior notice, as long as the vessels have been constructed, purchased or sent for repair outside the Black Sea. The less restrictive rules applicable to Black Sea states were agreed as, effectively, a concession to the Soviet Union, the only Black Sea state other than Turkey with any significant number of capital ships or submarines. The passage of civil aircraft between the Mediterranean and Black Seas is permitted, but only along routes authorised by the Turkish government.
The terms of the Convention were largely a reflection of the international situation in the mid-1930s. They largely served Turkish and Soviet interests, enabling Turkey to regain military control of the Straits and assuring Soviet dominance of the Black Sea. Although the Convention restricted the Soviets' ability to send naval forces into the Mediterranean sea – thereby satisfying British concerns about Soviet intrusion into what was considered a British sphere of influence – it also ensured that outside powers could not exploit the Straits to threaten the Soviet Union. This was to have significant repercussions during World War II when the Montreux regime prevented the Axis powers from sending naval forces through the Straits to attack the Soviet Union. The Axis powers were thus severely limited in naval capability in their Black Sea campaigns, relying principally on small vessels that had been transported overland by rail and canal networks. Auxiliary vessels and armed merchant ships occupied a grey area, however, and the transit of such vessels through the straits led to friction between the Allies and Turkey. Repeated protests from Moscow and London led to the Turkish government banning the movements of "suspicious" Axis ships with effect from June 1944 after a number of German auxiliary ships were permitted to transit the Straits.
Development of the Convention since 1936.
The Convention remains in force today, with amendments, though not without dispute. It was repeatedly challenged by the Soviet Union during World War II and the Cold War. As early as 1939, Joseph Stalin sought to reopen the Straits Question and proposed joint Turkish and Soviet control of the Straits, complaining that "a small state [i.e. Turkey] supported by Great Britain held a great state by the throat and gave it no outlet." After the Molotov–Ribbentrop Pact was signed by the Soviet Union and Nazi Germany, the Soviet Foreign Minister Vyacheslav Molotov informed his German counterparts that the USSR wished to take military control of the Straits and establish its own military base there. The Soviets returned to the issue in 1945 and 1946, demanding a revision of the Montreux Convention at a conference excluding most of the Montreux signatories, a permanent Soviet military presence and joint control of the Straits. This was firmly rejected by Turkey, despite an ongoing Soviet "strategy of tension". For several years after World War II, the Soviets exploited the restriction on the number of foreign warships by ensuring that one of theirs was always in the Straits, thus effectively blocking any nation other than Turkey from sending warships through the Straits. Soviet pressure expanded into full on demands to revise the Montreux Convention, which led to the Turkish Straits crisis of 1946, which led to Turkey abandoning its policy of neutrality. In 1947 it became the recipient of US military and economic assistance under the Truman Doctrine of "containment" and joined NATO, along with Greece, in 1952.
The passage of US warships through the Straits also raised controversy, as the convention forbids the transit of non-Black Sea nations' warships with guns of a calibre larger than eight inches (203 mm). In the 1960s, the US sent warships carrying 305 mm calibre ASROC missiles through the Straits, prompting Soviet protests. The Turkish government rejected the Soviet complaints, pointing out that guided missiles were not guns and that such weapons had not even existed at the time of the Convention's agreement so were not restricted.
In April 1982, the Convention was amended to allow Turkey to close the Straits at its discretion in peacetime as well as during wartime.
The United Nations Convention on the Law of the Sea (UNCLOS), which entered into force in November 1994, has prompted calls for the Montreux Convention to be revised and adapted to make it compatible with UNCLOS's regime governing straits used for international navigation. However, Turkey's long-standing refusal to sign UNCLOS has meant that Montreux remains in force without further amendments.
The safety of vessels passing through the Bosporus has become a major concern in recent years as the volume of traffic has increased greatly since the Convention was signed – from 4,500 in 1934 to 49,304 by 1998. As well as obvious environmental concerns, the Straits bisect the city of Istanbul with over 11 million people living on its shores; maritime incidents in the Straits therefore pose a considerable risk to public safety. The Convention does not, however, make any provision for the regulation of shipping for the purposes of safety and environmental protection. In January 1994 the Turkish government adopted new "Maritime Traffic Regulations for the Turkish Straits and the Marmara Region". This introduced a new regulatory regime "in order to ensure the safety of navigation, life and property and to protect the environment in the region" but without violating the Montreux principle of free passage. The new regulations provoked some controversy when Russia, Greece, Cyprus, Romania, Ukraine and Bulgaria raised objections. However, they were approved by the International Maritime Organisation on the grounds that they were not intended to prejudice "the rights of any ship using the Straits under international law". The regulations were revised in November 1998 to address Russian concerns.

</doc>
